{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Demo: Querying arXiv cs.CL Papers\n",
    "\n",
    "This notebook demonstrates the Retrieval-Augmented Generation (RAG) system built for searching through 50 arXiv cs.CL papers.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Configuration\n",
    "INDEX_DIR = Path(\"data/index\")\n",
    "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Resources\n",
    "\n",
    "Load the FAISS index, chunks, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"Loaded model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Load FAISS index\n",
    "print(\"\\nLoading FAISS index...\")\n",
    "index_path = INDEX_DIR / \"faiss_index.bin\"\n",
    "faiss_index = faiss.read_index(str(index_path))\n",
    "print(f\"Loaded index with {faiss_index.ntotal} vectors\")\n",
    "\n",
    "# Load chunks\n",
    "print(\"\\nLoading chunks...\")\n",
    "chunks_path = INDEX_DIR / \"chunks.json\"\n",
    "with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "\n",
    "# Load metadata\n",
    "print(\"\\nLoading metadata...\")\n",
    "metadata_path = INDEX_DIR / \"metadata.json\"\n",
    "with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "print(f\"Loaded metadata for {len(metadata)} chunks\")\n",
    "\n",
    "# Count unique papers\n",
    "unique_papers = len(set(m['paper_id'] for m in metadata))\n",
    "print(f\"\\nTotal unique papers: {unique_papers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(query: str, k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for relevant passages based on a query.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        k: Number of top results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing search results\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Normalize (index was normalized)\n",
    "    query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "    distances, indices = faiss_index.search(query_vector, k)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "        if idx < len(chunks):\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'distance': float(distance),\n",
    "                'paper_id': metadata[idx]['paper_id'],\n",
    "                'paper_title': metadata[idx]['paper_title'],\n",
    "                'chunk_index': metadata[idx]['chunk_index'],\n",
    "                'text': chunks[idx]\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_results(query: str, results: List[Dict]):\n",
    "    \"\"\"\n",
    "    Display search results in a readable format.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"Rank {result['rank']} | Distance: {result['distance']:.4f}\")\n",
    "        print(f\"Paper: {result['paper_title']}\")\n",
    "        print(f\"Paper ID: {result['paper_id']} | Chunk: {result['chunk_index']}\")\n",
    "        print(f\"\\nText excerpt:\")\n",
    "        # Show first 500 characters\n",
    "        text_preview = result['text'][:500] + \"...\" if len(result['text']) > 500 else result['text']\n",
    "        print(text_preview)\n",
    "        print(f\"\\n{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Queries\n",
    "\n",
    "Let's try several different types of queries to demonstrate the system's capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What are transformer models and how do they work?\"\n",
    "results1 = search_papers(query1, k=3)\n",
    "display_results(query1, results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"Explain attention mechanisms in natural language processing\"\n",
    "results2 = search_papers(query2, k=3)\n",
    "display_results(query2, results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3: Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"How do large language models learn from data?\"\n",
    "results3 = search_papers(query3, k=3)\n",
    "display_results(query3, results3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = \"What techniques are used for training language models?\"\n",
    "results4 = search_papers(query4, k=3)\n",
    "display_results(query4, results4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 5: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = \"How do we evaluate the performance of NLP models?\"\n",
    "results5 = search_papers(query5, k=3)\n",
    "display_results(query5, results5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics and Analysis\n",
    "\n",
    "Let's analyze the retrieval results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_queries = [\n",
    "    (query1, results1),\n",
    "    (query2, results2),\n",
    "    (query3, results3),\n",
    "    (query4, results4),\n",
    "    (query5, results5)\n",
    "]\n",
    "\n",
    "# Analyze paper distribution\n",
    "from collections import Counter\n",
    "\n",
    "retrieved_papers = []\n",
    "for query, results in all_queries:\n",
    "    for result in results:\n",
    "        retrieved_papers.append(result['paper_id'])\n",
    "\n",
    "paper_counts = Counter(retrieved_papers)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS OF RETRIEVAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal queries: {len(all_queries)}\")\n",
    "print(f\"Total results retrieved: {len(retrieved_papers)}\")\n",
    "print(f\"Unique papers in results: {len(paper_counts)}\")\n",
    "\n",
    "print(f\"\\nMost frequently retrieved papers:\")\n",
    "for paper_id, count in paper_counts.most_common(5):\n",
    "    # Find paper title\n",
    "    title = next(m['paper_title'] for m in metadata if m['paper_id'] == paper_id)\n",
    "    print(f\"  {count}x - {paper_id}\")\n",
    "    print(f\"      {title[:80]}...\")\n",
    "\n",
    "# Average distances\n",
    "all_distances = []\n",
    "for query, results in all_queries:\n",
    "    all_distances.extend([r['distance'] for r in results])\n",
    "\n",
    "print(f\"\\nRetrieval quality (L2 distances):\")\n",
    "print(f\"  Average distance: {np.mean(all_distances):.4f}\")\n",
    "print(f\"  Min distance: {np.min(all_distances):.4f}\")\n",
    "print(f\"  Max distance: {np.max(all_distances):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Query\n",
    "\n",
    "Try your own query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your own query here\n",
    "custom_query = \"Your question here\"\n",
    "custom_results = search_papers(custom_query, k=3)\n",
    "display_results(custom_query, custom_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
