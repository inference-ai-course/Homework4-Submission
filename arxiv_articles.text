Estimating the true number of principal components under the random design
Abstract
Principal component analysis (PCA) is frequently employed as a dimension reduction tool when the number of covariates is large. However, the number of principal components to be retained in PCA is typically determined in a researcher-dependent manner. To mitigate the subjectivity in PCA, this paper proposes a data-driven testing procedure to estimate the number of underlying principal components. While existing work such as GSell-Wager-Chouldechova-Tibshirani-2016, Taylor-Loftus-Tibshirani-2016 and Choi-Taylor-Tibshirani-2017 discuss similar tests under fixed design, this paper investigates an extension of their framework to a more general econometric setup with the random design. The proposed test is proved to achieve asymptotically exact type 1 error controls under a locally defined null hypothesis, with simulation examples indicating an asymptotic validity of our test.
1 Introduction
In recent years, there has been a growing body of empirical work analyzing data sets with a very large number of covariates, such as point-of-sale (POS) data and scanner panel data. When conducting statistical analysis based on such high-dimensional covariate information, it is rarely the case that the model of interest is fully specified in advance; instead, it is standard practice to select models through exploratory data analysis (EDA). A wide variety of EDA methods have been proposed, including the Akaike Information Criterion (AIC; Akaike-1973), the Bayesian Information Criterion (BIC; Schwarz-1978), the Lasso (Tibshirani-1996), and principal component analysis (PCA;Pearson-1901; Hotelling-1933), which is the focus of this study.
PCA is frequently employed as a dimension reduction tool when the number of covariates is large. A large number of covariates typically implies a high likelihood that many of them are highly similar, or strongly correlated. Using such variables directly as regressors can give rise to multicollinearity. In such situations, PCA is used to mitigate multicollinearity by constructing a small number of new covariates as linear combinations of the original covariates, with those new covariates orthogonal to each other. For example, McKenzie-2005 constructs a one-dimensional covariate capturing “long-term household economic status” by applying PCA to 30 indicators from Mexican household survey data, which consists of housing characteristics (e.g., number of rooms, building materials), access to infrastructure (e.g., water, electricity, telecommunications), and ownership of durable goods (e.g., vehicles, household appliances).
One important challenge in PCA is, as argued in Choi-Taylor-Tibshirani-2017, how to determine the number of principal components to retain. One criterion is the cumulative proportion, which is the proportion of the data’s “variation” explained when using the st through the -th principal components. In practice, analyses often use enough principal components to reach about cumulative proportion. While this practice is arguably subjective, several computational procedures have also been proposed. Jolliffe-2002 (Jolliffe-2002, Chapter 6) serves as a more general summary for the existing methods to choose the number of principal components.
This paper proposes a statistical testing procedure for selecting the number of principal components. Our method is valid under random designs, that is, when covariates are assumed to be realizations of underlying stochastic variables or when there is no justification for parametric noise structure on data. Our testing procedure achieves asymptotically exact type error controls.
The rest of this section reviews the related literature. In Section 2, we introduce a setup and several assumptions, and formalize our testing procedure. Section 3 provides a simulation study which visualizes the performance of our testing procedure. Section 4 concludes this article. The proof of the main result is collected in Appendix A.
Related literature
This paper is most closely related to the works of GSell-Wager-Chouldechova-Tibshirani-2016, Taylor-Loftus-Tibshirani-2016 and Choi-Taylor-Tibshirani-2017, who develop a Kac-Rice based framework for rank estimation of noisy matrices under fixed designs. In their setting, one observes a non-stochastic signal matrix of interest contaminated by additive Gaussian noise. Their goal is to infer the true rank of the underlying signal. Specializing their framework to the covariance-type formulation most relevant for our paper, let denote a non-random vector and define the signal matrix as , with its ordered eigenvalues . The parameter of interest is , which can be characterized via the nested hypotheses
or, equivalently,
to be tested sequentially for . One has access to a noisy version of signal matrix
and base inference on its ordered eigenvalues of . For each step , they introduce the conditional singular value (CSV) test statistic
and show that is exactly distributed as under . A key feature of their construction is that it delivers valid global size control for the entire sequential testing procedure: the conditional law of is derived given both the observed noisy spectrum and the event that all previous null hypotheses have been rejected, thereby accounting for the data-driven, nested nature of the selection. While the noise variance is unknown in practice, Choi-Taylor-Tibshirani-2017 propose plug-in estimators based on the trailing eigenvalues of , such as
which preserves the pivotality of the CSV statistic.
Our paper builds directly on, and departs substantively from, this framework. Conceptually, we retain the Kac-Rice based, sequential testing perspective on the rank of a covariance matrix. Methodologically, however, our setting differs in the following important respects. We consider a random design framework in which the observed matrix arises as an empirical covariance of random covariates rather than as a fixed signal plus Gaussian noise; this calls for a fundamentally different probabilistic treatment and asymptotic theory. In this way, the contribution of our paper can be viewed as adapting the core ideas of the Kac-Rice methodology to a random design context that is more directly aligned with typical econometric applications, and justifying it by asymptotic theory.
2 Proposed testing procedure
Suppose that we observe an i.i.d. sample from the random vector in the population. Here, is a given constant and we assume . For simplicity and without loss of generality, we assume that . We denote
Furthermore, let be the eigenvalues of sorted in descending order, and let be the eigenvalues of sorted in descending order. What we are interested in is the true rank of the covariance matrix:
To estimate , the analyst aims to sequentially conduct tests of the following nested hypotheses: for each step ,
| (2.1) |
The proposed procedure for testing (2.1) is as follows:
Test 1.
With a given level , and the following test statistic:
| (2.2) |
we reject if and accept otherwise.
The interpretation of the test statistic (2.2) is quite similar to the one in Choi-Taylor-Tibshirani-2017 (Choi-Taylor-Tibshirani-2017, Test 2.1): The test statistic compares the relative size of ranging between , and a small value of implies a large value of , supporting the alternative hypothesis (Choi-Taylor-Tibshirani-2017, p.2598).
To establish an asymptotic justification for Test 1, we introduce several assumptions.
Assumption 1 (Local null structure).
We assume that the null hypothesis can be interpreted as follows:
-
•
for each sample size , there exists a population distribution ;
-
•
under each , the population covariance matrix is well-defined;
-
•
among the eigenvalues of , those with indices satisfy
(2.3)
Assumption 2 (Regularity conditions).
For each , we impose the following assumptions. The observed () are i.i.d. samples from . Without loss of generality, assume . Assume that is finite and that
| (2.4) |
Furthermore, assume that
| (2.5) |
is finite and positive definite.
Assumption 3 (Population eigenvalue gap).
For each , there exists a given constant , which satisfies that, for each , under the null hypothesis ,
| (2.6) |
Proposition 1 (Global size control).
3 Simulation examples
We conduct simulation studies to estimate the rank of the population covariance matrix using Test 1 with level . The data generating process is specified as follows: Let be a given matrix with full column rank, and let be a random vector of latent factors following a multivariate -distribution, without assuming Gaussian noise. Define as the random vector of population explanatory variables; by construction, only out of the explanatory variables are linearly independent. In the simulation study, we set and consider , with 500 Monte Carlo replications for each setting.
The resulting performance of Test 1 can be seen in Table 1, which provides several insights into the proposed method. First, the results for suggest that the proposed procedure behaves in accordance with the underlying asymptotic theory. Second, when the true number of factors is relatively large, the simulation results indicate that the later stages of the sequential testing procedure has reduced its statistical power. This phenomenon is not at all counterintuitive: as the procedure proceeds to higher values of , the -th sample eigenvalues become smaller, which naturally makes it more difficult to reject the null hypothesis that the corresponding population eigenvalues are equal to zero. A plausible explanation is that the theoretical justification of the procedure is derived under the local null structure. In particular, Assumption 1 imposes with , which treats population eigenvalues that shrink faster than order as indistinguishable from zero in the asymptotic analysis. Within this framework, such rapidly vanishing but nonzero eigenvalues are asymptotically regarded as contributing to a reduced rank, which limits the extent to which one can expect uniform consistency of the proposed test’s power.
| False | True | True | True | True | True | True | |
| 63.0 | 0 | NA | NA | NA | NA | NA | |
| (315/500) | (0/315) | NA | NA | NA | NA | NA | |
| 98.2 | 0 | NA | NA | NA | NA | NA | |
| (491/500) | (0/491) | NA | NA | NA | NA | NA | |
| False | False | False | True | True | True | True | |
| 96.0 | 55.8 | 19.4 | 0 | NA | NA | NA | |
| (480/500) | (268/480) | (52/268) | (0/52) | NA | NA | NA | |
| 99.8 | 92.0 | 65.1 | 0 | NA | NA | NA | |
| (499/500) | (459/499) | (299/459) | (0/299) | NA | NA | NA | |
| 99.8 | 99.4 | 95.4 | 0 | NA | NA | NA | |
| (499/500) | (496/499) | (479/496) | (0/479) | NA | NA | NA | |
| False | False | False | False | False | True | True | |
| 98.0 | 87.6 | 62.2 | 33.7 | 17.8 | 0 | NA | |
| (490/500) | (429/490) | (267/429) | (90/267) | (16/90) | (0/16) | NA | |
| 97.8 | 95.1 | 87.7 | 80.4 | 98.2 | 0 | NA | |
| (489/500) | (465/489) | (408/465) | (328/408) | (322/328) | (0/322) | NA | |
| 98.8 | 97.1 | 88.8 | 80.5 | 100 | 0 | NA | |
| (494/500) | (480/494) | (426/480) | (343/426) | (343/343) | (0/343) | NA | |
| 99.8 | 100 | 98.4 | 86.6 | 100 | 0 | NA | |
| (499/500) | (499/499) | (491/499) | (425/491) | (425/425) | (0/425) | NA | |
| False | False | False | False | False | False | False | |
| 99.4 | 99.4 | 96.4 | 56.5 | 7.1 | 10.5 | 100 | |
| (497/500) | (494/497) | (476/494) | (269/476) | (19/269) | (2/19) | (2/2) | |
| 99.6 | 99.6 | 98.6 | 65.4 | 28.1 | 0 | NA | |
| (498/500) | (496/498) | (489/496) | (320/489) | (9/320) | (0/9) | NA |
Note: The numbers in parentheses indicate, for each step, the number of times it was reached and the number of times the null was rejected.
4 Conclusions
This paper has developed a sequential testing procedure for estimating the true rank of a covariance matrix in a random design framework, with the specific goal of providing a statistically principled tool for selecting the number of principal components. Building on the CSV test developed by Choi-Taylor-Tibshirani-2017, we have adapted their fixed design, noisy matrix framework to the empirically more relevant setting in which the observed matrix is an empirical covariance of stochastic covariates. Under a set of high level assumptions, including a local null structure for the trailing eigenvalues and a separation condition for the leading eigenvalues, we have shown that the proposed test statistic yields asymptotically exact type 1 error controls at each step of the nested testing procedure. In this way, our method provides a theoretically justified alternative to ad-hoc rules for choosing the number of principal components in applications where the covariates are stochastic and a parametric noise model is not available.
Future Work
The proposed sequential test for the rank of the covariance matrix under random design is intended as a foundational contribution toward rigorous inference following PCA based EDA. Several directions for future research, closely aligned with this broader agenda, naturally arise.
First, a direct extension is to integrate the present rank testing framework with selective inference for linear regression models that use PCA generated regressors. Following the spirit of Taylor-Loftus-Tibshirani-2016 and Choi-Taylor-Tibshirani-2017, one may characterize the selection event associated with choosing the number of principal components via the sequential testing procedure, and then construct confidence intervals for regression parameters defined on the selected principal component space. The goal is to obtain asymptotically valid selective inference procedures (See e.g., Lee-Sun-Sun-Taylor-2016; Tibshirani-Taylor-Lockhart-Tibshirani-2016; Taylor-Tibshirani-2018; Markovic-Xia-Taylor-2018; Kuchibhotla-Kolassa-Kuffner-2022; and McCloskey-2024 for details for selective inference.) which do not rely on fixed-design, and to provide theoretically justified inference for regression models estimated after data-driven dimension reduction.
Second, it is important to move beyond linear regression and develop selective inference methods for more general estimators, including generalized method of moments (GMM). In many applications, such as discrete choice models and demand estimation in empirical industrial organization (Berry-Levinsohn-Pakes-1995; Berry-Haile-2021), the use of PCA or factor models to summarize high-dimensional product characteristics is practically indispensable. Thus, another natural future goal is to derive selective inference procedures that remain valid when the target parameter is estimated by complex or nonlinear methods using covariates pre-processed through PCA or related techniques.
Third, the methodology can be extended to high-dimensional factor models and related latent structure settings. Existing procedures for determining the number of factors or the effective rank (Bai-Ng-2003; Bai-Li-2012; Fan-Ke-Sun-Zhou-2019; Fan-Ke-Wang-2020; Qiu-Li-Yao-2024; Li-Zhang-2025) focus primarily on consistent estimation and robustness properties. However, they typically do not provide a selective inference theory for downstream analyses that condition on the estimated factor structure. Another goal for future research is to adapt and generalize the present approach so as to explicitly incorporate the estimation of factor spaces and to deliver valid inference for selected parameters. Such developments would have broad implications for empirical work employing factor models in macroeconomics, finance, and other fields.
Taken together, these directions outline a research agenda in which the theoretical contributions of this paper, a rank estimation under random design with asymptotically exact type error controls, form the core building block for a comprehensive framework of selective inference after PCA based and factor based EDA, with direct relevance to econometric practice and empirical applications.
Acknowledgments
The author would like to thank Yoshihiko Nishiyama, Takahide Yanagi and Shunsuke Imai for helpful comments and discussion.
Appendix A Proofs
To prove Proposition 1, we introduce several lemmas as follow.
We omit the proof of Lemma 1, since it follows immediately from the Cramér-Wold device and the Lindeberg-Feller central limit theorem. ∎
Definition 1 (Null space).
Under the null hypothesis for each , we define the null space of as
| (A.1) |
We denote by the projection matrix onto the null space . By construction, it follows that under the null hypothesis , we have .
Lemma 2 (Projected CLT).
For each , under the null hypothesis ,
holds, where is a matrix defined on and
Lemma 3 (Limiting behavior of non-zero eigenvalues).
For , it holds that
Furthermore,
For each , by Lemma 1 and Weyl’s inequality222See e.g., Bhatia-1997 and Horn-Johnson-2013 for the standard matrix perturbation theory.,
from which the result follows. ∎
Lemma 4 (Sample eigenvalue gap).
For each , under the null hypothesis , for , it holds that
Lemma 5 (Limiting behavior of null eigenvalues).
Let the eigenvalues of , sorted in descending order, be denoted by . Then, for each , under the null hypothesis , it holds that
Consider . Under the null hypothesis , by the “local null” interpretation (Assumption 1),
holds. Moreover, by Lemma 1 and Weyl’s inequality,
holds. Recall also from Lemma 2 that
Combining these and applying the continuous mapping theorem, we obtain
as claimed. ∎
Lemma 6 (Limiting test statistic).
Define the “limiting test statistic” as
where
Then, under the null hypothesis for each , it holds that
We omit the proof, since it follows from the same Kac–Rice and conditional density calculations as in Choi-Taylor-Tibshirani-2017 (Choi-Taylor-Tibshirani-2017, Theorem 2.1). ∎
Lemma 7 (Plug-in scale estimator).
Under the null hypothesis for each , it holds that
Lemma 8 (Limiting behavior of test statistic).
Under the null hypothesis for each , it holds that
Starting from
we apply to obtain
Interpreting this as a function of , Lemma 3, Lemma 5, Lemma 7, and the continuous mapping theorem yield
∎
Robust Cauchy-Based Methods for Predictive Regressions111We are grateful to the Editor, Anindya Banerjee, and two anonymous referees for their helpful comments and constructive suggestions. We also thank Jean-Marie Dufour, Jenny Hau, Nour Meddahi, Aleksey Min, Ulrich K. Müller, Robert Taylor, Neil Shephard, and the participants at the 54th NES conference and iCEBA conferences for helpful discussions and comments. We also thank Yongok Choi for providing his code.
Abstract
This paper develops robust inference methods for predictive regressions that address key challenges posed by endogenously persistent or heavy-tailed regressors, as well as persistent volatility in errors. Building on the Cauchy estimation framework, we propose two novel tests: one based on -statistic group inference and the other employing a hybrid approach that combines Cauchy and OLS estimation. These methods effectively mitigate size distortions that commonly arise in standard inference procedures under endogeneity, near nonstationarity, heavy tails, and persistent volatility. The proposed tests are simple to implement and applicable to both continuous- and discrete-time models. Extensive simulation experiments demonstrate favorable finite-sample performance across a range of realistic settings. An empirical application examines the predictability of excess stock returns using the dividend–price and earnings–price ratios as predictors. The results suggest that the dividend–price ratio possesses predictive power, whereas the earnings–price ratio does not significantly forecast returns.
Abstract
This paper develops robust inference methods for predictive regressions that address key challenges posed by endogenously persistent or heavy-tailed regressors, as well as persistent volatility in errors. Building on the Cauchy estimation framework, we propose two novel tests: one based on -statistic group inference and the other employing a hybrid approach that combines Cauchy and OLS estimation. These methods effectively mitigate size distortions that commonly arise in standard inference procedures under endogeneity, near nonstationarity, heavy tails, and persistent volatility. The proposed tests are simple to implement and applicable to both continuous- and discrete-time models. Extensive simulation experiments demonstrate favorable finite-sample performance across a range of realistic settings. An empirical application examines the predictability of excess stock returns using the dividend–price and earnings–price ratios as predictors. The results suggest that the dividend–price ratio possesses predictive power, whereas the earnings–price ratio does not significantly forecast returns.
Keywords: predictive regressions, robust inference, near nonstationarity, heterogeneity, heavy tails, persistent volatility, endogeneity.
JEL Codes: C12, C22
Keywords: predictive regressions, robust inference, near nonstationarity, heterogeneity, heavy tails, persistent volatility, endogeneity.
JEL Codes: C12, C22
The Total Word Count: 12491
1 Introduction
Predictive regressions play a central role in empirical finance, providing a framework for assessing whether financial or macroeconomic variables can forecast future returns. Prominent applications include the forecasting of equity and aggregate returns (see, among others, CampbellYogo2006, CampbellYogo2006; GoyalWelch, GoyalWelch; campbell, campbell, Hirshleifer, Hirshleifer; KJ, KJ; Rapach, Rapach; MR, MR; Goyal2024, Goyal2024, and references therein) and tests of market efficiency (e.g., Fama1, Fama1, Fama, Fama2, the review in Martin, Martin, and references therein). Numerous studies have examined the econometric properties of predictive regressions for stock returns (see Phillips2015, Phillips2015, for a review), highlighting several statistical challenges that complicate inference on return predictability. Widely used predictors, including valuation ratios such as the dividend–price and earnings–price ratios, typically exhibit near–unit–root dynamics, and their innovations are correlated with stock returns over long horizons. This combination of persistence and endogeneity induces substantial biases in conventional hypothesis tests (see, e.g., stambaugh1999predictive, stambaugh1999predictive; KimPark1, KimPark1). Moreover, stock return volatility is stochastic and highly persistent (jacquier2004bayesian, jacquier2004bayesian; hansen2014estimating, hansen2014estimating), and cavaliere2004testing shows that such volatility can lead to severe size distortions in tests assuming stationarity. Predictive regression data also often exhibit heavy tails, jumps, structural breaks, and regime shifts, further undermining standard inference (op. cit.).
A large literature has addressed persistent endogeneity in predictive regressions. Notably, CampbellYogo2006, ChenDeo2009, PhillipsMagdalinos2009, and KMS2015, among others, proposed inference procedures designed to handle persistence and endogeneity. While these methods perform well in such environments, they do not adequately address other empirical features such as persistent volatility, structural breaks, or regime switching. The simulations in ibragimov2023new show that standard tests can suffer from severe size distortions under persistent volatility.
CJP2016 proposed an inference method (the Cauchy RT) based on the Cauchy estimator and a time-change transformation in a continuous-time framework to address endogenous persistence and volatility in the errors.222See also BuKimWang for an alternative method robust to endogenously persistent or heavy-tailed regressors and persistent volatility in continuous time. ibragimov2023new introduced another approach (the Cauchy VC), also based on the Cauchy estimator but with a nonparametric volatility correction. Unlike CJP2016, it applies to both continuous- and discrete-time models.
This paper proposes two practical tests that serve as robust alternatives to these methods. The proposed tests are robust to heterogeneous and persistent volatility, as well as to endogenous, persistent, and/or heavy-tailed regressors. Both employ Cauchy estimation procedures, as in CJP2016 and ibragimov2023new, to address endogeneity, persistence, and heavy tails. The two methods differ in their treatment of heterogeneous volatility: the first extends the -statistic-based group inference of IbragimovMuller2010 to asymptotically normal Cauchy estimators, while the second is a hybrid test combining Cauchy and OLS estimation that uses the Cauchy estimator for the coefficient and OLS residuals for the standard error.
The proposed methods are easy to implement and avoid the technical complexities of the time-change transformation in CJP2016 and the nonparametric volatility correction in ibragimov2023new. Although they rely on an asymptotically exogenous volatility condition, they perform well in finite samples and compare favorably to alternative procedures across empirically relevant scenarios, including mild violations of this condition. Moreover, the proposed methods apply to both continuous- and discrete-time models, as in ibragimov2023new. Overall, the two tests developed here and the inference procedures of CJP2016 and ibragimov2023new should be viewed as complementary, particularly in environments with heteroskedastic and persistent volatility.
The remainder of the paper is organized as follows. Section 2 discusses the predictive regression model and the Cauchy estimator; Section 3 develops the inference procedures and theoretical results; Section 4 extends them to multiple predictors and models with intercepts; Sections 5 and 6 present simulations and an empirical illustration; and Section 7 concludes. All proofs are in the Appendix.
2 Predictive Regressions and the Cauchy Estimator
2.1 Model and Issues
Throughout the paper, we consider -adapted processes defined on a filtered probability space equipped with an increasing filtration of sub--fields of . Our objective is to test the (un)predictability of the process (e.g., the time series of excess stock returns) based on a covariate process (e.g., the time series of price–dividend ratios). As usual, we consider the linear predictive regression model
| (1) |
Following the standard specification for a volatility model, we assume that
where is a volatility process and is a martingale difference sequence (MDS) with respect to . We impose the following regularity conditions on .
Assumption 2.1.
(a) ; (b) is -adapted and nonnegative; and (c) for any .
Conditions (a) and (b) are standard and ensure that the conditional variance of is identified: . Condition (c) is a conditional Lindeberg condition, which holds, for example, if is bounded for some with probability one. See ibragimov2023new and references therein for further discussion and examples of processes satisfying Assumption 2.1.
The hypothesis of unpredictability of corresponds to in regression (1). It is well-known that standard OLS -statistic inference is not robust to many empirically relevant features of financial data. For instance, the OLS estimator of is not asymptotically Gaussian under if is endogenous and (nearly) nonstationary (see elliott1994inference, elliott1994inference; PhillipsNear, PhillipsNear; GP, GP; PM, PM; KMS2015, KMS2015), or even if is stationary but has infinite variance (e.g., GrangerOrr, GrangerOrr; EKM, EKM; ibragimov2015heavy, ibragimov2015heavy). This non-Gaussianity persists even when the errors are homoskedastic with for all .333As usual, the endogeneity of refers to the existence of nonzero long-run covariance between the innovations of and . Furthermore, stock return data exhibit time-varying and stochastically persistent volatility, which causes the distribution of the OLS -statistic to deviate from standard normality, leading to size distortions in conventional tests (see CJP2016, CJP2016; ibragimov2023new, ibragimov2023new).
2.2 The Cauchy Estimator
Both inference methods proposed in this paper build upon the following Cauchy estimator of (assuming no intercept, i.e., ):
where denotes the sign function, for and for . The estimator can be interpreted as an instrumental variable (IV) estimator using as an instrument for (see, e.g., SoShin, SoShin; BREITUNG2015, BREITUNG2015; kim-meddahi-2019, kim-meddahi-2019; shephard2020, shephard2020).
Under Assumption 2.1, (denoted by ) is a unit-variance MDS with respect to . Define the continuous-time partial sum process by
which takes values in , the space of càdlàg functions on with values in . By the functional central limit theorem for martingales (Theorem 18.2 of billingsley1986convergence, billingsley1986convergence), we have in , where is a standard Brownian motion.
For the volatility process , define on . Then the Cauchy estimator can be expressed in terms of and as
Following ibragimov2023new, we assume that the volatility process is persistent in the sense that it admits a limiting process defined on such that jointly.
Assumption 2.2.
There exists a nonnegative process on such that
where is a standard Brownian motion adapted to the same filtration as .
Assumption 2.2 encompasses a wide class of models, including those with nonstationary volatility, regime switching, or structural breaks.444Assumptions 2.1 and 2.2 exclude some globally homoskedastic processes, such as stationary GARCH models. However, the hybrid testing procedure proposed later remains valid under , which includes conditionally heteroskedastic but globally homoskedastic processes, such as stationary GARCH models (see also Section 4 of ibragimov2023new, ibragimov2023new). It also covers cases with deterministic volatility , as in cavaliere-taylor-2007, cavaliere-taylor-2008, xu-phillips-2008, and harvey-leybourne-zu, among others.555Assumption 2.2 is a simplified version of the condition in Assumption 2 of cavaliere-taylor-2009. We focus on stochastically bounded volatilities with , excluding explosive volatility settings () for simplicity. It further includes nonstationary volatility processes such as those in hansen1995regression and chung2007nonstationary, where is a smooth positive transformation of a (near) unit root process. Overall, Assumptions 2.1 and 2.2 are general enough to allow for stochastic and discontinuous volatility—features commonly observed in financial returns.
Under Assumptions 2.1 and 2.2, the properly normalized Cauchy estimator satisfies
by standard results on the convergence of stochastic integrals (see hansen-1992, hansen-1992; kurtz1991weak, kurtz1991weak; ibragimov2023new, ibragimov2023new). The limit is in general a non-Gaussian martingale, becoming Gaussian only if and are independent. In that case, is a scale mixture of normals with variance , denoted
We formalize the independence assumption as follows.
Assumption 2.3.
The processes and in Assumption 2.2 are independent.
Assumption 2.3 requires the volatility process to be asymptotically independent of the martingale , but does not preclude finite-sample dependence. For example, consider
where is bounded and is independent of with , where and are independent. For any , the volatility process in this example satisfies Assumption 2.3, even though and may be dependent for any fixed .
In the following sections, we develop inference methods based on the Cauchy estimator. Section 3 focuses on predictive regressions with a single predictor and no intercept, while Section 4 extends the analysis to models with multiple predictors and an intercept.
3 Robust Inference for Predictive Regressions
3.1 Robust -Statistic Approach
The first approach relies on -statistic-based inference using group estimates of , as proposed by IbragimovMuller2010 (see also IM1, IM1; Section 3.3 of ibragimov2015heavy, ibragimov2015heavy). The method is based on normalized Cauchy estimators—specifically, the numerator of the Cauchy estimator divided by in the full-sample case:
| (2) |
Following the -statistic approach, we partition the sample into a fixed number of approximately equal groups of consecutive observations. The observation at time belongs to the th group if
We compute the normalized Cauchy statistic in (2) within each group:
| (3) |
The -statistic based on the group statistics is given by
| (4) |
where
Under the null hypothesis , the test rejects in favor of if , where denotes the two-sided -critical value at level , i.e. for (one-sided tests are analogous).
To study the asymptotic behavior of , we decompose
where
Under Assumption 2.1, forms a sequence of martingale differences uncorrelated across groups, yielding the following asymptotic characterization.
The statistics do not satisfy the standard condition in IbragimovMuller2010, which requires estimators such that
for some , , and independent of . By contrast, Lemma 3.1 shows that lack such a diverging normalization. Consequently, as shown in Proposition 3.2, the -statistic approach yields correct asymptotic size but is consistent only for a restricted class of covariates, excluding (near) unit-root processes. This inconsistency arises precisely because the asymptotics of do not involve a diverging sequence (see proofs of Proposition 3.2 and Corollary 3.3).
Nevertheless, with additional regularity conditions, if for positive random variables and a sequence , then the Cauchy estimator computed within each group satisfies
for . In general, however, are non-Gaussian, especially when is (near) unit root and endogenous. Applying the -statistic approach to thus yields consistency for broader classes of covariates but may incur size distortions due to non-Gaussianity.
Proposition 3.2.
(a) Under ,
(b) Under , suppose is stationary with and satisfies
Then
Proposition 3.2 shows that the -statistic approach is conservative under and consistent under when is stationary with a finite first moment. It is thus valid and robust to persistent heteroskedasticity and endogenously heavy-tailed covariates. However, it becomes inconsistent for highly persistent covariates, such as (near) unit-root processes. To illustrate, consider the generalized local-to-unity framework of dou2021generalized, where for and
| (5) |
with a stationary continuous-time Gaussian ARMA process.666See dou2021generalized for a detailed discussion.
Corollary 3.3.
When is highly persistent, converges to rather than diverging, with lower bound . Simulations in Section 4 confirm that rejection probabilities remain high even when is asymptotically bounded. For ,
| (6) |
The ratio form in (6) implies large realizations of in finite samples, producing high rejection rates even under inconsistency. Figure 1 plots the simulated density of when is Brownian motion.777Based on 100,000 simulated draws. The minimum simulated value is 1.15, and with .
3.2 A Hybrid Test
We now propose a simple hybrid test that remains consistent for a broad class of covariates. Under Assumptions 2.1–2.3,
where and .
A key feature of the Cauchy estimator is that its properly normalized limit distribution is invariant to the data-generating process of . By contrast, the OLS estimator’s variance depends on both and , complicating variance estimation even under homoskedasticity. For , the asymptotic variance depends solely on , requiring only heteroskedasticity-robust adjustments.888See shephard2020, Section 4.3, for related discussion.
We define the hybrid test statistic as
where as in (2), and
Here, estimates using OLS residuals. As noted by shephard2020, the Cauchy-based variance estimator performs poorly because the Cauchy estimator converges more slowly and less efficiently than OLS when is heavy-tailed or nearly integrated. Hence, we use OLS residuals to improve efficiency.999A related approach is employed by KMS2015 in the IVX framework of PM.
We assume:
Assumption 3.1.
.
Assumption 3.1 is very general and holds in many time-series settings. It is weaker than Assumption 3.2 of ibragimov2023new, which requires for . As shown in ibragimov2023new, this holds with when is either (near) unit root or stationary with finite variance; it also applies to certain stationary heavy-tailed processes (see, e.g., Samorodnitsky2007, Samorodnitsky2007).
The conclusions of Proposition 3.4 remain valid under weaker conditions. For instance, if Assumptions 2.1 and 3.1 hold and
where is independent of , then retains its asymptotic validity. These conditions include stationary volatility with . Hence, Assumptions 2.2 and 2.3 can be interpreted as primitive sufficient conditions accommodating persistent volatility in predictive regression data.
Remark. Proposition 3.4(a) also holds if uses instead of , since under . Moreover, the corresponding test remains consistent when is stationary with finite variance or follows a generalized local-to-unity process (dou2021generalized). However, it can be inconsistent for heavy-tailed . For instance, if is i.i.d. -stable with and independent of , then
by the generalized central limit theorem (see feller1971, feller1971; logan1973, logan1973; Davis1983, Davis1983; DavisResnick1986, DavisResnick1986). Thus, the use of (or another consistent estimator under both and ) is crucial for the consistency of the hybrid test.
4 Extensions
This section extends the inference methods developed in Section 3 to models with multiple predictors and to regressions including an intercept. Our goal is not to design efficient procedures but to provide simple and robust inference methods that rely on minimal assumptions on the predictors and volatility processes.
4.1 Predictive Regressions with Multiple Predictors
Consider a predictive regression with predictors :
| (7) |
The objective is to test the joint predictability of the covariates, that is,
We construct a testing procedure for based on the univariate inference methods in Section 3. Specifically, we estimate univariate predictive regressions
and test each null hypothesis
Clearly, implies for all . The converse also holds under mild regularity conditions, as shown below.
Lemma 4.1.
Consider model (7) and define . Suppose that for each , , for all , and that the matrix is invertible.101010Even when is constant, the univariate Cauchy estimator and associated tests remain well defined. In this case, the estimator simplifies to , implying , and Proposition 3.4 continues to hold. Hence, the lack of sign variation does not affect the validity of our methods in the univariate case. In the multiple-predictor case, however, the invertibility of imposes mild restrictions on sign changes across predictors. For practical applications, one can induce variation in the sign instrument by recentering the predictor, for example, , which preserves both the martingale structure and the asymptotic validity of the estimator. Then, the joint null hypothesis holds if and only if holds for all .
Lemma 4.1 justifies the use of multiple hypothesis testing based on univariate Cauchy estimators.111111See Harvey2015 for an application of the multiple-testing framework in predictive regressions, and KMS2015 for joint-predictability tests in the IVX framework. Note that the IVX approach may lose validity under heavy-tailed predictors or continuous-time data, whereas our method remains robust in such settings. In conjunction with the hybrid test introduced in Section 3.2, we compute the statistic for each parameter , where denotes the corresponding Cauchy estimator. Let denote its -value. The joint null hypothesis is rejected at level if , following the Bonferroni correction.
This approach directly extends the univariate robust inference procedure to a multivariate setting and requires only mild conditions for the equivalence between and . The Bonferroni correction imposes no assumptions on the joint distribution of the test statistics, which motivates its use here (see Holm1979, Holm1979; BenjaminiHochberg1995, BenjaminiHochberg1995; Shaffer1995, Shaffer1995).
We also note that if one additionally assumes that is invertible, the joint hypothesis can be tested directly using a Wald-type statistic:
In particular, under ,
and hence .121212As mentioned earlier, can be interpreted as an instrument. Therefore, one may use an alternative instrument, as in shephard2020, and construct a Wald-type test accordingly. We leave a systematic comparison between the Bonferroni-type multiple-testing procedure and the Wald-type joint test for future research.
4.2 Predictive Regressions with an Intercept
The analysis in Section 3 assumes that the intercept in (1). When , it must be properly accounted for. A natural starting point is the demeaned model
| (8) |
where for . However, is not an martingale difference sequence (MDS) with respect to , invalidating the martingale CLT used in Sections 2 and 3. Specifically, the Cauchy estimator becomes
which is problematic because: (i) is not an MDS, and (ii) is not -measurable. Thus, the theory in Section 3 is not directly applicable.131313Recursive demeaning using instead of does not resolve this issue since is not an MDS either.
To restore the MDS property, we instead difference the model:
and estimate this first-differenced (FD) model on alternating subsets of observations. We focus on the even-indexed observations and define the modified Cauchy estimator:
This estimator has two key properties. First, for even-indexed data, the regression error forms an MDS with respect to for .141414For odd-indexed data, forms an MDS with respect to , yielding an analogous estimator . Second, can again be viewed as an IV estimator, but it uses , which is -measurable, as the instrument.151515More generally, one may use for deterministic weights , provided . Hence, is an MDS with respect to .
The inference procedures of Section 3 remain valid for . In particular, the hybrid test in Section 3.2 can be implemented as
| (9) |
where , and
with the OLS estimator from the demeaned model (8). Note that is based on the full sample, whereas uses only even-indexed data. The asymptotic validity of this hybrid procedure is established next.
Corollary 4.2.
(a) Under ,
(b) Under ,
so that whenever .
Although the odd-indexed estimator has analogous properties, and are typically dependent, with the dependence structure determined by the DGP of . Hence, unless additional assumptions are imposed, we restrict attention to a single subset of observations—either with even or odd indices.161616Using only half of the data is not uncommon in predictive regressions. See, for example, ZhuCaiPeng2014 and Liu2019, who employ long-lag differencing to eliminate intercepts. In addition, Dufour uses a split-sample approach to address inference problems under a Markovian structure.
Consistency of the hybrid test with an intercept requires
This holds for most stationary processes if
The condition may fail for certain unit-root processes. For instance, for a random walk , it does not hold. More generally, in the local-to-unity model of Phillips_Magdalinos_2007book,
| (10) |
with satisfying Assumption LP therein, the consistency condition becomes
The first term diverges if and only if (see Lemma 3.2 of Phillips_Magdalinos_2007book, Phillips_Magdalinos_2007book). Since and may be dependent, typically , causing the second term to diverge as well. Hence, the condition fails only if and . In all other cases ( or nonzero covariance), the test remains consistent.
5 Finite Sample Performance
This section investigates the finite-sample performance of the proposed inference methods. Two sets of simulation experiments are conducted. The first set is based on a continuous-time model and compares our robust -statistic–based tests, for , and the hybrid test with the Cauchy RT test of CJP2016 and the Cauchy VC test of ibragimov2023new. The second set is based on a discrete-time predictive regression model with an intercept and compares our procedures with the IVX test of KMS2015.
5.1 Continuous-Time Experiments
5.1.1 Simulation Design
Following CJP2016 and ibragimov2023new, we consider the continuous-time predictive regression model
| (11) | ||||
where and are Brownian motions with . The constant term in the predictive regression is set to zero without loss of generality, and recursive demeaning is applied. The model is observed at interval , corresponding to daily observations, so that a sample of length years contains observations.
The volatility process follows one of the following specifications:
-
•
CNST (Constant volatility): , with .
-
•
SB (Structural break): , with and .
-
•
RS (Regime switching): , where is a two-state Markov chain independent of , with transition matrix
initialized at its invariant distribution, where , , and .
-
•
GBM (Geometric Brownian motion): , where is a Brownian motion correlated with such that and .
We set (corresponding to 60, 240, 600, and 1200 monthly observations) and for the persistence parameter in (11), and consider a two-sided test of against .
5.1.2 Results
We first assess the empirical size of each test under the null hypothesis . The results for the four volatility models (CNST, SB, RS, and GBM) and for are reported in Table 1. Overall, both the -statistic–based tests and the hybrid method exhibit satisfactory size performance, closely matching the nominal levels and performing comparably to the Cauchy RT and Cauchy VC tests. Among the -based procedures, moderate partition numbers ( or ) provide the most stable results, whereas smaller values tend to be mildly undersized. In the GBM case, where volatility is endogenously persistent, the -statistic–based tests become slightly conservative but remain competitive with the Cauchy RT and VC methods.
Next, we analyze the finite-sample power properties of the tests. We consider and under the same volatility specifications. The results are summarized in Tables 2–5. The proposed tests exhibit power comparable to that of the Cauchy RT and Cauchy VC procedures. For small samples (), the Cauchy RT and VC tests occasionally show higher power, but the difference diminishes as increases. In certain settings, our methods even outperform the existing approaches. For instance, dominates under , , and regime-switching volatility (Table 4), whereas the hybrid test performs best under , , , and regime-switching volatility.
In summary, all four robust inference procedures—Cauchy RT, Cauchy VC, , and —deliver accurate size control and strong discriminatory power under endogenously persistent regressors and persistent volatility. While the Cauchy RT requires high-frequency data and a time transformation, and the Cauchy VC involves nonparametric volatility filtering with a tuning parameter, our proposed -statistic and hybrid methods are much simpler to implement and require neither. Hence, these approaches are best viewed as complementary: the Cauchy RT and Cauchy VC are preferable in high-frequency environments, whereas our procedures provide robust and easily implementable alternatives in more general settings. It is also worth emphasizing that the proposed methods, like the Cauchy VC, are applicable to both continuous- and discrete-time models, whereas the Cauchy RT method is restricted to the continuous-time framework.
5.2 Discrete-Time Experiments
5.2.1 Simulation Design
We now examine the finite-sample performance of the proposed tests in a discrete-time setting with an intercept, comparing them to the IVX test of KMS2015. The data-generating process (DGP) is specified as
| (12) |
for , where corresponds to 5, 20, and 50 years of monthly data. We set and , and consider a one-sided test of against .171717The IVX test of KMS2015 performs well in two-sided testing for a broad class of models. However, as shown in DEMETRESCU2023105271, the IVX method exhibits severe size distortions in one-sided tests when regressors are highly persistent and endogenous. For this reason, we focus on the one-sided case to demonstrate the performance of our methods in this setting.
The innovation process follows an MA() process:
where are jointly normal with correlation . For the MA(2) case, and for ; for the MA(4) case, and for . The volatility process follows the same specifications as in the continuous-time simulations, except that the GBM model is excluded.
5.2.2 Results
The results, summarized in Tables 6–11, indicate that the proposed tests exhibit excellent size control under the null hypothesis across all DGPs, whereas the IVX test is substantially oversized, particularly when volatility is nonstationary or exhibits structural breaks. Furthermore, both the hybrid and -statistic approaches demonstrate nontrivial power, even though they are constructed using only half of the observations. Among the size-controlled procedures, the statistic consistently delivers the strongest performance.
Overall, these findings corroborate the theoretical robustness of our methods. They remain valid under heavy-tailed, endogenous, and persistent regressors, as well as under heteroskedastic and persistent volatility. In contrast, the IVX test performs well only under stationary volatility and light tails. Hence, our proposed robust procedures offer a practical and reliable alternative to existing inference methods for predictive regressions in both continuous- and discrete-time frameworks.
6 Empirical Application
To illustrate the empirical performance of the proposed tests relative to the Cauchy RT and Cauchy VC tests, we reexamine the dataset used by CJP2016 to test the predictability of stock returns using the dividend–price (D/P) and earnings–price (E/P) ratios as predictors. For stock returns, we employ the NYSE/AMEX value-weighted index and the S&P 500 index obtained from the Center for Research in Security Prices (CRSP). The dividend–price ratio is defined as the annual dividend divided by the current total market value. Further details on data construction are provided in Section 6.1 of CJP2016.
Following CJP2016, we estimate two types of predictive regressions: one based on all returns and another based only on returns generated from the diffusive component of stock prices, obtained by first testing for jumps and removing observations corresponding to detected jumps. In all cases, we apply one-sided tests.
The results are reported in Table 12. As shown in Panels C and D, none of the tests reject the null hypothesis of unpredictability for the S&P 500 data when the E/P ratio is used as a predictor. By contrast, when the D/P ratio serves as a predictor, the proposed tests— with and —reject the null of unpredictability for several cases: CRSP (yearly without jump removal; quarterly with jump removal) and S&P 500 (quarterly and yearly without jump removal; yearly with jump removal). In contrast, the Cauchy RT test fails to reject the null in all cases, while the Cauchy VC test yields qualitatively similar conclusions to our proposed tests, except that it additionally rejects the null for CRSP (monthly with jump removal) and S&P 500 (monthly with jump removal; quarterly with jump removal).
Consistent with our simulation evidence, the Cauchy RT test demonstrates strong finite-sample power but requires high-frequency data due to its reliance on a continuous-time approximation.181818For the Cauchy RT test in our simulations, we estimate the discretized time-changed regression using lower-frequency observations, with (approximately monthly), generated by the random time-sampling scheme described in Section 5 of CJP2016. The mixed empirical results—where the Cauchy RT test fails to reject the null while both the proposed methods and the Cauchy VC test do reject—may reflect the limited accuracy of the continuous-time approximation when applied to monthly, quarterly, or yearly data. Evaluating the robustness of the continuous-time approximation underlying the Cauchy RT test remains an interesting topic for future research.
7 Conclusion
This paper introduces two robust inference methods for predictive regressions, addressing key econometric challenges commonly encountered in empirical finance, such as endogenously persistent or heavy-tailed regressors and persistent volatility in errors. Building on the Cauchy estimation framework, we develop two simple yet theoretically rigorous procedures: a -statistic–based approach and a hybrid method. Both methods are computationally straightforward and applicable to continuous- and discrete-time models alike.
Simulation evidence demonstrates that the proposed tests perform well in finite samples, maintaining correct size and competitive power under a wide range of data-generating processes, including those characterized by stochastic volatility, structural breaks, and regime switching. Although our procedures require the assumption of asymptotically exogenous volatility, they exhibit excellent robustness and complement existing Cauchy-based methods, including the IVX method of KMS2015, the Cauchy RT test of CJP2016 and the Cauchy VC test of ibragimov2023new.
In an empirical application to stock return predictability, we use the dividend–price and earnings–price ratios as predictors for excess returns on major U.S. equity indices. The results indicate that the dividend–price ratio possesses predictive power, while the earnings–price ratio does not significantly forecast returns. Overall, the proposed inference procedures offer a practical, theoretically sound, and implementable alternative to existing methods for robust inference in predictive regressions.
Appendix: Proofs
Proof of Lemma 3.1.
Proof of Proposition 3.2.
Part (a) follows from Theorem 1 and the discussion in Section 2.2 of IbragimovMuller2010. For part (b), we deduce from Lemma 3.1 that
| (A.2) |
uniformly in , under . Recall that
Hence, the numerator of satisfies
| (A.3) |
To complete the proof, it suffices to show that . Indeed, for , we have
Proof of Corollary 3.3.
We aim to show that
| (A.4) |
and
| (A.5) |
Proof of Proposition 3.4.
It suffices to show that , since this implies
where, in particular,
Proof of Lemma 4.1.
We need only show that holds if holds for all . Let . By the moment restrictions, is the solution to
Given for all , holds for all if and only if .
Moreover,
and since is assumed invertible, the condition implies , completing the proof. ∎
Proof of Corollary 4.2.
| T | 5 | 20 | 50 | 5 | 20 | 50 | 5 | 20 | 50 | |
| CNST | Cauchy RT | 5.3 | 4.9 | 5.3 | 5.2 | 5.4 | 4.7 | 5.5 | 5.1 | 5.1 |
| Cauchy VC | 5.6 | 5.0 | 5.3 | 5.4 | 5.0 | 5.1 | 5.4 | 5.0 | 4.8 | |
| 4.8 | 4.6 | 5.2 | 4.7 | 5.0 | 5.1 | 4.8 | 4.9 | 5.0 | ||
| 4.7 | 4.7 | 5.0 | 4.7 | 5.1 | 4.9 | 5.0 | 5.2 | 4.9 | ||
| 4.8 | 4.8 | 5.1 | 4.9 | 5.0 | 4.9 | 4.7 | 4.9 | 4.9 | ||
| 5.2 | 4.8 | 5.2 | 5.0 | 4.8 | 5.1 | 5.2 | 4.9 | 4.8 | ||
| SB | Cauchy RT | 5.6 | 5.0 | 5.1 | 5.2 | 5.3 | 5.0 | 5.4 | 5.0 | 4.9 |
| Cauchy VC | 8.0 | 6.7 | 6.3 | 7.8 | 6.5 | 6.0 | 7.9 | 6.4 | 6.0 | |
| 3.6 | 3.7 | 3.9 | 4.0 | 4.1 | 3.7 | 3.8 | 3.7 | 3.7 | ||
| 4.7 | 4.2 | 4.6 | 4.3 | 4.6 | 4.5 | 4.2 | 4.2 | 4.5 | ||
| 4.7 | 4.6 | 4.6 | 4.8 | 4.7 | 4.5 | 4.3 | 4.6 | 4.5 | ||
| 6.1 | 5.0 | 4.9 | 6.1 | 5.5 | 5.1 | 6.2 | 5.5 | 5.0 | ||
| RS | Cauchy RT | 5.0 | 4.8 | 5.2 | 4.9 | 4.9 | 4.9 | 5.4 | 5.1 | 4.8 |
| Cauchy VC | 5.2 | 5.4 | 6.1 | 5.2 | 5.1 | 5.8 | 5.6 | 5.8 | 5.8 | |
| 4.5 | 4.5 | 5.1 | 4.6 | 4.4 | 4.9 | 4.6 | 5.3 | 4.6 | ||
| 4.8 | 4.6 | 5.0 | 4.5 | 4.8 | 4.8 | 4.6 | 4.9 | 4.7 | ||
| 5.0 | 4.4 | 4.9 | 4.6 | 4.6 | 4.9 | 5.1 | 5.1 | 4.5 | ||
| 5.0 | 4.9 | 5.2 | 5.2 | 4.7 | 4.9 | 5.2 | 5.3 | 5.0 | ||
| GBM | Cauchy RT | 4.4 | 4.7 | 4.4 | 4.3 | 4.5 | 4.4 | 4.6 | 4.5 | 4.5 |
| Cauchy VC | 5.4 | 5.5 | 6.1 | 5.7 | 5.7 | 5.9 | 5.7 | 5.9 | 6.5 | |
| 3.2 | 3.1 | 3.2 | 3.6 | 3.8 | 3.4 | 3.6 | 3.8 | 3.9 | ||
| 3.8 | 3.6 | 3.7 | 4.2 | 4.3 | 3.6 | 4.2 | 4.2 | 4.3 | ||
| 3.7 | 3.8 | 4.1 | 4.4 | 4.2 | 4.0 | 4.6 | 4.5 | 4.6 | ||
| 5.3 | 4.6 | 5.0 | 5.2 | 4.8 | 4.9 | 5.6 | 4.9 | 5.2 | ||
CNST, SB, GBM, and RS denote constant volatility, structural break, geometric Brownian motion, and regime switching, respectively.
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| Cauchy RT | 8.8 | 25.2 | 85.4 | 6.1 | 9.9 | 22.7 | 6.3 | 13.2 | 27.9 | |
| Cauchy VC | 9.1 | 25.6 | 86.2 | 6.5 | 10.0 | 23.5 | 8.2 | 13.4 | 26.8 | |
| 8.0 | 21.0 | 79.5 | 6.7 | 11.2 | 24.6 | 5.1 | 14.6 | 30.6 | ||
| 9.5 | 21.4 | 82.0 | 6.2 | 10.5 | 25.4 | 5.9 | 14.3 | 31.0 | ||
| 8.8 | 22.8 | 84.0 | 5.5 | 10.0 | 23.3 | 5.5 | 14.5 | 29.9 | ||
| 6.4 | 23.5 | 66.6 | 5.3 | 8.6 | 17.5 | 5.9 | 12.9 | 24.3 | ||
| Cauchy RT | 15.5 | 85.5 | 100.0 | 9.2 | 22.1 | 83.9 | 6.6 | 27.4 | 85.6 | |
| Cauchy VC | 17.2 | 86.2 | 100.0 | 8.8 | 23.1 | 84.3 | 11.9 | 27.7 | 84.2 | |
| 13.8 | 79.9 | 100.0 | 9.1 | 23.5 | 86.7 | 6.0 | 32.5 | 89.2 | ||
| 15.0 | 83.4 | 100.0 | 8.9 | 25.5 | 86.8 | 6.5 | 31.7 | 92.0 | ||
| 15.0 | 83.7 | 100.0 | 8.3 | 23.7 | 87.0 | 6.1 | 31.9 | 91.7 | ||
| 14.0 | 66.7 | 96.6 | 7.3 | 18.8 | 57.6 | 8.5 | 27.6 | 70.0 | ||
| Cauchy RT | 37.3 | 99.2 | 100.0 | 12.2 | 50.1 | 100.0 | 7.7 | 52.2 | 100.0 | |
| Cauchy VC | 40.2 | 99.3 | 100.0 | 12.6 | 51.9 | 100.0 | 16.5 | 56.3 | 100.0 | |
| 30.3 | 98.0 | 100.0 | 12.9 | 55.5 | 99.8 | 6.9 | 60.6 | 100.0 | ||
| 33.1 | 99.5 | 100.0 | 12.0 | 56.3 | 100.0 | 7.2 | 61.6 | 100.0 | ||
| 35.3 | 98.9 | 100.0 | 12.3 | 54.1 | 100.0 | 6.6 | 59.2 | 100.0 | ||
| 32.4 | 87.7 | 99.1 | 9.7 | 34.0 | 93.5 | 12.1 | 48.9 | 99.0 | ||
| Cauchy RT | 67.0 | 100.0 | 100.0 | 17.0 | 84.4 | 100.0 | 8.4 | 83.2 | 100.0 | |
| Cauchy VC | 68.8 | 100.0 | 100.0 | 16.6 | 84.7 | 100.0 | 22.4 | 83.3 | 100.0 | |
| 58.8 | 99.7 | 100.0 | 18.1 | 88.7 | 100.0 | 7.3 | 88.7 | 100.0 | ||
| 60.7 | 100.0 | 100.0 | 16.8 | 86.6 | 100.0 | 7.6 | 89.7 | 100.0 | ||
| 62.4 | 99.9 | 100.0 | 16.7 | 86.9 | 100.0 | 7.0 | 89.2 | 100.0 | ||
| 51.2 | 95.8 | 99.9 | 12.4 | 58.5 | 99.0 | 17.8 | 72.9 | 100.0 | ||
| Cauchy RT | 86.7 | 100.0 | 100.0 | 23.2 | 98.0 | 100.0 | 9.3 | 98.2 | 100.0 | |
| Cauchy VC | 87.9 | 100.0 | 100.0 | 24.4 | 98.5 | 100.0 | 29.4 | 97.4 | 100.0 | |
| 79.3 | 100.0 | 100.0 | 24.6 | 97.8 | 100.0 | 8.5 | 98.3 | 100.0 | ||
| 81.9 | 100.0 | 100.0 | 23.8 | 99.1 | 100.0 | 8.5 | 99.1 | 100.0 | ||
| 82.3 | 100.0 | 100.0 | 23.2 | 98.3 | 100.0 | 7.7 | 99.2 | 100.0 | ||
| 65.2 | 97.5 | 100.0 | 16.3 | 81.3 | 99.6 | 23.8 | 92.3 | 100.0 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| Cauchy RT | 8.7 | 20.0 | 67.7 | 8.3 | 11.6 | 35.9 | 8.7 | 17.2 | 52.4 | |
| Cauchy VC | 8.5 | 20.6 | 57.6 | 8.9 | 10.5 | 19.2 | 8.2 | 14.6 | 25.3 | |
| 5.6 | 11.3 | 45.6 | 4.4 | 8.7 | 15.7 | 6.4 | 8.4 | 23.3 | ||
| 6.6 | 14.8 | 45.3 | 6.7 | 9.1 | 17.5 | 6.2 | 10.5 | 22.7 | ||
| 8.2 | 14.6 | 44.1 | 6.9 | 8.9 | 19.5 | 6.6 | 10.1 | 22.9 | ||
| 5.8 | 13.7 | 31.4 | 7.1 | 8.1 | 13.5 | 5.8 | 10.2 | 16.7 | ||
| Cauchy RT | 14.4 | 65.1 | 96.6 | 11.7 | 35.7 | 88.4 | 14.5 | 52.2 | 99.6 | |
| Cauchy VC | 14.9 | 60.6 | 95.6 | 10.8 | 21.6 | 55.8 | 11.5 | 30.1 | 72.9 | |
| 9.5 | 44.1 | 88.5 | 6.8 | 16.3 | 57.4 | 9.0 | 21.0 | 68.3 | ||
| 11.4 | 45.6 | 89.8 | 8.9 | 17.9 | 55.9 | 9.2 | 23.6 | 69.4 | ||
| 12.7 | 42.3 | 92.1 | 8.4 | 18.3 | 55.9 | 8.8 | 22.2 | 72.2 | ||
| 8.9 | 32.4 | 81.9 | 8.9 | 14.0 | 34.3 | 7.9 | 16.6 | 49.7 | ||
| Cauchy RT | 32.1 | 87.0 | 99.3 | 15.5 | 68.6 | 98.8 | 24.2 | 90.1 | 100.0 | |
| Cauchy VC | 26.1 | 83.1 | 99.2 | 13.5 | 36.0 | 91.1 | 15.5 | 50.3 | 98.8 | |
| 16.9 | 72.9 | 97.0 | 9.2 | 32.7 | 89.0 | 12.0 | 43.1 | 95.2 | ||
| 20.7 | 74.5 | 99.1 | 11.9 | 34.5 | 90.0 | 12.2 | 41.7 | 98.0 | ||
| 20.0 | 74.3 | 99.6 | 10.0 | 31.9 | 90.7 | 12.6 | 41.4 | 98.8 | ||
| 14.7 | 60.2 | 98.1 | 10.8 | 23.4 | 73.7 | 10.8 | 33.2 | 92.1 | ||
| Cauchy RT | 50.2 | 95.6 | 99.7 | 22.8 | 89.8 | 100.0 | 37.7 | 99.7 | 100.0 | |
| Cauchy VC | 42.9 | 95.1 | 99.6 | 17.0 | 58.9 | 99.5 | 20.7 | 74.5 | 100.0 | |
| 30.2 | 88.8 | 98.5 | 12.6 | 56.2 | 96.1 | 16.6 | 67.4 | 99.8 | ||
| 31.7 | 90.4 | 99.7 | 14.7 | 53.8 | 98.4 | 17.0 | 65.0 | 100.0 | ||
| 31.8 | 91.1 | 100.0 | 13.0 | 54.4 | 99.0 | 18.5 | 66.6 | 100.0 | ||
| 23.2 | 81.4 | 99.8 | 13.0 | 35.6 | 96.9 | 14.3 | 51.8 | 100.0 | ||
| Cauchy RT | 65.4 | 98.6 | 99.9 | 34.6 | 97.4 | 100.0 | 55.1 | 100.0 | 100.0 | |
| Cauchy VC | 58.8 | 98.5 | 100.0 | 21.4 | 78.7 | 99.9 | 27.1 | 91.5 | 100.0 | |
| 45.5 | 95.3 | 98.8 | 17.6 | 74.0 | 98.2 | 21.7 | 86.2 | 100.0 | ||
| 46.1 | 96.4 | 99.8 | 18.0 | 75.7 | 99.7 | 22.7 | 87.5 | 100.0 | ||
| 44.7 | 97.9 | 100.0 | 18.0 | 73.9 | 100.0 | 24.9 | 87.9 | 100.0 | ||
| 32.2 | 92.4 | 100.0 | 15.7 | 56.6 | 99.8 | 18.2 | 73.4 | 100.0 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| Cauchy RT | 6.6 | 23.8 | 64.2 | 5.9 | 10.3 | 21.0 | 8.1 | 12.8 | 32.9 | |
| Cauchy VC | 11.0 | 49.9 | 85.9 | 7.8 | 12.9 | 33.0 | 7.4 | 13.0 | 34.5 | |
| 7.7 | 22.1 | 77.7 | 6.1 | 11.8 | 22.2 | 7.5 | 10.4 | 28.4 | ||
| 8.0 | 23.2 | 83.6 | 6.2 | 10.9 | 22.6 | 7.7 | 11.0 | 28.9 | ||
| 7.9 | 22.8 | 84.0 | 6.5 | 11.8 | 22.9 | 7.8 | 10.6 | 30.8 | ||
| 8.2 | 22.3 | 64.8 | 7.2 | 7.6 | 15.0 | 8.3 | 9.3 | 20.2 | ||
| Cauchy RT | 11.9 | 69.3 | 96.0 | 8.0 | 20.3 | 65.5 | 12.2 | 32.2 | 90.2 | |
| Cauchy VC | 25.7 | 84.6 | 97.8 | 10.0 | 30.2 | 84.1 | 10.8 | 28.2 | 86.6 | |
| 15.9 | 77.2 | 99.8 | 9.6 | 24.3 | 79.8 | 10.5 | 26.9 | 81.9 | ||
| 15.2 | 80.5 | 99.9 | 9.2 | 23.8 | 81.3 | 11.0 | 27.0 | 82.9 | ||
| 16.0 | 82.1 | 100.0 | 9.5 | 24.9 | 81.4 | 11.5 | 26.6 | 83.7 | ||
| 16.7 | 67.7 | 94.4 | 9.1 | 13.8 | 50.9 | 10.3 | 19.2 | 60.0 | ||
| Cauchy RT | 31.6 | 87.0 | 98.9 | 10.7 | 39.8 | 95.5 | 17.9 | 63.4 | 99.5 | |
| Cauchy VC | 47.3 | 94.5 | 98.5 | 12.1 | 58.3 | 95.4 | 14.9 | 53.5 | 98.7 | |
| 33.0 | 96.7 | 100.0 | 13.9 | 50.8 | 99.1 | 15.4 | 51.7 | 99.4 | ||
| 35.1 | 98.2 | 100.0 | 13.3 | 52.2 | 99.5 | 15.1 | 55.5 | 99.8 | ||
| 34.8 | 98.3 | 100.0 | 13.2 | 51.2 | 99.1 | 15.4 | 53.5 | 99.8 | ||
| 32.3 | 86.6 | 98.2 | 11.1 | 29.0 | 87.6 | 13.1 | 37.8 | 95.0 | ||
| Cauchy RT | 52.7 | 95.2 | 99.7 | 14.2 | 65.3 | 99.3 | 26.2 | 90.6 | 100.0 | |
| Cauchy VC | 65.1 | 97.2 | 99.0 | 17.3 | 81.7 | 97.4 | 19.3 | 78.2 | 99.6 | |
| 52.5 | 99.7 | 100.0 | 18.0 | 82.1 | 99.9 | 21.8 | 80.0 | 100.0 | ||
| 59.7 | 99.6 | 100.0 | 18.2 | 83.6 | 100.0 | 20.9 | 82.5 | 100.0 | ||
| 61.3 | 99.9 | 100.0 | 19.4 | 81.8 | 100.0 | 21.6 | 81.0 | 100.0 | ||
| 51.6 | 94.3 | 98.9 | 13.5 | 54.9 | 96.6 | 17.4 | 59.5 | 99.5 | ||
| Cauchy RT | 66.4 | 97.7 | 100.0 | 20.9 | 86.1 | 99.9 | 36.2 | 98.2 | 100.0 | |
| Cauchy VC | 76.8 | 98.0 | 99.2 | 21.5 | 91.1 | 98.7 | 25.0 | 92.1 | 99.7 | |
| 74.3 | 100.0 | 100.0 | 23.7 | 95.4 | 99.9 | 27.8 | 94.7 | 100.0 | ||
| 80.4 | 99.9 | 100.0 | 23.6 | 96.3 | 100.0 | 28.4 | 95.8 | 100.0 | ||
| 81.1 | 100.0 | 100.0 | 25.7 | 96.8 | 100.0 | 29.1 | 96.3 | 100.0 | ||
| 67.3 | 97.0 | 99.1 | 17.3 | 77.2 | 98.7 | 22.7 | 81.8 | 99.8 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| Cauchy RT | 7.8 | 19.2 | 59.9 | 6.0 | 9.6 | 24.6 | 8.5 | 14.0 | 41.8 | |
| Cauchy VC | 42.4 | 59.7 | 80.4 | 6.7 | 13.0 | 29.5 | 7.2 | 10.8 | 19.6 | |
| 11.0 | 27.1 | 67.7 | 4.4 | 7.5 | 21.4 | 5.7 | 8.4 | 18.8 | ||
| 10.3 | 28.5 | 72.0 | 5.4 | 8.8 | 21.7 | 4.9 | 8.1 | 19.2 | ||
| 10.2 | 29.2 | 73.2 | 5.5 | 9.3 | 21.2 | 5.8 | 9.2 | 19.9 | ||
| 8.8 | 25.5 | 61.8 | 5.6 | 7.5 | 13.3 | 6.2 | 7.0 | 14.7 | ||
| Cauchy RT | 13.7 | 59.1 | 92.4 | 8.2 | 24.7 | 73.4 | 13.2 | 40.9 | 92.8 | |
| Cauchy VC | 55.3 | 78.9 | 91.2 | 9.4 | 28.0 | 53.6 | 8.8 | 20.5 | 51.2 | |
| 21.6 | 65.5 | 94.6 | 7.0 | 18.8 | 68.3 | 8.4 | 16.3 | 57.5 | ||
| 23.9 | 69.7 | 95.5 | 7.4 | 21.2 | 70.9 | 7.0 | 18.6 | 63.2 | ||
| 22.4 | 71.3 | 96.5 | 7.9 | 21.1 | 72.2 | 8.2 | 19.3 | 64.7 | ||
| 21.2 | 59.2 | 88.0 | 6.7 | 12.0 | 35.0 | 8.0 | 14.5 | 40.5 | ||
| Cauchy RT | 29.0 | 80.8 | 97.9 | 11.2 | 48.3 | 95.0 | 18.7 | 72.2 | 99.2 | |
| Cauchy VC | 65.6 | 88.3 | 92.2 | 13.8 | 42.9 | 65.6 | 12.1 | 36.1 | 69.9 | |
| 38.9 | 85.5 | 97.3 | 10.7 | 43.0 | 89.6 | 10.4 | 35.2 | 81.2 | ||
| 40.2 | 88.4 | 98.5 | 11.0 | 43.6 | 93.0 | 10.5 | 39.8 | 88.0 | ||
| 42.0 | 89.0 | 99.1 | 10.0 | 44.4 | 92.8 | 11.1 | 39.6 | 91.5 | ||
| 38.9 | 77.0 | 92.1 | 8.3 | 22.3 | 64.9 | 10.8 | 26.3 | 69.7 | ||
| Cauchy RT | 47.9 | 91.7 | 99.1 | 16.0 | 70.5 | 98.5 | 27.8 | 91.5 | 99.7 | |
| Cauchy VC | 73.4 | 91.6 | 93.4 | 17.6 | 56.2 | 70.4 | 15.0 | 52.5 | 77.3 | |
| 53.4 | 91.8 | 98.4 | 14.6 | 68.5 | 94.4 | 13.9 | 57.8 | 83.9 | ||
| 57.4 | 94.9 | 99.3 | 14.9 | 71.3 | 97.1 | 15.1 | 63.9 | 93.9 | ||
| 58.9 | 95.2 | 99.6 | 14.4 | 72.6 | 98.2 | 15.9 | 62.9 | 96.6 | ||
| 51.2 | 86.3 | 93.5 | 10.3 | 37.4 | 74.8 | 13.1 | 42.1 | 82.3 | ||
| Cauchy RT | 60.1 | 96.6 | 99.5 | 23.7 | 87.6 | 98.9 | 40.3 | 97.9 | 99.9 | |
| Cauchy VC | 79.7 | 92.6 | 93.9 | 22.3 | 64.9 | 72.9 | 19.4 | 63.6 | 80.7 | |
| 67.5 | 94.8 | 99.0 | 20.2 | 81.4 | 96.0 | 18.3 | 71.0 | 85.5 | ||
| 69.9 | 97.2 | 99.7 | 19.7 | 85.7 | 98.6 | 20.0 | 80.7 | 95.0 | ||
| 72.9 | 97.2 | 99.9 | 20.2 | 87.5 | 99.1 | 21.2 | 79.9 | 98.2 | ||
| 59.9 | 90.7 | 94.4 | 12.8 | 54.6 | 80.2 | 16.8 | 57.5 | 89.5 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| 5.0 | 4.9 | 4.8 | 5.2 | 5.3 | 4.8 | 4.5 | 4.7 | 5.2 | ||
| 4.8 | 4.7 | 5.2 | 5.0 | 4.8 | 4.8 | 5.1 | 5.0 | 4.9 | ||
| 4.7 | 4.5 | 4.8 | 3.4 | 4.1 | 4.5 | 3.7 | 5.0 | 4.6 | ||
| 5.1 | 4.7 | 4.5 | 3.1 | 3.8 | 4.4 | 2.8 | 4.0 | 4.2 | ||
| 4.9 | 4.9 | 5.1 | 2.8 | 4.0 | 4.2 | 2.5 | 3.5 | 3.9 | ||
| IVX | 14.2 | 13.5 | 12.3 | 10.2 | 10.5 | 10.3 | 9.5 | 10.3 | 10.5 | |
| 8.4 | 7.9 | 8.4 | 17.0 | 17.5 | 16.6 | 24.9 | 24.9 | 25.8 | ||
| 8.2 | 8.0 | 8.4 | 17.4 | 16.0 | 17.1 | 25.4 | 25.7 | 26.0 | ||
| 15.4 | 15.6 | 15.9 | 12.7 | 14.2 | 15.4 | 18.7 | 20.5 | 21.9 | ||
| 20.3 | 20.9 | 20.2 | 13.8 | 15.1 | 15.8 | 18.2 | 21.4 | 21.6 | ||
| 24.8 | 24.9 | 26.0 | 15.0 | 16.7 | 18.4 | 18.3 | 20.9 | 22.3 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 13.4 | 13.4 | 13.9 | 39.3 | 41.1 | 39.7 | 60.5 | 61.1 | 62.0 | ||
| 13.7 | 13.5 | 13.7 | 39.6 | 39.1 | 39.6 | 62.0 | 62.8 | 62.5 | ||
| 33.8 | 33.5 | 33.6 | 29.7 | 32.4 | 33.0 | 45.1 | 48.9 | 50.7 | ||
| 47.1 | 47.4 | 47.1 | 35.0 | 36.4 | 37.0 | 47.6 | 51.9 | 52.6 | ||
| 58.8 | 58.6 | 59.4 | 42.2 | 42.0 | 43.0 | 52.3 | 53.8 | 56.0 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 20.2 | 20.1 | 20.8 | 63.3 | 65.2 | 65.0 | 85.9 | 87.6 | 87.7 | ||
| 20.7 | 20.4 | 20.6 | 63.3 | 64.0 | 64.3 | 86.5 | 87.8 | 87.8 | ||
| 52.8 | 52.0 | 52.0 | 49.0 | 52.4 | 53.0 | 68.4 | 74.0 | 75.4 | ||
| 71.3 | 70.8 | 70.7 | 58.8 | 59.8 | 59.6 | 73.6 | 77.4 | 79.2 | ||
| 83.7 | 83.2 | 83.4 | 69.8 | 68.5 | 68.5 | 80.0 | 80.8 | 82.7 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 27.7 | 27.3 | 28.3 | 80.3 | 82.1 | 82.9 | 95.4 | 96.6 | 97.2 | ||
| 28.0 | 27.3 | 28.4 | 80.1 | 81.4 | 82.6 | 95.9 | 97.0 | 97.1 | ||
| 68.1 | 67.0 | 66.7 | 65.5 | 68.5 | 69.1 | 83.7 | 88.5 | 89.4 | ||
| 86.3 | 85.0 | 85.1 | 76.8 | 77.4 | 77.7 | 88.8 | 91.1 | 92.6 | ||
| 94.3 | 93.9 | 94.1 | 86.2 | 85.3 | 85.6 | 93.2 | 94.0 | 94.8 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 34.3 | 33.7 | 34.7 | 89.5 | 91.7 | 92.1 | 98.6 | 99.2 | 99.3 | ||
| 34.8 | 33.8 | 35.4 | 89.1 | 91.2 | 92.0 | 98.7 | 99.1 | 99.3 | ||
| 78.5 | 77.8 | 77.4 | 76.7 | 79.5 | 80.5 | 90.9 | 94.7 | 95.7 | ||
| 93.5 | 92.9 | 92.9 | 87.7 | 87.8 | 88.7 | 95.4 | 96.6 | 97.5 | ||
| 98.2 | 98.1 | 98.0 | 94.8 | 93.9 | 94.2 | 97.7 | 98.1 | 98.6 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| 5.0 | 4.9 | 4.8 | 5.2 | 5.3 | 4.8 | 4.5 | 4.7 | 5.2 | ||
| 4.8 | 4.7 | 5.2 | 5.0 | 4.8 | 4.8 | 5.1 | 5.0 | 4.9 | ||
| 4.7 | 4.5 | 4.8 | 3.4 | 4.1 | 4.5 | 3.7 | 5.0 | 4.6 | ||
| 5.1 | 4.7 | 4.5 | 3.1 | 3.8 | 4.4 | 2.8 | 4.0 | 4.2 | ||
| 4.9 | 4.9 | 5.1 | 2.8 | 4.0 | 4.2 | 2.5 | 3.5 | 3.9 | ||
| IVX | 14.2 | 13.5 | 12.3 | 10.2 | 10.5 | 10.3 | 9.5 | 10.3 | 10.5 | |
| 7.1 | 6.7 | 7.0 | 12.3 | 12.5 | 12.2 | 16.5 | 16.6 | 17.2 | ||
| 7.0 | 7.0 | 7.2 | 12.4 | 11.5 | 12.3 | 16.8 | 17.2 | 17.5 | ||
| 11.5 | 11.2 | 12.0 | 9.1 | 10.6 | 11.4 | 13.0 | 14.1 | 15.0 | ||
| 14.4 | 14.4 | 14.1 | 9.6 | 10.9 | 11.7 | 11.6 | 14.1 | 14.5 | ||
| 16.7 | 16.5 | 17.6 | 10.1 | 11.7 | 13.0 | 11.3 | 13.6 | 14.6 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 10.1 | 9.8 | 10.3 | 25.4 | 25.8 | 24.6 | 39.4 | 39.4 | 40.5 | ||
| 10.3 | 10.0 | 10.4 | 25.4 | 24.2 | 25.4 | 40.3 | 40.3 | 40.8 | ||
| 22.5 | 22.6 | 23.0 | 18.7 | 21.1 | 22.0 | 28.7 | 31.9 | 33.6 | ||
| 31.2 | 31.4 | 30.4 | 21.5 | 23.2 | 24.1 | 29.0 | 33.4 | 33.6 | ||
| 38.7 | 38.8 | 39.2 | 25.1 | 26.4 | 28.0 | 31.0 | 33.4 | 34.9 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 14.2 | 14.2 | 14.7 | 42.4 | 44.1 | 43.1 | 64.4 | 65.1 | 66.4 | ||
| 14.4 | 14.4 | 14.4 | 42.8 | 42.5 | 42.8 | 65.9 | 66.9 | 66.5 | ||
| 36.4 | 36.0 | 36.0 | 32.2 | 34.7 | 35.2 | 48.3 | 52.5 | 54.5 | ||
| 50.3 | 50.5 | 50.3 | 37.8 | 39.1 | 40.0 | 51.2 | 55.2 | 56.5 | ||
| 62.5 | 62.3 | 63.3 | 46.0 | 45.4 | 46.1 | 56.9 | 57.9 | 60.0 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 19.0 | 18.9 | 19.6 | 59.7 | 61.4 | 61.6 | 82.9 | 84.3 | 84.6 | ||
| 19.4 | 19.1 | 19.1 | 59.5 | 60.2 | 60.6 | 83.4 | 84.7 | 84.8 | ||
| 49.9 | 49.0 | 49.3 | 45.7 | 48.9 | 49.7 | 65.1 | 70.4 | 72.2 | ||
| 67.9 | 67.5 | 67.1 | 55.1 | 56.2 | 55.9 | 69.9 | 73.8 | 75.9 | ||
| 80.4 | 80.3 | 80.5 | 65.8 | 64.5 | 64.3 | 76.5 | 77.2 | 79.4 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 24.1 | 24.1 | 24.9 | 73.1 | 75.7 | 75.7 | 92.5 | 93.8 | 94.4 | ||
| 24.5 | 24.0 | 24.6 | 73.7 | 74.3 | 75.5 | 92.8 | 94.1 | 94.1 | ||
| 61.6 | 60.4 | 60.7 | 58.5 | 61.6 | 62.0 | 77.8 | 83.2 | 84.4 | ||
| 80.4 | 79.2 | 79.4 | 69.3 | 70.5 | 70.1 | 83.1 | 86.1 | 87.8 | ||
| 90.6 | 90.1 | 90.3 | 79.9 | 78.8 | 79.0 | 88.7 | 89.3 | 90.8 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| 4.5 | 4.9 | 5.1 | 5.3 | 5.2 | 4.8 | 5.1 | 4.7 | 4.7 | ||
| 4.8 | 4.8 | 4.9 | 5.0 | 4.5 | 4.8 | 5.0 | 4.9 | 4.9 | ||
| 3.7 | 3.7 | 3.7 | 2.4 | 3.4 | 3.5 | 2.8 | 3.5 | 3.4 | ||
| 4.5 | 4.3 | 3.8 | 3.0 | 3.6 | 3.8 | 2.7 | 3.3 | 3.6 | ||
| 4.3 | 4.8 | 4.6 | 2.7 | 3.4 | 4.1 | 2.5 | 3.1 | 3.7 | ||
| IVX | 30.7 | 31.2 | 30.1 | 32.8 | 34.2 | 34.3 | 33.9 | 34.5 | 36.0 | |
| 7.7 | 8.1 | 8.7 | 13.7 | 13.7 | 13.2 | 18.5 | 18.7 | 19.3 | ||
| 8.0 | 8.2 | 8.4 | 13.6 | 13.0 | 12.8 | 19.3 | 19.7 | 19.7 | ||
| 11.5 | 11.6 | 11.7 | 9.2 | 10.5 | 10.8 | 12.9 | 15.1 | 15.6 | ||
| 16.0 | 16.0 | 16.0 | 10.8 | 12.1 | 12.4 | 13.3 | 16.1 | 16.6 | ||
| 19.6 | 20.2 | 20.5 | 12.2 | 13.8 | 14.6 | 14.6 | 16.1 | 17.6 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 13.5 | 13.5 | 14.3 | 29.2 | 29.3 | 28.5 | 44.3 | 45.0 | 46.0 | ||
| 14.0 | 13.8 | 13.9 | 28.2 | 28.1 | 27.3 | 46.1 | 46.6 | 46.1 | ||
| 24.8 | 24.7 | 24.8 | 21.1 | 23.1 | 23.4 | 32.0 | 35.3 | 36.3 | ||
| 36.3 | 36.6 | 36.4 | 27.3 | 28.2 | 28.0 | 35.0 | 39.3 | 39.6 | ||
| 44.8 | 45.7 | 46.1 | 31.5 | 33.6 | 34.2 | 39.5 | 41.0 | 43.0 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 20.4 | 20.4 | 21.5 | 46.8 | 48.1 | 47.6 | 68.9 | 70.8 | 72.2 | ||
| 21.3 | 20.8 | 21.0 | 46.9 | 46.3 | 47.5 | 71.2 | 71.5 | 72.5 | ||
| 39.4 | 38.6 | 39.3 | 35.6 | 37.6 | 38.0 | 51.1 | 56.1 | 57.6 | ||
| 56.3 | 57.2 | 56.4 | 45.7 | 46.4 | 45.9 | 57.6 | 62.3 | 63.3 | ||
| 67.5 | 69.2 | 69.3 | 53.9 | 55.3 | 55.1 | 63.8 | 65.9 | 67.7 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 27.6 | 27.7 | 28.9 | 62.7 | 64.9 | 64.7 | 84.4 | 87.3 | 88.0 | ||
| 28.3 | 28.0 | 28.0 | 63.2 | 63.2 | 64.5 | 85.7 | 87.3 | 87.9 | ||
| 51.7 | 51.4 | 51.6 | 48.5 | 50.0 | 51.8 | 65.0 | 71.3 | 72.7 | ||
| 71.7 | 72.5 | 71.4 | 62.0 | 62.2 | 62.1 | 74.0 | 77.3 | 79.1 | ||
| 82.3 | 83.1 | 83.1 | 71.6 | 72.3 | 72.0 | 81.1 | 82.0 | 83.9 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 34.4 | 34.3 | 35.5 | 74.0 | 76.9 | 77.2 | 92.1 | 94.6 | 95.4 | ||
| 35.1 | 34.6 | 34.3 | 74.7 | 75.9 | 77.3 | 93.0 | 94.6 | 95.3 | ||
| q=4 | 23.8 | 23.3 | 23.4 | 34.1 | 37.0 | 38.4 | 46.8 | 50.2 | 51.4 | |
| 61.7 | 61.1 | 61.2 | 58.6 | 60.4 | 62.5 | 74.1 | 80.5 | 82.4 | ||
| 81.7 | 82.3 | 81.3 | 73.6 | 73.7 | 74.4 | 84.0 | 86.5 | 88.2 | ||
| 90.6 | 91.1 | 91.4 | 83.2 | 83.1 | 83.0 | 90.1 | 91.1 | 91.9 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| 4.6 | 4.9 | 5.0 | 4.5 | 5.0 | 5.0 | 4.3 | 5.0 | 5.0 | ||
| 4.8 | 4.7 | 4.8 | 4.6 | 4.6 | 4.8 | 4.9 | 5.1 | 5.4 | ||
| q=4 | 3.3 | 3.1 | 3.0 | 2.2 | 2.5 | 3.0 | 2.4 | 2.7 | 3.0 | |
| 3.4 | 3.7 | 3.5 | 2.2 | 2.7 | 3.0 | 2.0 | 2.7 | 3.2 | ||
| 4.2 | 4.1 | 3.7 | 2.2 | 2.8 | 3.2 | 1.8 | 2.6 | 3.0 | ||
| 4.3 | 4.4 | 4.4 | 2.5 | 2.7 | 3.3 | 1.6 | 2.3 | 3.0 | ||
| IVX | 32.7 | 32.5 | 31.2 | 32.1 | 33.5 | 34.3 | 32.4 | 34.1 | 35.5 | |
| 9.8 | 10.1 | 11.0 | 7.9 | 8.9 | 9.0 | 9.0 | 11.4 | 12.0 | ||
| 10.2 | 10.2 | 10.4 | 8.0 | 8.3 | 8.8 | 10.4 | 11.8 | 12.4 | ||
| 15.1 | 15.3 | 15.5 | 5.5 | 6.7 | 7.5 | 5.8 | 8.1 | 9.0 | ||
| 21.6 | 22.5 | 22.1 | 7.1 | 9.0 | 9.3 | 6.1 | 8.9 | 9.9 | ||
| 26.3 | 28.5 | 28.5 | 9.0 | 10.3 | 12.1 | 6.6 | 9.6 | 10.7 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 18.9 | 19.2 | 20.8 | 13.9 | 16.0 | 15.7 | 18.1 | 23.3 | 25.3 | ||
| 19.3 | 20.0 | 20.0 | 14.0 | 14.7 | 15.8 | 20.0 | 24.4 | 26.2 | ||
| 34.2 | 34.0 | 34.5 | 11.9 | 14.1 | 15.1 | 13.3 | 18.6 | 19.8 | ||
| 48.3 | 50.4 | 50.2 | 18.2 | 20.2 | 21.7 | 16.1 | 21.9 | 23.3 | ||
| 59.6 | 61.5 | 62.9 | 22.8 | 26.5 | 28.3 | 18.2 | 24.6 | 26.9 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 29.3 | 29.7 | 30.9 | 22.0 | 25.4 | 25.5 | 31.0 | 39.5 | 43.1 | ||
| 29.4 | 29.7 | 30.0 | 21.9 | 24.4 | 25.8 | 33.9 | 40.5 | 43.3 | ||
| 50.5 | 50.7 | 50.9 | 21.5 | 24.0 | 25.3 | 24.4 | 31.9 | 33.6 | ||
| 69.6 | 71.3 | 71.2 | 33.1 | 35.7 | 37.3 | 30.8 | 38.3 | 40.3 | ||
| 81.2 | 83.1 | 83.2 | 41.5 | 46.4 | 47.7 | 36.5 | 44.0 | 47.4 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 38.4 | 38.1 | 39.8 | 31.5 | 36.6 | 36.8 | 45.5 | 56.6 | 60.8 | ||
| 38.5 | 38.7 | 38.8 | 31.8 | 35.8 | 37.5 | 49.0 | 58.0 | 60.4 | ||
| 63.2 | 63.5 | 63.0 | 31.9 | 34.5 | 36.6 | 36.1 | 45.2 | 47.7 | ||
| 82.2 | 83.8 | 83.8 | 49.2 | 51.5 | 52.3 | 46.4 | 54.8 | 56.8 | ||
| 91.6 | 92.8 | 92.7 | 60.3 | 64.8 | 65.0 | 54.6 | 62.8 | 65.2 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 45.9 | 45.2 | 46.9 | 41.8 | 47.2 | 48.6 | 59.1 | 70.3 | 74.9 | ||
| 45.5 | 45.7 | 46.3 | 41.9 | 47.2 | 48.4 | 62.4 | 71.8 | 74.2 | ||
| 71.5 | 71.7 | 70.9 | 41.7 | 45.3 | 46.4 | 46.5 | 56.7 | 59.3 | ||
| 89.5 | 90.3 | 90.3 | 62.4 | 64.9 | 64.7 | 60.0 | 68.1 | 69.8 | ||
| 96.2 | 96.8 | 96.6 | 74.1 | 77.3 | 78.2 | 70.3 | 76.7 | 78.3 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| 3.2 | 4.0 | 4.3 | 4.7 | 5.0 | 5.0 | 5.2 | 4.9 | 4.8 | ||
| 3.8 | 3.9 | 4.2 | 5.0 | 4.9 | 4.9 | 4.9 | 5.1 | 4.8 | ||
| 2.6 | 2.9 | 3.5 | 2.7 | 2.5 | 3.0 | 3.4 | 3.4 | 4.0 | ||
| 2.4 | 2.3 | 3.2 | 1.8 | 2.0 | 2.5 | 2.3 | 2.6 | 3.2 | ||
| 2.3 | 2.1 | 2.8 | 1.7 | 1.8 | 2.3 | 2.1 | 2.0 | 2.4 | ||
| IVX | 13.5 | 13.2 | 12.4 | 9.7 | 11.1 | 11.0 | 10.3 | 11.2 | 12.4 | |
| 5.8 | 6.8 | 7.4 | 16.1 | 17.1 | 16.6 | 23.4 | 24.1 | 24.4 | ||
| 6.4 | 6.7 | 6.9 | 16.8 | 16.8 | 16.6 | 23.9 | 24.9 | 23.6 | ||
| 10.2 | 10.8 | 12.1 | 11.0 | 11.5 | 12.3 | 16.2 | 17.4 | 19.0 | ||
| 13.3 | 13.8 | 15.0 | 11.5 | 10.7 | 11.8 | 17.0 | 17.1 | 18.3 | ||
| 16.1 | 16.0 | 18.8 | 12.9 | 12.0 | 12.9 | 17.7 | 15.4 | 18.0 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 10.4 | 11.3 | 12.0 | 39.6 | 40.9 | 39.3 | 56.7 | 60.4 | 60.6 | ||
| 11.0 | 11.7 | 11.8 | 39.7 | 39.6 | 39.8 | 57.3 | 60.9 | 59.6 | ||
| 26.2 | 27.6 | 28.7 | 28.6 | 30.4 | 30.1 | 41.8 | 45.1 | 47.2 | ||
| 37.5 | 39.1 | 40.4 | 34.4 | 33.2 | 33.4 | 46.9 | 47.9 | 48.9 | ||
| 47.6 | 49.3 | 50.2 | 41.3 | 38.2 | 38.3 | 53.6 | 49.7 | 50.2 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 17.3 | 18.2 | 19.0 | 64.3 | 67.1 | 63.9 | 81.5 | 86.0 | 87.0 | ||
| 17.9 | 18.6 | 18.7 | 63.8 | 65.3 | 64.4 | 82.2 | 86.8 | 86.2 | ||
| 45.8 | 46.7 | 47.9 | 49.7 | 51.9 | 50.3 | 65.8 | 70.3 | 72.3 | ||
| 63.9 | 65.3 | 66.4 | 60.3 | 58.8 | 58.2 | 74.7 | 75.4 | 76.2 | ||
| 76.6 | 77.6 | 78.2 | 70.9 | 67.5 | 65.9 | 82.3 | 80.3 | 78.4 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 25.0 | 25.4 | 26.5 | 80.2 | 83.1 | 81.7 | 92.0 | 95.5 | 96.4 | ||
| 25.6 | 26.2 | 25.9 | 79.4 | 82.0 | 82.1 | 92.5 | 95.5 | 96.1 | ||
| 62.3 | 62.9 | 63.6 | 66.5 | 68.0 | 67.3 | 80.4 | 84.3 | 86.0 | ||
| 81.2 | 82.7 | 83.6 | 79.0 | 78.2 | 76.9 | 89.0 | 90.0 | 90.2 | ||
| 91.1 | 91.9 | 92.4 | 88.4 | 85.9 | 84.8 | 94.6 | 93.5 | 93.3 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 32.6 | 32.5 | 33.9 | 88.9 | 91.4 | 90.7 | 96.5 | 98.6 | 99.1 | ||
| 33.4 | 33.6 | 32.7 | 88.2 | 90.9 | 90.6 | 96.8 | 98.6 | 98.9 | ||
| 73.9 | 75.2 | 75.6 | 78.3 | 79.5 | 78.8 | 88.7 | 91.8 | 92.9 | ||
| 91.0 | 91.9 | 92.2 | 89.7 | 89.0 | 88.2 | 95.7 | 95.9 | 96.1 | ||
| 96.9 | 97.1 | 97.4 | 95.7 | 94.6 | 94.2 | 98.3 | 98.0 | 97.9 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| T | 20 | 50 | 100 | 20 | 50 | 100 | 20 | 50 | 100 | |
|---|---|---|---|---|---|---|---|---|---|---|
| 3.8 | 3.9 | 4.3 | 4.3 | 5.1 | 5.3 | 4.6 | 4.8 | 4.9 | ||
| 4.0 | 4.2 | 4.3 | 4.8 | 4.9 | 4.7 | 4.9 | 4.8 | 4.5 | ||
| 3.5 | 3.5 | 3.5 | 2.3 | 2.3 | 2.5 | 2.3 | 2.4 | 3.2 | ||
| 3.1 | 3.4 | 3.2 | 1.8 | 2.0 | 2.6 | 2.0 | 1.9 | 2.6 | ||
| 3.5 | 3.2 | 3.4 | 1.6 | 1.8 | 2.1 | 1.6 | 1.8 | 2.0 | ||
| IVX | 15.6 | 13.6 | 13.4 | 9.8 | 10.5 | 11.1 | 9.6 | 11.2 | 12.4 | |
| 7.8 | 7.9 | 8.8 | 10.2 | 11.6 | 11.5 | 12.4 | 14.5 | 15.8 | ||
| 8.5 | 8.7 | 8.7 | 10.5 | 11.4 | 11.9 | 12.7 | 14.4 | 14.9 | ||
| 16.0 | 15.5 | 17.4 | 8.2 | 7.9 | 8.0 | 8.8 | 9.8 | 11.2 | ||
| 20.7 | 21.9 | 23.2 | 9.3 | 8.9 | 9.2 | 9.7 | 10.4 | 11.3 | ||
| 26.5 | 26.4 | 28.5 | 10.8 | 9.8 | 11.1 | 10.5 | 10.1 | 10.7 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 15.3 | 15.4 | 17.0 | 22.7 | 24.9 | 24.1 | 27.2 | 34.7 | 36.4 | ||
| 16.2 | 16.8 | 16.9 | 22.9 | 24.8 | 24.0 | 28.5 | 34.4 | 35.7 | ||
| 37.7 | 38.7 | 40.8 | 21.0 | 20.2 | 19.5 | 22.9 | 25.2 | 28.1 | ||
| 53.2 | 53.8 | 56.8 | 26.3 | 25.9 | 24.4 | 28.0 | 29.0 | 29.3 | ||
| 65.2 | 66.9 | 69.0 | 34.5 | 31.2 | 32.7 | 33.5 | 31.7 | 32.3 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 24.1 | 24.2 | 26.8 | 39.3 | 42.3 | 41.9 | 47.5 | 58.9 | 61.5 | ||
| 25.4 | 25.9 | 26.3 | 38.3 | 41.5 | 41.5 | 47.6 | 59.1 | 60.6 | ||
| 59.4 | 59.6 | 61.8 | 37.5 | 36.3 | 34.6 | 42.3 | 44.5 | 47.6 | ||
| 78.1 | 78.7 | 81.1 | 49.4 | 48.4 | 45.9 | 52.1 | 53.0 | 53.6 | ||
| 88.3 | 89.8 | 90.7 | 62.2 | 58.6 | 58.4 | 62.0 | 59.9 | 59.4 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 32.9 | 33.4 | 35.4 | 55.1 | 59.0 | 59.1 | 64.5 | 77.4 | 80.3 | ||
| 34.3 | 35.1 | 35.0 | 53.1 | 58.4 | 58.5 | 64.7 | 77.3 | 79.3 | ||
| 74.1 | 74.3 | 76.0 | 53.0 | 52.1 | 50.5 | 59.3 | 62.3 | 65.2 | ||
| 90.3 | 91.1 | 92.0 | 69.3 | 68.4 | 65.7 | 72.1 | 72.5 | 73.0 | ||
| 96.6 | 97.2 | 97.6 | 82.1 | 80.2 | 78.8 | 82.8 | 80.5 | 79.4 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | |
| 40.9 | 41.0 | 42.4 | 67.3 | 71.8 | 72.4 | 76.2 | 87.7 | 91.3 | ||
| 42.3 | 42.4 | 42.3 | 64.5 | 71.5 | 71.8 | 76.0 | 87.6 | 89.9 | ||
| 83.8 | 83.7 | 84.8 | 66.6 | 65.6 | 63.7 | 71.9 | 75.0 | 77.3 | ||
| 95.8 | 96.1 | 96.7 | 82.8 | 81.9 | 79.9 | 85.1 | 85.1 | 85.0 | ||
| 99.0 | 99.3 | 99.4 | 92.7 | 91.5 | 90.0 | 93.0 | 91.2 | 90.8 | ||
| IVX | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| Series | Frequency | Cauchy RT | Cauchy VC | ||||
| Panel A: D/P as predictor for the period of 1927-2011 | |||||||
| CRSP | Monthly | 0.005 | 1.06 | 0.85 | 0.42 | 0.44 | 0.43 |
| Quarterly | 0.007 | 0.48 | 1.26 | 1.29 | 1.34 | 1.09 | |
| Yearly | 0.063 | 0.99 | |||||
| S&P500 | Monthly | 0.003 | 0.74 | 1.14 | 0.89 | 0.82 | 0.72 |
| Quarterly | 0.008 | 0.65 | |||||
| Yearly | 0.042 | 0.82 | |||||
| Panel B: D/P as predictor for the period of 1927-2011 with jumps removed | |||||||
| CRSP | Monthly | 0.001 | 0.22 | 1.15 | 1.06 | 0.90 | |
| Quarterly | 0.015 | 0.93 | |||||
| Yearly | 0.005 | 0.05 | 0.56 | 0.56 | 0.53 | 0.45 | |
| S&P500 | Monthly | 0.002 | 0.48 | 1.35 | 1.14 | 1.02 | |
| Quarterly | 0.017 | 1.21 | 1.70 | 1.61 | 1.20 | ||
| Yearly | 0.008 | 0.11 | |||||
| Panel C: E/P as predictor for the period of 1950-2011 | |||||||
| S&P500 | Monthly | 0.000 | -0.05 | 0.32 | 0.26 | 0.22 | 0.30 |
| Quarterly | 0.007 | 0.39 | 0.39 | 0.38 | 0.35 | 0.35 | |
| Yearly | 0.059 | 0.83 | 0.76 | 1.04 | 0.23 | 0.84 | |
| Panel D: E/P as predictor for the period of 1950-2011 with jumps removed | |||||||
| S&P500 | Monthly | 0.001 | 0.16 | 0.01 | 0.14 | 0.18 | 0.11 |
| Quarterly | 0.000 | 0.02 | 0.71 | 0.78 | 0.61 | 0.48 | |
| Yearly | -0.038 | -0.38 | 0.69 | -0.05 | 0.55 | 0.44 |
Test results for return predictability of the NYSE/AMEX value-weighted index (CRSP) and S&P 500 using the Cauchy RT, Cauchy VC, (), and tests across different regression frequencies. Panels A–B use the dividend–price ratio (D/P), and Panels C–D use the earnings–price ratio (E/P) as predictors. Significance at the 5% and 1% levels is denoted by ‘‘∗’’ and ‘‘∗∗’’, respectively.
Dynamic Spatial Treatment Effects and Network Fragility: Theory and Evidence from the 2008 Financial Crisis
Abstract
The 2008 financial crisis exposed fundamental vulnerabilities in interconnected banking systems, yet existing frameworks fail to integrate spatial propagation with network contagion mechanisms. This paper develops a unified spatial-network framework to analyze systemic risk dynamics, revealing three critical findings that challenge conventional wisdom. First, banking consolidation paradoxically increased systemic fragility: while bank numbers declined 47.3 % from 2007 to 2023, network fragility measured by algebraic connectivity rose 315.8 %, demonstrating that interconnectedness intensity dominates institutional count. Second, financial contagion propagates globally with negligible spatial decay (boundary d* = 47,474 km), contrasting sharply with localized technology diffusion (d* = 69 km)—a scale difference of 688 times. Third, traditional difference-in-differences methods overestimate crisis impacts by 73.2 % when ignoring network structure, producing severely biased policy assessments. Using bilateral exposure data from 156 institutions across 28 countries (2007-2023) and employing spectral analysis of network Laplacian operators combined with spatial difference-in-differences identification, we document that crisis effects amplified over time rather than dissipating, increasing fragility 68.4 % above pre-crisis levels with persistent effects through 2023. The consolidation paradox exhibits near-perfect correlation (r = 0.97) between coupling strength and systemic vulnerability, validating theoretical predictions from continuous spatial dynamics. Policy simulations demonstrate network-targeted capital requirements achieve 11.3x amplification effects versus uniform regulations. These findings establish that accurate systemic risk assessment and macroprudential policy design require explicit incorporation of both spatial propagation and network topology.
Keywords: Financial networks, Systemic risk, Spatial treatment effects, Network contagion, 2008 Financial Crisis, Consolidation paradox
JEL Classification: G01, G21, G28, C31, C63, E44
Contents
- 1 Introduction
- 2 Literature Review
- 3 Theoretical Framework
- 4 Data and Empirical Methodology
- 5 Empirical Results: 2008 Financial Crisis Impact
- 6 Robustness Analysis
- 7 Policy Implications and Applications
- 8 Conclusion
- A Computational Algorithms
1 Introduction
The 2008 financial crisis stands as the defining economic event of the twenty-first century, triggering the deepest global recession since the Great Depression and fundamentally reshaping financial regulation worldwide. The crisis revealed that modern financial systems exhibit complex network properties where local perturbations—such as the collapse of Lehman Brothers on September 15, 2008—can cascade through interconnected institutions to generate system-wide instability. Understanding how shocks propagate through financial networks, how network structure evolves in response to crises, and whether consolidation enhances or undermines stability remains central to macroprudential policy design fifteen years later.
This paper develops and empirically implements a unified framework for analyzing systemic risk in financial networks that combines three methodological advances: continuous functional analysis of spatial treatment effects from Kikuchi (2024c) and Kikuchi (2024f), spectral characterization of network fragility from Kikuchi (2024i), and spatial difference-in-differences methods adapted for interconnected systems. By integrating these complementary perspectives, we provide the first comprehensive analysis of how the 2008 crisis altered financial network structure and demonstrate a consolidation paradox that challenges conventional wisdom about stability.
1.1 Motivation and Research Questions
The conventional narrative of post-crisis financial regulation emphasizes that consolidation enhanced stability by eliminating weak institutions and concentrating resources in systemically important financial institutions (SIFIs) subject to enhanced oversight. Bernanke (2010) argues that regulatory reforms including the Dodd-Frank Act successfully strengthened the system by improving capital adequacy and resolution mechanisms. Adrian & Shin (2010) documents that the crisis stemmed from excessive leverage and maturity transformation, problems addressable through higher capital requirements regardless of network structure.
We challenge this narrative by demonstrating that consolidation paradoxically increased systemic vulnerability measured through network fragility metrics. While the number of major financial institutions declined 47.3 % from 296 in 2007 to 156 in 2023, the algebraic connectivity of the global banking network—which governs shock propagation speed—increased 315.8 % over the same period. This implies that surviving institutions became more tightly coupled, with contagion spreading faster post-crisis despite fewer nodes. The finding has profound implications for macroprudential policy: interventions that reduce institution count without addressing coupling strength may inadvertently increase systemic risk.
Our analysis addresses four fundamental research questions. First, how did the 2008 financial crisis causally impact network fragility in global banking systems? Answering this requires overcoming the endogeneity problem that network structure reflects both crisis impacts and strategic institutional responses. We employ spatial difference-in-differences methods developed in Kikuchi (2024f) that treat the entire network as the unit of analysis, comparing pre-crisis (2007) and post-crisis (2009-2023) periods while accounting for spatial spillovers that violate standard parallel trends assumptions.
Second, what explains the consolidation paradox whereby fewer institutions generate higher fragility? We develop theoretical foundations showing that algebraic connectivity depends not only on node count but also on coupling strength measured by edge weights. When bilateral exposures intensify among surviving institutions, can rise even as falls. Using the spectral framework from Kikuchi (2024i), we derive closed-form expressions relating to exposure concentration and validate predictions using comprehensive bilateral data.
Third, how do spatial treatment effects in financial networks compare to technology diffusion patterns studied in Kikuchi (2024j)? That paper documents strong spatial decay in technology adoption with spatial boundary kilometers, reflecting localized demonstration effects and knowledge spillovers. We hypothesize that financial contagion exhibits fundamentally different spatial properties due to electronic payment systems and global capital markets that eliminate geographic frictions. Our estimates reveal spatial decay rate per kilometer, implying spatial boundary kilometers—effectively global propagation.
Fourth, what are the implications for macroprudential policy design? We demonstrate that network-targeted interventions achieve amplification factors 11.3 times larger than uniform policies by strategically exploiting spectral centrality. Banks with high eigenvector centrality contribute disproportionately to , making them priority targets for capital requirements or resolution planning. This finding extends results from Kikuchi (2024i) to show that continuous functional analysis provides actionable guidance for regulatory design.
1.2 Theoretical Framework
Our theoretical framework unifies three mathematical approaches that have traditionally been applied separately. From spatial economics, we adopt the continuous functional methods developed in Kikuchi (2024c), which characterize treatment effect propagation through partial differential equations analogous to fluid dynamics. The fundamental insight is that spillovers in interconnected systems satisfy diffusion equations where local perturbations spread according to network structure:
| (1) |
where represents the state variable (financial stress, default probability, liquidity shortage) at location and time , governs spatial decay through geographic distance, controls network diffusion through the graph Laplacian , and represents external forcing from policy interventions or exogenous shocks. This formulation nests both geographic and network channels, allowing us to estimate their relative importance.
From spectral graph theory, we adopt methods that characterize network properties through eigenvalue decomposition of the Laplacian matrix. As established in Fiedler (1973) and extended to financial networks in Kikuchi (2024i), the algebraic connectivity (second-smallest Laplacian eigenvalue) measures network fragility. Higher indicates tighter coupling and faster contagion propagation. The mixing time governs how quickly shocks equilibrate across the network, with pre-crisis yielding milliseconds and post-crisis yielding milliseconds—a four-fold acceleration in propagation speed.
From causal inference, we adopt spatial difference-in-differences methods adapted from Kikuchi (2024f) for settings where spatial dependence precludes unit-level treatment effect identification. The key insight is that while individual institution-level effects cannot be separately identified due to spillovers, aggregate network-level effects remain identifiable. We define the treatment effect as:
| (2) |
where the expectation is taken over entire network realizations rather than individual institutions. This formulation respects spatial structure while maintaining causal interpretation.
The integration of these three approaches yields testable predictions. From the diffusion equation (1), we predict that financial shocks exhibit minimal spatial decay () due to electronic transmission mechanisms, contrasting sharply with technology diffusion. From spectral theory, we predict that consolidation increases when exposure concentration rises faster than node count declines. From spatial difference-in-differences, we predict that ignoring network structure produces upward-biased estimates that overstate crisis impacts by attributing spillover effects to direct treatment.
1.3 Empirical Strategy and Data
We implement this framework using comprehensive bilateral exposure data from 156 major financial institutions across 28 countries spanning 2007-2023. The dataset comes from multiple sources: Bank for International Settlements (BIS) Consolidated Banking Statistics for cross-border exposures, individual bank financial statements for institution-specific balance sheet data, and Federal Reserve Bank of New York Bilateral Exposure Reports for US institution linkages. For each institution and counterparty , we observe total exposure at quarterly frequency.
From these bilateral exposures, we construct exposure-weighted networks where edge weights reflect the strength of financial connections. The adjacency matrix is where represents the geometric average normalized exposure. The degree matrix contains row sums of , and the Laplacian is . We compute the algebraic connectivity using the Lanczos algorithm for sparse symmetric matrices, as detailed in Section A.
Our identification strategy treats the 2008 financial crisis as a quasi-natural experiment that exogenously shocked the global financial system. Following Brunnermeier (2009), we date the crisis onset to September 15, 2008 (Lehman Brothers bankruptcy) and the acute phase as September 2008 through March 2009. This provides a clear treatment timing: institutions are unexposed before September 2008 and exposed afterward. The parallel trends assumption requires that network fragility would have evolved similarly in treated and control periods absent the crisis—testable using pre-2008 data.
The empirical specification is:
| (3) |
where indicates quarters after September 2008, includes time-varying controls (global GDP growth, VIX volatility index, sovereign debt levels), and standard errors are bootstrapped by resampling network realizations. The coefficient captures the causal impact of the crisis on network fragility.
1.4 Key Findings
Our empirical analysis yields four principal findings. First, the 2008 crisis caused a large, statistically significant, and persistent increase in network fragility. Algebraic connectivity rose from in 2007Q2 to in 2023Q4, representing a 315.8 % increase. The difference-in-differences estimate of the treatment effect is (95 % CI: [+733, +1,618], p 0.001), indicating that the crisis elevated fragility 68.4 % above counterfactual trends. Effects persist through 2023 with no evidence of reversion, demonstrating structural hysteresis consistent with Kikuchi (2024f).
Second, we document a consolidation paradox whereby network fragility increased despite substantial reduction in institution count. The number of major banks fell from 296 in 2007 to 156 in 2023 ( 47.3 %), yet rose 315.8 %. Decomposition analysis reveals that average bilateral exposure concentration increased 687 % over this period, with the exposure Herfindahl index rising from 0.043 to 0.339. Surviving institutions became dramatically more interconnected, with the average bank maintaining 47.3 counterparty relationships in 2023 versus 12.6 in 2007. The correlation between exposure concentration and is r = 0.97 (p 0.001), confirming that coupling strength drives fragility more than node count.
Third, spatial decay analysis reveals fundamentally different propagation patterns for financial contagion versus technology diffusion. Estimating equation (1) jointly for both channels, we find spatial decay rate per kilometer for financial networks (95 % CI: [0.000014, 0.000026]) versus per kilometer for technology adoption. This implies spatial boundary kilometers for finance versus kilometers for technology—two thousand times larger. Financial contagion propagates essentially instantaneously across the globe due to electronic payment systems, while technology adoption remains geographically localized due to tacit knowledge requirements. The finding demonstrates that spatial treatment effect methodologies developed in Kikuchi (2024c) apply broadly but with dramatically different parameter values across economic contexts.
Fourth, traditional difference-in-differences methods that ignore network structure produce severely biased estimates. The naive DID estimator that treats institutions as independent units yields (s.e. = 287), 73.2 % larger than the correctly-specified spatial DID estimate (s.e. = 218). This bias stems from attributing spillover effects to direct treatment: when Institution A experiences direct crisis impact, connected Institution B experiences indirect impact through bilateral exposures. The naive estimator incorrectly codes both as direct treatment, double-counting spillovers. Our spatial DID approach aggregates to network level before differencing, properly accounting for within-network propagation.
1.5 Contributions
This paper makes three main contributions to the literature. First, we provide the first comprehensive empirical analysis demonstrating that post-crisis consolidation increased rather than decreased systemic fragility. This challenges the dominant narrative in policy circles that consolidation enhances stability by eliminating weak institutions. Our finding that rose 315.8 % while bank count fell 47.3 % reveals a consolidation paradox with profound regulatory implications. Policies that reduce institution count without addressing exposure concentration may inadvertently increase systemic risk.
Second, we demonstrate the empirical relevance of continuous functional methods for financial network analysis. While Kikuchi (2024c) and Kikuchi (2024f) develop theoretical foundations for spatial treatment effects using Navier-Stokes equations, our paper provides the first large-scale validation using comprehensive bilateral exposure data. The finding that governs propagation speed with mixing time as predicted by spectral theory confirms that mathematical physics provides actionable insights for financial stability analysis.
Third, we establish that spatial treatment effect methodologies apply to financial networks but with dramatically different parameters than technology diffusion. The spatial decay rate differs by a factor of 2,150 (), reflecting fundamentally distinct propagation mechanisms. This demonstrates the generality of the continuous functional framework while highlighting context-dependence of specific parameter values.
1.6 Roadmap
The remainder of the paper proceeds as follows. Section 2 reviews relevant literature on financial networks, systemic risk measurement, and spatial treatment effects. Section 3 develops the theoretical framework, deriving key results on consolidation paradox and spatial boundaries. Section 4 describes data sources, network construction, and summary statistics. Section 5 presents the main empirical results on the 2008 crisis impact, consolidation paradox, and spatial propagation. Section 6 conducts extensive robustness checks including alternative network specifications, placebo tests, and sensitivity analysis. Section 7 discusses policy implications for macroprudential regulation, capital requirements, and resolution planning. Section 8 concludes. Appendices provide computational algorithms, additional robustness results, and theoretical proofs.
2 Literature Review
This paper contributes to three distinct but related literatures: financial networks and systemic risk, spatial treatment effects and causal inference with spillovers, and network dynamics following aggregate shocks. We discuss each strand and highlight how our framework advances understanding.
2.1 Financial Networks and Systemic Risk
The study of financial networks has emerged as a central research area following the 2008 crisis, with scholars applying graph theory and network science to understand contagion dynamics. Allen & Gale (2000) provide an early theoretical framework showing that complete networks may be more resilient than incomplete networks due to loss-sharing mechanisms, though this result reverses when initial shocks are sufficiently large. Freixas et al. (2000) extend this analysis to examine optimal network architecture from a social planner’s perspective, demonstrating that decentralized formation may generate excessive interconnectedness.
Elliott et al. (2014) develop a comprehensive framework for analyzing financial networks where institutions hold cross-holdings of debt and equity. They show that network structure determines whether small shocks dissipate or cascade through the system, with denser networks exhibiting nonmonotonic stability properties. When shocks are small, connectivity facilitates risk-sharing and enhances stability. When shocks exceed critical thresholds, connectivity transmits contagion and undermines stability. This theoretical insight motivates empirical investigation of actual network structures.
Acemoglu et al. (2015) analyze how network topology affects systemic risk in input-output networks and financial systems. They establish sufficient conditions under which shocks to individual firms have negligible aggregate effects (asymptotic resilience) versus conditions under which idiosyncratic shocks generate aggregate fluctuations. The key insight is that heavy-tailed degree distributions—characteristic of real-world networks—amplify tail risks beyond what diversification arguments suggest. Our analysis complements this work by providing empirical evidence that post-crisis consolidation created more heavy-tailed exposure distributions.
The empirical literature on financial network measurement faces significant data challenges since bilateral exposures are typically proprietary and incomplete. Upper & Worms (2004) develop estimation methods for the German interbank market, showing how to construct network approximations from aggregate balance sheet data when bilateral positions are unobserved. Garratt et al. (2014) apply these methods to map global banking networks using BIS consolidated banking statistics. Our dataset improves on these approaches by incorporating actual bilateral exposures from regulatory filings rather than estimated exposures.
Gai & Kapadia (2010) examine contagion dynamics in interbank networks through simulation analysis, demonstrating that highly connected but heterogeneous network structures exhibit fragility where the system appears robust in normal times but becomes vulnerable following negative shocks. Glasserman & Young (2015) refine these results by deriving analytical approximations for contagion probabilities as functions of network moments. Our spectral approach complements these simulation-based methods by providing closed-form characterizations through eigenvalue analysis.
Recent work has applied spectral methods from graph theory to characterize financial network structure. Hautsch et al. (2015) compute systemic risk contributions using principal component analysis of return covariances, effectively employing spectral decomposition. Sommese et al. (2021) explicitly analyze the Laplacian spectrum of interbank networks, demonstrating that algebraic connectivity correlates with systemic importance measures. Our contribution extends this line by connecting to treatment effect propagation speed through continuous functional analysis from Kikuchi (2024f).
2.2 Spatial Treatment Effects and Causal Inference
The problem of estimating treatment effects in the presence of spillovers has received increasing attention as researchers recognize that standard causal inference methods break down when treated units affect control units. Manski (2013) provides a comprehensive treatment of the identification problem, showing that spillovers create fundamental challenges for recovering causal effects without strong assumptions. He develops partial identification bounds that remain valid under weak assumptions about spillover structure.
Abadie et al. (2020) examine inference in experiments with spillovers, demonstrating that randomization-based approaches remain valid even when spillovers are present, provided estimation targets are carefully defined. They propose aggregating observations into clusters where within-cluster spillovers are allowed but between-cluster spillovers are assumed absent. Our spatial DID approach can be interpreted as implementing this insight by treating the entire network as a single cluster.
Recent work has developed continuous spatial approaches that model treatment effect decay as a function of distance. Butts et al. (2017) extends the classic difference-in-differences design to account for spatial spillovers using kernel weighting functions. Cao & Liu (2021) propose augmented inverse probability weighting estimators that remain consistent under spatial interference. Our framework builds on Kikuchi (2024c), who shows that spatial decay satisfies partial differential equations analogous to Navier-Stokes fluid dynamics.
Kikuchi (2024a) provides a unified framework connecting discrete network spillovers with continuous spatial diffusion, demonstrating that both approaches are special cases of a general evolution equation. Kikuchi (2024b) extends this framework to stochastic settings where treatment effects are random functionals. Our contribution applies these theoretical developments to financial networks, demonstrating their empirical relevance.
The methodological challenge in our context is that financial networks violate standard spillover assumptions in two ways. First, spillovers occur through bilateral exposures rather than geographic proximity, requiring network-based distance metrics. Second, network structure is endogenous to the crisis, as institutions adjust exposures in response to stress. We address the first challenge using spectral graph distance metrics from Kikuchi (2024i) and the second through event study designs that compare pre- and post-crisis periods.
2.3 Financial Crises and Network Evolution
A growing empirical literature examines how financial networks evolve during and after crises. Upper (2011) surveys simulation methods for assessing contagion risk in interbank markets, documenting substantial heterogeneity in estimated impacts depending on network specification. Demirgüç-Kunt et al. (2020) analyze bank performance during the COVID-19 pandemic, finding that better-capitalized banks with more diversified funding structures exhibited greater resilience. Our analysis focuses on the 2008 financial crisis rather than COVID-19, examining structural evolution over a longer time horizon.
Battiston et al. (2012) introduce the DebtRank algorithm for measuring systemic importance in financial networks, showing that it provides superior predictions of actual defaults compared to balance sheet metrics alone. Bardoscia et al. (2015) extend this framework to distinguish between different contagion channels, demonstrating that funding contagion through liquidity spirals may be more important than solvency contagion through direct exposures. Our spectral approach complements these node-level centrality measures by characterizing system-level fragility.
Chinazzi et al. (2020) use a global metapopulation disease transmission model to assess the effect of travel restrictions on COVID-19 spread, demonstrating limited effectiveness. While their focus is epidemiological rather than financial, the methodological parallel is instructive: network structure determines propagation dynamics, and interventions that modify network topology can be more effective than those that merely reduce node-level transmission. Our policy analysis exploits this insight to design network-targeted capital requirements.
Adrian & Shin (2010) examine liquidity risk during the 2008 crisis, documenting that market illiquidity created severe funding pressures even for solvent institutions. Gorton & Metrick (2012) argues that the crisis stemmed from information insensitivity in securitized debt markets breaking down, creating bank runs in shadow banking. Our network perspective complements these accounts by showing how bilateral exposures transmitted initial shocks across the system.
Bernanke (2010) provides a comprehensive analysis of the 2008 crisis causes and policy responses, emphasizing that the crisis stemmed from failures in risk management, excessive leverage, and regulatory gaps. He argues that post-crisis reforms including Dodd-Frank successfully addressed these vulnerabilities. Our consolidation paradox finding challenges this optimistic assessment by demonstrating that network fragility actually increased post-crisis.
2.4 Our Contribution
We contribute to these literatures in several ways. First, we provide the first comprehensive empirical analysis demonstrating that post-crisis consolidation increased network fragility despite reducing institution count. This consolidation paradox has not been documented previously and challenges conventional policy wisdom. Second, we demonstrate that continuous functional methods from mathematical physics provide empirically relevant tools for financial network analysis, validating theoretical frameworks from Kikuchi (2024c) and Kikuchi (2024f). Third, we show that spatial treatment effect methodologies apply to financial networks but with dramatically different parameters than technology diffusion, establishing the generality and context-dependence of these methods.
3 Theoretical Framework
This section develops the theoretical framework that underlies our empirical analysis. We begin by characterizing financial networks through spectral graph theory, then derive the relationship between network structure and fragility, and finally establish predictions about crisis impacts and consolidation dynamics.
3.1 Financial Networks as Weighted Graphs
A financial network at time is formally represented as a weighted directed graph where denotes the set of financial institutions, denotes the set of bilateral exposures, and is the weighted adjacency matrix with representing the exposure of institution to institution .
Definition 3.1 (Financial Network).
At time , the financial network is characterized by:
-
1.
Node set: with cardinality
-
2.
Edge set: with if institution has exposure to institution
-
3.
Weight matrix: with representing exposure magnitude
For empirical implementation, we normalize exposures to obtain the adjacency matrix :
| (4) |
where is the total exposure of institution and is the total exposure to institution . This geometric mean normalization ensures that and captures the relative importance of bilateral relationships.
The degree matrix is diagonal with entries:
| (5) |
representing the total connectivity of institution . The graph Laplacian is then defined as:
| (6) |
The Laplacian is a real symmetric matrix with non-negative eigenvalues . The multiplicity of the zero eigenvalue equals the number of connected components, so with multiplicity one for connected networks.
3.2 Spectral Characterization of Network Fragility
The second-smallest eigenvalue plays a special role in network dynamics, governing the rate at which perturbations propagate through the system. This quantity, known as the algebraic connectivity or Fiedler value after Fiedler (1973), measures how well-connected the network is.
Definition 3.2 (Algebraic Connectivity).
For a connected graph with Laplacian , the algebraic connectivity is:
| (7) |
where the minimization is over vectors orthogonal to the all-ones vector .
The interpretation of as a measure of network fragility follows from its role in diffusion processes. Consider a shock that creates heterogeneity across institutions, represented by a vector where measures institution ’s state (stress level, liquidity shortage, default probability). The evolution of this state vector is governed by the diffusion equation:
| (8) |
where represents external forcing. In the absence of forcing (), the solution is:
| (9) |
where are eigenvectors of and are constants determined by initial conditions. Since and for , the decay rate is dominated by :
| (10) |
where is the steady-state uniform distribution. The mixing time—the time required for shocks to equilibrate across the network—is therefore:
| (11) |
Higher implies faster equilibration and therefore greater fragility: shocks spread more rapidly through tightly connected networks.
3.3 The Consolidation Paradox
A central question in financial regulation concerns whether consolidation enhances or undermines stability. Conventional wisdom suggests that reducing the number of institutions should decrease systemic risk by eliminating weak banks and simplifying the network structure. We demonstrate theoretically that this intuition can be misleading: consolidation may increase fragility when bilateral exposures intensify among surviving institutions.
Theorem 3.1 (Consolidation Paradox).
Consider a financial network that undergoes consolidation from institutions at time to institutions at time . Let denote the average bilateral exposure at time . Then the algebraic connectivity satisfies:
| (12) |
Therefore, (increasing fragility) if and only if:
| (13) |
That is, fragility increases when average exposure intensity grows faster than the inverse of node reduction.
Proof.
For a regular graph with nodes where each node has degree and edge weights , the Laplacian has eigenvalues for . The algebraic connectivity is therefore:
| (14) |
for large . The degree scales with average exposure: . Substituting:
| (15) |
Therefore:
| (16) |
which establishes (12). The paradox condition (13) follows immediately. ∎
This theorem formalizes the consolidation paradox: fragility can increase despite fewer nodes if surviving institutions become sufficiently more interconnected. In our empirical application, while , satisfying condition (13) with substantial margin and explaining the observed 315.8 % increase in .
3.4 Spatial Decay and Network Propagation
Financial shocks propagate through both geographic channels (regional correlations, local market disruptions) and network channels (bilateral exposures, common creditor effects). We model this dual-channel propagation using a combined diffusion equation that nests both mechanisms.
Consider a financial system distributed over geographic space with institutions located at positions . Let denote the stress level at location and time . The evolution satisfies:
| (17) |
where governs geographic diffusion through the Laplacian operator , governs network diffusion through the graph Laplacian , and represents external forcing.
The spatial decay rate determines how quickly shocks attenuate with geographic distance. For a localized initial shock at the origin, the solution in the absence of network effects () is:
| (18) |
where is the total shock magnitude. At fixed time , stress decays exponentially with distance:
| (19) |
where the spatial boundary is:
| (20) |
For financial networks, we hypothesize that due to electronic payment systems and global capital markets that eliminate geographic frictions. This contrasts with technology diffusion where reflects localized knowledge spillovers.
Prediction 3.1 (Minimal Spatial Decay in Financial Networks).
For financial contagion, the spatial decay rate satisfies where subscripts index application domains. Consequently, , implying that financial shocks propagate globally while technology shocks remain localized.
3.5 Crisis Impact Through Spatial Difference-in-Differences
We model the 2008 financial crisis as an exogenous aggregate shock that affected all institutions simultaneously but with heterogeneous intensity. Let be an indicator for the post-crisis period starting at (September 2008). The treatment effect of interest is the crisis impact on network fragility:
| (21) |
where expectations are taken over network realizations. Standard difference-in-differences assumes no spillovers, allowing unit-level estimation. In financial networks, spillovers are fundamental—institutions transmit shocks through bilateral exposures by construction. This requires aggregating to network level before computing treatment effects.
Definition 3.3 (Spatial Treatment Effect).
The spatial treatment effect of the 2008 crisis on network fragility is:
| (22) |
where and denote post- and pre-crisis periods.
The key identification assumption is parallel trends at the network level:
Assumption 3.1 (Network-Level Parallel Trends).
In the absence of the crisis, network fragility would have evolved according to:
| (23) |
for constants that are the same in pre- and post-crisis periods.
3.6 Network-Targeted Policy Design
An important application of this framework is designing macroprudential interventions that optimally exploit network structure. Consider a policy that imposes capital requirements or activity restrictions on a subset of institutions. The policy effectiveness depends on how is selected.
Let denote the Fiedler vector (eigenvector corresponding to ). This vector partitions the network into communities where indicates one community and indicates the other. Institutions with large are structurally important for network connectivity.
Proposition 3.1 (Optimal Targeting).
To maximally reduce network fragility subject to treating a fixed fraction of institutions, target institutions with largest values. The reduction in satisfies:
| (24) |
where contains the top institutions ranked by .
This result provides operational guidance for regulators: to design capital requirements that most effectively reduce systemic risk, target institutions with high Fiedler centrality rather than conventional size metrics. We validate this prediction empirically by computing counterfactual values under alternative targeting schemes.
4 Data and Empirical Methodology
This section describes our data sources, network construction procedures, variable definitions, and summary statistics.
4.1 Data Sources
Our analysis combines four primary data sources providing comprehensive coverage of global financial institutions and their bilateral exposures over 2007-2023.
4.1.1 Bank for International Settlements Consolidated Banking Statistics
The BIS Consolidated Banking Statistics provide quarterly data on cross-border exposures of major banking groups to counterparties in over 200 countries. Reporting banks submit comprehensive information on their consolidated international claims, broken down by counterparty country, sector, and maturity. These data capture the global network structure of cross-border banking relationships.
For each reporting bank in country and quarter , we observe total international claims on counterparty country :
| (25) |
While these data do not provide institution-level bilateral exposures , they allow us to construct country-level networks and to calibrate exposure distributions using aggregate constraints.
4.1.2 Individual Bank Financial Statements
We collect audited financial statements for all banks included in the BIS reporting set, obtained from S&P Capital IQ, Bankscope (Bureau van Dijk), and individual bank 10-K/20-F filings. These provide institution-specific balance sheet information including:
-
•
Total assets and equity
-
•
Breakdown of asset classes (loans, securities, derivatives)
-
•
Geographic distribution of activities
-
•
Intra-group versus external exposures
This granular balance sheet data allows us to construct institution-level control variables and to validate the network exposures against reported aggregates.
4.1.3 Federal Reserve Bank of New York Bilateral Exposure Reports
For US banking institutions, we obtain bilateral exposure data from the Federal Reserve Bank of New York Supervisory Data. These confidential regulatory filings report detailed bilateral credit exposures between US banks and their major counterparties worldwide, collected under the authority of the Bank Holding Company Act. The data include:
-
•
Bilateral loans and credit lines
-
•
Derivatives exposures (netted by counterparty)
-
•
Securities holdings issued by financial counterparties
-
•
Guarantees and off-balance-sheet commitments
These data provide the most detailed view of bilateral network structure for US institutions, which represent approximately 25 % of global systemically important banks.
4.1.4 European Banking Authority Transparency Exercise
For European banks, we utilize exposure data from the EBA Transparency Exercise, conducted biannually since 2011 and annually since 2016. The EBA requires major European banking groups to disclose detailed exposure information including:
-
•
Sovereign exposures by country and maturity
-
•
Corporate credit exposures by sector
-
•
Interbank exposures
-
•
Asset quality indicators
These data cover approximately 130 European banks representing over 70 % of European banking sector assets. The transparency exercise was initiated following the European debt crisis to enhance market discipline through disclosure.
4.2 Sample Construction
From these sources, we construct a panel dataset of 156 major financial institutions across 28 countries observed quarterly from 2007Q1 through 2023Q4 (68 quarters). The sample selection follows several criteria designed to ensure data quality and representativeness.
First, we include only Global Systemically Important Banks (G-SIBs) as designated by the Financial Stability Board, plus additional large institutions whose failure would have systemic consequences. This ensures that sample institutions account for a substantial fraction of global financial intermediation. The 156 institutions in our sample represent approximately 78 % of total global banking assets and 85 % of cross-border claims as of 2023.
Second, we require continuous data availability throughout the sample period, dropping institutions that entered or exited during 2007-2023 through mergers, failures, or restructurings. While this creates survivorship bias, it is necessary for constructing consistent networks over time. We address this limitation in robustness checks by analyzing separate cross-sections for each year.
Third, we exclude institutions headquartered in countries with capital controls or limited financial integration, as network connections for these institutions may not reflect arm’s-length bilateral exposures. Specifically, we exclude banks from China, Russia, and several Middle Eastern countries where capital flows are significantly regulated.
Table 1 provides summary statistics on sample composition.
| Country | Number of Banks | Total Assets | Share of Global | G-SIB Status |
|---|---|---|---|---|
| 2023 | (USD Trillion) | Banking Assets | (Count) | |
| United States | 23 | 18.4 | 24.7 % | 8 |
| Japan | 12 | 11.2 | 15.0 % | 3 |
| United Kingdom | 18 | 9.8 | 13.2 % | 4 |
| France | 15 | 8.6 | 11.5 % | 4 |
| Germany | 14 | 7.3 | 9.8 % | 2 |
| Switzerland | 9 | 5.9 | 7.9 % | 2 |
| Canada | 11 | 4.2 | 5.6 % | 1 |
| Netherlands | 8 | 3.7 | 5.0 % | 1 |
| Spain | 10 | 3.1 | 4.2 % | 1 |
| Italy | 9 | 2.6 | 3.5 % | 1 |
| Others | 27 | 6.8 | 9.1 % | 3 |
| Total | 156 | 81.6 | 78.0 % | 30 |
-
•
Notes: This table reports sample composition as of 2023Q4. Assets are consolidated global assets from regulatory filings. Share of global banking assets computed using BIS aggregate statistics. G-SIB status from Financial Stability Board designations as of November 2023. Others category includes Australia, Sweden, Norway, Austria, Belgium, and Denmark.
4.3 Network Construction
From bilateral exposure data, we construct weighted directed networks for each quarter following a consistent methodology. The key challenge is that bilateral exposures are observed incompletely: we observe for many but not all institution pairs. We address this using a maximum entropy imputation procedure that preserves observed exposures while minimizing information content of unobserved links.
4.3.1 Exposure Aggregation
For each institution pair and quarter , we aggregate exposures across multiple channels to compute total bilateral exposure :
| (26) |
where each component represents exposures through different channels:
-
•
: Direct loans and credit lines from to
-
•
: Holdings by of debt securities issued by
-
•
: Net derivatives exposures from to after netting
-
•
: Guarantees and standby letters of credit
This aggregation ensures comprehensive capture of financial interconnections. Derivatives exposures are particularly important post-crisis as OTC derivatives markets represent a major contagion channel.
4.3.2 Missing Data Imputation
When bilateral exposures are unobserved, we impute using maximum entropy methods subject to observ equality constraints on row sums and column sums. Let denote the set of observed exposures. The imputation problem is:
| (27) |
subject to:
| (28) | ||||
| (29) | ||||
| (30) |
where and are observed total outward and inward exposures from balance sheet data. This approach, developed by Upper & Worms (2004) and Garratt et al. (2014), produces minimally informative imputations consistent with available information.
The solution to this constrained optimization is:
| (31) |
for unobserved pairs, where is total system exposure. This formula has an intuitive interpretation: bilateral exposures are proportional to the product of marginal exposures, analogous to assuming independence.
4.3.3 Adjacency Matrix Construction
From total exposures , we construct the normalized adjacency matrix using geometric mean normalization:
| (32) |
This normalization ensures comparability across institutions of different sizes and creates a symmetric matrix suitable for spectral analysis. The degree matrix is:
| (33) |
and the Laplacian is:
| (34) |
4.4 Variable Definitions
Our main outcome variable is network fragility measured by algebraic connectivity:
| (35) |
We compute this using the Lanczos algorithm for sparse symmetric matrices, as described in Section A. The algorithm converges rapidly (typically 15-20 iterations for ) and provides high numerical accuracy.
We construct several control variables and alternative network measures:
-
•
Network density: , the fraction of possible edges that are present
-
•
Average path length: , where is the shortest path distance between and
-
•
Clustering coefficient: , where is the number of triangles including node and is node ’s degree
-
•
Exposure Herfindahl index: , measuring exposure concentration
-
•
System leverage: , average leverage ratio
We also construct institution-level variables for robustness checks:
-
•
Total assets: in billions USD, inflation-adjusted to 2023 dollars
-
•
Equity capital ratio:
-
•
Return on assets:
-
•
Non-performing loan ratio:
4.5 Summary Statistics
Table 2 reports summary statistics for key variables separately for pre-crisis (2007Q1-2008Q2) and post-crisis (2008Q4-2023Q4) periods.
| Pre-Crisis (2007Q1-2008Q2) | Post-Crisis (2008Q4-2023Q4) | |||||
| Variable | Mean | Std. Dev. | N | Mean | Std. Dev. | N |
| Panel A: Network-Level Variables | ||||||
| Algebraic connectivity | 1,719 | 187 | 6 | 5,234 | 1,456 | 61 |
| Number of banks | 296 | 12 | 6 | 178 | 38 | 61 |
| Network density | 0.143 | 0.018 | 6 | 0.287 | 0.064 | 61 |
| Average path length | 2.76 | 0.31 | 6 | 1.94 | 0.28 | 61 |
| Clustering coefficient | 0.382 | 0.042 | 6 | 0.619 | 0.087 | 61 |
| Exposure Herfindahl | 0.043 | 0.007 | 6 | 0.198 | 0.089 | 61 |
| Panel B: Institution-Level Variables | ||||||
| Total assets (USD Billion) | 487 | 623 | 1,776 | 612 | 748 | 9,516 |
| Equity capital ratio | 0.064 | 0.023 | 1,776 | 0.089 | 0.031 | 9,516 |
| Return on assets | 0.011 | 0.008 | 1,776 | 0.006 | 0.012 | 9,516 |
| NPL ratio | 0.024 | 0.018 | 1,776 | 0.047 | 0.036 | 9,516 |
| System leverage | 15.6 | 2.3 | 1,776 | 11.2 | 1.9 | 9,516 |
-
•
Notes: This table reports summary statistics for network-level and institution-level variables. Pre-crisis period is 2007Q1 through 2008Q2 (6 quarters). Post-crisis period is 2008Q4 through 2023Q4 (61 quarters). Network-level variables computed once per quarter. Institution-level variables computed for each institution-quarter observation. All monetary values in 2023 USD using GDP deflator.
Several patterns emerge from these summary statistics. First, algebraic connectivity increased dramatically from mean 1,719 pre-crisis to mean 5,234 post-crisis, representing a 204 % increase. This confirms our main finding of elevated network fragility. Second, the number of banks declined sharply from 296 to 178, a 40 % reduction consistent with widespread consolidation. Third, network density nearly doubled from 0.143 to 0.287, indicating that surviving institutions became more interconnected. Fourth, the exposure Herfindahl index rose from 0.043 to 0.198, confirming increased concentration among fewer institutions.
Institution-level statistics show expected patterns: equity capital ratios increased from 6.4 % to 8.9 % post-crisis, reflecting stricter regulatory requirements under Basel III. Return on assets declined from 1.1 % to 0.6 %, consistent with lower profitability amid heightened competition and regulation. Non-performing loan ratios doubled from 2.4 % to 4.7 %, reflecting deteriorated asset quality. System leverage fell from 15.6 to 11.2, indicating deleveraging. These patterns are consistent with post-crisis regulatory reforms emphasizing capital adequacy and risk management.
Figure 1 provides a visual representation of network structure evolution over the sample period.
5 Empirical Results: 2008 Financial Crisis Impact
This section presents our main empirical results on the causal impact of the 2008 financial crisis on network fragility. We begin with baseline difference-in-differences estimates, then examine the consolidation paradox, analyze spatial decay patterns, and finally compare with traditional methods that ignore network structure.
5.1 Baseline Treatment Effect Estimates
Our baseline specification implements the spatial difference-in-differences design described in Section 3. We estimate:
| (36) |
where indicates the post-crisis period starting with the quarter following Lehman Brothers bankruptcy, is global GDP growth, is the CBOE Volatility Index measuring market uncertainty, and is average sovereign debt-to-GDP ratio for included countries. Standard errors are computed using block bootstrap with 1,000 replications, resampling entire quarters to account for within-quarter correlation.
Table 3 reports results.
| Dependent Variable: Algebraic Connectivity (lambda-2) | ||||
| (1) | (2) | (3) | (4) | |
| Post-2008 Crisis | 3,515 | 3,287 | 2,634 | 1,176 |
| (456) | (498) | (524) | (218) | |
| [p 0.001] | [p 0.001] | [p 0.001] | [p 0.001] | |
| Global GDP Growth | 187 | 156 | 143 | |
| (94) | (89) | (76) | ||
| VIX Index | 43.6 | 38.9 | ||
| (12.3) | (11.7) | |||
| Sovereign Debt/GDP | 892 | |||
| (267) | ||||
| Observations | 67 | 67 | 67 | 67 |
| R-squared | 0.726 | 0.748 | 0.781 | 0.824 |
| Pre-crisis mean | 1,719 | 1,719 | 1,719 | 1,719 |
| Treatment effect (%) | 204.5 | 191.2 | 153.2 | 68.4 |
-
•
Notes: This table reports spatial difference-in-differences estimates of the 2008 crisis impact on network fragility as measured by algebraic connectivity. Post-2008 Crisis is an indicator for quarters after 2008Q3 (Lehman Brothers bankruptcy). Standard errors in parentheses computed using block bootstrap with 1,000 replications. P-values in square brackets. All specifications include constant term (not reported). Pre-crisis mean is average algebraic connectivity for 2007Q1-2008Q2. Treatment effect (%) computed as coefficient divided by pre-crisis mean. Column (4) is our preferred specification with full controls.
Column (1) presents the unconditional difference-in-differences estimate without controls. The post-crisis indicator coefficient is (s.e. = 456, p 0.001), indicating that algebraic connectivity increased by 3,515 units following the crisis. Relative to the pre-crisis mean of 1,719, this represents a 204.5 % increase in network fragility. The effect is highly statistically significant and economically substantial.
Column (2) adds controls for global GDP growth, which enters negatively as expected: stronger economic growth reduces financial stress and thereby lowers interconnectedness. The post-crisis coefficient moderates slightly to (s.e. = 498) but remains highly significant. Column (3) adds the VIX volatility index, which enters positively, reflecting that higher market uncertainty increases network fragility. The treatment effect declines to (s.e. = 524), suggesting that part of the unconditional increase stems from elevated volatility.
Column (4) presents our preferred specification including all controls. The post-crisis coefficient is (s.e. = 218, p 0.001), corresponding to a 68.4 % increase in network fragility relative to the pre-crisis baseline after controlling for macroeconomic conditions and market volatility. This remains a large and highly significant effect, indicating that the 2008 crisis caused a structural increase in systemic vulnerability that persists beyond cyclical factors.
The sovereign debt-to-GDP ratio enters positively and significantly (, s.e. = 267), consistent with the hypothesis that sovereign stress increases financial sector interconnectedness as banks increase exposures to troubled sovereigns through moral hazard and regulatory arbitrage. This channel was particularly important during the European debt crisis (2010-2012) and may explain some persistence in elevated fragility.
5.2 Dynamic Treatment Effects
To examine how the crisis impact evolved over time, we estimate event study specifications allowing separate coefficients for each post-crisis year:
| (37) |
where corresponds to 2008 (the crisis year) and we normalize for identification. This specification tests parallel pre-trends (coefficients should be zero) and reveals dynamic treatment effect evolution.
Figure 2 presents results graphically.
Several patterns emerge from the event study. First, pre-crisis coefficients and are small and statistically indistinguishable from zero, providing strong support for the parallel trends assumption underlying causal identification. Point estimates are (95 % CI: [ 412, +126]) and (normalized), confirming that network fragility evolved similarly in the years immediately preceding the crisis.
Second, the treatment effect emerges sharply in 2008-2009 and persists throughout the sample period. The 2009 coefficient is (95 % CI: [+1,687, +3,981]), indicating that fragility rose by 2,834 units (165 %) in the immediate crisis aftermath. This large increase reflects the combination of bank failures (reducing denominator in ) and increased bilateral exposures among surviving institutions (raising numerator).
Third, remarkably, the treatment effect amplifies rather than dissipating over time. By 2015, the coefficient rises to (95 % CI: [+2,893, +5,359]), and by 2023 it reaches (95 % CI: [+4,102, +6,762]). This pattern demonstrates structural hysteresis: the crisis triggered network reorganization that became self-reinforcing rather than temporary. Surviving institutions increased their bilateral exposures creating tighter coupling, which in turn increased fragility, consistent with theoretical predictions from Kikuchi (2024f).
Fourth, there is no evidence of reversion toward pre-crisis fragility levels even fifteen years later. Traditional economic shocks exhibit mean reversion as markets adjust and policies respond. The persistent elevation of indicates that the 2008 crisis caused a permanent regime shift in financial network structure. This finding has important implications for long-run stability and suggests that absent policy intervention, elevated fragility may persist indefinitely.
5.3 The Consolidation Paradox
A central finding of our analysis is that network fragility increased dramatically despite substantial reduction in the number of banks. This consolidation paradox challenges conventional wisdom that reducing institution count enhances stability. We examine this phenomenon in detail.
Table LABEL:tab:consolidation_paradox decomposes the change in algebraic connectivity into components attributable to node count versus coupling strength.
| Period | Banks | Avg Bilateral | Algebraic | Predicted |
|---|---|---|---|---|
| (n) | Exposure (w) | Connectivity | lambda-2 | |
| 2007Q2 (Pre-Crisis) | 296 | 12.6 | 1,719 | 1,719 |
| 2023Q4 (Post-Crisis) | 156 | 47.3 | 7,151 | 7,151 |
| Change: | ||||
| Absolute | 140 | 34.7 | 5,432 | 5,432 |
| %age | 47.3% | 275.4% | 315.8% | 317.4% |
| Counterfactual: | ||||
| Only Node Reduction | 156 | 12.6 | 902 | |
| Only Coupling Increase | 296 | 47.3 | 6,456 | |
| Correlation (n, lambda-2) | 0.63 | |||
| Correlation (w, lambda-2) | 0.97 |
-
•
Notes: This table decomposes algebraic connectivity changes between pre-crisis (2007Q2) and post-crisis (2023Q4) periods. Banks (n) is count of institutions. Average Bilateral Exposure (w) is mean counterparty relationships per institution. Algebraic Connectivity is lambda-2 from network Laplacian. Predicted lambda-2 uses formula lambda-2 (w/n) scaled to 2007Q2 level. Counterfactual Analysis shows lambda-2 under hypothetical scenarios where only one factor changes. Correlations computed over quarterly time series (n = 67).
The top panel shows that while the number of banks declined 47.3 % from 296 to 156, average bilateral exposures increased 275.4 % from 12.6 to 47.3 counterparties per institution. Algebraic connectivity rose 315.8 % from 1,719 to 7,151. The predicted value using the theoretical approximation from Theorem 1 is 7,183, remarkably close to the actual value, validating the theoretical decomposition.
The counterfactual analysis isolates each mechanism. If only node reduction had occurred (holding constant at 2007 levels), algebraic connectivity would have declined to 906, a 47.3 % decrease proportional to the node reduction. This is the conventional wisdom: fewer institutions means less fragility. However, the actual outcome went in the opposite direction due to coupling effects.
If only coupling strength had increased (holding constant), algebraic connectivity would have risen to 6,461, a 276 % increase. This coupling effect dominated the node reduction effect, producing net increase of 315.8 %. The result is a consolidation paradox: fewer institutions but higher fragility.
The correlation analysis confirms these relationships. The correlation between node count and algebraic connectivity is negative (r = 0.63), as expected: more nodes mechanically reduce for fixed edge structure. However, the correlation between average exposure intensity and algebraic connectivity is strongly positive (r = 0.97, p 0.001), demonstrating that coupling strength is the primary driver of fragility.
Figure 3 visualizes this relationship.
The economic interpretation is that post-crisis regulatory reforms inadvertently increased systemic risk despite reducing institution count. Consolidation through mergers and failures eliminated smaller regional banks while preserving large international institutions. The surviving banks increased their bilateral exposures to each other, both because fewer counterparties were available and because Basel III liquidity requirements incentivized banks to maintain standing credit relationships. This created a tightly coupled core of systemically important institutions whose interdependence generates rapid contagion propagation.
5.4 Spatial Decay Analysis
We now examine how financial contagion propagates geographically, testing the prediction that spatial decay is minimal due to electronic payment systems eliminating geographic frictions. We estimate the dual-channel diffusion equation from Section 3:
| (38) |
where measures the change in algebraic connectivity for institution pairs separated by geographic distance , captures spatial decay through the parameter , and controls for network distance (shortest path length). We estimate this equation using nonlinear least squares.
Table 5 reports results comparing financial networks with technology diffusion from Kikuchi (2024j).
| Financial Networks | Technology | ||||
|---|---|---|---|---|---|
| Estimate | Std. Error | 95% CI | Estimate | Std. Error | |
| Spatial decay rate (kappa) | 0.00002 | 0.000006 | [0.000014, 0.000026] | 0.043 | 0.009 |
| Boundary (d-star, km) | 47,474 | 14,242 | [33,232, 61,716] | 69 | 17 |
| Network distance effect | 0.687 | 0.134 | [0.554, 0.820] | 0.328 | 0.095 |
| R-squared | 0.94 | 0.89 | |||
| Observations | 12,090 | 124,500 | |||
| Ratio (Finance/Technology) | 0.00047 | 687.8 |
-
•
Notes: This table compares spatial decay parameters for financial contagion (columns 1-3) versus technology diffusion (columns 4-5). Spatial decay rate (kappa) estimated from exponential decay function. Implied spatial boundary (d-star) computed as / kappa, representing distance at which effects decline to 1 % of origin intensity. Network distance effect measures impact of one additional step in network shortest path. Financial network estimates use 156 institutions times 77 quarters equals 12,090 observations. Technology diffusion estimates from Kikuchi (2024j) use 500 firms times 249 firm-quarters equals 124,500 observations. Standard errors from delta method.
The results reveal dramatic differences in spatial propagation between financial networks and technology diffusion. For financial contagion, the estimated spatial decay rate is per kilometer (95 % CI: [0.000014, 0.000026]), implying a spatial boundary of kilometers. This is larger than Earth’s diameter, indicating effectively global propagation with negligible geographic attenuation.
In contrast, technology diffusion exhibits spatial decay rate per kilometer, implying spatial boundary kilometers. Technology shocks dissipate within metropolitan-area distances due to localized knowledge spillovers and demonstration effects.
The ratio of decay rates is , implying that financial shocks travel 2,150 times further than technology shocks before attenuating to comparable levels. This fundamental difference reflects distinct propagation mechanisms: financial contagion occurs through instantaneous electronic payment systems connecting institutions worldwide, while technology diffusion requires face-to-face interactions and localized demonstration effects.
The network distance coefficient is larger for finance () than technology (), confirming that network topology matters more than geography for financial contagion. Each additional step in the network shortest path increases correlation by 0.687 for finance versus 0.328 for technology, indicating that bilateral exposure channels dominate geographic proximity.
Figure 4 visualizes these differences.
5.5 Comparison with Traditional Methods
A key contribution of our analysis is demonstrating that traditional difference-in-differences methods produce severely biased estimates when applied to network data. We compare our spatial DID approach with three alternatives: (1) institution-level DID ignoring network structure, (2) institution-level DID including network controls, and (3) synthetic control methods.
Table 6 reports results.
| Estimation Method | Treatment | Standard | Bias | Bias |
|---|---|---|---|---|
| Effect | Error | Magnitude | (%) | |
| Preferred Approach: | ||||
| Spatial DID (Network-Level) | 1,176 | 218 | ||
| Traditional Approaches: | ||||
| Institution DID (No Controls) | 2,034 | 287 | 858 | 73.0 |
| Institution DID (Network Controls) | 1,687 | 243 | 511 | 43.5 |
| Synthetic Control | 1,523 | 312 | 347 | 29.5 |
| Alternative Specifications: | ||||
| Spatial DID (Weighted) | 1,204 | 229 | 28 | 2.4 |
| Spatial DID (Robust SE) | 1,176 | 267 |
-
•
Notes: This table compares treatment effect estimates across alternative methods. Spatial DID (Network-Level) is our preferred specification from Table 3 Column (4), treating entire network as unit of analysis. Institution DID (No Controls) uses standard institution-level difference-in-differences ignoring network structure. Institution DID (Network Controls) adds node-level centrality measures as covariates. Synthetic Control uses synthetic control method with donor pool of pre-crisis observations. Spatial DID (Weighted) weights observations by institution assets. Spatial DID (Robust SE) uses heteroskedasticity-robust standard errors instead of bootstrap. Bias Magnitude equals estimate spatial DID benchmark. Bias (%) equals bias divided by spatial DID estimate times 100.
The institution-level DID with no controls yields (s.e. = 287), 73 % larger than the spatial DID estimate of . This substantial upward bias stems from double-counting spillovers: when Institution A experiences crisis impact, connected Institution B experiences indirect impact through bilateral exposures. The naive estimator incorrectly attributes both effects to direct treatment, inflating the estimate.
Adding network controls (degree centrality, betweenness centrality, eigenvector centrality) as covariates reduces bias to 43.5 % () but does not eliminate it. The residual bias reflects that standard regression methods cannot fully separate direct effects from network spillovers when institutions are fundamentally interconnected.
Synthetic control methods perform better but still exhibit 29.5 % upward bias (). The synthetic control approach constructs counterfactual outcomes by matching pre-crisis trajectories using weighted combinations of control observations. However, this requires that control units remain unaffected by treatment—violated in financial networks where crisis impacts propagate through exposures.
Alternative specifications of our spatial DID approach yield similar estimates: asset-weighted aggregation produces (2.4 % larger), and using heteroskedasticity-robust standard errors instead of bootstrap yields identical point estimate () with moderately larger standard error (267 versus 218). These specifications validate our baseline approach.
The methodological lesson is that network spillovers require specialized treatment. Standard causal inference methods developed for independent observations systematically overstate treatment effects by attributing spillover impacts to direct treatment. Our spatial DID approach resolves this by aggregating to network level, properly accounting for within-network propagation while maintaining causal interpretation.
6 Robustness Analysis
This section presents extensive robustness checks examining sensitivity to alternative specifications, sample definitions, and measurement approaches.
6.1 Alternative Network Specifications
Table 7 examines sensitivity to network construction choices including exposure thresholds, normalization methods, and weight transformations.
| Network Specification | lambda 2 | Treatment | Std. Error | Change from |
|---|---|---|---|---|
| (2007Q2) | Effect | Baseline | ||
| Baseline: | ||||
| Geometric mean normalization | 1,719 | 1,176 | 218 | |
| Alternative Normalizations: | ||||
| Arithmetic mean | 1,634 | 1,089 | 197 | 7.4 % |
| Maximum exposure | 1,891 | 1,243 | 229 | 5.7 % |
| Asset-weighted | 1,782 | 1,221 | 234 | 3.8 % |
| Alternative Thresholds: | ||||
| Top 10 % exposures only | 1,456 | 987 | 189 | 16.1 % |
| Top 25 % exposures only | 1,598 | 1,101 | 203 | 6.4 % |
| All exposures (no threshold) | 1,719 | 1,176 | 218 | |
| Alternative Weights: | ||||
| Binary (unweighted) | 1,287 | 843 | 174 | 28.3 % |
| Log-transformed | 1,615 | 1,098 | 206 | 6.6 % |
| Square-root transformed | 1,683 | 1,134 | 212 | 3.6 % |
-
•
Notes: This table examines sensitivity of results to network construction choices. Each row uses alternative specification to construct adjacency matrix and compute algebraic connectivity. lambda 2 (2007Q2) is pre-crisis baseline algebraic connectivity. Treatment Effect is coefficient on Post-2008 indicator from equation (36). Standard Error from bootstrap (1,000 replications). Change from Baseline computes %age difference in treatment effect relative to geometric mean normalization. All specifications include controls from Table 3 Column (4).
Results are qualitatively robust across alternative specifications. Using arithmetic mean normalization instead of geometric mean produces treatment effect (7.4 % smaller). Using maximum exposure normalization yields (5.7 % larger). Asset-weighted networks produce (3.8 % larger). All estimates remain large, positive, and highly statistically significant.
Focusing only on top exposures reduces algebraic connectivity and treatment effects as expected: excluding smaller bilateral relationships lowers network density and coupling. Using top 10 % exposures yields (16.1 % smaller), while top 25 % yields (6.4 % smaller). These specifications may be more appropriate if small exposures do not transmit meaningful contagion, though our baseline includes all exposures to fully capture network structure.
Alternative weight transformations produce similar results. Unweighted (binary) networks yield (28.3 % smaller), as binary coding discards information about exposure intensity. Log-transformation yields (6.6 % smaller), attenuating influence of largest exposures. Square-root transformation yields (3.6 % smaller), intermediate between unweighted and baseline.
6.2 Pre-Trends Tests and Placebo Analysis
Table 8 formally tests the parallel trends assumption using leads of the crisis indicator.
| Lead Period | Coefficient | Std. Error | 95 % CI | P-value |
|---|---|---|---|---|
| 2007Q1 (6 quarters before) | 67 | 143 | [ 353, 219] | 0.639 |
| 2007Q3 (4 quarters before) | 89 | 156 | [ 401, 223] | 0.571 |
| 2008Q1 (2 quarters before) | 42 | 134 | [ 226, 310] | 0.754 |
| 2008Q3 (Crisis quarter) | 2,834 | 476 | [ 1,882, 3,786] | 0.001 |
| Joint F-test (all leads) | F = 0.54 | 0.658 |
-
•
Notes: This table tests parallel trends assumption by regressing algebraic connectivity on leads of Post-2008 indicator plus controls. Each row shows coefficient on indicator for specified quarter. Leads (6, 4, 2 quarters before crisis) should be zero under parallel trends. Crisis quarter (2008Q3) shows sharp increase as expected. Standard errors from bootstrap (1,000 replications). Joint F-test examines whether all lead coefficients are jointly zero. Failure to reject (p = 0.658) supports parallel trends assumption.
All pre-crisis lead coefficients are small and statistically indistinguishable from zero. The lead 6 coefficient is (s.e. = 143, p = 0.639), lead 4 is (s.e. = 156, p = 0.571), and lead 2 is (s.e. = 134, p = 0.754). The joint F-test fails to reject that all leads are zero (F = 0.54, p = 0.658), strongly supporting the parallel trends assumption.
In contrast, the crisis quarter coefficient is (s.e. = 476, p 0.001), demonstrating sharp discontinuous increase in fragility coinciding with Lehman bankruptcy. This pattern—flat pre-trends followed by sharp increase—is the signature of a valid quasi-experiment.
Table 9 presents placebo tests using alternative crisis dates.
| Placebo Date | Treatment | Std. Error | 95 % CI | Expected |
|---|---|---|---|---|
| Effect | Result | |||
| 2006Q3 (2 years early) | 87 | 168 | [ 423, 249] | Zero |
| 2007Q3 (1 year early) | 134 | 187 | [ 240, 508] | Zero |
| 2008Q3 (Actual crisis) | 2,834 | 476 | [ 1,882, 3,786] | Positive |
| 2009Q3 (1 year late) | 176 | 203 | [ 230, 582] | Zero |
| 2010Q3 (2 years late) | 92 | 194 | [ 480, 296] | Zero |
-
•
Notes: This table reports placebo tests using artificial crisis dates. Each row estimates equation (36) with Post indicator defined relative to specified placebo date. Standard errors from bootstrap (1,000 replications). Expected Result column indicates whether significant effect is expected under correct identification. Only actual crisis date (2008Q3) should produce significant positive coefficient. Placebo dates before and after should produce insignificant coefficients. Results confirm this pattern, validating identification strategy.
Placebo tests confirm that only the actual crisis date produces significant effects. Using crisis dates 2 years early (2006Q3) yields (s.e. = 168, p = 0.605), 1 year early (2007Q3) yields (s.e. = 187, p = 0.474), 1 year late (2009Q3) yields (s.e. = 203, p = 0.387), and 2 years late (2010Q3) yields (s.e. = 194, p = 0.636). All placebo coefficients are small and statistically insignificant, while the actual crisis date yields (p 0.001). This validates that the observed effect represents causal impact of the 2008 crisis rather than spurious correlation or trend breaks.
6.3 Subperiod Analysis
Table 10 examines whether treatment effects vary across different post-crisis subperiods.
| Period | lambda 2 | Treatment | Std. Error | Relative to |
|---|---|---|---|---|
| Mean | Effect | Pre-Crisis | ||
| Pre-Crisis (2007Q1-2008Q2) | 1,719 | |||
| Acute Crisis (2008Q4-2009Q4) | 3,847 | 2,128 | 398 | 124 % |
| Recovery (2010Q1-2012Q4) | 4,523 | 2,804 | 412 | 163 % |
| European Debt Crisis (2013Q1-2015Q4) | 5,187 | 3,468 | 456 | 202 % |
| Post-Reform (2016Q1-2019Q4) | 6,034 | 4,315 | 489 | 251 % |
| COVID Era (2020Q1-2021Q4) | 6,723 | 5,004 | 523 | 291 % |
| Recent (2022Q1-2023Q4) | 6,984 | 5,265 | 537 | 306 % |
-
•
Notes: This table examines treatment effects separately for different post-crisis subperiods. lambda 2 Mean is average algebraic connectivity for specified period. Treatment Effect is coefficient on period indicator relative to pre-crisis baseline from specification with all controls. Standard Error from bootstrap (1,000 replications). Relative to Pre-Crisis computes %age change from pre-crisis mean of 1,719. Treatment effects increase monotonically over time, demonstrating amplification rather than dissipation.
Treatment effects increase monotonically across subperiods, rising from +124 % in the acute crisis phase to +306 % in the recent period (2022-2023). This pattern confirms dynamic amplification documented in the event study: rather than dissipating as markets adjust, the crisis impact intensified over fifteen years through network evolution and consolidation dynamics.
The acceleration during the European debt crisis (2013-2015) reflects increased interconnectedness as banks maintained exposures to troubled sovereigns despite rising risk. The further acceleration during post-reform period (2016-2019) paradoxically coincides with implementation of Basel III capital requirements, suggesting that regulations may have inadvertently increased coupling by forcing banks to concentrate relationships among approved counterparties.
6.4 Alternative Sample Definitions
Table 11 examines sensitivity to sample inclusion criteria.
| Sample Definition | Number of | Treatment | Std. Error | Change from |
|---|---|---|---|---|
| Institutions | Effect | Baseline | ||
| Baseline (G-SIBs plus large banks) | 156 | 1,176 | 218 | |
| Only G-SIBs | 30 | 1,423 | 287 | 21.0 % |
| All banks (including small) | 412 | 934 | 176 | 20.6 % |
| US banks only | 23 | 1,298 | 312 | 10.4 % |
| European banks only | 87 | 1,067 | 243 | 9.3 % |
| Balanced panel (no exits) | 142 | 1,154 | 214 | 1.9 % |
| Full sample (including exits) | 189 | 1,234 | 229 | 4.9 % |
-
•
Notes: This table examines sensitivity to sample inclusion criteria. Baseline includes 156 institutions designated as G-SIBs or with assets exceeding 100 billion USD. Only G-SIBs restricts to 30 Financial Stability Board designated systemically important banks. All banks includes smaller regional institutions. US/European subsamples restrict geography. Balanced panel drops institutions that exited during sample period. Full sample includes all institutions observed in any period. Standard errors from bootstrap. Change from Baseline computes %age difference in treatment effect relative to baseline sample.
Results are qualitatively similar across sample definitions. Restricting to G-SIBs only yields larger treatment effect (, 21 % above baseline), consistent with hypothesis that largest institutions experienced greater interconnectedness increases. Including all banks yields smaller effect (, 20.6 % below baseline), as small banks participate less in international wholesale funding markets. Geographic subsamples produce similar results to baseline: US banks (10.4 % above), European banks (9.3 % below).
Balanced panel and full sample specifications yield nearly identical estimates to baseline, indicating that survivorship bias is minimal. Banks that exited (through failure or merger) do not drive the main results.
7 Policy Implications and Applications
This section discusses policy implications of our findings for macroprudential regulation, capital requirements, resolution planning, and stress testing.
7.1 Network-Targeted Capital Requirements
A key application of our framework is designing capital requirements that optimally reduce systemic risk. Standard Basel III capital requirements apply uniform risk weights to different asset classes but do not account for network externalities: a bank’s contribution to systemic risk depends on its position in the network, not just its balance sheet.
We propose network-targeted capital requirements that impose higher requirements on institutions with high spectral centrality. Specifically, institution ’s capital requirement should be:
| (39) |
where is the baseline Basel III requirement (currently 8 % of risk-weighted assets), is institution ’s Fiedler centrality (component of eigenvector ), and is a policy parameter governing the strength of network adjustment.
Table LABEL:tab:network_capital simulates the impact of network-targeted requirements under alternative values of .
| Policy Scenario | Avg Req | Std Dev | lambda-2 | Reduction | Banks |
|---|---|---|---|---|---|
| (%) | (%) | Level | (%) | Affected | |
| Baseline (Uniform 8%) | 8.0 | 0.0 | 7,151 | 156 | |
| Network-Targeted () | 8.4 | 1.3 | 6,234 | 12.8 | 47 |
| Network-Targeted () | 9.1 | 2.1 | 5,438 | 24.0 | 62 |
| Network-Targeted () | 9.8 | 2.9 | 4,782 | 33.1 | 73 |
| Network-Targeted () | 10.6 | 3.7 | 4,231 | 40.8 | 89 |
| Size-Based (Top 30 banks) | 11.2 | 4.8 | 5,987 | 16.3 | 30 |
| Leverage-Based (Lev 15) | 10.3 | 3.9 | 6,123 | 14.4 | 52 |
-
•
Notes: This table simulates counterfactual capital requirements under alternative targeting schemes. Baseline applies uniform 8 % requirement on all banks. Network-Targeted adds adjustment proportional to squared Fiedler centrality (v-2-i), controlled by alpha. lambda-2 Level is algebraic connectivity under counterfactual policy. Reduction is %age decline in lambda-2 relative to baseline. Banks Affected counts institutions facing requirements above baseline. Size-Based targets 30 largest banks (G-SIBs). Leverage-Based targets high-leverage institutions.
Network-targeted requirements with reduce algebraic connectivity 33.1 % from 7,151 to 4,782 while requiring only 73 institutions (47 % of the sample) to hold capital above the 8 % baseline. This compares favorably to size-based targeting (top 30 banks), which achieves only 16.3 % reduction despite imposing substantial requirements (11.2 % average) and higher compliance costs (389 billion USD versus 356 billion).
The superiority of network targeting stems from exploiting spectral structure. Banks with high Fiedler centrality sit at network bottlenecks where they contribute disproportionately to connectivity. Reducing their bilateral exposures through capital requirements disrupts contagion pathways more effectively than targeting largest banks, which may have large balance sheets but peripheral network positions.
7.2 Stress Testing with Network Dynamics
Current stress testing frameworks evaluate individual institutions’ ability to withstand adverse scenarios but do not fully incorporate network amplification of shocks. We propose augmenting stress tests with network diffusion analysis using the continuous functional framework.
Consider a stress scenario where institution experiences initial shock . The system response evolves according to the diffusion equation:
| (40) |
where represents institutions’ stress levels at time and is the shock vector. The equilibrium stress distribution is:
| (41) |
The amplification factor—ratio of total equilibrium stress to initial shock—is:
| (42) |
Table 13 computes amplification factors for different shock scenarios.
| Shock Scenario | Initial | Equilibrium | Amplification | Most Affected |
|---|---|---|---|---|
| Stress | Stress | Factor | Institutions | |
| Single large bank failure | 100 | 1,142 | 11.4 | Connected G-SIBs |
| Three regional bank failures | 75 | 623 | 8.3 | Regional networks |
| Sovereign default (European) | 200 | 2,387 | 11.9 | European banks |
| Market liquidity shock | 150 | 1,698 | 11.3 | Market makers |
| Average across scenarios | 131 | 1,463 | 10.7 |
-
•
Notes: This table computes network amplification factors for different stress scenarios using the network diffusion model in equation (40). Initial Stress is total shock magnitude (arbitrary units). Equilibrium Stress is steady-state total stress after network propagation. Amplification Factor is ratio of equilibrium to initial stress, measuring network magnification. Most Affected Institutions indicates which bank types experience largest stress increases. Scenarios calibrated to plausible magnitudes based on 2008 crisis events.
Network amplification factors range from 8.3 to 11.9 across scenarios, averaging 10.7. This implies that a shock affecting one institution with initial severity 100 generates equilibrium system stress of 1,070 after propagating through bilateral exposures. Current stress tests that ignore network amplification underestimate systemic impacts by approximately one order of magnitude.
The policy implication is that stress testing frameworks should incorporate network diffusion analysis to accurately assess system-wide consequences. Regulators should evaluate not just whether individual banks survive shocks but whether the network as a whole remains stable accounting for cascading failures and exposure amplification.
7.3 Resolution Planning and Too-Big-to-Fail
Our consolidation paradox finding has direct implications for resolution planning. The Dodd-Frank Act requires G-SIBs to prepare ”living wills” describing how they would be wound down in bankruptcy without taxpayer support. However, our analysis suggests that traditional resolution planning may be insufficient when network fragility is high.
Consider the impact of resolving (orderly shutting down) institution on system fragility. Removing node changes the Laplacian from to where row and column are deleted. The change in algebraic connectivity is:
| (43) |
where is institution ’s Fiedler centrality. Institutions with large contribute substantially to network fragility, making their resolution beneficial for systemic stability.
Table LABEL:tab:resolution_priority ranks institutions by resolution impact.
| Institution | Assets | Fiedler | Resolution | lambda-2 | Fragility | Priority |
| (Anonymous) | (USD T) | Centrality | Impact | Reduction | Reduction | Rank |
| (v-2-i) | (squared) | (%) | ||||
| Bank A | 3.2 | 0.187 | 0.035 | 7,151 to 6,901 | 3.5 | 1 |
| Bank B | 2.8 | 0.173 | 0.030 | 7,151 to 6,937 | 3.0 | 2 |
| Bank C | 2.4 | 0.159 | 0.025 | 7,151 to 6,972 | 2.5 | 3 |
| Bank D | 3.6 | 0.156 | 0.024 | 7,151 to 6,980 | 2.4 | 4 |
| Bank E | 2.1 | 0.148 | 0.022 | 7,151 to 6,994 | 2.2 | 5 |
| … | … | … | … | … | … | … |
| Average (Top 10) | 2.9 | 0.164 | 0.027 | 2.7 | ||
| Average (All 156) | 0.52 | 0.048 | 0.002 | 0.2 |
-
•
Notes: This table ranks institutions by their contribution to network fragility, measured through squared Fiedler centrality. Institution identities anonymized for confidentiality. Total Assets in trillions USD as of 2023Q4. Fiedler Centrality is absolute value of eigenvector component v-2-i. Resolution Impact equals v-2-i squared, measuring change in lambda-2 from removing institution. lambda-2 Reduction shows counterfactual algebraic connectivity after resolution. Fragility Reduction is %age decline in lambda-2. Priority Rank indicates resolution ordering that maximally reduces systemic risk. Top 10 institutions account for 27 % of fragility reduction despite representing 6.4 % of total banks.
The top 10 institutions ranked by Fiedler centrality contribute 27 % of potential fragility reduction through resolution, despite representing only 6.4 % of banks. This concentration indicates that resolution planning should prioritize these systemically important network hubs. Importantly, the ranking differs from simple size ranking: Bank D has largest balance sheet (3.6 trillion USD) but only fourth-highest network impact because its exposures are relatively diversified.
The policy implication is that ”too-big-to-fail” should be redefined as ”too-connected-to-fail.” Resolution priorities should target network centrality rather than asset size. This requires collecting bilateral exposure data to compute spectral centrality measures—currently not systematically done by most regulators.
7.4 International Coordination
Our finding of effectively global contagion propagation (spatial boundary 47,474 kilometers) implies that financial stability is a global public good requiring international coordination. Unilateral national regulations may be ineffective when shocks transmit rapidly across borders through bilateral exposures.
Consider two countries, Home and Foreign, that can each impose capital requirements. The payoff to Home from imposing requirement when Foreign imposes depends on network structure:
| (44) |
where is the cost of imposing requirement and represents expected crisis costs proportional to network fragility. The Nash equilibrium satisfies first-order conditions, but will be inefficiently low due to coordination failure: neither country internalizes the benefit of reducing fragility for the other.
Table 15 simulates equilibrium under alternative coordination regimes.
| Coordination Regime | Capital | lambda 2 | Expected | Efficiency |
|---|---|---|---|---|
| Requirement | Level | Crisis Cost | Gain | |
| (%) | (USD Billion) | (%) | ||
| No Coordination (Nash) | 8.4 | 6,847 | 342 | |
| Bilateral Agreement | 9.6 | 5,923 | 296 | 13.5 |
| Full Coordination (Social Optimum) | 11.2 | 4,891 | 245 | 28.4 |
| Basel III (Actual) | 8.0 | 7,151 | 358 | 4.7 |
-
•
Notes: This table compares outcomes under alternative international coordination regimes using a two-country game-theoretic model. No Coordination is Nash equilibrium where each country optimizes unilaterally. Bilateral Agreement has countries jointly optimize capital requirements. Full Coordination is social optimum maximizing joint welfare. Basel III (Actual) shows current policy. Capital Requirement is equilibrium or optimal requirement. lambda 2 Level is resulting network fragility. Expected Crisis Cost computed as probability times severity times GDP. Efficiency Gain is %age reduction in expected cost relative to no coordination. Model calibrated using estimated parameters.
Full international coordination achieves 28.4 % reduction in expected crisis costs relative to Nash equilibrium through higher capital requirements (11.2 % versus 8.4 %) and lower fragility (lambda 2 = 4,891 versus 6,847). Bilateral agreements achieve intermediate gains (13.5 %). Notably, actual Basel III requirements (8.0 %) produce even higher costs than Nash equilibrium, suggesting that international coordination is currently insufficient.
The policy implication is that financial regulation requires strengthened international institutions. The Basel Committee on Banking Supervision provides a coordination mechanism, but lacks enforcement power. Our analysis suggests that coordinated capital requirements targeting network centrality could reduce expected crisis costs by nearly 30 % relative to uncoordinated policies—a substantial efficiency gain justifying institutional investment in coordination mechanisms.
8 Conclusion
This paper develops and empirically implements a unified framework for analyzing systemic risk in financial networks by integrating spatial treatment effect methodology with spectral network fragility analysis. Building on continuous functional methods from Kikuchi (2024c), Kikuchi (2024f), and Kikuchi (2024i), we characterize contagion dynamics through the spectral properties of network Laplacian operators and identify causal impacts using spatial difference-in-differences methods adapted for interconnected systems.
Our empirical analysis of the 2008 financial crisis yields four principal findings. First, the crisis caused a large, statistically significant, and persistent increase in network fragility measured by algebraic connectivity. Fragility rose 68.4 % above pre-crisis baselines (95 % CI: [42.7 %, 94.1 %], p 0.001) and continued amplifying rather than dissipating over fifteen years, demonstrating structural hysteresis where systems settle into new equilibria rather than reverting automatically.
Second, we document a consolidation paradox: while the number of major banks declined 47.3 % between 2007 and 2023, network fragility increased 315.8 %. This occurred because bilateral exposure concentration rose 687 %, with surviving institutions becoming more tightly coupled. The correlation between exposure concentration and fragility is r = 0.97 (p 0.001), confirming that coupling strength drives systemic risk more than institution count. This finding challenges conventional wisdom that consolidation enhances stability and suggests that post-crisis regulatory reforms may have inadvertently increased systemic vulnerability.
Third, spatial decay analysis reveals that financial contagion exhibits negligible geographic attenuation with spatial decay rate kappa = 0.00002 per kilometer, implying spatial boundary d-star = 47,474 kilometers—effectively global propagation. This contrasts sharply with technology diffusion documented in Kikuchi (2024j), which exhibits spatial boundary of only 69 kilometers. The 2,150-fold difference in propagation range reflects distinct mechanisms: financial contagion occurs through instantaneous electronic payment systems, while technology diffusion requires localized face-to-face interactions.
Fourth, traditional difference-in-differences methods that treat institutions as independent units produce treatment effect estimates biased upward by 73.2 %. This bias stems from double-counting spillovers: when one institution experiences crisis impact, connected institutions experience indirect impacts through bilateral exposures. Naive estimators incorrectly attribute both effects to direct treatment. Our spatial difference-in-differences approach resolves this by aggregating to network level before differencing, properly accounting for within-network propagation while maintaining causal interpretation.
The theoretical framework provides several insights. Theorem 1 establishes the consolidation paradox: fragility increases when consolidation occurs if average bilateral exposure intensity grows faster than the inverse of node reduction. The condition determines whether fewer institutions generate higher fragility. In our data, this condition holds with substantial margin ( versus ), explaining the observed increase.
The integration of spatial decay and network contagion through equation (17) yields testable predictions about relative channel importance. For financial networks, we find and large , indicating dominant network channel with minimal geographic friction. For technology diffusion, Kikuchi (2024j) finds large and moderate , indicating dominant spatial channel with important network effects. This demonstrates that the continuous functional framework applies broadly across economic contexts but with dramatically different parameter values.
The policy implications are substantial. First, network-targeted capital requirements that impose higher requirements on institutions with high spectral centrality achieve 33.1 % reduction in fragility while affecting only 47 % of institutions. This compares favorably to size-based targeting, which achieves 16.3 % reduction despite higher compliance costs. Second, stress testing frameworks should incorporate network diffusion analysis to accurately assess system-wide impacts, as our estimates suggest network amplification factors average 10.7 across shock scenarios. Third, resolution planning should prioritize network centrality rather than asset size, redefining ”too-big-to-fail” as ”too-connected-to-fail.” Fourth, international coordination on capital requirements could reduce expected crisis costs by 28.4 % relative to uncoordinated policies.
Our analysis has several limitations that suggest directions for future research. First, we do not observe all bilateral exposures directly, requiring maximum entropy imputation for missing links. While this approach produces minimally informative estimates, access to complete supervisory data would strengthen inference. Second, we treat network structure as exogenous to the crisis, while institutions likely adjusted exposures strategically in response to stress. Developing identification strategies that account for endogenous network formation remains an important challenge. Third, we focus on cross-border exposures among major institutions, omitting domestic interbank markets and shadow banking connections that may transmit contagion. Extending the analysis to include these channels would provide more comprehensive systemic risk assessment.
Fourth, our spatial difference-in-differences approach assumes parallel trends at network level, which we validate using pre-crisis data. However, unobserved time-varying factors could violate this assumption. Instrumental variables strategies exploiting plausibly exogenous variation in network structure could provide additional identification. Fifth, we analyze a single crisis (2008 financial crisis), limiting external validity. Replicating the analysis for other financial crises—such as the 1997 Asian financial crisis or 2011 European debt crisis—would test generalizability of the consolidation paradox and other findings.
Sixth, we do not model heterogeneous institution responses to the crisis. Banks differed in their strategic responses based on business models, regulatory constraints, and management quality. Allowing for treatment effect heterogeneity could reveal important distributional consequences. Seventh, our stress testing analysis uses linear diffusion dynamics, while actual contagion may exhibit nonlinearities such as threshold effects where shocks exceeding critical values trigger cascading failures. Incorporating nonlinear dynamics would improve realism.
Despite these limitations, our analysis demonstrates that continuous functional methods from mathematical physics provide empirically relevant tools for analyzing systemic risk in financial networks. The finding that consolidation paradoxically increased fragility challenges dominant policy narratives and suggests that post-crisis regulatory reforms may have inadvertently undermined the stability they sought to enhance. Network-targeted interventions that exploit spectral structure offer promising alternatives to conventional size-based regulations. More broadly, the framework applies across economic domains—from technology diffusion to financial contagion—providing unified mathematical foundations for understanding spatial treatment effects in interconnected systems.
References
- Abadie et al. (2020) Abadie, A., Athey, S., Imbens, G. W., & Wooldridge, J. (2020). Sampling-based versus design-based uncertainty in regression analysis. Econometrica, 88(1), 265–296.
- Acemoglu et al. (2015) Acemoglu, D., Ozdaglar, A., & Tahbaz-Salehi, A. (2015). Systemic risk and stability in financial networks. American Economic Review, 105(2), 564–608.
- Adrian & Shin (2010) Adrian, T., & Shin, H. S. (2010). Liquidity and leverage. Journal of Financial Intermediation, 19(3), 418–437.
- Allen & Gale (2000) Allen, F., & Gale, D. (2000). Financial contagion. Journal of Political Economy, 108(1), 1–33.
- Bardoscia et al. (2015) Bardoscia, M., Battiston, S., Caccioli, F., & Caldarelli, G. (2015). Pathways towards instability in financial networks. Nature Communications, 6, 6627.
- Battiston et al. (2012) Battiston, S., Puliga, M., Kaushik, R., Tasca, P., & Caldarelli, G. (2012). DebtRank: Too central to fail? Financial networks, the FED and systemic risk. Scientific Reports, 2, 541.
- Bernanke (2010) Bernanke, B. S. (2010). Causes of the recent financial and economic crisis. Statement before the Financial Crisis Inquiry Commission, September 2.
- Brunnermeier (2009) Brunnermeier, M. K. (2009). Deciphering the liquidity and credit crunch 2007-2008. Journal of Economic Perspectives, 23(1), 77–100.
- Butts et al. (2017) Butts, K., Dickstein, M., & Shapiro, J. (2017). Revisiting event study designs. NBER Working Paper No. 23474.
- Cao & Liu (2021) Cao, Y., & Liu, Y. (2021). Estimating dynamic treatment effects in event studies with heterogeneous treatment effects. Journal of Econometrics, 225(2), 175–199.
- Chinazzi et al. (2020) Chinazzi, M., Davis, J. T., Ajelli, M., Gioannini, C., Litvinova, M., Merler, S., … & Vespignani, A. (2020). The effect of travel restrictions on the spread of the 2019 novel coronavirus (COVID-19) outbreak. Science, 368(6489), 395–400.
- Demirgüç-Kunt et al. (2020) Demirgüç-Kunt, A., Pedraza, A., & Ruiz-Ortega, C. (2020). Banking sector performance during the COVID-19 crisis. World Bank Policy Research Working Paper, No. 9363.
- Elliott et al. (2014) Elliott, M., Golub, B., & Jackson, M. O. (2014). Financial networks and contagion. American Economic Review, 104(10), 3115–3153.
- Fiedler (1973) Fiedler, M. (1973). Algebraic connectivity of graphs. Czechoslovak Mathematical Journal, 23(2), 298–305.
- Freixas et al. (2000) Freixas, X., Parigi, B. M., & Rochet, J. C. (2000). Systemic risk, interbank relations, and liquidity provision by the central bank. Journal of Money, Credit and Banking, 32(3), 611–638.
- Gai & Kapadia (2010) Gai, P., & Kapadia, S. (2010). Contagion in financial networks. Proceedings of the Royal Society A, 466(2120), 2401–2423.
- Garratt et al. (2014) Garratt, R., Mahadeva, L., & Svirydzenka, K. (2014). Mapping systemic risk in the international banking network. Journal of Money, Credit and Banking, 46(s1), 231–257.
- Glasserman & Young (2015) Glasserman, P., & Young, H. P. (2015). How likely is contagion in financial networks? Journal of Banking & Finance, 50, 383–399.
- Gorton & Metrick (2012) Gorton, G., & Metrick, A. (2012). Securitization. In G. M. Constantinides, M. Harris, & R. M. Stulz (Eds.), Handbook of the Economics of Finance (Vol. 2, pp. 1–70). Elsevier.
- Hautsch et al. (2015) Hautsch, N., Schaumburg, J., & Schienle, M. (2015). Financial network systemic risk contributions. Review of Finance, 19(2), 685–738.
- Kikuchi (2024a) Kikuchi, T. (2024a). A unified framework for spatial and temporal treatment effect boundaries: Theory and identification. arXiv preprint arXiv:2510.00754.
- Kikuchi (2024b) Kikuchi, T. (2024b). Stochastic boundaries in spatial general equilibrium: A diffusion-based approach to causal inference with spillover effects. arXiv preprint arXiv:2508.06594.
- Kikuchi (2024c) Kikuchi, T. (2024c). Spatial and temporal boundaries in difference-in-differences: A framework from Navier-Stokes equation. arXiv preprint arXiv:2510.11013.
- Kikuchi (2024d) Kikuchi, T. (2024d). Nonparametric identification and estimation of spatial treatment effect boundaries: Evidence from 42 million pollution observations. arXiv preprint arXiv:2510.12289.
- Kikuchi (2024e) Kikuchi, T. (2024e). Nonparametric identification of spatial treatment effect boundaries: Evidence from bank branch consolidation. arXiv preprint arXiv:2510.13148.
- Kikuchi (2024f) Kikuchi, T. (2024f). Dynamic spatial treatment effect boundaries: A continuous functional framework from Navier-Stokes equations. arXiv preprint arXiv:2510.14409.
- Kikuchi (2024g) Kikuchi, T. (2024g). Dynamic spatial treatment effects as continuous functionals: Theory and evidence from healthcare access. arXiv preprint arXiv:2510.15324.
- Kikuchi (2024h) Kikuchi, T. (2024h). Emergent dynamical spatial boundaries in emergency medical services: A Navier-Stokes framework from first principles. arXiv preprint arXiv:2510.XXXXX.
- Kikuchi (2024i) Kikuchi, T. (2024i). Network contagion dynamics in European banking: A Navier-Stokes framework for systemic risk assessment. arXiv preprint arXiv:2510.19630.
- Kikuchi (2024j) Kikuchi, T. (2024j). Dual-Channel Technology Diffusion: Spatial Decay and Network Contagion in Supply Chain Networks. arXiv preprint arXiv:2510.XXXXX.
- Manski (2013) Manski, C. F. (2013). Identification of treatment response with social interactions. The Econometrics Journal, 16(1), S1–S23.
- Sommese et al. (2021) Sommese, E., Cerchiello, P., & Torri, G. (2021). A spectral analysis of interbank networks. Expert Systems with Applications, 168, 114270.
- Upper (2011) Upper, C. (2011). Simulation methods to assess the danger of contagion in interbank markets. Journal of Financial Stability, 7(3), 111–125.
- Upper & Worms (2004) Upper, C., & Worms, A. (2004). Estimating bilateral exposures in the German interbank market: Is there a danger of contagion? European Economic Review, 48(4), 827–849.
Appendix A Computational Algorithms
A.1 Eigenvalue Computation
For computing the algebraic connectivity of large graphs ( nodes), we use the Lanczos algorithm for sparse symmetric matrices:
A.2 Bootstrap Inference
For constructing confidence intervals robust to clustering and heteroskedasticity:
A.3 Maximum Entropy Network Imputation
For imputing missing bilateral exposures:
Semi-Supervised Treatment Effect Estimation with Unlabeled Covariates via Generalized Riesz Regression
Abstract
This study investigates treatment effect estimation in the semi-supervised setting, where we can use not only the standard triple of covariates, treatment indicator, and outcome, but also unlabeled auxiliary covariates. For this problem, we develop efficiency bounds and efficient estimators whose asymptotic variance aligns with the efficiency bound. In the analysis, we introduce two different data-generating processes: the one-sample setting and the two-sample setting. The one-sample setting considers the case where we can observe treatment indicators and outcomes for a part of the dataset, which is also called the censoring setting. In contrast, the two-sample setting considers two independent datasets with labeled and unlabeled data, which is also called the case-control setting or the stratified setting. In both settings, we find that by incorporating auxiliary covariates, we can lower the efficiency bound and obtain an estimator with an asymptotic variance smaller than that without such auxiliary covariates.
1 Introduction
A core interest in causal inference is estimating treatment effects, including the average treatment effect (ATE, Imbens & Rubin, 2015). In the standard setup, we estimate such treatment effects from triples of covariates, a treatment indicator, and outcomes. As in other statistical analyses, it is highly important to gather a sufficient amount of data. While randomized controlled trials are the gold standard, they are often infeasible. Therefore, in many practical scenarios, we use observational data to perform causal inference. However, observational data are also not necessarily easy to collect. In particular, treatment variables and the corresponding outcomes are often costly, whereas covariates are usually easy to gather.
Under this practical scenario, we consider estimating ATEs more accurately using auxiliary unlabeled covariates, even when treatment variables and outcomes are missing. This setting corresponds to semi-supervised learning in machine learning, where we utilize both labeled and unlabeled data (Chapelle et al., 2006). In many applications, such unlabeled covariates are easy to gather. For example, in the United States, we may aim to estimate the ATE for the effect of a new scholarship. Although we may know the covariates for an enormous number of students, we can assign treatment, scholarship, to only a limited number of them. In such cases, we aim to estimate the ATE more accurately by using the unlabeled covariates.
We find that, under appropriate conditions, using unlabeled data allows us to construct an ATE estimator whose asymptotic variance, or equivalently, asymptotic mean squared error (MSE), is smaller than that of an estimator that ignores unlabeled data, as shown by Hahn (1998). To support this finding, we develop an asymptotic efficiency bound, a lower bound on the asymptotic variance, when using unlabeled covariates, propose ATE estimators, and show that the resulting asymptotic variances match the efficiency bound. In the methodological and theoretical arguments, we consider two practical scenarios, called the one-sample and two-sample scenarios. In the one-sample scenario, we interpret the unlabeled covariates as part of a dataset with missing variables, outcomes and the treatment indicator. In the two-sample scenario, we assume that labeled and unlabeled data are two independent datasets.
Our efficient ATE estimators are developed based on the efficient influence function implied by the efficiency bound. This object is also called a Neyman orthogonal score in the debiased machine learning literature (Chernozhukov et al., 2018). The Neyman orthogonal scores include nuisance parameters, regression functions and a Riesz representer, which must be estimated before obtaining the ATE estimators. For the Riesz representer estimation, we employ generalized Riesz regression in Kato (2025b; a), which generalizes the Riesz regression in Chernozhukov et al. (2021). We further extend generalized Riesz regression to the semi-supervised setting.
We list our contributions as follows:
-
•
We develop efficiency bounds for regular ATE estimators in this setting, which also yield Neyman orthogonal scores.
-
•
We construct an asymptotically efficient estimator using the Neyman orthogonal scores.
-
•
We extend generalized Riesz regression for estimating nuisance parameters, including the Riesz representer and regression functions.
Related work.
The related topics of this study include debiased machine learning, efficiency under the two-sample case (stratified sampling scheme), treatment effect estimation with missing values, density-ratio estimation, and semi-supervised learning.
In treatment effect estimation, we typically aim to attain the -rate with the smallest asymptotic variance, or equivalently, asymptotic MSE. We provide an efficiency bound, which is a lower bound on the asymptotic variance among regular estimators. As discussed in Uehara et al. (2020), when there are two independent datasets, we cannot apply the usual efficiency bounds developed for a single dataset. To derive efficiency bounds in such settings, existing studies employ the efficiency theory under the stratified sampling scheme (Wooldridge, 2001). Using this scheme, efficiency bounds have been proposed for various settings, including multiple log data, active learning, and learning from positive and unlabeled data. This study also employs this technique to develop efficiency bounds.
The efficiency bounds are derived from the efficient influence functions. Certain efficient influence functions take forms that allow the removal of bias caused by the estimation errors of the nuisance parameters. Debiased machine learning is a framework for estimating treatment effects by utilizing such properties (Chernozhukov et al., 2018). We refer to efficient influence functions with these properties as Neyman orthogonal scores. Chernozhukov et al. (2022b) reframes this framework by characterizing Neyman orthogonal scores using the Riesz representer. Chernozhukov et al. (2021) proposes Riesz regression, an end-to-end method for estimating the Riesz representer. Kato (2025a; b) propose generalized Riesz regression by regarding the Riesz representer estimation problem as Bregman divergence minimization (Sugiyama et al., 2011).
This study generalizes treatment effect estimation under covariate shift (Uehara et al., 2020; Kato et al., 2024) and in the positive-unlabeled (PU) learning setup (Kato et al., 2025). PU learning is a classical problem, originally studied in Imbens & Lancaster (1996), and recently reframed by du Plessis et al. (2015) as a modern statistical machine learning framework. Our sampling scheme arguments are significantly inspired by the works in this literature.
This study is also related to semi-supervised regression, as studied in Azriel et al. (2022) and Kawakita & Kanamori (2013), (Horvitz & Thompson, 1952), and treatment effect estimation with missing values, as studied in Heckman (1974), Robins et al. (1994), and Kennedy (2020). Note, however, that our setting is different from these and more closely related to the literature mentioned in the previous paragraphs.
2 Problem Setting
In this section, we formulate our problem setting. We define potential outcomes and observations separately by following the Neyman–Rubin causal model (Neyman, 1923; Rubin, 1974). Then, we discuss estimating the ATE from the observations.
2.1 Potential Outcomes
There is a binary treatment . Let us define the corresponding potential outcome by . Let be a -dimensional covariate, where is the space. For each , assume that the conditional distribution of given has its density, and let be the probability density function.
2.2 Average Treatment Effect
This study focuses on the estimation of the average treatment effect, which is the expected value of . We take the expectation over a distribution whose covariate probability density is given by
We call it the evaluation covariate density. This density function can differ from . We make the assumptions for in the following sections.
Under a given covariate density , the ATE is defined as follows:
where denotes the expectation taken over the distribution whose covariate density is .
2.3 Observation
This section defines the sample, that is, observations of , , and . To define observations rigorously, we need to consider the censoring setting carefully. To discuss data augmentation within the theory of semiparametric efficiency, we introduce two DGPs. The first DGP is the one-sample scenario, where there is only one dataset, and in this dataset, we randomly choose units who are assigned to treatment or control, that is, units whose outcomes we can observe. We also refer to this setting as the censoring setting. The second DGP is the two-sample scenario, where there are two independent datasets; one of the datasets contains data with covariates, treatment indicator, and outcomes, while the other only contains covariates. We also refer to this setting as the case-control setting or the stratified sampling scheme. We define these two DGPs below.
One-sample scenario.
In the one-sample scenario, we observe a single dataset , defined as follows:
where is an observation indicator, , and is the observable treatment indicator and outcome, defined as
is a treatment indicator, and is the outcome defined as
Here, NA denotes a missing value. Equivalently, we can write as
In this setting, we assume .
Note that and are observable, while and are not observable when .
Two-sample scenario
In the two-sample scenario, we observe two stratified datasets, and :
where and are the sample sizes of each dataset, and is the observed outcome defined as
and is a treatment indicator.
Difference between the two settings
We show an illustration that demonstrates the difference between the one-sample and two-sample scenarios in Figure 1. In both settings, we can identify and estimate the ATE in the standard way if we ignore the unlabeled auxiliary covariates. That is, in the one-sample scenario, we can estimate the ATE only by using , while in the two-sample scenario, we can estimate the ATE only by using . However, we demonstrate that by using the unlabeled auxiliary covariates, we can reduce the asymptotic variance effectively.
A summary of the differences is provided below:
- One-sample scenario:
-
A single dataset is observed, where some observations do not include the treatment and outcome variables (i.e., contain only unlabeled covariates).
- Two-sample scenario:
-
Two separate datasets are observed: one consists of fully labeled data, and the other contains only unlabeled covariates.
Remark (PU learning).
Our terminology of the censoring and case-control settings comes from that in PU learning (Niu et al., 2016). In both settings, the goal is to learn a conditional class probability or a classifier using only positive and unlabeled data. In censoring PU learning, we consider one dataset from which labeled data are observed (Elkan & Noto, 2008). In case-control PU learning, we assume that there exist two independent datasets, one labeled and the other unlabeled (du Plessis et al., 2015). The case-control PU learning setting is also studied in Imbens & Lancaster (1996).
Notations and Assumptions
Throughout this study, let denote the distribution of a random variable . For simplicity, we assume that the distribution of a continuous random variable has a probability density, whose notation depends on the random variable. For a probability density or mass function , we denote the expectation over by . If the dependence is clear from the context, we omit and simply denote it as . Similarly, let be the variance operator. Let us denote the true mean and variance of conditioned on by and , respectively.
We make the following regularity assumption.
Assumption 2.1.
There exist constants and such that and for any , and hold.
3 One-Sample Scenario
First, we consider the one-sample setting for the DGP, which is also referred to as the censoring setting. We redefine the DGP with its notations and assumptions in Section 3.1. Then, for this DGP, we develop an efficiency bound in Section 3.2. We propose our estimator in Section 3.3 and show consistency in Section 3.5 and asymptotic normality in Section 3.6.
3.1 Notation and Assumption
This section introduces and summarizes the notations and assumptions, while recapping the DGP of the one-sample scenario. As defined in Section 2, the DGP of this scenario is
Let be the probability of observation , be the propensity score defined in the observed samples, and let be the joint probability of and . Under this notation, the probability density is written as
For simplicity, we assume that the evaluation density is the marginal density of the covariates.
Assumption 3.1 (Evaluation density in the one-sample scenario).
The evaluation density is given as .
We also make the following assumptions.
Assumption 3.2 (Unconfoundedness and missing at random (MAR)).
It holds that (unconfoundedness) and (MAR).
Assumption 3.3 (Common support).
There exists a universal constant such that for all , holds almost surely.
Note that this assumption also implies the existence of a universal constant such that .
3.2 Efficiency Bound
First, we derive the efficiency bound for regular estimators, which provides a lower bound on asymptotic variances. The efficiency bound is characterized via the efficient influence function (van der Vaart, 1998), derived as follows (proof is provided in Appendix B):
Lemma 3.1.
Recall that . The following proposition is Theorem 25.20 in van der Vaart (1998), which connects the efficient influence function to the efficiency bound.
Proposition 3.2 (Theorem 25.20 in van der Vaart (1998).).
Let be some random variable, and be a model of its DGP. The (semiparametric) efficient influence function is the gradient of with respect to the model , which has the smallest -norm. It satisfies that for any regular estimator of a parameter of interest regarding a given parametric submodel, , where is the second moment of the limiting distribution of .
Using this proposition, we can derive the following efficiency bound from Lemma 3.1.
Theorem 3.3 (Efficiency bound in the one-sample scenario).
Here, note that the efficient influence function depends on the unknown , which are referred to as nuisance parameters. Since the efficient influence function satisfies the equation , if the nuisance parameters are known and the exact expectation is computed, we can obtain by solving for that satisfies this equation. Thus, the efficient influence function provides significant insights for constructing an efficient estimator. Furthermore, the accuracy of the estimation of the nuisance parameters affects the estimation of , the parameter of interest.
3.3 ATE Estimator
Based on the efficient influence function, we propose an ATE estimator defined as
where and are estimators of and . Note that the estimators can depend on . This estimator is an extension of the augmented inverse probability weighting estimator, also called a doubly robust estimator (Bang & Robins, 2005). We say that an estimator is efficient if its asymptotic variance aligns with .
For estimating the regression function , we can employ methods for conditional ATE estimation (Wager & Athey, 2018; Curth & van der Schaar, 2021; Kennedy et al., 2024), as well as standard regression methods using parametric or nonparametric models (Tsybakov, 2008; Schmidt-Hieber, 2020). We can also use targeted maximum likelihood estimation to refine this estimation (van der Laan & Rose, 2011).
For estimating , we can use logistic regression or other advanced methods, such as the covariate balancing propensity score (Imai & Strauss, 2011; Hainmueller, 2012) and Riesz regression (Chernozhukov et al., 2021). As Zhao (2019), Bruns-Smith et al. (2025), and Kato (2025a) show, Riesz regression and covariate balancing methods are in a dual relationship, and Riesz regression can be interpreted as a special case of density ratio estimation (Kato, 2025d). For details, see Section 3.4 and Appendix A.
3.4 Generalized Riesz Regression
We explain how to construct estimators for. In this study, we employ generalized Riesz regression, also referred to as Bregman-Riesz regression (Kato, 2025a; b). In the efficient estimation of causal parameters, Neyman orthogonal scores play an important role and typically correspond to the efficient score. Specifically, asymptotically efficient estimators must be asymptotically linear with respect to the Neyman orthogonal scores. When the parameter of interest is linear in the regression functions, the Neyman orthogonal score can be decomposed into the Riesz representer and regression functions. In our framework, the Neyman orthogonal score is given by
where we replace with in the original definition of , and is the Riesz representer. Riesz regression, as proposed by Chernozhukov et al. (2021), is a method for estimating the Riesz representer in an end-to-end manner. Kato (2025a) shows that Riesz regression is a specific instance of density ratio estimation and can be generalized via Bregman divergence minimization (Sugiyama et al., 2011). Kato (2025b) further reformulates and extends this approach as direct debiased machine learning (DDML) via generalized Riesz regression.
Generalized Riesz regression.
Generalized Riesz regression estimates by minimizing the Bregman divergence between the true Riesz representer and its model . That is, the estimation error of is measured using the Bregman divergence. For a twice differentiable convex function with bounded derivative, the population objective for Riesz representer estimation is written as
| (1) | ||||
| (2) |
The empirical counterpart replaces expectations with sample averages. Here, we used . Minimizing this objective over a hypothesis class yields an estimator of , that is,
where GRR denotes generalized Riesz regression.
The use of generalized Riesz regression allows us to naturally incorporate unlabeled covariates into the estimation of the Riesz representer. This is because, in equation 1, we can approximate using unlabeled covariates, whereas requires labeled data. That is,
where the second term can be evaluated using both labeled and unlabeled data. Note that unlabeled covariates can be utilized even when is estimated via maximum likelihood. However, the generalized Riesz regression approach is arguably more appropriate in an end-to-end formulation. Also see Kawakita & Kanamori (2013).
Let denote the model class for . If we set , then
This population objective corresponds to Riesz regression as in Chernozhukov et al. (2021). We refert to this objective as least squares Riesz (LS-Riesz) regression.
Now, redefine as the set of such that and , a condition that should hold under the common support assumption. For (), the corresponding Bregman divergence is
We refer to this objective as Kullback-Leibler Riesz (KL-Riesz) regression, since the choice of makes yields the KL divergence.
By replacing the expectations with the sample mean and minimizing the empirical objective for , we can estimate .
Interpretation.
As Kato (2025b) discusses, LS-Riesz regression corresponds to the stable balancing weights proposed in Zubizarreta (2015), and KL-Riesz corresponds to the entropy balancing weights in Hainmueller (2012). These correspondences were originally shown in the covariate balancing literature, such as in Zhao (2019) and Bruns-Smith et al. (2025). They can be derived from duality relationships.
Note that the duality depends on the model class used for , namely . For the duality between LS-Riesz and stable balancing weights, linear models must be used for , whereas for the duality between KL-Riesz and entropy balancing weights, logistic models for are required.
3.5 Consistency and double robustness
First, we prove the consistency result, that is, holds as . We can obtain this result relatively easily compared to asymptotic normality. We make the following assumption, which holds for most estimators of the nuisance parameters.
Assumption 3.4.
There exist universal constants such that and hold almost surely. As , either of the following holds for all :
Then, the following consistency result holds. This result is given as a special case of Theorem 3.5; therefore, we omit the proof.
Theorem 3.4 (Consistency in the one-sample setting).
This consistency structure is referred to as double robustness.
3.6 Asymptotic Normality
Next, we establish the asymptotic normality of our estimator. Unlike consistency, this requires stronger assumptions on the nuisance estimators, especially for the propensity score.
To prove asymptotic normality or -consistency, we must control the complexity of the nuisance parameter estimators. One simple approach is to assume the Donsker condition; however, it is well known that this condition often fails in high-dimensional regression settings. In such cases, asymptotic normality can still be attained using sample splitting, a common technique in this field (Klaassen, 1987), which has recently been refined by Chernozhukov et al. (2018) as cross-fitting.
Cross-fitting. We estimate and using cross-fitting. Cross-fitting is a variant of sample splitting (Chernozhukov et al., 2018). We randomly partition into folds (subsamples), and for each fold , the nuisance parameters are estimated using all other folds. Let the estimators for fold be denoted by and . Let be the index set of samples belonging to fold .
Various estimation methods may be used, including neural networks and Lasso, as long as they satisfy the convergence rate conditions in Assumption 3.5. The pseudocode is shown in Algorithm 1.
Asymptotic normality. We present results for the case with cross-fitting, but similar results hold under the Donsker condition.
We make the following assumptions:
Assumption 3.5.
For each , as , the following hold:
-
•
and .
-
•
for .
We define the estimator as
and show that the asymptotic normality holds as follows:
Theorem 3.5 (Asymptotic normality in the one-sample scenario).
The proof is provided in Appendix C. The asymptotic variance of matches the efficiency bound. Therefore, Theorem 3.5 also implies that is asymptotically efficient.
We now discuss alternative ATE estimators.
Remark (Inefficiency of the Inverse Probability Weighting (IPW) estimator).
The IPW estimator is defined as
Unlike our proposed efficient estimator, this estimator does not use the conditional outcome estimators (Horvitz & Thompson, 1952). When and are known, it is unbiased. However, it suffers from a large asymptotic variance:
Here, , with equality when is zero for all . Thus, the IPW estimator is inefficient relative to . Moreover, if is unknown, stronger assumptions are needed to establish asymptotic normality compared to our efficient estimator.
Remark (Regression Adjustment (RA) estimator).
Another alternative is the RA estimator, defined as
also known as the naive plug-in or direct method estimator. Its asymptotic normality heavily depends on the estimators .
4 Two-Sample Scenario
Next, we consider the two-sample scenario for the DGP, which is also referred to as the case-control setting and stratified sampling scheme. We reintroduce the notation and assumptions required for our analysis in Section 4.1. Section 4.2 presents the efficiency bound, and Section 4.3 provides an ATE estimator under this setting. We establish consistency in Section 4.4 and asymptotic normality in Section 4.5. Finally, we compare the one-sample and two-sample scenarios in Section 4.6.
4.1 Notation and Assumptions
As introduced in Section 2, the DGP for the two-sample scenario is defined as
Let denote the propensity score. Then, the joint density can be written as
For the evaluation density, we make the following assumption.
Assumption 4.1 (Evaluation density in the two-sample scenario).
There exists such that
Define the set of such values as
In addition, for all such that , holds.
We assume so that efficiency can be gained by using the unlabeled covariates. If , it merely indicates that only the labeled covariates are informative for ATE estimation.
We also impose the following assumptions.
Assumption 4.2 (Unconfoundedness).
The potential outcomes satisfy .
Assumption 4.3 (Common support).
There exists a universal constant such that .
4.2 Efficiency Bound
Let denote the joint density of . Define the following quantity:
where note that .
Following Uehara et al. (2020), we derive the efficiency bound using the efficiency arguments under the two-sample scenario (stratified sampling scheme). In this scheme, there appear two efficient influence functions for and , respectively. The proof is provided in Appendix D.
Lemma 4.1.
As in the one-sample scenario, the efficient influence function directly yields the following efficiency bound.
Theorem 4.2 (Efficiency bound in the two-sample scenario).
For simplicity, throughout this study, we assume that is known. This assumption is justified because the evaluation density is selected by the researcher, and for specific choices, becomes evident. For example, if , then holds.
In some situations, it may be necessary to estimate empirically using . In such cases, the convergence rate of the estimator for can affect the asymptotic normality results presented below. If is estimated solely from the available samples, the asymptotic normality may fail to hold due to the slow convergence rate. However, if additional information allows us to accelerate the convergence, the asymptotic results can be preserved. Since the feasibility of such acceleration depends on the specific application, we omit further details.
4.3 ATE Estimator
Based on the efficient influence function, we define the estimator as
Here, and denote estimators of and , where and indicate their dependence on each dataset. As in the one-sample setting, these estimators depend on , , , and . However, compared to the one-sample setting, there are more indices involved, which makes the expressions more complicated. Therefore, we suppress the indices whenever their dependence is clear from the context. Unlike in the one-sample scenario, we do not use the observation indicator , since it is deterministically known whether a unit belongs to the treatment or control group. This distinction leads to theoretical differences from the one-sample scenario.
4.4 Consistency
We impose the following assumption.
Assumption 4.4.
As , it holds that or .
Then, the following consistency result holds.
4.5 Asymptotic Normality
Next, we establish the asymptotic normality of the estimator.
Assumption 4.5.
For each , as , the following hold: for each ,
We now establish asymptotic normality in the following theorem, with the proof provided in Appendix E. In this result, we consider the asymptotic regime where the sample sizes and approach infinity while maintaining a fixed ratio .
Theorem 4.4 (Asymptotic normality in the two-sample scenario).
Thus, the proposed estimator is efficient with respect to the efficiency bound derived in Theorem 4.2.
Corollary 4.5.
If almost surely, then , and
4.6 Comparison with the One-Sample Scenario
The difference between the one-sample and two-sample scenarios appears in the formulation of ATE estimators, the setup of Riesz regression, and the corresponding efficiency arguments. In addition, as introduced in Section 5, they lead to different approaches when the amount of unlabeled covariate data far exceeds the amount of labeled data.
5 Infinitely Many Unlabeled Data
In many applications, we often have access to infinitely many unlabeled data points, as they are less costly to collect than fully labeled ones. This section extends the previously discussed methods to the case where the sample size of the unlabeled data grows much faster than that of the labeled data.
Remark.
The idea behind the proposed method in this section is to modify the normalization and asymptotic framework in order to establish asymptotic normality. The methods discussed here are also related to those proposed in Hadad et al. (2021) and Zhan et al. (2024). In those works, the authors consider estimating treatment effects (policy values) using data collected via multi-armed bandits and construct an estimator based on a weighted average of score functions, where the weights are chosen to ensure asymptotic normality even when the sample size for suboptimal arms (treatments) converges to zero at a certain rate. Although their setting, methodology, and theoretical framework differ from ours, we share certain conceptual ideas with their approaches.
5.1 One-Sample Scenario
In the one-sample scenario, estimation with infinitely many unlabeled data points corresponds to the case where diverges much faster than . We consider an asymptotic framework where we first let with fixed, and then let , while normalizing the ATE estimator by .
We do not directly consider asymptotics with respect to . Instead, we examine the limits of and separately, as stated in the following theorem.
Corollary 5.1.
5.2 Two-Sample Scenario
In the two-sample scenario, for the independent datasets and , we first let , and then let . Under this asymptotic regime, we obtain the following result.
Corollary 5.2.
6 Efficiency Gain
By using auxiliary unlabeled covariates, we can reduce the asymptotic variance of ATE estimators. As Hahn (1998) shows, for a labeled dataset , the efficiency bound of ATE estimators is given as , and an efficient ATE estimator satisfies
In both one-sample and two-sample scenarios, we can gain efficiency by using unlabeled covariates. Specifically, while we cannot reduce the term corresponding to , we can reduce the term corresponding to . This efficiency gain becomes more apparent when we consider the case with many unlabeled covariates, as in Section 5.
7 Covariate Shift Adaptation
8 Conclusion
This study investigates semiparametric efficient estimation of ATE when auxiliary unlabeled covariates are accessible. We consider both one-sample and two-sample scenarios, and derive semiparametric efficiency bounds for each. Based on the corresponding efficient influence functions, we construct asymptotically efficient estimators via Neyman orthogonal scores. Our approach leverages generalized Riesz regression for estimating nuisance parameters, allowing flexible incorporation of unlabeled covariates. We further analyze the asymptotic properties under settings where the number of unlabeled samples diverges, and demonstrate the superiority of our estimator in terms of efficiency compared to standard inverse probability weighting and regression adjustment methods. The proposed framework unifies and extends existing methods for treatment effect estimation under covariate shift, missing labels, and semi-supervised settings.
References
- Azriel et al. (2022) David Azriel, Lawrence D. Brown, Michael Sklar, Richard Berk, Andreas Buja, and Linda Zhao. Semi-supervised linear regression. Journal of the American Statistical Association, 117(540):2238–2251, 2022.
- Bang & Robins (2005) Heejung Bang and James M. Robins. Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4):962–973, 2005.
- Bruns-Smith et al. (2025) David Bruns-Smith, Oliver Dukes, Avi Feller, and Elizabeth L Ogburn. Augmented balancing weights as linear regression. Journal of the Royal Statistical Society Series B: Statistical Methodology, 04 2025.
- Chapelle et al. (2006) Olivier Chapelle, Bernhard Schölkopf, and Alexander Zien. Semi-Supervised Learning. MIT Press, 2006.
- Chernozhukov et al. (2018) Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 2018.
- Chernozhukov et al. (2021) Victor Chernozhukov, Whitney K. Newey, Victor Quintas-Martinez, and Vasilis Syrgkanis. Automatic debiased machine learning via riesz regression, 2021. arXiv:2104.14737.
- Chernozhukov et al. (2022a) Victor Chernozhukov, Whitney Newey, Víctor M Quintas-Martínez, and Vasilis Syrgkanis. RieszNet and ForestRiesz: Automatic debiased machine learning with neural nets and random forests. In International Conference on Machine Learning (ICML), 2022a.
- Chernozhukov et al. (2022b) Victor Chernozhukov, Whitney K. Newey, and Rahul Singh. Automatic debiased machine learning of causal and structural effects. Econometrica, 90(3):967–1027, 2022b.
- Curth & van der Schaar (2021) Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneous treatment effects: From theory to learning algorithms. In Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS), 2021.
- du Plessis et al. (2015) Marthinus Christoffel du Plessis, Gang. Niu, and Masashi Sugiyama. Convex formulation for learning from positive and unlabeled data. In International Conference on Machine Learning (ICML), 2015.
- Elkan & Noto (2008) Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In International Conference on Knowledge Discovery and Data Mining (KDD), 2008.
- Hadad et al. (2021) Vitor Hadad, David A. Hirshberg, Ruohan Zhan, Stefan Wager, and Susan Athey. Confidence intervals for policy evaluation in adaptive experiments. Proceedings of the National Academy of Sciences (PNAS), 118(15), 2021.
- Hahn (1998) Jinyong Hahn. On the role of the propensity score in efficient semiparametric estimation of average treatment effects. Econometrica, 66(2):315–331, 1998.
- Hainmueller (2012) Jens Hainmueller. Entropy balancing for causal effects: A multivariate reweighting method to produce balanced samples in observational studies. Political Analysis, 20(1):25–46, 2012.
- Heckman (1974) James Heckman. Shadow prices, market wages, and labor supply. Econometrica, 42(4):679–694, 1974.
- Horvitz & Thompson (1952) Daniel G. Horvitz and Donovan J. Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American Statistical Association, 47(260):663–685, 1952.
- Huang et al. (2007) Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Schölkopf, and Alex J. Smola. Correcting sample selection bias by unlabeled data. In NeurIPS, pp. 601–608. MIT Press, 2007.
- Imai & Strauss (2011) Kosuke Imai and Aaron Strauss. Estimation of heterogeneous treatment effects from randomized experiments, with application to the optimal planning of the get-out-the-vote campaign. Political Analysis, 19(1):1–19, 2011.
- Imbens & Lancaster (1996) Guido W. Imbens and Tony Lancaster. Efficient estimation and stratified sampling. Journal of Econometrics, 74(2):289–318, 1996.
- Imbens & Rubin (2015) Guido W. Imbens and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press, 2015.
- Kanamori et al. (2009) Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct importance estimation. Journal of Machine Learning Research, 10(Jul.):1391–1445, 2009.
- Kato (2025a) Masahiro Kato. Direct bias-correction term estimation for propensity scores and average treatment effect estimation, 2025a. arXiv: 2509.22122.
- Kato (2025b) Masahiro Kato. Direct debiased machine learning via bregman divergence minimization, 2025b. aXiv: 2510.23534.
- Kato (2025c) Masahiro Kato. Nearest neighbor matching as least squares density ratio estimation and riesz regression, 2025c. arXiv: 2510.24433.
- Kato (2025d) Masahiro Kato. A unified theory for causal inference: Direct debiased machine learning via bregman-riesz regression, 2025d.
- Kato & Teshima (2021) Masahiro Kato and Takeshi Teshima. Non-negative bregman divergence minimization for deep direct density ratio estimation. In International Conference on Machine Learning (ICML), 2021.
- Kato et al. (2024) Masahiro Kato, Akihiro Oga, Wataru Komatsubara, and Ryo Inokuchi. Active adaptive experimental design for treatment effect estimation with covariate choice. In International Conference on Machine Learning (ICML), 2024.
- Kato et al. (2025) Masahiro Kato, Fumiaki Kozai, and Ryo Inokuchi. Puate: Semiparametric efficient average treatment effect estimation from treated (positive) and unlabeled units, 2025. arXiv:2501.19345.
- Kawakita & Kanamori (2013) Masanori Kawakita and Takafumi Kanamori. Semi-supervised learning with density-ratio estimation. Machine Learning, 91(2):189–209, 2013.
- Kennedy (2020) Edward H. Kennedy. Efficient nonparametric causal inference with missing exposure information. The International Journal of Biostatistics, 16(1), 2020.
- Kennedy et al. (2024) Edward H. Kennedy, Sivaraman Balakrishnan, James M. Robins, and Larry Wasserman. Minimax rates for heterogeneous causal effect estimation. The Annals of Statistics, 52(2):793 – 816, 2024.
- Kiryo et al. (2017) Ryuichi Kiryo, Gang Niu, Marthinus Christoffel du Plessis, and Masashi Sugiyama. Positive-unlabeled learning with non-negative risk estimator. In Advances in Neural Information Processing Systems (NeurIPS), 2017.
- Klaassen (1987) Chris A. J. Klaassen. Consistent estimation of the influence function of locally asymptotically linear estimators. Annals of Statistics, 15, 1987.
- Lee & Schuler (2025) Kaitlyn J. Lee and Alejandro Schuler. Rieszboost: Gradient boosting for riesz regression, 2025. arXiv: 2501.04871.
- Lin et al. (2023) Zhexiao Lin, Peng Ding, and Fang Han. Estimation based on nearest neighbor matching: from density ratio to average treatment effect. Econometrica, 91(6):2187–2217, 2023.
- Neyman (1923) Jerzy Neyman. Sur les applications de la theorie des probabilites aux experiences agricoles: Essai des principes. Statistical Science, 5:463–472, 1923.
- Niu et al. (2016) Gang Niu, Marthinus Christoffel du Plessis, Tomoya Sakai, Yao Ma, and Masashi Sugiyama. Theoretical comparisons of positive-unlabeled learning against positive-negative learning. In Advances in Neural Information Processing Systems (NeurIPS), 2016.
- Rhodes et al. (2020) Benjamin Rhodes, Kai Xu, and Michael U. Gutmann. Telescoping density-ratio estimation. In Advances in Neural Information Processing Systems (NeurIPS), 2020.
- Robins et al. (1994) J. M. Robins, A. Rotnitzky, and L. P. Zhao. Estimation of regression coefficients when some regressors are not always observed. Journal of the American Statistical Association, 89:846–866, 1994.
- Rubin (1974) Donald B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology, 66:688–701, 1974.
- Schmidt-Hieber (2020) Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation function. Annals of Statistics, 48(4):1875–1897, 2020.
- Sugiyama et al. (2011) Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio matching under the bregman divergence: A unified framework of density ratio estimation. Annals of the Institute of Statistical Mathematics, 64, 10 2011.
- Sugiyama et al. (2012) Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density Ratio Estimation in Machine Learning. Cambridge University Press, 2012.
- Tsybakov (2008) Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Publishing Company, Incorporated, 1st edition, 2008.
- Uehara et al. (2020) Masatoshi Uehara, Masahiro Kato, and Shota Yasui. Off-policy evaluation and learning for external validity under a covariate shift. In Conference on Neural Information Processing Systems (NeurIPS), 2020.
- van der Laan & Rose (2011) M.J. van der Laan and S. Rose. Targeted Learning: Causal Inference for Observational and Experimental Data. Springer Series in Statistics. Springer New York, 2011.
- van der Vaart (1998) Aad W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998.
- Wager & Athey (2018) Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using random forests. Journal of the American Statistical Association, 113(523):1228–1242, 2018.
- Wooldridge (2001) Jeffrey M. Wooldridge. Asymptotic properties of weighted m-estimation for standard stratified samples. Econometric Theory, 2001.
- Yamada et al. (2011) Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama. Relative density-ratio estimation for robust distribution comparison. In Advances in Neural Information Processing Systems (NeurIPS), volume 24. Curran Associates, Inc., 2011.
- Zhan et al. (2024) Ruohan Zhan, Zhimei Ren, Susan Athey, and Zhengyuan Zhou. Policy learning with adaptively collected data. Management Science, 70(8):5270–5297, 2024.
- Zhao (2019) Qingyuan Zhao. Covariate balancing propensity score by tailored loss functions. The Annals of Statistics, 47(2):965 – 993, 2019.
- Zubizarreta (2015) José R. Zubizarreta. Stable weights that balance covariates for estimation with incomplete outcome data. Journal of the American Statistical Association, 110(511):910–922, 2015.
Appendix A DDML
This section explains the DDML framework proposed in Kato (2025b), which refines the arguments about Riesz regression and direct density ratio estimation discussed in Kato (2025a). The core of debiased machine learning is to construct an estimator using the Neyman orthogonal scores (Chernozhukov et al., 2018). For this problem, Kato (2025a; b) establishes the DDML framework, which consists of targeted Neyman estimation and generalized Riesz regression.
A.1 Targeted Neyman estimation
Targeted Neyman estimation formulates the nuisance parameters estimation problem as minimizing the discrepancy between the true Neyman orthogonal scores and their model-based counterparts. Since the Neyman orthogonal score is zero in expectation, we only need to estimate the nuisance parameters so that the sample mean of the Neyman orthogonal score with plug-in parameters is zero. In our setting, we estimate the nuisance parameters and , aiming for to be zero, where , , and are the estimators of , , and . Note that we need to ensure that is asymptotically linear for . As discussed in Kato (2025b), the term is decomposed as
A.2 Iterative Procedure for Regression Function and Riesz Representer Estimation
This section explains an approach for estimating the regression function and the Riesz representer using the iterative procedure proposed in Kato (2025b). We do not adopt this approach in the main text, as it complicates the arguments, but we recommend its use.
Targeted maximum likelihood (TMLE)
We can make the term () zero in expectation, and we can make the term () zero using the TMLE-based ATE estimator. Assume that is known. If we set as
where
and is an initial estimate of . If we set and , the terms () and () are automatically zero.
Iterative Algorithm
As explained above, we have
Therefore, our target is to minimize
Here, there are two problems. In minimizing
we do not know . In addition, in the TMLE part, we do not know .
If we know , we can estimate using the weighted version of generalized Riesz regression proposed in Kato (2025b). In the context of this study, we weight the loss for by . We omit the details here and refer to Kato (2025b) and Kato (2025d).
Finally, we suggest the following iterative procedure for steps, following Kato (2025b):
-
•
Obtain an initial estimate of and denote it by .
-
•
For each ,
-
–
Estimate using weighted generalized Riesz regression with weight .
-
–
Estimate by the TMLE procedure with and as
-
–
A.3 Riesz Regression as Density Ratio Estimation
Riesz regression can be interpreted as a special case of direct density ratio estimation algorithms (Sugiyama et al., 2012; Huang et al., 2007; Kanamori et al., 2009). Therefore, we can employ various estimation techniques as in Yamada et al. (2011), Kiryo et al. (2017), Rhodes et al. (2020), and Kato & Teshima (2021), as well as the methods proposed for Riesz regression (Chernozhukov et al., 2022a) and Lee & Schuler (2025). From this perspective, we can also interpret the nearest neighbor matching ATE estimator as a special case of Riesz regression. These arguments are based on Lin et al. (2023), which finds that the nearest neighbor matching ATE estimator can be interpreted as a density ratio estimation method. For details, see Kato (2025c).
Appendix B Proof for Lemma 3.1: Efficient Influence Function in the One-Sample Scenario
We provide the proof of Lemma 3.1. Our proof strategy is inspired by the approaches in Hahn (1998) and Kato et al. (2025).
Proof procedure
Their proof considers a nonparametric model for the distribution of potential outcomes and defines regular parametric submodels. The procedure involves the following steps: (i) characterizing the tangent set for all regular parametric submodels, (ii) verifying that the parameter of interest is pathwise differentiable, (iii) confirming that the proposed semiparametric efficient influence function lies within the tangent set, and (iv) calculating the expectation of the squared influence function.
Proof.
In Section 2, we defined the probability density function for as
where and are the conditional densities of and . Recall that for each , we have
For this density function, we consider the parametric submodels:
where has the following probability density function:
so that there exists such that
We can define such a parametric submodel, as shown in van der Vaart (1998).
Then, we define score functions (the derivative of the log likelihood function) as follows:
where
Using the parametric submodels and their score functions, we denote the tangent space as .
Under the parametric submodels, We redefine the ATE as a function of as
Them, the derivative of the ATE function is given as
where
and .
From the Riesz representation theorem, there exists a function such that
| (3) |
There exists a unique function such that , called the efficient influence function. We specify the efficient influence function as
We prove that is actually the unique efficient influence function by verifying that satisfies equation 3 and .
Proof of equation 3:
We have
where we used
Finally, we have
Proof of :
Set
Then, holds. ∎
Appendix C Proof of Theorem 3.5: Efficient ATE Estimator under the One-Sample Scenario
For simplicity, we consider two-fold cross-fitting; that is, . Without loss of generality, we assume that the sample size is even, and let . For each , we denote the subset of the dataset in cross-fitting as
We defined the estimator as
where recall that
We have
Here, if it holds that
| (4) |
then we have
from the central limit theorem for i.i.d. random variables.
Let denote the -th fold of . Here, we have
To show equation 4, we show the following two inequalities separately:
| (5) | ||||
| (6) |
Here, the LHS of the first inequality is referred to as the empirical process term, while the LHS of the second inequality is referred to as the second-order remainder term.
C.1 Proof of equation 5
Proof.
We aim to show that for any ,
| (7) |
We show equation C.1 by showing that for any ,
| (8) |
We prove equation C.1 using Chebychev’s inequality. From Chebychev’s inequality we have
Since observations are i.i.d. and the conditional mean of the target part is zero, we have
| (9) | ||||
The term equation 9 converges to zero in probability as if
as . Here, we used the boundedness conditions of each function and the following computation. Them, we complete the proof.
We explain the last step of the above proof below. Let and denote the first and second terms in the expectation of equation 9, respectively. Then, we have
Here, we have
By showing that , we prove the statement. To show , we use the following concrete form of :
Then, we have
Here, we can show that the following term converges to zero in probability, which follows directly from the convergence in probability of each nuisance-parameter estimator:
Then, we show that the remaining parts converge to zero in probability. Let us denote the parts as
Next, we have
Then, from the parallelogram law, we have
Here, we can bound
by
where is constant independent of , and we used the boundedness of and . Similarly, we can bound each of the remaining terms. Thus, we complete the proof. ∎
C.2 Proof of equation 6
Proof.
We have
where we used Hölder’s inequality. ∎
Appendix D Proof of Lemma 4.1: Efficient Influence Function in the Two-Sample Scenario
This section provides the proof of Lemma 4.1, which gives the efficient influence function in the two-sample scenario. Our proof is inspired by those in Uehara et al. (2020) and Kato et al. (2024).
As defined in Section 2, the sampling scheme of the two-sample scenario assumes the availability of the following two datasets:
where
D.1 Parametric Submodels
We consider regular parametric submodels with a parameter . We denote parametric submodels whose probability density functions are given as
| (10) |
and . Under the parametric submodels, is the conditional probability density function of given , is the propensity score, is the probability density of in observation data, and is the probability density of in evaluation data. Then, the score function are given as
and .
D.2 ATE Function
Next, we consider the ATE under the parametric submodel. Under our defined parametric submodel, let be our parameter of interest as a function of , which is given as
We have the following lemma for the derivative of .
Lemma D.1.
In the two-sample scenario, there are two independent datasets, which makes the arguments about the semiparametric efficiency difficult. To address this issue, we regard the whole samples as one observation and consider the case where we observe observations; that is, for each , we define a sample as
and we assume that we can observe a dataset . Here, is generated from a distribution whose probability density function is given as
The corresponding parametric submodels are given as
Then, the corresponding score functions are given as
where , , , and are defined as
The tangent set for this parametric submodel at is given as
where we have
Here, is pathwise differentiable at if there exists a function such that and for all regular parametric submodels
| (11) |
We refer to as the semiparametric efficient influence function. As the following proposition, the semiparametric efficient influence function gives a lower bound for the asymptotic variance of regular estimators.
This states that is a lower bound in estimating . We call the semiparametric efficiency bound because what we want to consider is a lower bound of .
Lastly, we derive the semiparametric efficient influence function in the following lemma.
Lemma D.2.
Under the parametric submodel defined in equation 10, we have
For , the (scaled) efficiency bound is
D.3 Proof for Lemma D.2
We prove Lemma D.2 using the techniques of Hahn (1998) and Uehara et al. (2020). The basic procedure follows that in Appendix D.
We begin by considering a nonparametric model for the distribution of potential outcomes and define regular parametric submodels. We then compute the gradient (as a candidate for the semiparametric efficient influence function) of the target parameter , characterize the tangent set for all regular parametric submodels, verify that the parameter of interest is pathwise differentiable, confirm that the efficient influence function lies in the tangent space, and calculate the expected squared value of the influence function.
As discussed in Appendices D.1, we define parametric submodels as
Then, recall that in Appendix D.1, we define the score function as
where recall that , , , and are defined as
Then, based on the above preparation, we prove Lemma˜D.2 by guess and verify.
Proof of Lemma˜D.2.
Let us denote the expectation taken over the joint distribution of by . We first verify that . Here, we have
Therefore, we have
We also have
Hence, we have
Under our choice of , the LHS of equation D.3 becomes
Since data are generated independently, and holds, we have
For , it holds that
and
Therefore, we have
Here, from Lemma D.1, we have
Lastly, we verify that . We define the functions as
Then, . The proof is complete. ∎
Appendix E Proof of Theorem 4.4: Efficient ATE estimator under the Two-Sample Scenario
Recall that we have defined the ATE estimators as
We aim to show
Recall that
We have
Here, if it holds that
| (13) | ||||
| (14) |
then we have
from the central limit theorem for i.i.d. random variables. We show equation 13 and equation 14 using the same argument as in the proof of Theorem 3.5 and the analysis of double machine learning under the stratified scheme provided in Uehara et al. (2020). As the procedure is nearly identical to the proof in the one-sample scenario, we omit further details. By taking minimizing the variance, we obtain Theorem 4.4.
Unlocking the Regression Space
Abstract
This paper introduces and analyzes a framework that accommodates general heterogeneity in regression modeling. It demonstrates that regression models with fixed or time-varying parameters can be estimated using the OLS and time-varying OLS methods, respectively, across a broad class of regressors and noise processes not covered by existing theory. The proposed setting facilitates the development of asymptotic theory and the estimation of robust standard errors. The robust confidence interval estimators accommodate substantial heterogeneity in both regressors and noise. The resulting robust standard error estimates coincide with White’s (1980) heteroskedasticity-consistent estimator but are applicable to a broader range of conditions, including models with missing data. They are computationally simple and perform well in Monte Carlo simulations. Their robustness, generality, and ease of implementation make them highly suitable for empirical applications. Finally, the paper provides a brief empirical illustration.
Keywords: robust estimation, regression space, structural change, time-varying parameters, non-parametric estimation
JEL Classification: C13, C14, C50
1 Introduction
Regression analysis is the cornerstone of statistical theory and practice. Ordinary least squares (OLS) has been applied, within various regression contexts, to build an extensive toolkit, for the exploration of economic and financial datasets. The basic theory underlying OLS estimation and inference in regression models has been largely established for over half of a century (see e.g. Lai and Wei (1982)). The problem of robust estimation has long been a focus of empirical work in economics, beginning with the seminal work by White (1980). Its importance is well understood in applied econometrics. At the same time, several important concerns have been raised by applied researchers. Angrist and Pischke (2010) noted that “Leamer (1983) diagnosed his contemporaries’ empirical work as suffering from a distressing lack of robustness to changes in key assumptions”, and Leamer (2010) later reflected that “sooner or later, someone articulates the concerns that gnaw away in each of us and asks if the Assumptions are valid.” Similarly, Karmakar et al. (2022) observed, that the assumption of parameter constancy, or “stationarity is often an oversimplified assumption that ignores systematic deviations of parameters from constancy”. Clearly, this concern extends beyond parameter stability to encompass the stability of regressors, regression noise, and the underlying modelling assumptions.
In this paper, we focus on the inherent capacity of regression modelling to accommodate the effects of structural change in settings with both fixed and time-varying parameters. Many such structural changes influence not only the model parameters but also the regression space itself. This space comprises both the regressors and regression noise, and improper treatment of these components may result in incorrect inferences, misinterpretations, and forecasting distortions. We therefore examine which specifications of regression space can flexibly account for structural change while still enabling estimation of both fixed and time-varying regression parameters, the construction of confidence intervals, and the computation of standard errors.
Among recent developments, Wu (2005), Hall et al. (2012), and others, have proposed advanced theoretical methods for the estimation of the fixed parameters, while Cattaneo et al. (2018), Jochmans (2019) developed procedures to estimate both fixed parameters and standard errors in regression models with an increasing number of covariates and heteroscedasticity. Meanwhile, Li et al. (2020), Sun et al. (2021) and Linton and Xiao (2019) introduced new modelling frameworks that explicitly account for structural change. A common response to concerns about heteroskedasticity in the recent literature is the use of heteroscedasticity-robust variance and standard error estimators for linear regression models, see Eicker (1963), White (1980), MacKinnon (2012) and Cattaneo et al. (2018), among others.
There is also a sizeable and growing literature on the estimation of time-varying coefficient regression models, including works of Fan and Zhang (1999), Vogt (2012), among others. This literature further explores tests for different types of parameter variation, see e.g. Bai and Perron (1998), Zhang and Wu (2012), Zhang and Wu (2015), Hu et al. (2024). In addition, specification tests and tests for parameter instability have received significant attention, with important contributions by Hansen (2000), Georgiev et al. (2018), Hidalgo et al. (2019), Boldea et al. (2019), Fu et al. (2023), and others.
The modelling of deterministic, smooth parameter evolution has a long history in statistics. Early examples include linear processes with time-varying spectral densities, introduced by Priestley (1965). This framework is essentially nonparameteric and it has been further developed by Robinson (1989), Robinson (1991), Dahlhaus (1997), Dahlhaus et al. (2019), Dahlhaus and Richter (2023), some of whom refer to these processes as locally stationary. The estimation of time-varying parameters, as well as fixed parameters under heteroskedasticity in time series models, has been studied in Dahlhaus and Giraitis (1998), Xu and Phillips (2008), Giraitis et al. (2020), among other. Nonlinear time-varying time series models have also been developed by Doukhan and Wintenberger (2008), Bardet and Wintenberger (2009), Vogt (2012) and Karmakar et al. (2022). Despite these advances, such approaches have not been not been widely adopted in applied economics, where random coefficient models remain more prevalent.
Various methods have been proposed over the years to identify and handle structural change. Early contributions assumed that changes were deterministic, rare, and abrupt. Testing for parameter breaks dates back to the pioneering work by Chow (1960), with further contributions by Brown et al. (1975), Ploberger and Krämer (1992), among others. More recent approaches allow for random evolution of parameters, where changes may be discrete, as in Markov Switching models by Hamilton (1989) or threshold models by Tong (1990), or continuous as in smooth transition models by Terasvirta (1998), or those driven by unobservable shocks, as in random coefficient models by Nyblom (1989a). For example, Cogley and Sargent (2005) use random coefficient models to study stochastic volatility, while Primiceri (2005) examines whether changes in parameters or in the variance of shocks - policy-induced or otherwise - contributed to the period of macroeconomic calmness known as the “Great Moderation” after 1985. In these frameworks, parameters typically evolve as random walks or autoregressive processes.
Building on this literature, Giraitis et al. (2014), Giraitis et al. (2018), Dendramis et al. (2021), and others have developed a theoretical time series framework for random coefficient models and their estimation using kernel-based methods, which performs well in finite samples. These methods are computationally simple and straightforward to implement in applied research. For example, Chronopoulos et al. (2022) demonstrated the empirical prevalence of persistent volatility, suggesting that GARCH-type volatility structures may be less common than previously thought. Nevertheless, a full treatment of estimation and inference within a general regression framework has, surprisingly, not yet been provided.
In this paper, we provide a rigorous validation of the asymptotic normality of the feasible -statistic for the estimation of both fixed and time-varying parameters in linear regression models within an extended regresion space of regressors and regression disturbances. Our main objective is to describe, in transparent terms, the extended regression space under which such normality is preserved. The class of admissible regressors and regression noises is broad. Regressors are obtained by rescaling and shifting stationary short-memory sequences, while regression errors are generated by arbitrary rescaling of a stationary martingale difference sequence. The restrictions imposed on the scale factors and mean processes are weak, allowing these to be either deterministic or stochastic, and to vary over time, possibly abruptly or through non-stationary (e.g. unit-root) dynamics. Some assumptions on the scale factors are necessary and resemble the Lindeberg condition in the classical Lindeberg–Feller central limit theorem. Importantly, the robust feasible -statistic retains the same form and limiting distribution as in the standard setting. The infeasible robust standard errors coincide with the heteroskedasticity-consistent standard error estimator of White (1980). Our assumptions do not rely on mixing or near-epoch dependence conditions, which prevail throughout the existing literature. Given the generality of the regression space, these assumptions typically require no additional empirical verification.
The estimation framework for fixed regression parameters is developed in Section 2, which introduces the extended regression space, the underlying assumptions, and the main theoretical results. Section 3 establishes the estimation theory for time-varying regression parameters within the same framework. The proofs highlight how the results for the fixed-parameter case naturally extend to time-varying settings, with only negligible additional terms.
Our results are complementary to existing frameworks. The novelty lies in providing a methodological foundation that confirms the validity of robust regression estimation in an extended regression space. The fundamental theory in this area traces back to Lai and Wei (1982), who studied regression models with heteroskedastic martingale difference noise under eigenvalue-based assumptions. Alternative methods, such as bootstrap procedures, see Hall et al. (2012); Boldea et al. (2019), are widely used in regression analysis but may not be directly applicable to such a general class of regressors and regression noises. In contrast, we demonstrate that White-type standard errors remain applicable and computationally straightforward.
All theoretical results are supported by detailed, rigorous proofs. Monte Carlo simulations confirm that the proposed robust regression estimators perform well in finite samples. Overall, the framework developed in this paper is particularly suited to modelling economic and financial data, where heterogeneity, structural change, and dependence are inherent features.
The remainder of this paper is organised as follows. Section 2 presents the regression setting with the extended regression space, accommodating heterogeneity and dependence, and outlines the theoretical results for infeasible and feasible -statistics in the case of fixed parameters. Section 3 extends the analysis to the time-varying regression parameters. Section 4 addresses regression modelling with missing data patterns. Section 5 illustrates the flexibility of our robust estimation method by its application to the estimation of an AR(p) model generated by a stationary martingale difference noise. Sections 6 presents Monte Carlo simulation results. In Section 7, we provide an empirical application of the robust regression framework to modelling asset returns. Finally, Section 8 concludes. Proofs and additional simulations are provided in the Supplemental Material.
2 OLS estimation in general regression space
In this section, we focus on ordinary least squares (OLS) estimation in an environment that permits general heterogeneity in regression modelling. We analyze the model
| (1) |
where is a -dimensional parameter vector, is a stochastic regressor and is an uncorrelated noise term. To include an intercept, the first component can be set as . We refer to the collection of jointly as “the regression space”.
An applied researcher may want to work within a regression space that accommodates a wide range of regressors and regression noises, without being hindered by restrictive technical assumptions. Ideally, such a setting should permit regressors exhibiting non-stationarity and undefined generic structural change, while enabling estimation and inference under weak theoretical constraints that do not require empirical verification.
Our goal is to extend the OLS estimation procedure to a broad regression framework defined by baseline assumptions aligned with empirical research practice. These assumptions cover a wide variety of types of potentially non-stationary regression variables encountered in applied work. The framework achieves a level of generality comparable to that in Giraitis et al. (2024), which addresses testing for absence of correlation and cross-correlation under general heterogeneity.
We begin with specifying the structure of an uncorrelated regression noise . Suppose that
| (2) |
where is a zero mean stationary uncorrelated martingale difference noise, and is a deterministic or stochastic scale factor independent of . The following assumption formalizes these conditions.
Assumption 2.1.
is a stationary martingale difference (m.d.) noise with respect to some -field filtration , such that
The sequence is independent of . Moreover, variable has a probability density function satisfying for all , for some .
The information set is generated by the past history and possibly other variables.
A typical example of an m.d. noise in applied work is provided by the ARCH/GARCH family and the class of stochastic volatility processes. The specification (2) therefore allows for conditional heteroskedasticity in .
We next specify the regressors which form the key structural component of our regression space. For , the regressors can be written as
| (3) |
where is a stationary sequence, are deterministic or stochastic scale factors, and is a vector of deterministic or stochastic means. We assume that are independent of . To include an intercept in model (1), we set
| (4) |
We further suppose that in (3) except for the intercept (4), where .
In summary, the admissible regressors in our setting are obtained by shifting and rescaling a short-memory stationary process by the mean process and the scale factor :
The underlying stationary sequence is the fundamental component structuring regressors . Estimation of the regression parameter requires only mild assumptions on , and short-memory dependence assumption on , satisfied by ARMA and related stationary time series models. This framework eliminates the need for additional empirical validation.
Definition 2.1.
A (univariate) covariance stationary sequence has short memory (SM) if
Assumption 2.2.
is an measurable sequence with and .
(i) For , the sequences and are covariance stationary and have short memory (SM).
(ii) The matrix is positive definite.
The novelty of this regression framework lies in the structural specification (3), which accommodates regressors that may be deterministic or stochastic, and stationary or non-stationary. This flexibility arises from allowing a broad class of scale factors and mean processes which brings the OLS estimation closer to empirical practice.
The estimation framework also accommodates triangular arrays of means and scale factors: . Throughout the paper, we assume that these quantities may depend on the sample size . For brevity of notation, the subscript is omitted.
The underlying stationary noise component in the regressors in (3) is weakly exogenous with respect to the stationary m.d. noise in . The mean and scale factors are independent of , though they may be dependent on . Overall, are mutually independent of , while potential dependence among , and is unrestricted.
The processes and can be interpreted as conditional mean and variance, , and of , where denotes the information set generated by the means and scale factors.
Denote for ,
| (5) | |||||
We write if and .
Assumption 2.3.
The scale factors and are deterministic or stochastic non-negative variables such that, for ,
| (7) | |||||
Assumptions (7)–(7) impose only mild restrictions on the means and scale factors . In particular, condition (7) resembles the Lindeberg condition in the classical Lindeberg–Feller central limit theorem, as it excludes the possibility that the OLS estimation is dominated by a single extreme observation of .
The first restriction on in (7) is necessary. For example, consider the regressor , with scale factors and , so that . In this case, the OLS estimator of is inconsistent, and such a scale factor does not satisfy (7).
The second condition (7) ensures that OLS estimation is driven by the stochastic component of the regressor , rather than by deterministic or stochastic drift in .
In the presence of an intercept, condition (7) further implies that , since , , , and .
To estimate , we use the standard OLS estimator
| (8) |
computed from the sample .
Consistency. We first establish the consistency of the OLS estimator .
Theorem 2.1.
This result implies that the -th component of the OLS estimator is -consistent, that is, . The convergence rate may deviate from the conventional rate and may differ across components. From the definition of and , it follows that
| (10) |
Asymptotic normality. The asymptotic normality of an element of the OLS estimator, as well as the computation of its standard errors, requires additional assumptions on the scale factors and the stationary processes .
Assumption 2.4.
(i) For , the sequences , and are covariance stationary and have short memory (SM). (ii) For
| (11) |
Assumption 2.4 is not required when is i.i.d. Together, Assumptions 2.3 and 2.4(ii) exclude cases in which the mean process or a few extreme observations of or , dominate the estimation of the regression parameter. Overall, these assumptions are mild. They accommodate both deterministic and stochastic means and scale factors that may change abruptly over time unlike other theoretically rigorous treatments which restrict structural change to be deterministic and smooth. This flexibility makes the framework particularly suitable for modelling financial data, as it allows for volatility jumps, commonly observed in empirical finance (see, e.g., Eraker et al. (2003)). In modern macroeconomic VAR models, the scale factor in the uncorrelated noise representation is typically assumed to be stochastic (see, e.g., Chan et al. (2024), Carriero et al. (2024)), which our framework naturally encompasses.
Lemma 2.1 below shows that Assumptions 2.3 and 2.4(ii) holds for regressors and noises with bounded moments satisfying (10). The following example provides additional sufficient conditions.
Example 2.1.
Assumptions 2.3 and 2.4(ii) are satisfied by regressors and noises whose scale factors and means satisfy where do not depend on or .
When , for all , Assumptions 2.3 and 2.4(ii) hold for scale factors satisfying
This condition is, for example, satisfied when follows a unit root process defined by , where } is a sequence of i.i.d random variables with finite moments of order . The idea of modelling parameters as unit root processes was discussed, for example, in Nyblom (1989b).
We now describe the infeasible standard errors using the notation:
| (12) |
where denotes the -th element of the matrix . The infeasible standard error of is defined as , i.e., the square root of the corresponding diagonal element of .
The generality of our regression setting limits the multivariate asymptotic theory that can be established for . While a full joint distribution of is not available, we can derive asymptotic normality for linear combinations and then construct feasible inference for individual component .
Existing literature typically imposes stronger assumptions on regressors and errors such as mixing regressors (White, 2014, Theorem 3.78), locally stationary regressors in (Zhang and Wu, 2012, eq. (2.3)), or near-epoch dependent errors in (Hall et al., 2012, Assumption 8).
Theorem 2.2.
Suppose that the assumptions of Theorem 2.1 and Assumption 2.4 hold. Then, for any , the OLS estimator satisfies
| (13) |
In particular, for , the -statistic for satisfies
| (14) |
Property (13) is difficult to implement in practice because it requires estimation of the unknown matrices , except in the special case with only the -th element nonzero. In this case, (13) reduces to (14), and the infeasible standard error can be consistently estimated by
| (15) |
The feasible standard error is the square root of the diagonal element of .
Corollary 2.1.
Under the assumptions of Theorem 2.2, for , as ,
| (16) |
This result is the main contribution of Section 2. It enables straightforward computation of standard errors and the construction of confidence intervals for in the extended regression framework. Notably, the estimator coincides with the heteroskedasticity-consistent standard error estimator of White (1980).
Remark 2.1.
The consistency rate for the parameter may take the form for any , ranging from super-slow to super-fast convergence. To illustrate this, consider the regression model
where , , and , , are i.i.d. . Then and for , producing different convergence rates across parameters. Even in this simple case, the usual multivariate asymptotic normality for does not hold.
Corollary 2.1 allows us to establish the asymptotic power and consistency of a test for testing the hypothesis
i.e., whether the -th element of the regression parameter is equal to a specific value .
Corollary 2.2.
Suppose that . Then, under the assumptions of Corollary 2.1,
| (17) |
We conclude this section with a lemma that provides simple sufficient moment-type conditions for the validity of Assumptions 2.3 and 2.4(ii). In particular, condition (10) implies (19).
Lemma 2.1.
In particular, (19) is satisfied if ,
The regular estimator of standard errors in OLS regression estimation is given by
| (20) |
Unlike the robust standard errors , these conventional standard errors may produce coverage distortions, particularly when heteroskedasticity or heterogeneity in , , or is present, see Section 6. This underscores the robustness and strong empirical performance of the normal approximation in (16).
In this section, we have provided a rigorous validation of the asymptotic normality of feasible -statistics for the components of the OLS estimator in linear regression models with general heterogeneity. The assumptions imposed are mild yet flexible, allowing a wide class of (possibly nonstationary) regressors and noise processes beyond those typically considered in the existing literature. Some conditions on scale factors are analogous to the Lindeberg condition and remain necessary. Our framework complements, rather than replaces, prior approaches; for instance, near-unit-root regressors Georgiev et al. (2018) require a distinct theoretical treatment. Although bootstrap methods, see, e.g., Hall et al. (2012), Boldea et al. (2019), are widely applied in regression analysis, they may not extend to the heterogeneous structures considered here. By contrast, we demonstrate that the heteroskedasticity-consistent standard errors of White (1980) remain applicable and computationally straightforward.
In this paper we focus on the regression model (1), where the regression noise in (1) is uncorrelated. Extending the asymptotic theory to account for dependence in is a natural next step and is currently under consideration.
Detailed proofs of all results are provided in the Online Supplement.
3 Time-varying OLS estimation in extended regression space
This section demonstrates further advantages of the theory of regression estimation with a fixed parameter, developed in Section 2. Thanks to the flexible setting, estimation of time-varying parameters naturally follows from our theory for fixed-parameter regression in the extended space, along with bounding of some negligible terms.
In the previous section, we discussed the estimation of the regression model (1), , with a fixed parameter . We now extend the model by allowing the regression parameter to vary over time. Specifically, we consider the model
| (21) |
where the regressors and the regression noise , as defined in (3) and (2), remain unchanged. That is, they belong to the same regression space as in Section 2.
The primary objective is to develop a point-wise estimation procedure for the path of the time-varying parameter in model (21), while preserving the same regression space introduced in Section 2.
The literature on estimation of time-varying regression parameters is extensive. It primarily focuses on estimation and testing for parameter stability under relatively strong assumptions on the regressors and regression noise. For instance, regressors are assumed to be locally stationary in (Vogt (2012), model (3)), stationary and strongly mixing in (Fu et al. (2023), Assumption A.1) and strictly stationary in (Hu et al. (2024), Assumption P). It is clear that the class of regressors considered in our setting is broader, and they may be neither mixing nor stationary.
The objective of this section is to describe the extended regression space of regressors and disturbances that ensures the asymptotic normality of the feasible -statistic estimating the components of the time-varying parameter . We show that, as long as the regressors and the disturbance follow the structure and , the class of admissible means and scale factors is very broad and characterized by weak restrictions that may not require empirical verification.
Further extensions of the regression space are possible. For example, the weakly exogenous component of the regressors in our paper is assumed to be a short-memory process. In contrast, Hu et al. (2024) demonstrate that estimation of the time-varying parameter also permits weakly exogenous, strictly stationary regressors that exhibit long-memory behavior.
While most assumptions on the regressors and regression noise remain unchanged from Section 2, the estimator requires some modifications. Under an additional smoothness assumption on , the time-varying OLS estimator of parameter at time is the standard OLS estimator for a fixed regression parameter, obtained by regressing on :
| (22) |
The weights are generated as follows:
| (23) |
where is a bandwidth parameter such that and . The kernel function is bounded and there exist and such that
| (24) | |||||
For example, (24) is satisfied by functions and where is the probability density function of the standard normal distribution.
We impose a smoothness assumption on the time-varying parameter , which may be either deterministic or stochastic.
Assumption 3.1.
For some and for ,
| (25) |
where does not depend on .
Next, we briefly outline how our asymptotic theory for the time-varying robust estimator builds on the results from Section 2 on fixed-parameter regression estimation and the smoothness assumption (25). To demonstrate this, we introduce the following regression model with a fixed parameter :
| (26) |
Notice that the OLS estimator of the fixed parameter satisfies:
| (27) |
Since , the time-varying estimator given in (22) satisfies:
| (28) | |||||
Notice that in (28) does not depend on . Additionally, the regression space in estimation of the fixed parameter in Section 2 permits rescaling, so premultiplying by the kernel weights does not change the structure of regressors and : they still satisfy the settings (3) and (2). Consequently, the model (26) is covered by the regression model (1) with a fixed parameter, and the asymptotic results for follow from Section 2. The main technical task in this section is to show that the remainder term in (28) is negligible, which follows from the smoothness assumption (25).
The regressors and regression noise belong to the same regression space as defined in as in Section 2. While the assumptions on the stationary process and the martingale difference noise remain unchanged, for simplicity, we replace the previous conditions on the scale factors and the means with simple sufficient assumptions similar to those used in Lemma 2.1. As before, the scale factors can be deterministic or stochastic, may vary with , and are independent of .
Denote
Assumption 3.2.
and are such that, for ,
| (29) | |||||
| (30) |
where does not depend on .
It is straightforward to show that (30) is valid if for all .
To describe the infeasible standard errors , we use:
where denotes the -th element of the matrix . The infeasible standard error is defined by the diagonal element of the matrix .
The next theorem establishes the consistency rate and asymptotic normality property for the components of the time-varying OLS estimator , and allows for arrays of integers , which may depend on .
Theorem 3.1.
The consistency rate in (31) is determined by the bandwidth parameter and the smoothness parameter in (25). The condition ensures that in (32) the bias term remains negligible.
As in the fixed-parameter case, for from the extended regression space, the asymptotic normality can be established in point-wise estimation for each individual component of .
The unknown standard error can be consistently estimated by:
| (33) |
The feasible standard error is defined by the diagonal element of .
Corollary 3.1.
Under assumption of Theorem 3.1, for , and it holds:
| (34) |
Corollary 3.1 allows us to establish the asymptotic power of the test of the hypothesis
based on the -statistics .
Corollary 3.2.
Suppose that for as . Then, under assumption of Corollary 3.1,
| (35) |
The estimator used to obtain robust standard errors in (33) is a time-varying version of heteroskedasticity-consistent estimator of standard errors by White (1980). Simulation results confirm that it does not produce coverage distortions in the estimation of under the settings considered in this section.
In conclusion, we provide examples of smoothly varying deterministic and stochastic parameters that satisfy Assumption 3.1.
Example 3.1.
A standard example of a deterministic time-varying parameter which satisfies Assumption 3.1, is , , where is a deterministic smooth function that has property . Such satisfies (25) with .
A standard example of a stochastic smooth parameter is a re-scaled random walk , , where is an i.i.d. sequence with and . It satisfies (25) with , that is for ,
The above results are equipped with thorough and mathematically rigorous proofs, which can be found in the Online Supplement.
The key new features in the estimation of time-varying parameter are similar to those highlighted in the estimation of the fixed parameter in Section 2. Although the computation is straightforward, establishing the validity of the robust standard errors in the extended regression space of is challenging because the scale factors in model (21) are unknown and potentially random, and highly general, while the asymptotic behaviour of the may not be well-defined. The asymptotic normality of a single component of the estimator can still be established, even though a full multivariate asymptotic theory is not available. Unlike most existing literature, is permitted to evolve as a smoothly varying stochastic process.
4 Regression with missing data
In the previous sections, we showed that the extended regression space enables the estimation of both fixed and time-varying regression parameters. It offers several theoretical advantage, in particular, the ability to estimate regression models in the presence of missing data. Given the importance in empirical regression analysis in situations where some observations or regressors are missing, see, e.g., Enders (2022), we now present new and somewhat unexpected results on regression estimation with missing data. We show that the foundational assumptions underlying the constriction of regression space also allow us to accommodate an a broad range of missing data patterns.
In this section we suppose that instead of the full sample , we observe a subsample
| (36) |
of dependent variable and regressor . Our primary interest is to estimate both fixed and time-varying regression parameters using the subsample (36).
To that end, we represent the observed data as partially observed sample
| (37) |
where is missing-data indicator. In (36) it is defined as
| (38) |
We set if both and are observed, otherwise . Throughout this section, is treated as a sequence of random or deterministic variables, allowing for regularly missing, block-wise missing, or randomly missing data patterns.
In order for the theoretical results of the previous section to apply, we impose the following assumptions on the missing data indicator , the regressors in (3) and the regression noise in (2).
Assumption 4.2.
(i) and for some , where does not depend on .
(ii) and , where does not depend on .
Estimation of a fixed parameter. Suppose that follows the regression model (1) with a fixed parameter as in Section 2. Our primary interest is to estimate the parameter using subsample (36). In view of (1), we can write the partially observed regression model as
| (39) |
In (39), the regressors and the noise can be represented as
| (40) | |||||
They belong to the regression space described in (2) and (3). Therefore, parameter and the correspondent standard errors in model (39) can be estimated using the OLS estimator and :
Theorem 4.1.
Remark 4.1.
Theorem 4.1 shows that ignoring missing data does not affect the estimation of the fixed parameter. That is, the researcher can compute the estimators and directly using subsample :
Estimation of a time-varying parameter. Assume now that follows the regression model (21) with time-varying parameter , where regressors and regression noise are as in (3) and (2). We are interested in estimating the parameter in the presence of missing data using the subsample (36). Similarly to (39), we base the estimation on the partially observed regression model with a time-varying parameter,
| (43) |
where regressors and the noise are defined as in (40). They belong to the regression space described by (2), and (3) and thus results of Section 3 on the estimation of time-varying parameter apply.
We show in the following theorem that under Assumptions 4.1 and 4.2, parameter and standard errors can be estimated point-wise at each time provided that the missing data pattern satisfies the following condition:
| (44) |
This condition holds, for example, if for for some .
5 Estimation of a stationary AR() model with an m.d. noise
In this section we focus on another practical application of our regression framework developed in Section 2. We show that it covers the estimation of parameters of a stationary AR() model driven by a stationary martingale difference noise :
| (49) |
where parameters are such that the model (49) has a stationary solution. Xu and Phillips (2008) developed estimation theory for AR model , when where is smoothly varying deterministic sequence and a m.d. sequence has property a.s. Giraitis et al. (2018) were among the first to analyze the distortions of standard errors caused by m.d. noise in estimation of ARMA models. This paper shows that the variance of the parameter vector converges to a well-defined limit; however, its complex structure complicates the estimation of the limiting variance and the corresponding standard errors in empirical applications. They restricted the estimation of standard errors to AR(1) and MA(1) models. In the case of AR model, using our method we are able to estimate standard errors for any without relying on asymptotic approximations which is the main novelty and contribution of this section. Notice that the model (49) can be written as a special case of the regression model (1),
| (50) |
Here, the parameter is fixed, and the regressors are stationary random variables. It is straightforward to verify that the regressors
for satisfy the regression assumption (3). In the theorem below, we assume that the standard stationarity conditions on parameters of the AR() model (49) are satisfied, see e.g. Theorem 3.1.1 in Brockwell and Davis (1991), which ensure the existence of a stationary solution
| (51) |
We assume that satisfies Assumption 2.1 and satisfy Assumptions 2.2 and 2.4(i). These assumptions impose only mild restrictions on the m.d. noise , and their validity can be verified for typical examples of uncorrelated m.d. noise, such as ARCH-type processes.
Theorem 5.1.
The Monte Carlo results presented in Section 6.4 demonstrate that the robust OLS estimation produces correct confidence intervals for , whereas the standard OLS method exhibits coverage distortions, when the noise is not i.i.d. This finding indicates that the robust OLS estimator has a broader range of applicability than merely addressing heteroscedasticity, and that it can also be effectively used in regression settings not covered by the standard OLS estimation and inference theory.
It is worth noting that the papers by Doukhan and Wintenberger (2008), Bardet and Wintenberger (2009) and Karmakar et al. (2022) provide advanced theoretical results on the modelling and estimation of general nonlinear time-varying time series models; however, they address the linear AR model (49) only in the trivial case of an i.i.d. noise .
6 Monte Carlo Simulations
In this section, we explore the finite sample performance of the robust and standard OLS estimation methods in regression settings, outlined in Sections 2 and 3. We examine the impact of time-varying deterministic and stochastic parameters, means, scale factors and heteroskedasticity of the regression noise on estimation. Comparison of simulation results for standard and robust estimation methods shows that, despite the generality of our regression setting, estimation based on the robust standard errors produces well-sized coverage intervals for fixed and time-varying regression parameters and , while application of the standard confidence intervals leads to severe distortion of coverage rates.
6.1 Estimation of a fixed parameter
We generate arrays of samples of regression model with fixed parameter and an intercept:
| (53) |
We set the sample size to and conduct replications and set the nominal coverage probability at . (Estimation results for are available upon request). We also include a more complex example in the online supplement.
This model includes three parameters and three regressors. We set and define
where and . The stationary martingale difference noise in is generated by a GARCH() process
| (55) |
Model 6.1.
follows (53) with deterministic scale factors. We set: and .
Model 6.2.
follows (53) with stochastic scale factors. We set
The generating noises are i.i.d. and independent of .
Models 6.1 and 6.2 are regression models with fixed parameters. Examples of plots of the simulated dependent variable, regressor and regression noise are shown in Figure 1 and 2 ( and have similar patterns). To verify the validity of the asymptotic normal approximation of Corollary 2.1 in finite samples, we compute empirical coverage rates (CP) for confidence intervals used in robust OLS estimation, for parameter . For comparison, we compute the coverage rates CPst for standard confidence intervals based on the standard errors (20) used in standard OLS estimation. The robust and standard OLS procedures share the same estimator , and whence Bias, root mean square error (RMSE) and standard deviation (SD). Their confidence intervals differ because the variances (and standard errors) in their normal approximations are different.
Table 1 reports estimation results for Model 6.1 which contains determinist scale factors. It shows that coverage rate CP for robust confidence intervals is close to the nominal , while the coverage rate CPst of the standard confidence intervals drops below . The Bias, RMSE, and SD are small.
Table 2 shows estimation results for Model 6.2 which includes stochastic scale factors. It shows that the coverage rate CP for robust confidence intervals is close to the nominal , whereas the standard estimation method produces coverage distortions for parameters and .
| Parameters | Bias | RMSE | CP | CPst | SD |
| -0.00570 | 0.04579 | 95.0 | 79.2 | 0.04544 | |
| 0.00206 | 0.03407 | 95.4 | 72.7 | 0.03401 | |
| 0.00204 | 0.03495 | 94.0 | 72.9 | 0.03489 |
| Parameters | Bias | RMSE | CP | CPst | SD |
| -0.00420 | 0.05117 | 94.6 | 92.2 | 0.05100 | |
| 0.00208 | 0.03205 | 94.6 | 87.4 | 0.03199 | |
| 0.00071 | 0.01542 | 94.8 | 85.3 | 0.01541 |
To assess power, we vary in Model 6.1 from to and record how often the test rejects . Figure 3 reports results for ROLS and OLS at sample sizes . When , ROLS achieves a good size close to the nominal , while the size based on OLS results starts around and remains heavily oversized even as increases. For , power rises monotonically with for both methods. In Figure 3, the blue solid lines represent power based on ROLS, and the red solid lines correspond to standard OLS. Considering the OLS estimation has large size distortion, we compute its adjusted power, shown by the red dotted lines. With small sample size , OLS appears more powerful for , whereas ROLS catches up and achives good power when . For and , both methods already achieve good power around . Overall, ROLS provides reliable size and competitive power across different sample sizes. Similary results are observed for Model 6.2.
6.2 Estimation of a time-varying parameter
In this section we examine the validity of the normal approximation for the estimator , (22), of time-varying parameter , as established in Corollary 3.1 of Section 3. We replace the fixed regression parameter in the model (53) by a time-varying parameter :
| (56) |
where and are defined using and as in (6.1).
We consider two simulation models. Model 6.3 assumes deterministic parameters and scale factors, while Model 6.4 combines deterministic and stochastic parameters and scale factors.
Model 6.4.
where are stationary ARFIMA processes with memory parameter .
We estimate using the estimator , (22), where the weights are computed with the Gaussian kernel function with bandwidth , .
Figure 4 displays parameter estimation results for a single simulation from Model 6.3. It depicts the estimates (red line) against the true parameters (blue line), obtained with the bandwidth , and their point-wise confidence intervals (grey dashed lines), computed using the robust standard errors. The robust time-varying confidence intervals cover the true parameters , , for most of the time points.
Figure 5 reports the point-wise empirical coverage rates (blue line) in time-varying robust estimation of parameters which are close to the nominal for most of the time points. Figure 6 shows the RMSE’s for different choices of the bandwidth , . As expected, the RMSE depends on the smoothness of the parameter and often is minimized by moderately large values of , for example, .
Figure 7 reports estimation results for a single simulation from Model 6.4, and Figure 8 displays point-wise empirical coverage rates for robust confidence intervals. For deterministic parameters and , estimation quality is good and results are similar to those obtained for Model 6.3. For the stochastic parameter , the robust point-wise confidence intervals cover the path of stochastic parameter for most of the time points, see Figure 7(7). Figure 8(8) shows that coverage rates of robust time-varying confidence intervals for might be slightly affected by stochastic variation in the parameter and scale factors. Nevertheless, they are still satisfactory and reasonably close to the nominal coverage.
6.3 Estimation of regression parameter with missing data
| Parameters | Bias | RMSE | CP | CPst | SD |
| -0.00818 | 0.04983 | 94.60 | 74.60 | 0.04915 | |
| 0.00356 | 0.03875 | 94.00 | 67.90 | 0.03859 | |
| 0.00246 | 0.03840 | 93.80 | 70.00 | 0.03832 |
| Parameters | Bias | RMSE | CP | CPst | SD |
| -0.00567 | 0.05732 | 94.30 | 66.60 | 0.05704 | |
| 0.00144 | 0.04251 | 95.20 | 63.50 | 0.04249 | |
| 0.00289 | 0.04128 | 94.80 | 64.70 | 0.04118 |
To examine the impact of missing data on the robust and standard OLS estimation based on partially observed data we use two types of missing data patterns over the time period .
Type 1. The block of data is missing.
Type 2. single observations are missing at randomly selected times.
Tables 3 and 4 report robust and standard estimation results for Model 6.1 with fixed parameter. Table 3 shows that block missing data (Type 1) do not lead to noticeable changes in Bias, RMSE and SD, and the coverage rate for robust confidence intervals remains around . At the same time, the coverage rate CPst of the standard confidence intervals is substantially distorted.
Table 4 shows that randomly missing data do not affect the coverage rate of robust confidence intervals which remains to the nominal , while the coverage rate of the standard confidence intervals drops to around . This emphasises the flexibility of the robust OLS estimation of the fixed parameter in the presence of block or randomly missing data.
Figure 9 shows the coverage rates in time-varying robust estimation with block missing data (Type 1, shaded region) for . The coverage is close to the nominal , with some distortion for parameters and and a larger distortion for parameter within the shaded region. The distortion peaks at the centre of the block, as expected. Although the width of missing data block, , exceeds the bandwidth used in estimating , the coverage distortion seems to be offset by the smooth down-weighting of the data, and the performance of the robust time-varying OLS estimation exceeds expectations.
Figure 10 reports the path of the estimator and the point-wise robust confidence intervals, for a single simulation. The robust confidence intervals become wider in the shaded region, which likely explains the satisfactory coverage performance during that period.
Figure 11 shows that randomly missing data (Type 2) do not distort the robust time-varying OLS estimation. For all three parameters and time periods , the coverage rate is close to the nominal. Overall, robust estimation of time-varying parameter does not appear be affected by randomly missing data.
6.4 Estimation of a stationary AR() model
We assess the performance of the robust and standard procedures in the case of a stationary AR() model:
| (57) |
where , is a stationary martingale difference noise. The regressors include an intercept and the two past lags of . By Theorem 5.1, the parameter can be estimated by using the robust estimation method.
Table 5 shows that the coverage rate for the robust OLS estimation is close to the nominal , while the standard OLS estimation exhibits extensive coverage distortion for and .
| Parameters | Bias | RMSE | CP | CPst | SD |
| -0.00808 | 0.05250 | 94.9 | 92.3 | 0.05187 | |
| 0.00104 | 0.04183 | 94.5 | 75.0 | 0.04182 | |
| 0.00356 | 0.03091 | 94.8 | 88.8 | 0.03070 |
7 Empirical experiment
In this section, we analyze the structure and dynamics of daily SP 500 log returns, , from 02/01/1990 to 31/12/2019, (sample size ). We employ robust regression estimation to assess whether the returns can be modelled using a time-varying regression model of the form
| (58) |
where is an i.i.d. noise, and the time-varying mean and scale factor are independent of . Our objective is to estimate the time-varying mean , the scale factor , and to test for the absence of autocorrelation in the absolute residuals , thereby assessing the fit of the model (58) to the data.
It returns follows the model (58) with i.i.d. noise , then the absolute residuals ’s are uncorrelated then for :
Conversely, if the noise exhibits ARCH effects (stationary conditional heteroskedasticity), the sequence becomes autocorrelated, and the null hypothesis of uncorrelated absolute residuals would be rejected.
We estimate the the time varying mean using the time-varying OLS estimator with bandwidths . Figure 12(12) shows the estimated path of and the associated confidence intervals for bandwidth indicating that is very likely to change over time.
Assumption (58) implies that
Therefore, and thus follows a time-varying regression model of the form
| (59) |
where represents a time-varying intercept, denotes the scale factor, and is an i.i.d. noise. Hence can be consistently estimated using the time-varying OLS estimator . Figure 12(12) displays the estimated path of and the corresponding confidence intervals for with bandwidth , revealing pronounced time variation in the scale factor .
Figure 13(13) reports testing results for zero correlation at lags in the residual sequence . We employ the standard test and robust test procedures developed in Giraitis et al. (2024). Given that the sample size is large () and is estimated non-parametrically with bandwidth , we restrict the correlation analysis to the subsample . Both tests provide no evidence of significant correlation within this subsample, suggesting that the model (58) fits the returns well during this time period.
The same is not likely to be true if follows a GARCH(1,1) process, as confirmed by the following experiment. We fit a GARCH(1,1) model to the demeaned returns ,
We generate a simulated GARCH(1,1) sample , apply the regression model (59) to the absolute values , and compute the residuals, . Figure 13(13) shows that both standard and robust tests detect significant correlation in residuals , confirming the presence of conditional heteroskedasticity in the simulated GARCH data.
8 Conclusion
The robust OLS and time-varying OLS estimation and inference methods developed in this paper offer considerable flexibility for modelling economic and financial data. They allow for general heterogeneity in regression components and for structural change of regression coefficients over time. Moreover, the generalization of the structure of regressors and error terms further expands the range of empirical settings to which robust OLS regression framework can be applied. In particular, the paper develops asymptotic theory for general regression models with stochastic regressors possibly including a time varying mean, and provides data-based robust standard errors that enable the construction of confidence intervals for regression parameters. The Monte Carlo analysis demonstrates the strong performance of the robust estimation approach under complex settings, and confirms the asymptotic normality property and consistency of the proposed estimators.
References
- Angrist and Pischke (2010) Angrist, J. D. and J. S. Pischke (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. Journal of Economic Perspectives 24(2), 3–30.
- Bai and Perron (1998) Bai, J. and P. Perron (1998). Estimating and testing linear models with multiple structural changes. Econometrica 66, 47–78.
- Bardet and Wintenberger (2009) Bardet, J.-M. and O. Wintenberger (2009). Asymptotic normality of the quasi-maximum likelihood estimator for multidimensional causal processes. The Annals of Statistics 37, 2730–2759.
- Boldea et al. (2019) Boldea, O., A. Cornea-Madeira, and A. Hall (2019). Bootstrapping structural change tests. Journal of Econometrics 213(2), 359–397.
- Brockwell and Davis (1991) Brockwell, P. and R. Davis (1991). Time Series: Theory and Methods. Springer.
- Brown et al. (1975) Brown, R. L., J. Durbin, and J. M. Evans (1975). Techniques for testing the constancy of regression relationships over time. Journal of the Royal Statistical Society, Series B 37(2), 149–163.
- Carriero et al. (2024) Carriero, A., T. E. Clark, M. Marcellino, and E. Mertens (2024). Addressing covid-19 outliers in bvars with stochastic volatility. The Review of Economics and Statistics 106(5), 1403–1417.
- Cattaneo et al. (2018) Cattaneo, M., M. Jansson, and W. Newey (2018). Inference in linear regression models with many covariates and heteroscedasticity. Journal of the American Statistical Association 113(523), 1350–1361.
- Chan et al. (2024) Chan, J. C. C., G. Koop, and Y. Xuewen (2024). Large order-invariant bayesian vars with stochastic volatility. Journal of Business and Economic Statistics 42(2), 825–837.
- Chow (1960) Chow, G. C. (1960). Tests of equality between sets of coefficients in two linear regressions. Econometrica 28, 591–605.
- Chronopoulos et al. (2022) Chronopoulos, I., L. Giraitis, and G. Kapetanios (2022). Choosing between persistent and stationary volatility. Annals of Statistics 50(6), 3466–3483.
- Cogley and Sargent (2005) Cogley, T. and T. J. Sargent (2005). Drifts and volatilities: monetary policies and outcomes in the post WWII US. Review of Economic Dynamics 8(2), 262–302.
- Dahlhaus (1997) Dahlhaus, R. (1997). Fitting time series models to nonstationary processes. Annals of Statistics 25(1), 1–37.
- Dahlhaus and Giraitis (1998) Dahlhaus, R. and L. Giraitis (1998). On the optimal segment length for parameter estimates for locally stationary time series. Journal of Time Series Analysis 19(6), 629–655.
- Dahlhaus and Richter (2023) Dahlhaus, R. and S. Richter (2023). Adaptation for nonparametric estimators of locally stationary processes. Econometric Theory 39, 1123–1153.
- Dahlhaus et al. (2019) Dahlhaus, R., S. Richter, and W. Wu (2019). Towards a general theory for locally stationary processes. Bernoulli 25(2), 1013–1044.
- Dendramis et al. (2021) Dendramis, Y., L. Giraitis, and G. Kapetanios (2021). Estimation of time-varying covariance matrices for large datasets. Econometric Theory 37(6), 1100–1134.
- Doukhan and Wintenberger (2008) Doukhan, P. and O. Wintenberger (2008). Weakly dependent chains with infinite memory. Stochastic Processes and their Applications 118(11), 1997–2013.
- Eicker (1963) Eicker, F. (1963). Asymptotic normality and consistency of the least squares estimators for families of linear regressions. Annals of Mathematical Statistics 34, 447–456.
- Enders (2022) Enders, C. K. (2022). Applied Missing Data Analysis. Guilford Publications.
- Eraker et al. (2003) Eraker, B., M. Johannes, and N. Polson (2003). The impact of jumps in volatility and returns. Journal of Finance 58(3), 1269–1300.
- Fan and Zhang (1999) Fan, J. and W. Zhang (1999). Statistical estimation in varying coefficient models. Annals of Statistics 27(5), 1491–1518.
- Fu et al. (2023) Fu, Z., Y. Hong, L. Su, and X. Wang (2023). Specification tests for time-varying coefficient models. Journal of Econometrics 235, 720–744.
- Georgiev et al. (2018) Georgiev, I., D. Harvey, S. Leybourn, and A. Robert Taylor (2018). Testing for parameter instability in predictive regression models. Journal of Econometrics 204, 101–118.
- Giraitis et al. (2014) Giraitis, L., G. Kapetanios, and T. Yates (2014). Inference on stochastic time-varying coefficient models. Journal of Econometrics 179(1), 46–65.
- Giraitis et al. (2018) Giraitis, L., G. Kapetanios, and T. Yates (2018). Inference on multivariate heteroscedastic time varying random coefficient models. Journal of Time Series Analysis 39(2), 129–149.
- Giraitis et al. (2020) Giraitis, L., G. Kapetanios, and T. Yates (2020). Asymptotic theory for time series with changing mean and variance. Journal of Econometrics 219, 281–313.
- Giraitis et al. (2024) Giraitis, L., Y. Li, and P. C. B. Phillips (2024). Robust inference on correlation under general heterogeneity. Journal of Econometrics 240(1), 105691.
- Giraitis et al. (2018) Giraitis, L., M. Taniguchi, and M. S. Taqqu (2018). Estimation pitfalls when the noise is not iid. Japanese Journal of Statistics and Data Science 1, 59–80.
- Hall et al. (2012) Hall, A., S. Han, and O. Boldea (2012). Inference regarding multiple structural changes in linear models with endogenous regressors. Journal of Econometrics 270(2), 281–302.
- Hamilton (1989) Hamilton, J. D. (1989). A new approach to the economic analysis of nonstationary time series and the business cycle. Econometrica 57, 357–384.
- Hansen (2000) Hansen, B. (2000). Testing for structural change in conditional models. Journal of Econometrics 97(1), 93–115.
- Hidalgo et al. (2019) Hidalgo, J., Y. Lee, and M.-H. Seo (2019). Robust inference for threshold regression models. Journal of Econometrics 210, 291–309.
- Hu et al. (2024) Hu, Z., I. Kasparis, and Q. Wang (2024). Time-varying parameter regressions with stationary presistent data. Econometric Theory publisked online, 1–17.
- Jochmans (2019) Jochmans, K. (2019). Heteroscedasticity-robust inference in linear regression models with many covariates. Journal of the American Statistical Association 117(538), 887–896.
- Karmakar et al. (2022) Karmakar, S., S. Richter, and W. B. Wu (2022). Simultaneous inference for time-varying models. Journal of Econometrics 227(2), 408–428.
- Lai and Wei (1982) Lai, T. L. and C. Z. Wei (1982). Least squares estimates in stochastic regression models with applications to identification and control of dynamic systems. Annals of Statistics 10(1), 154–166.
- Leamer (1983) Leamer, E. E. (1983). Let’s take the con out of econometrics. The American Economic Review 73(1), 31–43.
- Leamer (2010) Leamer, E. E. (2010). Tantalus on the Road to Asymptopia. Journal of Economic Perspectives 24(2), 31–46.
- Li et al. (2020) Li, D., P. Phillips, and J. Gao (2020). Kernel-based inference in time-varying coefficient cointegrating regression. Journal of Econometrics 215, 607–632.
- Linton and Xiao (2019) Linton, O. and Z. Xiao (2019). Efficient estimation of nonparametric regression in the presence of dynamic heteroskedasticity. Journal of Econometrics 213, 608–631.
- MacKinnon (2012) MacKinnon, J. G. (2012). Thirty years of heteroscedasticity-robust inference. in Recent Advances and Future Directions in Causality, Prediction, and Specification Analysis, eds. X. Chen and N. R. Swanson, New York: Springer, 437–461.
- Nyblom (1989a) Nyblom, J. (1989a). Testing for the constancy of parameters over time. Journal of the American Statistical Association 84(405), 223–230.
- Nyblom (1989b) Nyblom, J. (1989b). Testing for the constancy of parameters over time. Journal of the American Statistical Association 84(405), 223–230.
- Ploberger and Krämer (1992) Ploberger, W. and W. Krämer (1992). The CUSUM test with OLS residuals. Econometrica 60, 271–285.
- Priestley (1965) Priestley, M. B. (1965). Evolutionary spectra and non-stationary processes. Journal of the Royal Statistical Society, Series B 27(2), 204–229.
- Primiceri (2005) Primiceri, G. E. (2005). Time varying structural vector autoregressions and monetary policy. The Review of Economic Studies 72(3), 821–852.
- Robinson (1989) Robinson, P. M. (1989). Nonparametric estimation of time-varying parameters. In P. Hackl (Ed.), Economic Structural Change: Analysis and Foresting, pp. 253–264. Springer Berlin.
- Robinson (1991) Robinson, P. M. (1991). Time-varying nonlinear regression. In P. Hackl and A. H. Westland (Eds.), Economic Structural Change: Analysis and Forecasting, pp. 179–190. Springer Berlin.
- Sun et al. (2021) Sun, Y., Y. Hong, T.-H. Lee, S. Wang, and X. Zhang (2021). Time-varying model averaging. Journal of Econometrics 222, 974–992.
- Terasvirta (1998) Terasvirta, T. (1998). Modelling economic relationships with smooth transition regressions. In A. Ullah and D. E. A. Giles (Eds.), Handbook of Applied Economic Statistics, pp. 507–552. Marcel Dekker.
- Tong (1990) Tong, H. (1990). Non-linear Time Series: A Dynamical System Approach. Oxford University Press.
- Vogt (2012) Vogt, M. (2012). Nonparametric regression for locally stationary time series. The Annals of Statistics 40(5), 2601–2633.
- White (1980) White, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. Econometrica 48, 817–838.
- White (2014) White, H. (2014). Asymptotic Theory for Econometricians. Academic Press.
- Wu (2005) Wu, W. (2005). M-estimation of linear models with dependent errors. Annals of Statistics 25(2), 495–521.
- Xu and Phillips (2008) Xu, K.-L. and P. Phillips (2008). Adaptive estimation of autoregressive models with time-varying variances. Journal of Econometrics 142, 265–280.
- Zhang and Wu (2012) Zhang, T. and W. Wu (2012). Inference of time-varying regression models. Annals of Statistics 40(3), 1376–1402.
- Zhang and Wu (2015) Zhang, T. and W. Wu (2015). Time-varying nonlinear regression models: Nonparametric estimation and model selection. Annals of Statistics 43(2), 741–768.
Online Supplement to
“Unlocking the Regression Space”
Liudas Giraitis1, George Kapetanios2, Yufei Li2, Alexia Ventouri2
1Queen Mary
University of London
2King’s College London
This Supplement provides proofs of the results given in the text of the main paper. It is organised as follows: Section 9, 10, 11 provide proofs of the main theorems. Section 12 contains auxiliary technical lemmas used in the proofs.
Formula numbering in this supplement includes the section number, e.g. , and references to lemmas are signified as “Lemma 10.#”, e.g. Lemma 10.1. Theorem references to the main paper include section number and are signified, e.g. as Theorem , while equation references do not include section number, e.g. , .
In the proofs, stands for a generic positive constant which may assume different values in different contexts.
9 Proofs of Theorems 2.1 and 2.2, Corollaries 2.1 and 2.2, and Lemma 2.1
Proof of Theorem 2.1. Notice that in view of (1),
Recall definition (5) of and . Then
| (9.1) | |||||
since by (7) of Assumption 2.3, by (12.7) of Lemma 12.2. Moreover, by (12.6) and (12.3),
This completes the proof of the consistency claim (9) of the theorem.
Recall that for symmetric matrices , and a vector it holds:
where denotes the spectral norm and the Euclidean norm of the matrix .
Recall the definition of the information set .
Proof of Theorem 2.2. Proof of (13). By (9.1),
Moreover, by the same argument as in the proof of (9.1),
| (9.2) | |||||
Hence,
| (9.3) | |||||
| (9.4) |
This together with (9.3) implies:
Write
To prove (13), it remains to show that
| (9.5) |
Notice that is an m.d. sequence with respect to the -field
:
| (9.6) |
The latter follows noting that the variables are -measurable since they are function of . Similarly, since ’s are measurable (see Assumption 2.2), the variables are also -measurable. Finally, by assumption, and are mutually independent, and therefore by Assumption 2.1. This shows that the conditional expectation property is preserved for and completes the argument showing that is a martingale difference sequence with respect to the -field .
Therefore, by Corollary 3.1 of Hall and Heyde (1980), to prove (9.5), it suffices to show that
| (9.7) | |||
Observe that (a) holds with a non-random limit . Thus, the verification of the condition (3.21) of Corollary 3.1, that the -fields are nested, for and , is unnecessary; see remark on page 59 in Hall and Heyde (1980). To verify (a), notice that
Then, setting , we can write,
| (9.8) |
Recall that by (9.2), . We show in (12.14) of Lemma 12.2 that
Together with (9.4), this implies
which proves (a).
Next we prove (b). We have
By definition of , On the other hand, by (12.18) of Corollary 12.1, for any ,
where is measurable, and, thus, also measurable. Then,
Hence,
by (12.54) of Lemma 12.3. This completes the proof (b) and the claim (13) of the theorem.
The claim (14) follows from (13) by setting where and for . Then and , where is the -th diagonal element of . Then,
by (13). This completes the proof of the theorem.
Proof of Corollary 2.1. We will show that
| (9.9) |
which together with (14) implies (16):
To prove (9.9), we will verify that
| (9.10) |
which implies the following property for diagonal elements:
In (12.11) of Lemma 12.2 it is shown that
| (9.11) |
for any , where do not depend on and Set , where for and . Then , and by (9.11), . This proves (9.9):
In addition, the bounds (9.11) imply that :
Proof of (9.10). Set . By (7) of Assumption 2.3, . We have
By (12.6), (12.3), (12.12) and (12.10) of Lemma 12.2,
We will show that
| (9.12) |
This implies (9.10):
Proof of (9.12). By definition,
Notice that
since by assumption (7) and by (12.8) of Lemma 12.2. Hence, to verify (9.12), it suffices to show that
| (9.13) |
Recall the equality . Denote . Then,
Hence,
where by Theorem 2.1, and
by (12.53) of Lemma 12.3. This implies (9.13) and completes the proof of the corollary.
Proof of Corollary 2.2. Let be the true value of the -th component of the parameter , and suppose that . Write
By (16) of Corollary 2.1, and . Hence,
Then, which proves the claim of Corollary 2.2.
Proof of Lemma 2.1. Proof of (7). It suffices to show that
| (9.14) |
Notice also that ,
| (9.15) |
In addition, by assumption (19) of lemma, . Thus,
We will show that which implies (9.14). Observe that for any ,
By assumption (18), where does not depend on . Hence,
which implies and proves (7).
Proof of (11). It suffices to verify that
| (9.16) |
By assumption (19) of lemma, . This together with (9.15) implies that
We will show that which implies and proves (9.16).
10 Proofs of Theorem 3.1 and Corollaries 3.1 and 3.2
Proof of Theorem 3.1. Recall the notation introduced in Section 3. Set
Then we can write
Recall the estimator given in (22). In Section 3 we introduced an auxiliary regression model with a fixed parameter :
| (10.1) |
Recall the OLS estimator of the fixed parameter in this model, given in (27):
In (28) we showed that following relation:
| (10.2) |
The remainder arises due to time variation in the parameter and is negligible. We will obtain an upper bound for this term. The term is the main component We will analyse it using the results of Section 2. Overall, equation (10.2) shows that properties of are determined by the properties of , with an additional negligible term .
First we will show that the components of and satisfy the following properties. For ,
| (10.3) | |||||
| (10.4) |
Proof of (10.3). Recall that , and
By Lemma 10.1, under assumptions of theorem, and the scale factors satisfy Assumptions 2.3 and 2.4(ii). Thus, by Theorem 2.1,
where and
| (10.6) |
Indeed, by (30) of Assumption 3.2. On the other hand, (9.17) implies that , where the last relation easily follows using definition of and (24). Hence , which proves (10.6).
This complete the proof of the first claim in (10.3), while the second claim holds by (14) of Theorem 2.2. The third claim holds since by (16) of Corollary 2.1 and (10.6),
| (10.7) |
To verify (10.8), recall notation of the diagonal matrix
Notice that
because by Assumption 3.2. On the other hand,
Next, bound
We have . Recall that by Assumption 3.2, by Assumption 3.1, and it is trivial to show that under (24),
This implies
| (10.9) |
which proves the second claim in (10.8).
We now are ready to prove the claims (31) and (32) of the theorem. First, together with (10.2), the properties (10.3) and (10.4) establish the consistency result (31):
To prove the asymptotic normality property (32), recall assumption . Then
under assumption . Then,
by (10.3) which proves the asymptotic normality property (32) of the theorem. Noting that , as shown in (10.7), this completes the proof of the theorem.
Proof of Corollary 3.1. In the proof of Theorem 3.1 we wrote the time-varying regression model as a regression model
| (10.10) |
with a fixed parameter . We showed that the regressors and the noise satisfy assumptions of Theorem 2.2 and that the contribution of the term is asymptotically negligible. That allowed us to establish the asymptotic normality property (32) of Theorem 3.1 for using results of Section 2.
Clearly, to prove Corollary 3.1, it suffices to verify the second claim in (34),
Proof of the corresponding result in the case of fixed parameter in Corollary 2.1 shows that we need to verify the validity of (9.12) for our regression model (10.10), i.e. to show that
| (10.11) |
where , , and .
Set . Write
By (9.12), Hence, to prove (10.11), we need to show that
| (10.12) |
By Assumption 3.2, . Hence,
We will show that which implies (10.12). Notice that
| (10.13) | |||||
Using the inequality , we can bound in (10.13),
Next we evaluate in (10.13). Let be large number. Then,
Hence,
Since , this yields
Recall that and . Denote Then,
Hence,
| (10.14) | |||||
By (31) of Theorem 3.1, , and can be made arbitrarily small by selecting large . We will show that
| (10.15) |
Combining this with (10.14), we obtain
so that the right hand side can be made arbitrarily small by selecting a large enough and letting . This proves (10.12).
To bound observe that by Assumption 3.1, , where and and recall (10.9). Then,
when . This proves (10.15) for .
To bound and , recall that by Assumption 3.2, and which implies that . Moreover, under (24) it holds and . Hence,
This completes the proof of (10.15) and the corollary.
Proof of Corollary 3.2. Let be the true value of the -th component of the time-varying parameter . Suppose that for as . Write
By (34) of Corollary 3.1, and . Hence,
Then, which proves the claim of the Corollary 3.2.
Lemma 10.1.
11 Proofs of Theorems 4.1, 4.2 and Theorem 5.1
Proof of Theorem 4.1. Suppose that follows the regression model (1). In the presence of missing data, estimation of the parameter is based on a regression model with the fixed parameter (39):
| (11.1) |
where the regressors and the noise take the form
| (11.2) | |||||
and is the missing data indicator. Under Assumptions 4.1 and 4.2 of the theoren, are independent of . Therefore, belongs to the regression space described in (2) and (3) of Section 2.
We estimate the fixed parameter using the estimator defined in (4):
| (11.3) |
We will show that satisfy Assumptions 2.1, 2.2, 2.3 and 2.4 of Theorem 2.2 of Section 2. Then, the required result (42) for of this theorem follows directly from the claims (16) of Corollary 2.1.
It remains to show the validity of the assumptions in group (b), i.e. that the means and the scales satisfy Assumptions 2.3 and 2.4(ii). By Assumption 4.2, we have and . Moreover, and where do not depend on and . For , define:
| (11.4) |
Notice that
where is the size of the subsample (36). By assumption of the theorem, . Thus,
| (11.5) | |||||
| (11.6) |
which confirms the validity of Assumptions 2.3 and 2.4(ii); see Lemma 2.1. This completes the proof of the theorem.
Proof of Theorem 4.2. Now, suppose that follows the regression model (21) with a time-varying parameter . In the presence of missing data, estimation of the time-varying parameter is based on a model (43):
| (11.7) |
Here, the regressors and the noise are the same as in (11.2). We showed in the proof of the Theorem 4.1 that belongs to the regression space described in (2) and (3) of Section 2.
The estimator of the time-varying parameter is given in (45):
| (11.8) |
We will show that satisfy Assumptions 2.1, 2.2, 2.4(i), 3.1 and 3.2 of Theorem 3.1. Then, the results (46), (47) and (48) for of Theorem 4.2 follow from the results (31) of Theorem 3.1 and (34) of Corollary 3.1.
Observe that Assumptions 2.1, 2.2, 2.4(i) on are part of Assumption 4.1 of this theorem, which also includes Assumption 3.1 for .
It remains to show that satisfy Assumption 3.2. This requires to prove the validity of (11.5) and (11.6) under Assumption 4.2 of this theorem, which we showed in the proof of Theorem 4.1.
Proof of Theorem 5.1. We consider a stationary AR() model (49),
where is a stationary sequence with respect to the information set . Write it as a regression model (1),
| (11.9) |
with fixed parameter and regressors . Under assumption (51) of theorem, AR() model has a stationary solution:
| (11.10) |
and regressors
for , satisfy regression assumption (3). From (11.10) it follows that the regressors are measurable. Moreover, under the assumptions of the theorem, satisfy Assumptions 2.1, 2.2, 2.3 and 2.4 of Theorem 2.2 in Section 2. Finally, we show that . Recall that by the assumption of theorem, is a stationary sequence such that It is known that if , for some , then
where does not depend on ; see e.g., Lemma 2.5.2 in Giraitis et al. (2012). Hence and from .
12 Proofs of Section 2: Auxiliary lemmas
This section contains auxiliary lemmas used in the proofs of the main results for Section 2. For the ease of referencing, we include the statement of Lemma 12.1(i) established in Giraitis et al. (2024).
Lemma 12.1.
Assume that sequences and are mutually independent.
(i) If is a covariance stationary short memory sequence, then
| (12.1) |
(ii) If , then
| (12.2) |
Proof of Lemma 12.1. The claim (i) of Lemma 12.1 was derived in (Giraitis et al. (2024), Lemma A5). To prove (ii), denote . Then,
This implies
This completes the proof of (12.2) and the lemma.
Lemma 12.2.
(i) There exists such that and such that for any , ,
| (12.3) | |||||
| (12.4) |
Moreover,
| (12.5) | |||||
| (12.6) | |||||
| (12.7) | |||||
| (12.8) |
(ii) In addition, if Assumption 2.4 holds, then there exists such that and such that for any , ,
| (12.9) | |||||
| (12.10) | |||||
| (12.11) |
Moreover,
| (12.12) | |||||
| (12.13) | |||||
| (12.14) |
Before the proof of lemma, we will state the following corollary. Denote
| (12.15) |
Notice that under (7) of Assumption 2.3,
| (12.16) |
Corollary 12.1.
Proof of Lemma 12.2(i). Proof of (12.3). Set . By definition,
| (12.19) |
Then
| (12.20) | |||||
where . Using (12.20), we can write
| (12.21) | |||
| (12.22) |
We split the proof into two cases when regression model (1) does not include intercept and when intercept is included.
Case 1 (no intercept): .
Case 2 (intercept included): , .
Case 1. Let . Then (12.21) implies
| (12.23) |
In this instance,
where . By Assumption 2.2(ii), the matrix is positive definite. Therefore, there exists such that for any ,
Hence, setting , we derive
since and . With (12.23) this proves the first claim in (12.3):
| (12.24) |
Matrix is symmetric and, thus, it has real eigenvalues. The bound (12.24) implies that the smallest eigenvalue of has property . Therefore is positive definite, and the largest eigenvalue of has property , which implies that . This proves the second claim in (12.3).
Case 2 (intercept included): . Recall that in presence of intercept, and .
Proof of (12.3). Set , . Recall that
| (12.25) |
We will show that there exists such that for any and ,
| (12.26) | |||||
| (12.27) |
where is defined as in (12.15). These bounds imply (12.3). Indeed, suppose that . By (12.25), this is equivalent to
Then, by (12.26),
On the other hand, if , then in (12.27),
which together with (12.27) implies
Therefore,
This implies that there exists such that
| (12.28) |
Proof of (12.26). Below we will show that there exists such that
| (12.29) |
In addition, observe that in Case 2,
| (12.30) |
Then from (12.22), using (12.29) and (12.30) we arrive at (12.26):
Proof of (12.27). By (12.21) and (12.29),
By Cauchy inequality and (12.30),
Since , then Hence, using notation introduced in (12.15), we obtain
which together with (12) and (12.29) proves (12.27):
Proof of (12.29). Recall, that in presence of intercept, and Denote and . Then
where is a block diagonal matrix and . By assumption, the matrix is positive definite. Denote . Then,
Observe that since . Recall that . Hence, by (12.24),
for some which does not depend on and . This implies (12.29).
Summarizing, note that by (12.24) and (12.28),
| (12.32) |
where does not depend on . Notice that by (12.16). This proves the first claim in (12.3).
Proof of the second claim in (12.3) is the same as in Case 1.
Proof of (12.5), (12.6), (12.7) and (12.8). Denote by the -th element of the matrix
| (12.33) |
To prove (12.5), it remains to show that
| (12.34) |
Case 1: . Then, by (12.20), we have
Therefore, setting , we can write
| (12.35) | |||||
By assumption, sequences , and are covariance stationary short memory sequences with zero mean, and the weights are independent of , are independent of and are independent of , Thus, applying Lemma 12.1 to , we obtain
Denote . Then,
Notice that . Observe that by (7) and by (7) of Assumption 2.3. This implies which proves (12.34).
Case 2. Let .
To prove (12.5), it suffices to show that , in (12.33) have property (12.34): . Recall that in presence of intercept we have .
First, observe that for , are the same as in (12.35) and whence by (12.34). Second, since . Finally, for , we have
Then,
By assumption, is a covariance stationary short memory sequence with , and and are mutually independent. Therefore, by Lemma 12.1,
which proves (12.34). This completes the proof of (12.5) in Case 2.
Proof of (12.6). It follows using the same argument as in Case 1.
Proof of (12.7). To prove that , write
It suffices to show that
| (12.36) |
We have
By Assumptions, 2.1 and 2.2, the sequences , are covariance stationary short memory sequences with zero mean, the weights are independent of , and are independent of .
Proof of (12.8). Observe that by (12.5) and (12.4) of Lemma 12.2, . Therefore,
This proves (12.8) and completes the proof of the part (i) of the lemma.
Proof of Lemma 12.2 (ii). Proof of (12.9). We can write
Let be a small number which will be selected below. Then,
Thus,
| (12.37) | |||||
We will show that there exist and such that , and for any , and ,
| (12.38) | |||||
| (12.39) |
Using these bounds in (12.37), we obtain
| (12.40) |
First we prove (12.38). Setting
we can write
Observe that the variables satisfy assumptions of Lemma 12.2(i). Hence by (12.32),
| (12.41) |
where does not depend on . Notice that by (12.16). This proves (12.38).
To prove (12.39), recall that . Bound
In (12.52) of Lemma 12.3 we show that , where does not depend on and is defined in (12.15). Thus, selecting
we obtain which proves the bound (12.39). Notice that can be made small by selecting large in (12.41).
In turn, by (12.40),
where is defined in (12.41). This implies
| (12.42) |
for some which does not depend on . Notice that is measurable, and by (12.16). This proves the first claim in (12.9). The second claim follows using the same argument as in the proof of (12.3).
Proof of (12.11). Write , . By (12.3), (12.4), (12.9) and (12.10),
| (12.43) | |||||
| (12.44) |
We will show that
| (12.45) |
Since this proves the first claim in (12.11). To verify (12.45), notice that the smallest eigenvalue of the matrix and the largest eigenvalue of the inverse matrix are related by the equality . By (12.44), . Thus, for ,
where which proves (12.45). Finally, by (12.43), for , which proves the second bound in (12.11).
Proof of (12.12), (12.13) and (12.14). Write
To prove (12.12), it suffices to verify that
| (12.46) |
Recall that and , where . Hence,
By (12.20),
Then,
Therefore, setting , it follows that
To prove (12.46), it suffices to show that
| (12.47) |
By Assumption 2.4, , and are covariance stationary short memory zero mean sequences, and these sequences are mutually independent of the weights , and . Moreover, definition of and and (7) of Assumption 2.3 imply that
and by (11) of Assumption 2.4,
Thus, (12.47) follows by using Lemma 12.1 and applying a similar argument as in the proof of (12.5). This completes the proof of (12.12).
Proof of (12.14). Write
| (12.48) |
By (12.12), We will show that
| (12.49) |
which together with (12.48) implies (12.14): We have, , where . Write
Then (12.49) follows if we show that
| (12.50) |
We have and . So,
Hence, denoting , we obtain
Observe, that sequences , , , are sequences of uncorrelated random variables with zero mean and constant variance. For example, by assumption, are measurable. Then, for ,
Then using the same argument as in the proof of (12.47) it follows
This completes the proof of the part (ii) and of the lemma.
Proof of Corollary 12.1. The claim (12.17) is shown in (12.32), and the claim (12.18) is shown in (12.42).
Lemma 12.3.
Under Assumptions of Theorem 2.1, the exists such that
| (12.51) | |||||
| (12.52) |
for sufficiently small , where does not depend on and and ,
.
This implies
| (12.56) | |||||
Notice that
| (12.57) |
by definition (12.15) of and and because
Moreover, , by (12.16). Clearly, (12.56) and (12.57) prove (12.51).
Proof of (12.52). Denote
Recall, that by assumption, is a stationary sequence, and by Assumption 2.2(i), . Then,
We will show that for sufficiently small ,
Indeed, by Assumption 2.1, the variable has probability distribution density and when for some . Without restriction of generality assume that . Then,
Therefore, , and as in (12.56), we obtain
which proves (12.52).
Proof of (12.53). We will prove the first claim (the proof of the second claim is similar). By (12.55), Let be a large number. Then, . Therefore,
| (12.58) |
By (11) of Assumption 2.4 and (12.57),
| (12.59) |
Since and are mutually independent, then by (12.2) of Lemma 12.1,
| (12.60) |
We will show that
| (12.61) |
where , and does not depend on . Together with (12.58) this implies
Next we prove (12.61). Set . Then, letting , we obtain
since, as , , and . This implies (12.61).
Proof of (12.54). Denote by the left hand side of (12.54). By (12.55), Let be a large number. Then,
Observe, that is measurable. Then,
13 Additional Monte Carlo simulation results
In this section, we further evaluate the finite-sample performance of our robust OLS estimation method using two examples of regression models with fixed parameters, where the regressors and regression noise exhibit complex, non-standard structures.
Example 1. As in the Monte Carlo section of the main paper, we generate arrays of samples from a regression model with a fixed parameter and an intercept, using a sample size of and replications. We first consider the following model:
| (13.1) |
We specify the scale factor in the regression noise as a deterministic trend , and a stationary martingale difference noise is generated from a GARCH() process
| (13.2) |
Define the regressors as and for , where
| (13.3) |
Figure 14 displays plots of a sample of variables , , and for generated by Model (13.1)-(13), which exhibit clear patterns of non-stationary behavior. The Monte Carlo simulation results for sample size based on 1000 replications are reported in the Table 6. Since the regressors and regression noise in this model satisfy the assumptions of Corollary 2.1, as expected, the Monte Carlo simulation results confirm excellent performance of the robust OLS estimator. In particular, the empirical coverage of the confidence intervals is close to the nominal , whereas the standard OLS estimator exhibits significant coverage distortions.
| Parameters | Bias | RMSE | CP | CPst | SD |
| -0.00036 | 0.02738 | 94.3 | 89.8 | 0.02738 | |
| 0.00050 | 0.01681 | 93.8 | 79.5 | 0.01680 | |
| -0.00003 | 0.00682 | 95.6 | 85.5 | 0.00682 |
Example 2. Next, we provide an example of a regression model in which the components of the fixed regression parameter are estimated at different rates. Consider regression model (13.1) with , defined as in Example 1. Set , and let the means and scale factors , be defined as follows:
| (13.4) |
This model satisfies the assumptions of Corollary 2.1 (see also Remark 2.1 in the main paper). Therefore, the corresponding -statistics for have the following property:
| (13.5) |
where, the robust standard errors are inversely proportional to the consistency rate
In this model, the intercept associated with the regressor is estimated at the consistency rate ; the parameter linked with the regressor (with ) at the rate , and the parameter linked with the regressor (with ) at the rate . The rate is super-fast, , when ; standard, , when ; super-slow, , when ; and logarithmic, , when . Monte Carlo results reported in Table 7 confirm the validity of the normal approximation (13.5) in finite samples (, based on replications). In particular, the coverage of the robust confidence intervals is close to the nominal level for all three parameters and for all values of considered in the construction of the regressor . In contrast, the coverage rates based on the standard OLS method exhibit noticeable distortions, especially for and .
As expected, smaller values of are associated with slower consistency rates , wider confidence intervals, and larger standard deviations for the estimator of .
| Parameters | Bias | RMSE | CP | CPst | SD | |
| -0.00331 | 0.08888 | 94.1 | 93.3 | 0.08882 | ||
| 2.4E-06 | 0.00004 | 95.3 | 86.4 | 0.00004 | ||
| -0.00008 | 0.00128 | 94.5 | 85.6 | 0.00128 | ||
| -0.00406 | 0.08976 | 94.5 | 93.4 | 0.08967 | ||
| 2.4E-06 | 0.00004 | 95.8 | 86.9 | 0.00004 | ||
| 0.00272 | 0.03384 | 94.9 | 85.9 | 0.03373 | ||
| -0.00397 | 0.08884 | 94.6 | 93.9 | 0.08875 | ||
| 2.6E-06 | 0.00004 | 95.6 | 85.9 | 0.00004 | ||
| 0.01275 | 0.14219 | 95 | 87.2 | 0.14162 | ||
| -0.00319 | 0.08628 | 95 | 94.7 | 0.08622 | ||
| 3.0E-06 | 0.00004 | 95.5 | 86.3 | 0.00004 | ||
| 0.04468 | 0.43022 | 95.1 | 91.3 | 0.42790 |
Table 8 reports the estimation results for the parameters for sample sizes , when the regressor is generated with and is estimated with the super-slow rate . The coverage rates for the robust OLS method are close to the nominal level in all cases. As expected, as increases, the standard errors of all three parameter estimates decrease; however, for , which is estimated with the super-slow rate , the reduction in the standard deviation is relatively slow.
| Parameters | Bias | RMSE | CP | CPst | SD | |
| -0.01307 | 0.23584 | 94.9 | 95.1 | 0.23548 | ||
| 0.00008 | 0.00073 | 92.5 | 86.6 | 0.00073 | ||
| 0.03138 | 0.22902 | 94.6 | 91.7 | 0.22686 | ||
| -0.00740 | 0.12187 | 95.1 | 94.6 | 0.12164 | ||
| 0.00001 | 0.00010 | 94.3 | 86.3 | 0.00009 | ||
| 0.01302 | 0.16151 | 94.8 | 88.1 | 0.16098 | ||
| -0.00397 | 0.08884 | 94.6 | 93.9 | 0.08875 | ||
| 2.6E-06 | 0.00004 | 95.6 | 85.9 | 0.00004 | ||
| 0.01275 | 0.14219 | 95 | 87.2 | 0.14162 | ||
| -0.00319 | 0.06599 | 93.2 | 92.2 | 0.06592 | ||
| 0.00000 | 0.00001 | 94.1 | 83.3 | 0.00001 | ||
| 0.00913 | 0.12430 | 94.8 | 84.3 | 0.12396 |
Figure 15 displays plots of a single sample of the variables and for generated by Model (13.4) for . These samples exhibit clear patterns of non-stationary behavior.
References
- Giraitis et al. (2012) Giraitis, L., H. L. Koul, and D. Surgailis (2012). Large Sample Inference for Long Memory Processes. Imperial College Press.
- Giraitis et al. (2024) Giraitis, L., Y. Li, and P. C. B. Phillips (2024). Robust inference on correlation under general heterogeneity. Journal of Econometrics 240(1), 105691.
- Hall and Heyde (1980) Hall, P. and C. C. Heyde (1980). Martingale Limit Theory and its Application. Academic press.
Boundary Discontinuity Designs: Theory and Practice††thanks: This chapter was prepared for the invited session on “Causal Inference and Statistical Decisions” at the 2025 World Congress of the Econometric Society, Seoul (Korea). We thank Alberto Abadie, David Card, Xiaohong Chen, Boris Hanin, Thomas Holmes, Kosuke Imai, Rafael Lalive, Oliver Linton, Xinwei Ma, Francesca Molinari, Jörg Stoye, Jeff Wooldridge, and Joseph Zweimuller for comments and discussions. Financial support from the National Science Foundation through grants SES-2019432, DMS-2210561, and SES-2241575 is gratefully acknowledged. Yu was also partially supported through the Data-Driven Social Science initiative at Princeton University.
Abstract
We review the literature on boundary discontinuity (BD) designs, a powerful non-experimental research methodology that identifies causal effects by exploiting a thresholding treatment assignment rule based on a bivariate score and a boundary curve. This methodology generalizes standard regression discontinuity designs based on a univariate score and scalar cutoff, and has specific challenges and features related to its multi-dimensional nature. We synthesize the empirical literature by systematically reviewing over empirical papers, tracing the method’s application from its formative uses to its implementation in modern research. In addition to the empirical survey, we overview the latest methodological results on identification, estimation and inference for the analysis of BD designs, and offer recommendations for practice.
Keywords: regression discontinuity, treatment effects estimation, causal inference.
Contents
1 Introduction
The Regression Discontinuity (RD) design is one of the leading observational methods for program evaluation and causal inference (see Abadie-Cattaneo_2018_ARE; Hernan-Robins_2020_Book, for overviews and further references). In its canonical form, a binary treatment is assigned to units whose value of a score is equal to or above a known cutoff, and not assigned to units whose score is below the cutoff. Under the key assumption that all observable and unobservable pretreatment characteristics vary smoothly around the cutoff, the discontinuous change generated by the hard-thresholding treatment assignment rule based on the univariate score and scalar cutoff can be used to learn about causal treatment effects. Lee-Lemieux_2010_JEL offer an early review with a list of standard RD empirical applications, Cattaneo-Titiunik_2022_ARE give a recent overview of the RD methodological literature, and Cattaneo-Idrobo-Titiunik_2020_CUP; Cattaneo-Idrobo-Titiunik_2024_CUP provide a practical introduction to modern RD methods and practice.
The Boundary Discontinuity (BD) design generalizes the RD design to the case where the score is bivariate, and the treatment is assigned to units according to the location of their score relative to a known boundary that splits its support into two disjoint regions. This setup is sometimes called the Multi-Score RD design (Papay-Willett-Murnane_2011_JoE; Reardon-Robinson_2012_JREE; Wong-Steiner-Cook_2013_JEBS), or the Geographic RD design (Keele-Titiunik_2015_PA; Keele-Titiunik-Zubizarreta_2015_JRSSA; Keele-Titiunik_2016_PSRM; Keele-etal_2017_AIE; Galiani-McEwan-Quistorff_2017_AIE; Rischard-Branson-Miratrix-Bornn_2021_JASA; Diaz-Zubizarreta_2023_AOAS). In this chapter, we discuss the empirical strategies most commonly employed for the analysis and interpretation of BD designs, and review the most recent theoretical and methodological results characterizing their econometric properties (Cattaneo-Titiunik-Yu_2025_BDD-Distance; Cattaneo-Titiunik-Yu_2025_BDD-Location; Cattaneo-Titiunik-Yu_2025_BDD-Pooling).
We introduce the BD design formally in Section 2, and then outline our review of the empirical literature in Economics, Political Science, Education, and other disciplines. Table 5 reports over empirical papers, which form the basis for the organization of this chapter. We find that the overwhelming majority of papers collapse the bivariate score into a univariate distance score measuring the shortest distance to the treatment assignment boundary, and then report a single average treatment effect estimated by pooling all the observations that are close to the boundary, regardless of their specific location. In contrast, only a handful of empirical papers investigate heterogeneity by localizing to specific regions or points along the boundary, despite the potential importance of this rich information for policy evaluation and decision-making.
Following our review of the empirical literature, in Section 3 we discuss pooling-based methods. We begin by discussing the seminal work of Card-Krueger_1994_AER, who appear to be the first to employ the BD design conceptually, although they did not localize to the assignment boundary for estimation purposes. We then discuss holmes1998effect and black1999better, who appear to be the first two papers to employ localization to the assignment boundary explicitly as their main empirical strategy. Subsequent empirical work has recognized the importance of localization to the boundary, but has considered different empirical strategies for treatment effect estimation via local flexible regression methods. One of the most influential papers is dell2010persistent, who incorporated a polynomial expansion of the bivariate location score as part of the local regression specification. More recently, an alternative empirical approach views the estimation based on distance to the boundary as a pooled univariate RD design, and thus estimates treatment effects using a local regression including a polynomial expansion of the univariate distance score. Recent examples include ito2020willingness and dehdari2022origins. Many empirical papers also include boundary-segment fixed effects in their local regression specifications, and some include interactions with the treatment indicator and/or between the polynomial expansions of the univariate distance score and the bivariate location score. Furthermore, for simplicity, or because distinct treatments occur in different regions of the boundary, some papers reduce the BD design to boundary-segment-specific univariate RD designs—see ou2010leave, LondonoVelezRodriguezSanchez_2020_AEJ and salti2022impact. Putting all together, Section 3 discusses eight distinct pooled local regression specifications commonly found in empirical work.
Building on our review of empirical strategies used for pooled estimation in BD designs, in Section 3.1 we also overview the main methodological results in Cattaneo-Titiunik-Yu_2025_BDD-Pooling for pooling-based estimation and inference. These methods are viewed as univariate RD designs based on the shortest distance to the assignment boundary, and hence are related to identification, estimation, and inference over a specific one-dimensional submanifold curve on the plane, defined by the assignment boundary. Recognizing this mathematical structure, Cattaneo-Titiunik-Yu_2025_BDD-Pooling establish their formal results leveraging concepts from geometric measure theory (federer2014geometric). After reviewing these ideas, we briefly discuss how estimation and inference can be conducted using the state-of-the-art methods from the RD design literature (Calonico-Cattaneo-Titiunik_2014_ECMA; Calonico-Cattaneo-Farrell-Titiunik_2019_RESTAT; Calonico-Cattaneo-Farrell_2020_ECTJ). Some of this work is conceptually related to a recent paper by chen2025semiparametric, who study estimation and inference for integral functionals on submanifolds when using nonparametric sieve estimation.
Section 4 discusses heterogeneity analysis and subsequent aggregation of causal treatment effects along the boundary. Although the boundary average treatment effect parameter emerging from pooling-based methods discussed in Section 3 is often of interest, and provides a useful aggregate measure of average treatment effects, the richness of the two-dimensional assignment in BD designs offer the opportunity to explore heterogeneous treatment effects along the assignment boundary. In geographic applications, this heterogeneity is directly linked to the geographic location of the units, providing useful information about how treatment effects vary in space; in non-geographic settings, the heterogeneity is directly linked to features of the units that are captured by the bivariate score variable, and hence the interpretation is necessarily application-specific. Thus, Section 4 begins by introducing the boundary average treatment effect curve, which captures the average treatment effect for each point along the assignment boundary. We also discuss how other aggregate causal effects can be recovered from that building block parameter, such as the weighted boundary average treatment effect and the largest average treatment effect.
After introducing the main ideas and causal parameter of interest, Section 4 reviews the theoretical and methodological takeaways from Cattaneo-Titiunik-Yu_2025_BDD-Distance and Cattaneo-Titiunik-Yu_2025_BDD-Location. These two recent papers investigate identification, estimation, and inference for heterogeneous treatment effects along the assignment boundary, and transformations thereof, from two connected but distinct perspectives: Cattaneo-Titiunik-Yu_2025_BDD-Distance studies boundary-point specific distance-based methods, which are directly motivated by the poloing-based approaches discussed in Section 3, while Cattaneo-Titiunik-Yu_2025_BDD-Location studies methods based on the bivariate location directly. For each approach, the estimation and inference results are valid both pointwise and uniformly along the assignment boundary, in addition to providing the basis for constructing other treatment effect estimators.
Finally, Section 5 offers recommendations for practice, and concludes. The methods discussed in this chapter can be implemented using the general-purpose software packages rdrobust, rdhte, rdmulti, and rd2d, located at https://rdpackages.github.io/, where replication files and related references are also available.
2 The Boundary Discontinuity Design
Each unit in the study, , has a continuous bivariate score that takes values in the set . The assignment of units to the treatment or control condition depends on the location of their score relative to a known one-dimensional boundary curve that splits into two disjoint regions: , with and the control and treatment disjoint (connected) regions, respectively, and , where denotes the boundary of the set . (We follow convention and assume that the boundary belongs to the treatment group, that is, and .) The observed outcome variable is .
An ancestor of the BD design was used by Card-Krueger_1994_AER in their seminal study of the effects of the minimum wage on employment. In their analysis, the treatment of interest is the increase of the state minimum wage in New Jersey that went into effect on April 1, 1992, and rose the minimum wage to $5.05. The authors compared a sample of fast food restaurants in New Jersey to a sample of fast food restaurants in eastern Pennsylvania, a state that shares a border with New Jersey and where the minimum wage did not increase. After the increase in New Jersey, the average starting wage was $5.08 in the New Jersey sample and $4.62 in the Pennsylvania sample.
The rationale for comparing restaurants in New Jersey to restaurants in eastern Pennsylvania was that, by focusing on close geographic areas, these restaurants would be subject to similar economic conditions that would otherwise confound the effects of changes to the minimum wage. Although Card-Krueger_1994_AER also relied on a difference-in-differences design comparing both samples before and after the increase, they explicitly mentioned the close geographic proximity between the areas to justify their choice of comparison group, as they believed that fast-food stores in eastern Pennsylvania formed a “natural basis for comparison with the experiences of restaurants in New Jersey” due to Pennsylvania being a “nearby state[s]” (Card-Krueger_1994_AER, p. 773).
The use of close proximity as the basis for reducing potential confounders and enhancing the credibility of causal interpretations bears a direct connection to the BD design. The main difference is that Card-Krueger_1994_AER included non-adjacent areas in New Jersey and Pennsylvania (e.g., the New Jersey sample included restaurants in the New Jersey shore, far away from the Pennsylvania border), and did not directly use the unit’s distance to the boundary in their analysis. The explicit use of distance to the assignment boundary is central to the BD design, which is why we consider Card-Krueger_1994_AER to be an ancestor rather than an instance of the BD design itself: while the authors leverage the change in treatment assignment induced by a geographic boundary, they did not explicitly localize to the boundary itself.
holmes1998effect and black1999better are the earliest empirical examples that we could find employing the BD design. holmes1998effect used a BD design to study the effect of pro-business policies on manufacturing, comparing states that adopted right-to-work laws with adjacent states that did not. Using the longitude and latitude coordinates of each county’s centroid, he calculated the minimum distance of the centroid to the assignment border, and focused the analysis on counties close to the border as a way to “control for differences across states in these various characteristics that are unrelated to policy” (holmes1998effect, p. 671). Similarly, black1999better used a BD design to infer the quality of public schools from housing prices. She compared house prices on opposite sides of school attendance district boundaries, restricting the sample to houses close to the boundary. The justification for this localization was that houses on opposite sides of the boundary within a small area around it would be similar in all characteristics except for school quality—as the latter changes discontinuously at the attendance district boundary. In these examples, the bivariate score is the pair of latitude and longitude coordinates that determines the location of each unit (county or house), and the assignment boundary is the geographic border between two adjacent states or school attendance districts; the treatment is the policy or feature that changes abruptly at the border—labor policy for holmes1998effect and school quality for black1999better. This type of BD design is often referred to as a Geographic RD design.
One of the most common non-geographic applications of the BD design occurs in education when a treatment is given on the basis of two exam scores. For example, ou2010leave studies the effect of a high school exit exam on the likelihood of dropping out of high school. The exam has a mathematics and a language arts component, each of which is graded separately; students must achieve a minimum proficiency in each section in order to pass the exam. The bivariate score in this case is the pair of mathematics and language test scores, and the treatment is passing the exam.
Figure 1 illustrates the boundary and treatment assignment regions in two stylized settings corresponding to the geographic and non-geographic examples. Figure 1(a) illustrates the geographic BD design case, where units are split into adjacent treated and control areas according to their location with respect to a geographic border, as in black1999better and holmes1998effect. (This figure shows the New Jersey-Pennsylvania border as considered by Card-Krueger_1994_AER.) Figure 1(b) illustrates the non-geographic setting, using as example the study by ou2010leave where students receive two exam scores and a treatment is given only to those individuals who score above a minimum cutoff in each exam. In geographic applications, the bivariate score is always composed of the geographic coordinates of the units. In non-geographic applications, the bivariate score could include different types of variables; in addition to test scores from multiple exams, non-geographic bivariate scores include shares of airline passengers in origin and destination cities (snider2015barriers), tax rates and income ratios (egger2015impact), systolic and diastolic blood pressure measurements (dai2022effects), different component scores in a means testing formula (salti2022impact; kampfen2024heterogeneous), and population counts at different time periods (hinnerich2014democracy).
Figure 1 illustrates an important distinction between the geographic and non-geographic examples. In non-geographic examples, the function that describes the boundary tends to be known, and is typically linear with few irregularities and very few kink points. In contrast, in geographic BD designs the boundary is typically the border between political or administrative subdivisions; as a consequence, its shape is only available as coordinates on a map, and its geometry can be complicated. In the best case possible, it will be piecewise linear with many kink points, but it will be more irregular in many applications. In his early study, holmes1998effect already recognized this challenge and discussed explicitly the complex boundary shapes created by geographic borders. Indeed, since the influential work by mandelbrot1967long; mandelbrot1983fractal, there has been an ongoing debate among mathematicians and philosophers trying to decide whether geographic borders (and other shapes in nature) are fractals (see avnir1998geometry, and references therein). From a methodological perspective, as recently formalized in Cattaneo-Titiunik-Yu_2025_BDD-Distance; Cattaneo-Titiunik-Yu_2025_BDD-Location; Cattaneo-Titiunik-Yu_2025_BDD-Pooling, the geometry of the assignment boundary in BD designs has fundamental implications for identification, estimation, and inference. We will review some of these recent findings in the upcoming sections.
In the standard RD design, the distance between the univariate score and the single cutoff is naturally always measured by the Euclidean distance, and observations are “close” to the cutoff when their univariate score implies a “small” distance . In contrast, in the BD design, “closeness” to the boundary can be defined in different ways, depending on whether the parameter of interest is defined relative to a specific point on the boundary, or as some aggregate thereof along the entire boundary, such as an average or supremum. Furthermore, due its multivariate nature, the analysis can depend on the specific notion of distance used.
In the next sections, we discuss the two main approaches to the analysis of BD designs. The first approach uses the variation in treatment assignment induced by entire boundary simultaneously, and thus pools all observations sufficiently “close” the boundary in a single analysis, producing a single average treatment effect across all boundary points. This pooling approach relies on localization to by first computing the distance between each observation’s location and the nearest point on the boundary, denoted by , and then retaining only those observations for which this distance is no larger than a specific bandwidth, denoted by . The advantage of this approach is that it naturally mimics the univariate RD design, using the bivariate location of each unit to build a univariate score that is used directly in a “standard” univariate RD analysis. This approach thus produces a single average treatment effect estimate.
In contrast, a second approach focuses on estimating the average treatment effect at each point on the boundary, performing localization relative to the specific point of analysis, thereby retaining observations whose distance to that point is no greater than a chosen bandwidth. Estimation and inference are conducted separately at each boundary point , which captures the heterogeneity of treatment effects along the boundary. Aggregate causal effects can then be recovered by averaging (or applying other transformations to) these boundary-point specific average treatment effects, including as a special case the average treatment effect along the entire boundary that is the focus of the pooling approach. The boundary-point-specific approach can be seen as focusing on heterogeneous average treatment effects that can then be used as building blocks to construct more aggregated causal parameters. The advantage of this approach is that it captures the full richness of the BD design by first learning the heterogeneity of the average treatment effect along the boundary, which usually summarizes unit characteristics of high policy relevance such as geographic location, academic performance, or economic need.
2.1 BD Designs in Practice
We searched the academic literature in multiple social sciences to learn how applied researchers implemented the BD design. We conducted our search in four general queries. In our initial Economics query, we included ten leading journals in Economics—Journal of Political Economy, Review of Economic Studies, Econometrica, American Economic Review, Quarterly Journal of Economics, Review of Economics and Statistics, American Economic Journal: Applied Economics, American Economic Journal: Economic Policy, American Economic Journal: Macroeconomics, and American Economic Journal: Microeconomics. In our initial Political Science query, we searched six leading Political Science journals—American Political Science Review, American Journal of Political Science, Journal of Politics, Political Science Research and Methods, British Journal of Political Science, and Quarterly Journal of Political Science. In both cases we searched for terms like “boundary discontinuity”, “multivariate regression discontinuity”, and “multiscore regression discontinuity”, and focused our attention on papers published since . We also expanded our search to include other articles that were repeatedly cited by the articles collected in our initial queries.
We also searched in the field of Education, as methodological researchers in that area were among the earliest scholars who discussed the multi-score RD design in non-geographic settings. For this search, we started with the seminal methodological papers in Education by Reardon-Robinson_2012_JREE and Wong-Steiner-Cook_2013_JEBS, and collected papers that both cited this methodological papers and were cited by them. Finally, we also included some papers in environmental science, to illustrate uses of the design at the intersection between the natural and social sciences. Our review is not meant to provide an exhaustive list of the literature. This literature is vast, and cannot be fully captured by our approach since we excluded several journals and restricted the time period. Our goal is simply to provide an overview of recent empirical work using the BD design across the social sciences to gauge the most common strategies used for empirical analysis.
The results of our search are summarized in Table 5. Our search yielded 82 published papers. Of these, we found that the overwhelming majority (76 articles or approximately 93%) used a pooling approach, focusing on the average effect along the entire boundary and using the univariate closest distance to the boundary as the running variable. Fewer than 10 articles were explicit in accounting for the heterogeneity along the boundary; of these, only 5 reported effects for different points or segments along the boundary (gonzalez2021cell; grout2011land; Keele-Titiunik_2015_PA; snider2015barriers; velez2019tuning). In sum, our review of the practical literature indicates that most applied researchers adopt a pooling approach, performing localization by grouping all observations sufficiently “close” to the boundary and then estimating a single average treatment effect. These common practices motivate the taxonomy we used in the rest of this chapter.
3 Pooling-Based Methods
Given a sample of outcomes and bivariate scores, , a distance function is used to measure the closeness between any two points in . The most common example is the Euclidean distance for , , although in some applications this is not the most appropriate measure. For example, ambrus2020loss are interested in how close residences in a city are to each other in terms of how long it takes for a person to walk between them; this requires calculating a walking distance, which is different from the Euclidean distance which would not take into account, say, that the path must follow city blocks and cannot go through buildings. More broadly, in geographic settings it is common practice to employ specific geodetic distances (Banerjee_2005_Biometrics).
Once a distance function has been chosen, researchers who employ the pooling approach define the closest signed distance to the boundary,
for . For a unit , measures how far ’s location is to the boundary point that is closest to , regardless of where that boundary point is located. Following the standard RD design logic, but now expanded to the BD design (a bivariate setting), localization occurs simultaneously along the entire assignment boundary because the univariate distance to is used to construct the region such that , which covers the boundary , where denotes a bandwidth parameter. This region, called a “tubular neighborhood” in mathematics (federer2014geometric), is a natural generalization of the standard localization approach used in univariate RD designs, where the region is simply when the score is scalar and is the univariate cutoff.
Figure 2 illustrates the idea graphically. In both cases, the tubular neighborhood represents the area that includes the observations used in the pooling-based analysis. Figure 2(a) shows a “large” bandwidth , while Figure 2(b) shows a “small” bandwidth . All units contribute simultaneously, regardless of their specific location. In particular, two units and with the same signed distance to the boundary, , will be assigned to the same group (treatment or control, depending on the sign of ) and will be units away from the boundary, but could be arbitrarily far from each other.
Figure 3 presents the two resulting RD plots (Calonico-Cattaneo-Titiunik_2015_JASA) in the pooling approach, based on the outcome data and the univariate distance score (the cutoff is by construction of ). These plots include a fourth-order global polynomial fit (solid line) for visualization purposes only; those global approximations are not recommended for estimation of treatment effects because they tend to exhibit poor performance due to the Runge’s phenomenon. Instead, as anticipated in Figure 2 and demonstrated in Figure 3, the idea is to localize around the “cutoff” (i.e., the one-dimensional boundary curve) to conduct estimation and inference.
In the pooling approach, the analysis is conducted using all the observations with score within the tubular neighborhood determined by the bandwidth , that is, for all units with . However, for this subsample, the specific treatment effect estimation approach can vary substantially. To describe and unify the most common approaches in the empirical literature illustrated in Table 5 as well as other approaches recommended from a methodological perspective, we define the following generic weighted least squares regression notation:
where is the outcome variable, is a (column) vector of independent variables, and are weights. A localized regression analysis can be implemented by setting or, more generally, for a kernel function determining the relative weight given to each observation. The bandwidth determines the degree of localization, as illustrated in Figure 2. When contains polynomial expansions of , the resulting estimation approach is often called nonparametric local polynomial regression but, as we will see shortly, empirical researchers often employ other variables in their local regression fit within the tubular determined by .
We also define the treatment assignment indicator
All empirical approaches naturally include in to estimate treatment effects; the majority also include other variables such as , , boundary-segment fixed effects (formally defined below), as well as certain transformations and interactions thereof. For simplicity, we omit discussing the inclusion of predetermined covariates, but we note that researchers often further augment the basic local regression specifications with pretreatment variables such as census characteristics, terrain ruggedness and elevation, time fixed effects, among other features. For a discussion of the role of preintervention covariates in the standard RD design see Calonico-Cattaneo-Farrell-Titiunik_2019_RESTAT for efficiency gains and Calonico-Cattaneo-Farrell-Palomba-Titiunik_2025_wp for treatment effect heterogeneity.
The pooling approach for analyzing BD designs begins by choosing the localization bandwidth to determine the units with that will be used in the subsequent estimation. Until now, there were no formal methods for choosing the bandwidth in an objective and data-driven way, so researchers often considered a range of “reasonable” values. For example, black1999better reported results in three different bandwidths, keeping observations within , , and miles of the nearest point on the boundary. As in standard univariate RD designs, different choices of trade off bias and variance in the estimation, with a larger leading to more bias but less variance, and a smaller having the opposite effect. Cattaneo-Titiunik-Yu_2025_BDD-Pooling recently established formal results characterizing this tradeoff and provided formal guidance for choosing the bandwidth when implementing pooling BD methods. We discuss these methods in Section 3.1.
Once the bandwidth has been chosen, researchers pool all units with distance within and then compare those assigned to the treatment group () to those assigned to the control group (). Because the bandwidth localizes the analysis to only a “small” region on either side of the boundary , the empirical analysis compares barely treated units to barely control units which, as in the standard univariate RD design, is the basis for the causal interpretation of the comparison, as all confounders are assumed to vary smoothly at the boundary while the treatment assignment changes abruptly from zero to one.
The most basic analysis proceeds by assuming that the units within the shrinking tubular of “width” are as-if randomly assigned to treatment and control. This approach is akin to the “local randomization” framework in standard RD designs (Lee-Lemieux_2010_JEL; Cattaneo-Frandsen-Titiunik_2015_JCI; Cattaneo-Titiunik-VazquezBare_2017_JPAM). Thus, a natural treatment effect estimator is the difference-in-means of the outcome between treated and control units with distance to the boundary no larger than the bandwidth:
| (1) |
where the coefficient on is
with and . This specification can be interpreted as a local-constant polynomial fit on ; it does not include the score in the fit because under the assumption of local randomization this score is uncorrelated with the outcome.
For example, black1999better reported results using specification (1), calling it a “rough, nonparametric estimate of the value of better schools” (p. 590). She also augmented the basic specification to include “segment fixed effects”, that is, binary variables indicating whether an observation is closest to one of several segments partitioning the assignment boundary. To describe this approach formally, we let , , be disjoint segments that partition the assignment boundary, that is, . Then, is the boundary-segment allocation random index for each unit , where it is understood that when the contains more than one element, the smallest item is chosen. The vector of segment fixed effects for observation is , which collects the boundary segment binary indicators determined by ’s location. The difference-in-means specification with segment fixed effects is
| (2) |
where the intercept is removed due to its perfect collinearity with the boundary-segment fixed effects. Setting reduces to the common intercept in specification (1).
For example, black1999better used a modified version of specification (2) where she included the test scores of the school associated with observation instead of the binary treatment assignment indicator . The causal interpretation of this specification rests on the assumption that, after localizing the analysis to include only observations near the boundary of school attendance districts, the only factor that can explain a difference in average house prices between the treated and control areas is school quality, because school quality changes discontinuously at the boundary while all other determinants of house prices are equal on average. In this sense, using test scores instead of the treatment assignment indicator in specifications (1) and (2) is in the spirit of an instrumental variables strategy where is used as an instrument for test scores (which proxy school quality), and the localization to a small bandwidth around the boundary ensures that units assigned to treatment and units assigned to control are similar in all other characteristics that might affect house prices.
The seminal research design used by black1999better is prior to the development of modern methods for RD analysis, which occurred predominantly during the s; see Cattaneo-Titiunik_2022_ARE for an overview. In more recent practice, the simple specifications (1) and (2) are not commonly used because researchers tend to adopt a continuity-based approach rather than a local randomization approach for interpretation. Under the continuity-based approach, the local constant fit has poor control of the misspecification bias, and hence practitioners prefer more flexible local regression specifications.
Later empirical work using BD designs made more explicit the connection between geographic discontinuities in treatment assignment and the classical RD design, following more closely the methods of univariate continuity-based RD analysis, which emphasized including the RD score in the polynomial specification to allow for arbitrary dependence between the RD score and outcome even within the bandwidth. Although there is considerable diversity in the specifications used in more recent practice, most researchers that include the score adopted one of two main strategies: a polynomial fit of the outcome on the bivariate score , or a polynomial fit of the outcome on the univariate distance .
In an influential contribution, dell2010persistent augmented specification (2) by including the bivariate score . She studied the long-run impact of the mita, a forced mining labor system instituted by the Spanish in Peru and Bolivia between and , using the historical boundary that determined which communities were forced to send labor to define treated and control areas. She studied the effect of the mita on contemporary outcomes measured at the household level, including consumption and childhood stunting. Her main local regression specification was
| (3) |
which includes the treatment indicator, boundary segment fixed effects, and a polynomial expansion of the bivariate score (with in her case). The estimated coefficient of interest is always the coefficient that accompanies the treatment indicator . In our notation, with . The inclusion of (a polynomial basis of) the bivariate location score is meant to control for “the smooth effects of geographic location” dell2010persistent. A notable characteristic of this specification is that it does not include interactions between the bivariate score and the treatment indicator, which imposes the restriction that the polynomials be identical in the treated and control areas. As for bandwidth selection, dell2010persistent considered km of the mita boundary first, but this choice is later reduced to km and km in subsequent analyses.
Specification (3) has become a common strategy to analyze BD designs. For example, mendez2022multinationals use it to study the effect of private sector companies in the development of local amenities, using a land concession to the multinational United Fruit Company in Costa Rica; paulsen2023foundations use it to estimate the effect of school funding on future earnings and political participation in New York, comparing areas that received higher funding to neighboring areas that did not; and de2018agents compare areas in South Africa where traditional leaders remain highly influential to areas where they are not, and estimate the electoral impact of strong political ties between traditional leaders and the African National Congress (ANC). Other examples are listed in Table 5. All of these examples follow specification (2) by including a flexible polynomial expansion of latitude and longitude, as well as boundary-segment fixed effects, but without allowing for interactions with the treatment indicator .
An alternative approach is to directly mimic the standard univariate RD design, taking the scalar (signed) distance to the closest point on the boundary for every unit, , as the univariate score, and employing local polynomial regression. The most basic local regression specification in this context takes the form:
| (4) |
with the polynomial expansion reducing to the simpler vector of the univariate distance . As in the case of specification (3), specification (4) can also be augmented with boundary-segment fixed effects:
| (5) |
In both cases, the treatment effect estimator is the coefficient estimate accompanying . Figure 5 offers a graphical representation of this empirical strategy.
Although specifications (4) and (5) aim to mimic the univariate RD design, they differ from best practices in an important respect. In the RD design, the recommended specification is a polynomial fit of the outcome on an intercept, the treatment assignment indicator, a polynomial expansion of the score, and the interaction between this expansion and the treatment assignment indicator. The latter interaction is crucial to allow the polynomial fit for control units to be different from the polynomial fit for treated units. None of the specifications discussed so far allow for this flexibility, potentially introducing a larger misspecification bias.
The most flexible specification based on takes the form:
| (6) |
where the estimated coefficient of interest continues to be the one accompanying . In practice, specification (6) is often implemented with and, depending on whether standard software for RD design analysis is used, the specification may employ a non-uniform weighting kernel (e.g., a triangular kernel that down-weights observations as their distance from the boundary increases). Putting aside the inclusion of boundary-segment fixed effects, Figure 6 illustrates the importance of including the interaction term in the treatment effect estimation, as in specification (6). Comparing Figures 5 and 6 shows that the two approaches can lead to different local approximations, with specifications (1) through (5) potentially exhibiting a larger bias than specification (6).
Specification (6) is one of the most commonly used in recent practice. Examples include ayres2021environmental, eugster2017culture, jones2022factor, lalive2008extended, and lowes2021concessions. We note that several articles write their empirical specifications as (4) or (5), but they actually implement specification (6) in their analyses (as clarified in text, footnotes or replication files). These researchers employ standard practices and software from the univariate RD design literature, which follows specification (6).
Specification (3) can also be augmented to include the interaction term between the treatment assignment and the polynomial expansion of the bivariate score :
| (7) |
where the estimated treatment effect coefficient accompanying enjoys better bias properties due to the added flexibility in the local regression specification. However, this more flexible approach is not as common in the empirical literature—some exceptions are gonzalez2021cell, egger2015impact, and jones2022effects.
In some applications, researchers estimate treatment effects in BD designs by considering two separate univariate RD designs, that is, keeping one dimension fixed and studying the other dimension of the bivariate score. For example, LondonoVelezRodriguezSanchez_2020_AEJ investigate the effects of Ser Pilo Paga (SPP), a government subsidy in Colombia that provided tuition support for post-secondary students to attend a government-certified higher education institution. Eligibility was based on both merit and economic need: in order to qualify for the program, students had to obtain a high grade in Colombia’s national standardized high school exit exam, SABER 11, and they also had to come from economically disadvantaged families, as measured by the survey-based wealth index SISBEN. The resulting BD design looks exactly as in Figure 1(b) with . The authors analyzed it by considering each cutoff separately and then pooling all observations in the other dimension, leading to two univariate RD designs: one with score in the sample , and the other with score in the sample . This approach is equivalent to splitting the analysis along each of the two linear segments of and then employing a pooling approach. This idea leads to the following local regression specification:
| (8) |
where the indicator variable is removed due to the perfect collinearity with , since contains indicator variables capturing a partitioning of , and denotes the Kronecker product. The treatment effect of interest is now the collection of coefficients accompanying , one for each segment. Specification (8) is roughly equivalent to employing specification (6) times, in each case only using the data corresponding to one of segments. (The only minor difference is whether observations are assigned to only one segment, or reused for different segment-specific treatment effect estimation implementation.) For instance, ou2010leave, LondonoVelezRodriguezSanchez_2020_AEJ and salti2022impact employ this boundary-segment specific pooling-based approach with , analyzing each of the two linear segments as in Figure 1(b). Another empirical example employing this approach is .
Finally, some researchers have also included both the univariate distance and the bivariate score in their local regression specifications—see, for example, ehrlich2018persistent and dehdari2022origins. In addition, it is possible to conceptualize more flexible specifications by including other interaction terms between , , and analogous to specification (8). These approaches are uncommon in practice according to our literature review (Table 5), so we do not discuss them further.
3.1 Methodological Results
While there has been a proliferation of different empirical strategies relaying on the pooling of observations closest to the treatment assignment boundary, little is known about their properties and relative merits for the analysis of BD designs. Cattaneo-Titiunik-Yu_2025_BDD-Pooling recently investigated the main empirical approaches employing pooling-based methods, offering formal identification, estimation, and inference results. Their methodological results rely on techniques from geometric measure theory (federer2014geometric), which are used to characterize how the different treatment effect estimates emerging from specifications (1) through (6) behave in large samples, that is, when as . These asymptotic approximations form the basis for the continuity-based approach in standard univariate RD designs (Hahn-Todd-vanderKlaauw_2001_ECMA), and are nowadays the most common way of characterizing the properties of causal treatment effect estimators in all RD settings. This section summarizes some of the findings in Cattaneo-Titiunik-Yu_2025_BDD-Pooling.
The main challenge when studying pooling-based methods for BD designs is that the localization is done using a sequence of shrinking tubular neighborhoods covering the one-dimensional submanifold , as illustrated in Figure 2. An important implication is that the geometry of the treatment assignment boundary and the specific distance function used can substantially affect statistical properties. In this section, we omit most technicalities, and focus instead on the main ideas and conclusions to aid empirical researchers interpret and implement pooling-based methods.
Because the treatment assignment boundary could be highly irregular, particularly in Geographic RD designs, it is necessary to restrict its geometry to establish formal properties. A minimal assumption commonly used in mathematics is to require to be a rectifiable curve, that is, to assume that it has finite length. More formally, is a rectifiable curve if and only if it can be reparameterized by a Lipschitz continuous function. This restriction allows for formally defining and computing integrals of functions along the one-dimensional domain . More importantly, under additional technical conditions, regularity on the geometry of allows for those integrals to be computed over shrinking tubular neighborhoods and for limits to be well-defined, which is at the core of the results in Cattaneo-Titiunik-Yu_2025_BDD-Pooling. Under regularity conditions, the authors establish the following result:
| (9) |
where , for each , is a tubular covering ,
and denotes its Jacobian, and denotes the -dimensional Hausdorff measure. The constant can also be characterized, but it is not important for the upcoming discussion (it can be set to for the analysis of pooling-based methods). Note that .
The integral is a natural generalization of the standard line integral, where serves as the rigorous “length element”, enabling a robust framework for integration over complex one-dimensional domains in the plane. In particular, if is piecewise linear (as in Figures 1(b) and 2), the integral can be reduced to the sum of integrals over the linear segments using a natural smooth curve parametrization and standard Riemann integration (when is Riemann integrable). For notational simplicity, we write
but with the understanding that the left-hand-side integral is just notation for the rigorously defined right-hand-side integral.
The limiting integral in (9) can be used to give precise meaning to the probability limit of the treatment effect estimates from any of the pooling specifications discussed previously. However, we need to introduce standard potential outcomes notation (see, e.g., Hernan-Robins_2020_Book, for a review) to give a causal interpretation to the probability limit of those parameters. For , let and denote the potential outcomes for unit under control and treatment assignment, respectively, and hence the observed outcome is
To streamline the presentation, we consider only the simplest specification (1), leading to the local difference-in-means estimator , to illustrate the methodological findings in Cattaneo-Titiunik-Yu_2025_BDD-Pooling. The other more flexible local regression specifications will enjoy similar properties.
The probability limit of can be naturally characterized as a ratio of integrals along the boundary. As and leveraging the logic of the law of large numbers, will be close in probability to , and
where the limit is taken as , denotes the Lebesgue density of , , and the conclusion follows by (9) upon setting and . While is redundant in this case, and , we nonetheless make it explicit to highlight that a term will appear when a non-uniform kernel is used. Similarly, . Furthermore, by the same logic, the two (properly rescaled) numerators of will be close in probability to their expectations, where
and, analogously,
The above calculations require precise regularity conditions and technical work, including restrictions on the geometry of and the distance function to guarantee that the expressions are well-defined and the limits are valid.
Putting the above calculations together,
The parameter is called the Boundary Average Treatment Effect (BATE) by Cattaneo-Titiunik-Yu_2025_BDD-Pooling. This parameter was heuristically introduced in the education literature by Wong-Steiner-Cook_2013_JEBS, who expressed it as
using the notation . Around the same time, Keele-Titiunik_2015_PA also discussed the BATE parameter in the context of Geographic RD designs.
The parameter captures a density-weighted average of average treatment effects at each point along the boundary, and thus aggregates the potentially heterogeneous treatment effects captured by . Cattaneo-Titiunik-Yu_2025_BDD-Location call the Boundary Average Treatment Effect Curve. As discussed in Section 4, this functional causal parameter can be seen as a building block for several causal parameters of interest in BD designs.
Going beyond identification, Cattaneo-Titiunik-Yu_2025_BDD-Pooling also study estimation accuracy and uncertainty quantification for pooling-based methods. For point estimation, the authors shows that
when as , and where denotes up to higher order terms in probability, denotes a leading bias term, and denotes the asymptotic variance that is bounded and bounded away from zero. Characterizing the precise order of the bias is difficult without additional restrictions on the treatment assignment boundary : the authors show that in general. Furthermore, as in the case of standard univariate RD designs, and under additional regularity conditions, the order of the bias may be improved when and are included in the local regression specification, as in (6), leading to . Another interesting feature is that the rate of decay of the variance component is the same as in one-dimensional nonparametric estimation: this improvement in convergence rate comes from the aggregation along the boundary of the bivariate nonparametric function . See Cattaneo-Titiunik-Yu_2025_BDD-Location and chen2025semiparametric for more discussion.
For bandwidth selection, under regularity conditions on and , the bias may admit a valid expansion as in the standard RD design literature. Therefore, optimal bandwidth selection procedures based on mean square error (MSE) or coverage error minimization may be used for implementation of pooling methods. See Calonico-Cattaneo-Farrell_2020_ECTJ for a review.
For distribution theory and inference, consider the usual test statistic
where denotes any of the usual variance estimators from (local, weighted) least squares regression methods. Under regularity conditions, and if and , , where denotes the standard Gaussian cumulative distribution function. Importantly, as discussed by Calonico-Cattaneo-Titiunik_2014_ECMA in the context of standard univariate RD designs, the usual confidence intervals
with , will not be valid whenever there is local misspecification. More precisely, the validity of the usual confidence intervals require the small bias condition , which is not satisfied when the bandwidth is chosen to be MSE-optimal (or is otherwise “large”). Therefore, valid uncertainty quantification depends on the choice of bandwidth and regression specification, because the bias would be of order , under regularity conditions on , , and the underlying conditional expectation functions.
If the MSE-optimal bandwidth is used, which in the context of pooling-based methods would depend on the order of the bias , then the small bias condition will not be satisfied, and thus the resulting inference procedures will over-reject the null hypothesis, sometimes substantially. Following Calonico-Cattaneo-Titiunik_2014_ECMA, a simple and practical solution is to employ robust bias correction. The two-step procedure is simple and intuitive: first, the pooled treatment effect is estimated using a choice of polynomial approximation and its associated MSE-optimal bandwidth; second, confidence intervals and hypothesis tests are constructed by estimating the pooled treatment effect and variance estimator using the same bandwidth choice but with a polynomial of larger order . In practice, the most common choice is for MSE-optimal treatment effect estimation, and for robust bias-corrected uncertainty quantification. This inference approach has several theoretical advantages (Calonico-Cattaneo-Farrell_2018_JASA; Calonico-Cattaneo-Farrell_2022_Bernoulli), and has been validated empirically (Hyytinen-Tukiainen-etal2018_QE; DeMagalhaes-etal_2025_PA). Standard RD software for estimation and inference in univariate score settings (rdrobust, rdhte) available at https://rdpackages.github.io/ can be used directly, provided the regularity conditions are satisfied.
4 Heterogeneity and Aggregation Along the Boundary
Our review of the literature indicates that most empirical researchers using BD designs employ pooling-based methods, thereby focusing on (density-weighted) average treatment effects along the entire (or a few segments of the) boundary . In most implementations, the estimation begins by calculating the distance to the nearest boundary point for every observation, keeping only observations for which this distance is less than a bandwidth, and then continues by pooling all observations in a single local regression analysis, mimicking standard univariate RD methods.
Although the focus on a single scalar parameter is a convenient way of summarizing the information provided by the BD design, this approach fails to exploit its full richness. In this section, we discuss a more general approach for the analysis of BD designs that starts by studying the average treatment effect at each point on the boundary, and then uses this functional causal parameter as the building block for aggregation along the boundary, hence recovering other causal treatment effects. This section summarizes the recent results on identification, estimation and inference discussed in Cattaneo-Titiunik-Yu_2025_BDD-Distance; Cattaneo-Titiunik-Yu_2025_BDD-Location.
The starting point is to consider the Boundary Average Treatment Effect Curve (BATEC):
which was already implicitly introduced as part of the probability limit emerging from the pooling approaches. However, the functional causal parameter is of independent interest because it captures the average treatment effects at each point on the boundary. Despite capturing potentially interesting causal evidence of heterogeneous treatment effects along the assignment boundary, only a handful of empirical papers have investigated the BATEC or variations thereof. Exceptions include Keele-etal_2017_AIE, velez2019tuning, and gonzalez2021cell.
Another feature of the BATEC is that it provides the key building block for constructing other causal parameters of interest by aggregation along the boundary. For example, Cattaneo-Titiunik-Yu_2025_BDD-Location discuss the following two parameters.
-
•
Weighted Boundary Average Treatment Effect (WBATE):
where denotes a user-chosen weighing scheme.
-
•
Largest Boundary Average Treatment Effect (LBATE):
The WBATE parameter represents a weighted average of the (potentially heterogeneous) treatment effects at each boundary point. In particular, setting recovers the BATE parameter . In practice, other weighting schemes may also be of interest. See Reardon-Robinson_2012_JREE and Wong-Steiner-Cook_2013_JEBS for early methodological discussions, and Rischard-Branson-Miratrix-Bornn_2021_JASA for other examples considered in the context of Bayesian methods for BD designs. The insightful discussion of Qiu-Stoye_2026_BookCh--Discussion provides another application of the WBATE parameter. The LBATE parameter captures the “best” causal treatment effect along the assignment boundary, and thus can be useful for evaluating or developing targeted policies. See, for example, Andrews-Kitagawa-McCloskey_2024_QJE.
Figure 7 gives a graphical representation of the BATEC, WBATE, and LBATE in the context of the same stylized BD design used in Section 3. Because is a function, estimation and inference proceeds by discretizing the boundary to construct local regression estimates for each point on the grid of evaluation points chosen. Figure 7(a) demonstrates the idea with evenly-spaced grid points, denoted by . As discussed in the upcoming subsections, distance-based and location-based methods employ these points to conduct estimation and inference, both pointwise and uniformly along the assignment boundary.
Figure 7(b) illustrates graphically the three target causal parameters , , and . The functional causal parameter indicates the presence of heterogeneity along a region of the boundary, while and give two distinct notions of aggregation of those heterogeneous treatment effects. This stylized example is motivated by the SPP study by LondonoVelezRodriguezSanchez_2020_AEJ introduced above, where . Re-analyzing the SPP application, Cattaneo-Titiunik-Yu_2025_BDD-Location found little evidence of heterogeneous treatment effects along the dimension, but declining treatment effects along the dimension. This empirical finding indicates that the average treatment effects along the boundary are roughly similar for all students with low academic performance () and regardless of their wealth level (), while heterogeneity in treatment effects is present for the wealthier eligible students () as their academic performance increases ().
The discussion so far has focused on causal parameters defined along the entire , but in some applications researchers have considered different segments of . Specification (8), and its motivation from the implementation used by LondonoVelezRodriguezSanchez_2020_AEJ and others, gives one example. Employing the BATEC parameter as a building block, it is natural to define causal parameters as integrals or suprema of over a specific region of . All the methods developed in Cattaneo-Titiunik-Yu_2025_BDD-Distance; Cattaneo-Titiunik-Yu_2025_BDD-Location immediately apply to those cases, offering estimation and inference methods for segment-specific causal parameters based on the core BATEC parameter.
4.1 Distance-Based Methods
The distance used by all pooling approaches is based on the distance of each observation to its nearest point on the boundary, denoted by . Researchers sometimes first construct the signed distance to a pre-specified point on the boundary
for each unit , and then use this distance to construct a tubular covering to implement pooling methods. One example of this approach is ehrlich2018persistent. While the resulting tubular will often be geometrically different from , all the ideas and results discussed in Section 3 continue to apply.
However, the intermediate boundary-point-specific signed distance for each unit can also be used directly to estimate and conduct inference for the BATEC. The idea is to view the outcomes and the boundary-point-specific scalar distance variables, , as a one dimensional RD design for each point . For example, this approach was used by Keele-Titiunik_2015_PA and velez2019tuning to study the effect of TV exposure on voter turnout, using reception and media market boundaries that determine whether citizens are exposed to specific television programming. In these examples, each unit’s score is a latitude-longitude pair that determines their place of residence, the outcome is whether the individual turned out to vote, and the boundary is the media market or reception boundary that separates the treated from the control region. In Keele-Titiunik_2015_PA, the treatment is exposure to presidential candidate television ads during the presidential election, and in velez2019tuning the treatment is exposure to Spanish-language television. In both cases, researchers first selected a grid on the boundary (e.g. Figure 7(a)), and then used the local regression specification:
| (10) |
for each . The key difference is that estimation is conducted with localization for each evaluation point on the boundary, as opposed to localization to the entire boundary as in specifications (1) through (8). Therefore, the estimated coefficient accompanying gives a point estimate of for each
Cattaneo-Titiunik-Yu_2025_BDD-Distance study the large sample properties of the distance-based estimation procedure for BATEC obtained from specification (10). They give necessary and/or sufficient conditions for identification, estimation, and inference in large samples, both pointwise and uniformly along the boundary. Their identification result requires minimal regularity of the boundary , the distance function , and the underlying data generating process, building on continuity-based identification in standard univariate RD designs (Hahn-Todd-vanderKlaauw_2001_ECMA). Cattaneo-Titiunik-Yu_2025_BDD-Distance also show that the bias of the distance-based point estimator will depend on the smoothness of the assignment boundary, while the variance of the estimator will be of order . Putting these two results together, it follows that bandwidth selection methods for standard univariate RD designs will either minimize the mean square error of the estimator or deliver undersmoothing, depending on the geometry of the assignment boundary and other assumptions. For inference, the authors establish asymptotic validity of confidence intervals and confidence bands, provided the bias of the distance-based point estimator is small enough (which, in turn, crucially depends on the geometry of ).
4.2 Location-Based Methods
While the distance-based estimator constructed using specification (10) provides a natural bridge between the pooling methods and heterogeneity analysis, Cattaneo-Titiunik-Yu_2025_BDD-Distance demonstrate that this approach would require a smooth assignment boundary for higher-order bias reduction. Otherwise, the distance-based approach will generate a consistent estimator with possibly a high bias, and therefore require a small bandwidth for estimation and inference validity. A solution to this problem is to use local bivariate regression methods.
Cattaneo-Titiunik-Yu_2025_BDD-Location study the properties of local regression treatment effect estimators directly employing the location information encoded in for each unit. The authors develop pointwise and uniform estimation and inference methods for both the BATEC and transformations thereof, such as the WBATE and LBATE. Their local regression specification takes the form:
| (11) |
where is the grid of points on the boundary (e.g., Figure 7(a)). This local polynomial specification crucially depends on directly via the th order polynomial expansion . The weights are determined by the bivariate kernel function , and the localization to each point continues to be is controlled by the bandwidth . In particular, is a valid choice, demonstrating an interesting connection with distance-based methods as implemented via specification (10). The estimated coefficient accompanying , denoted by , gives the location-based point estimate of , for each .
In the location-based setting, identification of for follows directly from a generalization of continuity-based identification results in the standard univariate RD design (see Hahn-Todd-vanderKlaauw_2001_ECMA; Papay-Willett-Murnane_2011_JoE; Reardon-Robinson_2012_JREE; Wong-Steiner-Cook_2013_JEBS; Keele-Titiunik_2015_PA; Cattaneo-Keele-Titiunik-VazquezBare_2016_JOP). Under minimal regularity conditions on the boundary and data generating process, Cattaneo-Titiunik-Yu_2025_BDD-Location establishes (pointwise and) uniform convergence rates of the form , demonstrating that the location-based estimator does not require stringent smoothness restrictions on the boundary to achieve higher order debiasing. Furthermore, the authors establish pointwise and integrated mean square error expansions for the estimator, and develop MSE-optimal bandwidth selection methods. By combining these results, they develop optimal point estimation of the BATEC, both pointwise and uniform, based on specification (11).
For uncertainty quantification, Cattaneo-Titiunik-Yu_2025_BDD-Location consider confidence intervals and confidence bands of the form:
where denotes any of the usual variance estimators from (weighted) least squares regression methods, and is the quantile used depending on inferential goal. For confidence intervals, the usual Gaussian quantile is asymptotically valid (i.e., ), while for confidence bands, a new (larger) quantile needs to be constructed to capture the joint uncertainty features of the estimators for different values along the boundary . As in the case of pooling-based and distance-based methods, a key condition needed for validity of these confidence intervals and bands is the small (misspecification) bias property. Cattaneo-Titiunik-Yu_2025_BDD-Location address this issue by relying on robust bias correction (Calonico-Cattaneo-Titiunik_2014_ECMA; Calonico-Cattaneo-Farrell_2022_Bernoulli), a construction that proceeds in two steps.
-
•
Step 1. For a choice (usually ), the MSE-optimal bandwidth is computed, and then for that choice of bandwidth the point estimator is computed using specification (11). As a result, is an asymptotically MSE-optimal point estimator of .
-
•
Step 2. For a choice (usually ), the point estimator and variance estimator are computed using specification (11) with the same bandwidth used in Step 1. As a result, inference procedures are robust bias-corrected, and hence asymptotically valid.
Cattaneo-Titiunik-Yu_2025_BDD-Location give technical and computational details, which we omit to conserve space, and because they do not fundamentally change the core methodological ideas.
Finally, can also be used to estimate and conduct inference for WBATE and LBATE via plug-in methods. Cattaneo-Titiunik-Yu_2025_BDD-Location provide sufficient conditions and study the validity of such methods. In particular, the WBATE corresponds to an integral over a submanifold of the nonparametric estimator . See chen2025semiparametric for related theoretical results when employing series approximations instead of local polynomial regression as commonly done in RD settings.
The general-purpose R package rd2d implement both distance-based and location-based methods (https://rdpackages.github.io/). See Cattaneo-Titiunik-Yu_2025_rd2d for more discussion. Figure 8 gives a stylized example of the type of results that can be obtained via distance-based and location-based methods, building on the same setup as in Figure 1. These figures are motivated by empirical work using the SPP application (LondonoVelezRodriguezSanchez_2020_AEJ).
5 Recommendations for Practice
Our review of the empirical literature employing BD designs showed that the overwhelming majority of studies focus exclusively on the identification, estimation and inference for the BATE using pooling-based methods (Section 3). This scalar parameter captures a density-weighted average of potentially heterogeneous treatment effects along the assignment boundary. While modern empirical work has recognized the importance of localization and flexible local parametrization, the current literature often exhibits three key limitations: (i) local regression specifications frequently omit the interaction between the treatment assignment variable and the flexible regression terms or , risking significant misspecification bias; (ii) the bandwidth is often chosen in an ad hoc manner; and (iii) the role of misspecification bias from a “large” bandwidth, such as an MSE-optimal choice, is often ignored, which can lead to over-rejection of the null hypothesis. Cattaneo-Titiunik-Yu_2025_BDD-Pooling provide a formal study of pooling-based methods that explicitly addresses these limitations. Moving forward, we recommend that empirical researchers implementing pooling-based methods always use specifications (6) or (7), select an MSE-optimal bandwidth, and conducting inference using robust bias-corrected methods.
More broadly, as noted by Cattaneo-Titiunik-Yu_2025_BDD-Distance; Cattaneo-Titiunik-Yu_2025_BDD-Location, a more general approach to analyzing and interpreting BD designs is to first focus on the BATEC parameter, and then consider other aggregate causal treatment effects as transformations thereof. As summarized in Section 4, distance-based and location-based methods capture the full richness of the BD design by estimating average treatment effects at each point along the assignment boundary. These methods also recover the BATE parameter as a special case, in addition to other interesting causal parameters such as the LBATE. These methods can be implemented using the general-purpose R package rd2d available at https://rdpackages.github.io/, and discussed in Cattaneo-Titiunik-Yu_2025_rd2d. In particular, location-based methods are the most robust and general approach; we therefore recommend them for future empirical work leveraging BD designs.
| Article | Distance | Specifications | Bandwidth | Heter | ||||||||||
| \endfirsthead Table 0 – Continued from previous page | ||||||||||||||
| Article | Distance | Specifications | Bandwidth | Heter | ||||||||||
| \endhead Continued on next page… | ||||||||||||||
| \endfoot Note: MSE denotes optimal mean squared error bandwidth choice. Binding score refers to the approach of constructing a single univariate score as the minimum of all scores that determine treatment assignment, known as the binding score, and then conducting a univariate RD analysis using the binding score (see, e.g., Reardon-Robinson_2012_JREE). A specification number with the symbol ⋆ indicates it has been modified in a some way; for details, we refer readers to the article. GPA denotes grade point average. \endlastfootCard-Krueger_1994_AER | Employment | Minimum wage increase | Location of towns/counties | NA | (1), (2) | NA | No | |||||||
| holmes1998effect | Manufacturing activity | Right-to-work policies | Lat, Lon | Euclidean | (1), (2), (6) | Manual | Yes | |||||||
| black1999better | House prices | School quality | Lat, Lon | Euclidean | (1), (2) | Manual | No | |||||||
| kane2003school | House prices | School quality | Lat, Lon | Euclidean | (1), (2) | Manual | No | |||||||
| kane2006school | Housing prices | School quality | Lat, Lon | Euclidean | (1), (2)⋆ | Manual | No | |||||||
| bayer2007unified | House prices | School quality | Lat, Lon | Euclidean | (1), (2) | Manual | No | |||||||
| lalive2008extended | Unemployment duration | Unemployment benefits | Lat, Lon | Driving time | (6) | Manual | No | |||||||
| dell2010persistent | Consumption, childhood stunting | Forced mining labor | Lat, Lon | Euclidean | (3) | MSE | No | |||||||
| ou2010leave | High school dropout | Failure of high school exit exam | Mathematics score, Language score | Euclidean | (8) | Manual | No | |||||||
| dube2010minimum | Employment | Increase in minimum wage | Lat, Lon | County adjacency | (1), (2) | Manual | No | |||||||
| grout2011land | Property values | Land-use regulations | Lat, Lon | Euclidean | (8) | Manual | Yes | |||||||
| robinson2011evaluating | English proficiency | English proficiency reclassification | Scores on five language tests | Euclidean | Binding score | MSE | No | |||||||
| eugster2011demand | Demand for social insurance | Culture | Lat, Lon | Driving kilometers | (6) | Manual | No | |||||||
| hinnerich2014democracy | Political regimes | Income redistribution | Population in two separate years | Euclidean | (8) | MSE | No | |||||||
| ferwerda2014political | Resistance activity | Devolution of governing authority | Lat, Lon | Euclidean | (6) | MSE, Manual | No | |||||||
| snider2015barriers | Airline industry outcomes | Access to airport facilities granted to new airlines | Share of passengers in top two carriers in origin and destination city | (1), (11) | MSE, Manual | Yes | ||||||||
| barone2015telecracy | Vote shares | Switch from analog to digital TV | Lat, Lon | Euclidean | (5) | Manual | No | |||||||
| michalopoulos2016long | Civil conflict | Ethnic partitioning | Lat, Lon | Euclidean | (1), (2) | Manual | No | |||||||
| egger2015impact | Foreign investment | Controlled foreign company rules | Tax rate, Income measures | NA | (7) | MSE | No | |||||||
| Keele-Titiunik_2015_PA | Voter turnout | Political TV advertisement | Lat, Lon | Chordal | (10) | MSE | Yes | |||||||
| macdonald2016effect | Crime | Policing | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| evans2017smart | STEM major | Financial incentive | GPA, Family contribution score | Euclidean | (8) | MSE | No | |||||||
| eugster2017culture | Unemployment | Culture | Lat, Lon | Road distance | (6) | Manual | No | |||||||
| kumar2018restrictions | Mortgage default | Limits on home equity borrowing | Lat, Lon | Euclidean | (4), (3) | Manual | No | |||||||
| ehrlich2018persistent | Economic density | Subsidies | Lat, Lon | Euclidean | (4)+ | MSE, Manual | No | |||||||
| dell2018historical | Economic outcomes | Dai Viet administrative institutions | Lat, Lon | Euclidean | (3), (5), (5)+ | Manual | No | |||||||
| dell2018nation | Public goods, political attitudes | War strategies | Lat, Lon | Euclidean | (3) | Manual | No | |||||||
| clinton2018politics | Voter registration and turnout | Medicaid expansion | Lat, Lon | Euclidean | (6) | Manual | No | |||||||
| spenkuch2018political | Voter turnout | Television advertisement | Lat, Lon | Euclidean | (2), (6) | MSE, Manual | No | |||||||
| de2018agents | Party vote shares | Political alignment | Lat, Lon | Euclidean | (3) | Manual | No | |||||||
| eugster2019culture | Tax competition | Culture | Lat, Lon | Road | (1) in DID | Manual | No | |||||||
| giuntella2019sunset | Sleep duration, Health outcomes | Timing of natural light | Lat, Lon | Euclidean | (6) | MSE, Manual | No | |||||||
| johnson2019effects | High School graduation | English learner classification | Scores on two language tests | Euclidean | Binding score | MSE | No | |||||||
| velez2019tuning | Voter turnout | Spanish-language television station | Lat, Lon | Euclidean | (1), (2), (10) | MSE, Manual | Yes | |||||||
| dupraz2019french | Schooling | Colonial legacies | Lat, Lon | Euclidean | (6), (3) | MSE | No | |||||||
| moscona2020segmentary | Conflict | Segmentary lineage organization | Lat, Lon | Euclidean | (6) | Manual | No | |||||||
| ambrus2020loss | Property value | Cholera epidemic | Lat, Lon | Walking | (6) | MSE, Manual | No | |||||||
| dell2020development | Schools, education levels | Sugar cultivation for Dutch cultivation system | Lat, Lon | Euclidean | (3) | Manual | No | |||||||
| he2020watering | Pollution | Water quality monitoring | Lat, Lon | Euclidean | (6) | MSE | No or NA | |||||||
| ito2020willingness | Willingness to pay for clean air | Lat, Lon | Euclidean, road | (6) | MSE, Manual | No | ||||||||
| wuepper2020countriesErosion | Soil erosion | Country borders | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| albertus2020land | Conflict | Land reform | Lat, Lon | Euclidean | (6) | MSE, Manual | No | |||||||
| wuepper2020countriesPollution | Crop yield gaps, nitrogen pollution | Country borders | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| schafer2020time | Voter turnout | Time zones | Lat, Lon | Chordal | (6) | MSE, Manual | No | |||||||
| letsa2020mechanisms | Attitudes towards local power | Colonial legacies | Lat, Lon | Euclidean | (3) | Manual | No | |||||||
| LondonoVelezRodriguezSanchez_2020_AEJ | College enrollment | Financial subsidy | High school exit exam score, wealth index | Euclidean | (8) | MSE | No | |||||||
| gonzalez2021cell | Election fraud | Cell phone coverage | Lat, Lon | Euclidean | (6), (7) | MSE | Yes | |||||||
| ayres2021environmental | Land value | Adjudication of groundwater rights | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| laliberte2021long | School quality | Educational achievement | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| aaronson2021effects | Characteristics of urban neighborhoods | Red lining maps | Lat, Lon | Euclidean | (2) in DID | Manual | No | |||||||
| appau2021long | Agricultural productivity | Bombing intensity | Lat, Lon | Absolute difference between centroid and 17th parallel north latitude | (4) (first stage) | Manual | No | |||||||
| lowes2021concessions | Education, health, wealth | Concession to extract rubber | Lat, Lon | Euclidean | (6) | MSE, Manual | No | |||||||
| sides2022effect | Election outcomes | Television advertisement | Lat, Lon | Euclidean | (2) | Manual | No | |||||||
| zheng2022valuation | House prices | Charter school entry | Lat, Lon | Euclidean | Event study in sample near boundary | Manual | No | |||||||
| dehdari2022origins | Regional identity | Actions of nation states | Lat, Lon | Euclidean | (6) | MSE, Manual | No | |||||||
| mendez2022multinationals | Living standards | Land concession to multinational firm | Lat, Lon | Euclidean | (3) | Manual | No | |||||||
| jones2022factor | Profits, irrigation efficiency | Irrigation | Lat, Lon | Euclidean | (6) | Manual | No | |||||||
| henn2023complements | State capacity | Remoteness from administrative headquarters | Lat, Lon | Euclidean | (6)⋆ | Manual | No | |||||||
| jones2022effects | Enrollment, graduation | Tuition scholarship | High school GPA, SAT/ACT score | NA | (7) | Manual | No | |||||||
| dai2022effects | Lifestyle outcomes | Hypertension diagnosis | Systolic and Diastolic blood pressure | Euclidean | (8) | MSE | No | |||||||
| salti2022impact | Economic outcomes | Cash assistance | Two scores in means testing formula | NA | (8)⋆ | MSE | No | |||||||
| castro2022effect | Teacher retention, student learning | Recruitment bonus | Population, distance to capital | Euclidean | (8) | MSE | No | |||||||
| moussa2022impact | Children health outcomes | Cash assistance | Two scores in means testing formula | NA | (8)⋆ | MSE | No | |||||||
| mangonnet2022playing | Protected area designation | Political alignment | Lat, Lon | Euclidean | (2) | Manual | No | |||||||
| baragwanath2023collective | Forest growth | Property rights | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| larsen2024long | Educational attainment | Grade retention | Mathematics and English Language Arts test scores | Euclidean | (8) | MSE | No | |||||||
| woller2023cost | Vote buying | Cost of voting | Lat, Lon | Euclidean | (4) | Manual | No | |||||||
| paulsen2023foundations | Voter turnout, economic outcomes | State funding for common schools | Lat, Lon | Euclidean | (3) | Manual | No | |||||||
| prillaman2023strength | Political participation | Participation in women-only credit groups | Lat, Lon | Euclidean | (2), (3) | Manual | No | |||||||
| mcalexander2023international | Rebel actions | UN partition line of Palestine | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| murphy2023dual | Special education placement | English learner designation | Various scores of language assessment | NA | Binding score | MSE | No | |||||||
| bjerre2024attendance | School enrollment | School change | Lat, Lon | Euclidean | (2) in DID | Manual | No | |||||||
| kampfen2024heterogeneous | Blood pressure | Blood pressure diagnosis | Systolic and Diastolic blood pressure | NA | Binding score | MSE | No | |||||||
| wuepper2024public | Deforestation | Country borders | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| cox2024bound | Voter mobilization | Social networks | Lat, Lon | Fastest driving | (1)⋆ | NA | No | |||||||
| doucette2024parliamentary | Urbanization, commercialization | Duchy of Württemberg | Lat, Lon | Euclidean | (4)+ | MSE, Manual | No | |||||||
| doucette2024pre | Party support | Inclusive institutions | Lat, Lon | Euclidean | (4)+ | Manual | No | |||||||
| grasse2024state | Poverty, economic development | Mass repression | Lat, Lon | Euclidean | (5) | Manual | No | |||||||
| jardim2024local | Wages, employment | Increase in minimum wage | Lat, Lon | Driving time | (1)⋆, (2)⋆ | Manual | No | |||||||
| ring2025wealth | Savings | Wealth tax assessment | Lat, Lon | Euclidean | (6) | Manual | No | |||||||
| yamagishi2025persistent | Land price | Buraku neighborhoods | Lat, Lon | Euclidean | (1), (6) | MSE, Manual | No | |||||||
| loumeau2025regional | Commuting flows, residential decisions | Departmental borders | Lat, Lon | Euclidean | (5) | MSE | No | |||||||
| boix2025political | Jewish National identity | Political emancipation | Lat, Lon | Euclidean | (6) | MSE | No | |||||||
| muller2025building | Share of predominant ethnic group | Administrative borders | Lat, Lon | Euclidean | (6) | Manual | No |
The Exact Variance of the Average Treatment Effect Estimator in Cluster RCT
Abstract
In cluster randomized controlled trials (CRCT) with a finite populations, the exact design-based variance of the Horvitz-Thompson (HT) estimator for the average treatment effect (ATE) depends on the joint distribution of unobserved cluster-aggregated potential outcomes and is therefore not point-identifiable. We study a common two-stage sampling design-random sampling of clusters followed by sampling units within sampled clusters-with treatment assigned at the cluster level. First, we derive the exact (infeasible) design-based variance of the HT ATE estimator that accounts jointly for cluster- and unit-level sampling as well as random assignment. Second, extending aronow2014sharp, we provide a sharp, attanable upper bound on that variance and propose a consistent estimator of the bound using only observed outcomes and known sampling/assignment probabilities. In simulations and an empirical application, confidence intervals based on our bound are valid and typically narrower than those based on cluster standard errors.
1 Introduction
Cluster randomized controlled trials are prevalent in applied economics [baranov2020maternal, attanasio2020estimating, dhar2022reshaping]. Populations are partitioned into natural groups—cities, villages, schools, classrooms—and randomization occurs at the cluster level while outcomes are measured on individuals within clusters. We take a finite‑population, design‑based perspective with two sources of randomness, following abadie2020sampling, abadie2023should: (i) sampling of the empirical study from a finite target population and (ii) treatment assignment. We explicitly allow for two‑stage sampling: first sample clusters, then sample units within sampled clusters; treatment is assigned at the cluster level.
We analyze design‑based inference for the finite‑population ATE when exposure is assigned by cluster and data are collected via two‑stage sampling. The estimand is the average of unit‑level potential outcomes in the finite population. Under known sampling and assignment probabilities, the Horvitz–Thompson (HT) estimator is unbiased. However, its exact variance depends on cluster‑aggregated potential outcomes that are not jointly observed (the familiar “cross‑arm covariance” problem), so the exact variance cannot be consistently estimated without additional assumptions.
A finite‑population perspective is natural for cluster randomized experiments with two‑stage sampling. It has long roots in survey sampling and has become increasingly important in causal inference and program evaluation, where researchers seek design‑faithful uncertainty quantification that conditions on the realized finite population and the actual sampling/assignment mechanisms. In parallel, a growing line of work emphasizes finite‑sample, design‑based inference for experiments as implemented. For example, heckman2021using develop worst‑case randomization tests for compromised or partially documented experiments, modeling experimenters who “satisfice” on covariate balance and delivering exact small‑sample procedures that are robust to rerandomization and post‑assignment transfers. Their framework motivates finite‑sample, design‑based inference when assignment protocols are only partially known, and it explicitly targets settings where conventional asymptotics can be misleading. More broadly, recent contributions clarify when design‑ versus sampling‑based uncertainty is appropriate [abadie2020sampling], develop theory for rerandomization and studentized randomization tests [morgan2015rerandomization, li2018asymptotic, chung2016multivariate], study experiments “as implemented” rather than “as intended” [e.g., heckman2010analyzing], and assess the reliability of asymptotics in small experimental samples [young2019channeling]. Our paper contributes to this agenda by deriving exact finite‑population variance formulas and sharp (and conservative) variance estimators for cluster randomized trials with two‑stage sampling, and by establishing asymptotic normality under weak moment and heterogeneity conditions—supplying practical, design‑based uncertainty quantification tailored to the settings most common in field experiments.
In particular, we derive the exact design‑based variance of the HT estimator of the ATE that incorporates randomness from cluster sampling, unit sampling, and cluster‑level assignment. Relative to designs without sampling or without clustering, two‑stage sampling introduces identifiable within‑ and between‑cluster variance components. Extending the sharp‑bounds approach of aronow2014sharp to two‑stage sampling with cluster assignment, we obtain a sharp, consistently estimable upper bound for the variance by applying the bounds to the joint distribution of the estimated cluster averages—thereby embedding both cluster‑ and unit‑sampling noise. We then construct a consistent estimator of this bound using only observed data and known inclusion/assignment probabilities. In simulations and in an application, Wald intervals that plug in our bound estimator are typically tighter than intervals based on clustered standard errors (Liang–Zeger), which can be overly conservative in this design, while maintaining nominal coverage.
Our paper is closely connected to the literature on cluster randomization and design-based inference. A large literature studies experiments in which treatment is assigned at the cluster level and outcomes are measured on units within clusters. Early work formalized design-based estimands under cluster assignment, and subsequent papers developed randomization-based variance expressions and estimators for cluster-level designs [middleton2015unbiased, su2021model, schochet2022design, wang2024design]. Much of this literature treats the sample as given, i.e. conditioning on the set of observed clusters and units and focuses on assignment-induced uncertainty. We differ by explicitly integrating two-stage sampling uncertainty, sampling clusters, then sampling units within sampled clusters, into the variance of the Horvitz-Thompson (HT) estimator for ATE. This adds variance components that persist even if assignment were held fixed, and it changes the object that a variance estimator must target.
Our analysis follows the finite-population, design-based perspective emphasized by abadie2020sampling, abadie2023should and by the survey-sampling literature on two-stage designs [ohlsson1989asymptotic, chauvet2020inference]. Those references clarify that sampling and assignment are distinct randomization mechanisms whose contributions to uncertainty should be kept separate. Relative to these, we specialize to cluster-level assignment and deliver (i) the exact design-based variance of the HT estimator that aggregates both stages of sampling ans treatment assignment, and (ii) a sharp, consistently estimable upper bound for that variance when it is not point-identified due to missing cluster-aggregated potential outcomes.
aronow2014sharp show that when cross-potential-outcome terms are unobserved, the variance of common design-based estimators may not be point-identified but admits a sharp upper bound under the randomization. We extend that sharp-bounds logit to two-stage sampling with cluster-level treatment assignment: the unobserved terms are cluster-aggregated potential outcomes, and the feasible bound must respect both sampling inclusion probabilities and treatment probabilities, being applied to the distribution of the estimated quantities. Our consistent estimator targets this bound, and hence supports valid yet typically tighter intervals than generic clustered standard errors in our settings.
Moreover, bugni2022inference analyze the cluster-randomized experiments from a superpopulation perspective in which potential outcomes and even sample size are random variables. As they note (Remark 3.4), finite-population precision gains stemming from observing a large share of the population cannot be consistently estimated because counterfactual potential outcomes are missing. We adopt the finite-population perspective and show how to recover a portion of precision gains via a sharp design-based variance bound, i.e., we trade point identification of the variance for a tight, attainable bound that is consistently estimable under the actual design.
The HT estimator weights observed outcomes by the inverse of the product of sampling and assignment probabilities. With cluster-level assignment, each sampled cluster reveals only one of its two cluster-aggregated potential outcomes, so the covariance term that involves the joint distribution of the potential outcomes in the variance cannot be pinned down. Our sharp bounds identifies the largest variance compatible with the design, the realized treatment assignments, and the observed outcomes. Because the bound respects the actual sampling and assignment probabilities, it is often materially less conservative than generic clustered standard errors. In practice, many applied papers report cluster standard errors computed from unit-level regressions, sometimes with or without survey weights. As emphasized by abadie2020sampling, abadie2023should and others, such standard errors mix model-based approximation with design-based features and can be over- or under-conservative depending on the design, especially under unequal inclusion probabilities or when the estimand is finite-population.
Throughout the paper, we consider no interference across clusters, known sampling and assignment probabilities, possibly unequal across clusters, and treatment assignment at the cluster level (compliance issues are outside our scope). The sampling scheme is conducted without replacement at each stage and may allow probability-proportional-to-size draws.
The structure of this paper is as follows. Section 2 formalizes the two-stage sampling and cluster-level assignment and introduce notation. Section 3 defines the HT estimator for the finite-population ATE and gives its exact design-based variance. Section 4 develops asymptotic properties under increasing numbers of clusters in a sequence of finite populations. Section 5 extends aronow2014sharp to derive a sharp variance bound and proposes a consistent estimator of the bound. Section 6 presents Monte Carlo evidence. Section 7 applies our methods to a school-level cluster randomized controlled trial and compares bound-based intervals to clustered standard errors.
2 Set up
We consider a finite population of size , consisting of clusters. Cluster , contains units, and we take , , and all as fixed and known. This assumption is plausible in many field settings (e.g., development economics) where researchers typically gather population information from the study area, have census lists, or have access to administrative records. For asymptotic results, we consider a sequence of such finite populations, with the number of clusters going to infinity, allowing for heterogeneous cluster size .
Randomness arises from both sampling and treatment assignment. The sampling process has two stages. Firstly, clusters are randomly selected from clusters without replacement. Secondly, in a sampled cluster , units are sampled at random from units without replacement. The treatment assignment is independent of both sampling stages by design. The treatment is on the cluster level: out of the sampled clusters are randomly assigned to treatment without replacement, and the remaining clusters are assigned to control. All units within a given cluster receive the same treatment status. Only the sampled units are observed and used for analysis.
In practice, the intervention is delivered to the entire cluster in some experiments, even though only a subset of units is sampled for measurement; and in some designs, the intervention is administered only to the sampled units. Our analysis formally accommodates both cases. However, we note that in the latter setting, extending conclusions to the entire cluster may involve extrapolation when considering potential spillover effects–issues that go beyond the scope of this paper. These considerations are related to the broad literature on two-stage randomization designs, which explicitly study within-cluster and between-cluster treatment allocations in the existence of spillover.
Let and denote the potential outcomes for unit in cluster under treatment and control, respectively. The potential outcomes are taken as fixed. We introduce several indicator variables to describe the sampling and assignment process.
Define as the indicator variable denoting whether cluster is sampled () or not () in the first-stage sampling, and as the treatment assignment indicator for cluster , where if is assigned to treatment and if is assigned to control. Let be the indicator of whether unit in cluster is sampled in the second-stage sampling, conditional on that cluster is sampled () in the first stage. The overall sampling indicator for unit in cluster is then denoted as .
The sampling and treatment assignment processes induce the following probabilities. The probability that cluster is sampled is
where is the number of sampled clusters out of total clusters.
Within a sampled cluster , the probability of unit , which belongs to cluster , being sampled is
where is the number of sampled units within cluster and is the total number of units in that cluster.
Conditional on being sampled, the probability that cluster is assigned to treatment is
where clusters are assigned to treatment and clusters are assigned to control.
Because both sampling and treatment assignment are carried out without replacement, the inclusion events are dependent, affecting the joint inclusion probabilities of pairs of clusters and units. Specifically, the probability that both clusters and () are sampled is
Within cluster , the probability that both units and () are sampled is
For units in different clusters, the probability that unit in cluster and unit in cluster are both sampled is
Conditional on both clusters and being sampled, the probability that both clusters are assigned to treatment is
the probability that both clusters are assigned to control is
and the probability that is assigned to treatment and is assigned to control is
In this two-stage design, the sampling process first determines which clusters and which units within them are observed, while treatment is assigned independently at the cluster level. The without-replacement nature of both sampling and assignment means that inclusion and treatment probabilities for different clusters or units are not independent. The above probabilities form the basis for the Horvitz-Thompson weighting scheme used in our estimator and for deriving its exact design-based variance.
3 The Estimator of the ATE and its Exact Variance
3.1 The estimator
The population parameter of interest is the average treatment effect (ATE), defined as
| (1) |
where and denote the potential outcomes of unit in cluster under treatment and control, respectively. The potential outcomes are treated as fixed in the finite population, while all randomness arises from the sampling and treatment assignment in the experimental design.
We estimate using the Horvitz-Thompson (HT) estimator:
This estimator is unbiased, because by construction , , and .
Our goal in this section is to calculate the exact variance of and then propose a feasible and consistent estimator of that variance for inference purposes.
To simplify notation and account for heterogeneity in cluster sizes, define the average cluster size
and the average within-cluster sample size
The quantity represents the total number of units in the population. In contrast, does not correspond to the actual number of sampled units in the realized data, because only out of clusters are drawn in the first stage. Instead, it represents the design-level number of sampling opportunities, which is the total number of units that would be sampled if all clusters were included in the first stage. The actual number of sampled units is random and equals , which depends on which clusters are sampled, i.e., the realizations of . This distinction is important because and are fixed by design, not random. They are specified in advance to characterize the sampling probabilities and to simplify expressions of the estimator’s variance. In many applications, especially in development economics, researchers know all cluster sizes from census or administrative data, so is treated as a known quantity. Also, in practice, the within-cluster sample sizes are almost always determined before the first-stage cluster sampling is carried out. Typically, one of the following conventions is adopted at the design stage: fixed number of sampled units per cluster ; fixed sampling proportion , where is a constant sampling rate applied across clusters; predetermined list of sample sizes, where a set of values are chosen in advance based on available information about cluster sizes, survey logistics, or budget constraints. Because these choices are made ex-ante, before any cluster is actually sampled, it is natural and appropriate to treat , , and as known, nonrandom design parameters.
To derive the exact variance of the ATE estimator in a way that separates contributions from the two sampling stages, we introduce scaled version of the potential outcomes. For each unit in cluster and treatment status , define the scaled potential outcomes
The scaled cluster total is then
and the scaled cluster mean is
The mean of scaled cluster totals equals the mean outcome over all units.
With these definitions, the ATE can be expressed as an average of cluster-level treatment effects:
where is the treatment effect for cluster .
We first consider an infeasible benchmark estimator. If all units within sampled clusters are observed, then the population cluster totals and would be known. In that case, the infeasible Horvitz-Thompson estimator using the true cluster totals is
| (2) |
where . In this expression, randomness arises only from the first-stage cluster sampling and treatment assignment of clusters, since no within-cluster sampling occurs.
In practice, the true cluster totals are not observed because only a subset of units within each sampled cluster is surveyed. To obtain unbiased estimates of cluster-level totals, we use a within-cluster Horvitz-Thompson estimator:
| (3) |
Replacing each unobserved cluster total by its within-cluster estimate yields an equivalent, cluster-level representation of the overall ATE estimator:
| (4) |
This form makes explicit the two distinct sources of randomness in the estimator: between-cluster randomness, which is due to sampling and treatment assignment of clusters; within-cluster randomness, which is due to the second-stage sampling of units used to construct .
By expressing the estimator in this way, we can cleanly separate the contributions of each stage to the total design-based variance. The next subsection derives the exact variance of both (the infeasible estimator) and (the feasible estimator with unit sampling).
3.2 The exact variance
We begin by showing that both and are unbiased estimators of the average treatment effect, and then derive their exact variances. The results are summarized in the following proposition.
Proposition 1.
Suppose the first stage sampling probability , and the cluster-level treatment assignment probability is . Then both estimators and are unbiased for the finite-population average treatment effect :
The exact variance of the infeasible estimator (which uses true cluster totals) is
| (5) |
When within-cluster unit sampling is also performed, let denote the second-stage sampling probability and . The exact variance of the feasible estimator is:
| (6) | ||||
When only the first-stage sampling of clusters and the treatment assignment are random, the variance of in (5) can be understood as having three additive components. The first term is between-cluster variation under treatment. measures the dispersion of the scaled cluster outcomes under treatment. It captures how heterogeneous treated clusters would be if all could be observed. The second term is between-cluster variation under control. is the analogous quantity for control clusters. The third term is the adjustment for the covariance between treatment and control potential outcomes. equals , and removes the double-counting of variability that arises when the two potential outcomes are positively correlated within clusters. Intuitively, when clusters that perform well under control also perform well under treatment, this term reduces the overall variance. The factors and reflect the design probabilities: smaller sampling or treatment probabilities increase variance because fewer clusters contribute to information to each term.
If there is no sampling, i.e., and all clusters are observed, our estimator coincides with the difference-in-means estimator studied by su2021model. The corresponding variance expression for , denoted as , is identical to theirs, except that our expression includes the exact finite population correction factor of rather than . abadie2023should studied a difference-in-means estimator but under Bernoulli randomization, where each cluster is independently assigned to treatment or control. They approximate the variance by conditioning on the realized number of treated and control units. This yields a variance expression that is algebraically similar to ours, but omits the finite population correction, which aligns with the inherent nature of Bernoulli assignment. A similar distinction arises when only a subset of clusters is sampled in the first stage (). In this case, the effective weights in the first two variance components of depend on both the cluster sampling and treatment probabilities-specifically, the products and . These terms capture how the uncertainty from sampling and assignment jointly contributes to the overall design-based variance.
When second-stage sampling is introduced, the true cluster totals are no longer directly observed; instead, they are estimated from within-cluster samples using . Consequently, the variance of contains additional components that reflect the extra randomness introduced by the within-cluster sampling process (). Intuitively, because each cluster’s total outcome is now estimated from a subsample of its units, the precision of depends on the within-cluster sampling fraction , and the heterogeneity of outcomes among units in that cluster. These features appear in the additional variance terms involving the within-cluster variance of scaled outcomes, , which measure how dispersed individual outcomes are around the cluster mean.
If all sampled clusters share the same within-cluster sampling i.e. for all -then the adjustment term simplifies to a constant scaling factor. In that case, our variance expressions coincides with the one derived in abadie2023should, except that our formulation explicitly includes finite population correction factors arising from considering sampling and treatment assignment without replacement.
4 Asymptotic Analysis
We study the asymptotic properties of the Horvitz-Thompson estimator under a sequence of finite, clustered populations. The number of clusters grows to infinity, while the number of units per cluster may either remain fixed or grow with . This is a finite population framework in which cluster counts, rather than individual counts, drive asymptotics.
Our asymptotic analysis builds on two complementary strands of the literature. First, the survey-sampling strand, exemplified by ohlsson1989asymptotic and chauvet2020inference, establishes central limit theorems for two-stage sampling in finite populations. Second, li2017general develops a general framework for finite-population asymptotics in design-based causal inference, identifying conditions under which estimators remain asymptotically normal as the population size grows. Related work in cluster-randomized experiments includes middleton2015unbiased and su2021model, who analyze asymptotics under cluster-level treatment assignment, and abadie2023should, who develop a design‑based framework that explicitly incorporates both clustered sampling and clustered (possibly partial) assignment.
Building on these foundations, we establish asymptotic normality for our two‑stage Horvitz–Thompson estimator. To do so, we impose a set of regularity conditions on (i) moments of potential outcomes, (ii) sampling and treatment probabilities, and (iii) the degree of cluster‑size heterogeneity. These assumptions are stated next.
Assumption 1.
.
Assumption 1 imposes a finite fourth-moment bound on unit-level potential outcomes. It ensures that extremely large or heavy-tailed outcomes do not dominate the behavior of the estimator. As insu2021model, imposing the moment condition at the unit level ensures that the scaled cluster totals are well-behaved. By contrast, abadie2023should assume a stronger uniform boundedness condition on the potential outcomes—stronger than a finite fourth-moment requirement—thereby excluding heavy-tailed distributions. Our weaker condition allows for moderate heterogeneity across units and clusters, provided the overall moment bound holds.
We next state the assumptions on the sampling probabilities and treatment assignment.
Assumption 2.
The probability of cluster sampling satisfies , with .
Assumption 3.
The probability of treatment .
Assumption 4.
Conditional on the cluster being sampled, .
Assumption 2 allows the first-stage sampling rate to decline with , but not too quickly. The restriction with ensures that the number of sampled clusters still grows sufficiently fast to support a central limit theorem. Assumption 3 is the standard overlap condition, ruling out near‑degenerate treatment shares among sampled clusters. Assumption 4 requires the number of units sampled in the sampled clusters to be bounded away from . This is a minimal requirement preventing empty clusters in the observed data. In practice, it is usually automatically satisfied since the within-cluster sample sizes are fixed in advance at the design stage.
We also need to control cluster-size heterogeneity, which plays a critical role in establishing asymptotic normality. Let , where is the average cluster size, and define . Here, is cluster ’s relative size compared to the average, while captures the overall degree of heterogeneity in cluster size. If all clusters are of equal size, i.e., , then . Large values of indicate that a few clusters are disproportionately large, which can dominate the sampling variation and invalidate standard central limit approximations if left uncontrolled.
Assumption 5.
.
In comparison, abadie2023should assumed that , which implies that cluster sizes are uniformly bounded relative to each other. Our condition is weaker: we permit some heterogeneity and even allow the largest cluster size to grow with , provided it does so slowly enough. When all clusters are sampled (), our rate condition simplifies to , which coincides with the assumption in su2021model. Because our setting includes cluster sampling, we must simultaneously control the sampling probability (Assumption 2) and the degree of size heterogeneity (Assumption 5). Because our setting includes cluster sampling, we must simultaneously control the sampling rate: . Intuitively, the condition means that as the number of clusters grows, both the sampling rate and the distribution of cluster sizes must be balanced so that the total information is not concentrated in a small number of large clusters.
The condition ensures a non-degenerate design variance, which means that the variance arising from the first-stage sampling and treatment assignment does not vanish as the number of clusters grows. This assumption is the finite-population analogue of requiring that the design provides enough independent information for a central limit theorem to apply, much as in classical two-stage survey-sampling theory.
Define the across-cluster variance and covariance components:
A sufficient condition for is either: (i) and , where ; or (ii) and , where . These conditions ensure that the design variance does not collapse wither because treatment shares become perfectly balanced () or because sampling covers all clusters () while the treated and control potential outcomes have co-movements.
To see why the above suffices, suppose and define . Using , we could have
where the inequality uses by Cauchy-Schwarz. The expression in the bracket, , is equal to if ; and it is strictly positive if .
The proof of Lemma 1 follows by applying the finite-population CLT in li2017general to the cluster-level Hotvitz-Thompson statistics. The argument parallels Theorem 1 of su2021model, who derive the same result for cluster-randomized experiment without a cluster sampling stage.
We now turn to the feasible estimator . Relative to , the estimator includes second-stage unit sampling, which introduces additional randomness and within-cluster dependence. Proving asymptotic normality therefore requires two ingredients: (i) the CLT for the first‑stage component (Lemma 1); (ii) a Lyapunov-type condition ensuring that the added variability from estimating cluster totals using within-cluster subsamples is asymptotically negligible—formally analogous to conditions in two-stage survey-sampling CLTs. Together, these will yield asymptotic normality of as .
Theorem 1.
Theorem 1 generalizes classical two‑stage survey‑sampling CLTs [ohlsson1989asymptotic, chauvet2020inference] to a causal estimand with cluster‑level assignment. It also parallels the main asymptotic normality result of abadie2023should, who consider clustered sampling and assignment in a more general design-based framework. Our analysis differs, however, in focusing specifically on cluster-randomized experiments with explicit two-stage sampling. Consequently, our rate conditions are expressed in terms of the number of clusters and the heterogeneity of their sizes, rather than only the total number of sampled units. abadie2023should likewise require the number of sampled clusters to diverge, but because their framework is broader (not limited to CRTs), their asymptotic statements are framed in a more general sampling and treatment assignment environment.
Lemma 1 and Theorem 1 jointly show that under mild design-based conditions, the Horvitz–Thompson estimator is asymptotically normal—first when cluster totals are known, and then when they are estimated via within-cluster sampling. In empirical terms, this result justifies using standard normal-based inference for two-stage cluster-randomized designs, provided that the number of sampled clusters is sufficiently large and no small subset of clusters dominates the sample.
5 Estimators of Exact Variance
In clustered randomized controlled trials, we never observe both or for the same cluster . Even without within-cluster subsampling, each cluster reveals only one potential cluster total. Following aronow2014sharp, we construct sharp bounds on the exact variance of .
Throughout, expectations and covariances are design-based (i.e., taken over the sampling and assignment mechanisms). For brevity, define the design covariances
and, for the second stage,
A convenient rearrangement of the exact variance of is
where
and
Here, and collect the first-stage (cluster) contributions together with the second-stage (unit) contributions within each arm. The term denotes the cross-arm covariance of cluster totals, taken across clusters.
With unit-level sampling within the sampled clusters, or cannot be observed for any unless the second-stage sampling probability . Instead, we only observe the within-cluster Horvitz-Thompson estimates of the scaled cluster totals, denoted by . If, counterfactually, we could draw independent subsamples of units from each cluster to estimate both and for the same cluster , then , . In that hypothetical setting, a plug-in estimator
would be unbiased and consistent for . Here,
In practice, however, we cannot observe both potential cluster totals for any given cluster. With second-stage sampling, each cluster provides only one observed aggregate, either or , depending on its assignment. Formally,
Hence, is infeasible because it relied on cross-arm covariance terms involving both and for the same . To address this, we apply Fréchet-Hoeffding covariance bounds proposed by aronow2014sharp to construct an interval estimator for this infeasible variance. These bounds describe the maximal and minimal possible covariance consistent with the observed marginal distributions of and .
We next characterize the maximal and minimal possible covariance between the estimated cluster aggregates under treatment and control. Define the empirical cumulative distribution functions (CDFs) of and as
where is the indicator function. The corresponding left-continuous inverses (quantile functions) of these CDFs are defined as
Using these marginal distributions, the Fréchet-Hoeffding bounds identify the extreme possible covariances between and compatible with the observed marginals: We then define the maximal and minimal possible covariances:
Maximal Covariance (Positive Association):
Minimal Covariance (Negative Association):
These bounds provide the most precise interval for given the observed data, without making additional assumptions about the joint distribution of and . Different from aronow2014sharp, we are now constructing bounds on the covariance between and , instead of the covariance between true scaled cluster totals.
By replacing in with and respectively, we could obtain and , delivering the sharpest bounds on .
In practice, we do not observe both potential aggregates for any cluster, since we only observe for treated sampled clusters and for control sampled clusters. We therefore construct the empirical cumulative distribution functions of the estimated totals and as
These weights ensure that the empirical distributions of the observed treated and control clusters estimate the population marginals of and . The corresponding left-continuous inverses of and are
where and are the ordered aggregates for the treated and control clusters.
To approximate the integrals above, let be the ordered distinct values in the set , which partitions .
Define the corresponding stepwise quantiles
Then the sample analogues of the covariance bounds are
Again, with the first-stage sampling and the treatment assignment, and is not directly observed. An unbiased and consistent estimator for is
Similarly, an unbiased and consistent estimator for is
The above variance estimators are the so-called Horvitz-Thompson variance estimator, and the counterparts in survey sampling can be found in chauvet2020inference. Each line mirrors the classic HT variance form: a cluster-level piece that uses pairwise cluster inclusion covariances , and a unit-level piece that uses pairwise unit inclusion covariances , with the appropriate design probabilities in the denominators. Under our regular conditions, these estimators are unbiased for the corresponding variance components and consistent as .
Combining the arm-specific pieces with the sharp covariance bound estimators and , yields an interval estimator and as:
Remark 1.
An alternative route is to apply the aronow2014sharp bounds directly to the covariance of the true (unobserved) cluster totals. This yields variance bounds . To estimate those bounds in our setting, two adjustments are required: (i) replace the unobserved marginals of those of by the marginals of the estimated cluster totals , which incorporate second-stage within-cluster sampling; and (ii) move from the population of clusters to the sampled clusters using HT weights, which account for first-stage sampling and treatment assignment. Operationally, these steps lead to the same feasible interval estimators we propose. Under stronger growth conditions, i.e., and (so that within-cluster estimation error vanishes and the empirical marginals of converge to those of , the interval estimators converge to .
Our approach, however, deliberately targets the bounds defined with the estimated cluster aggregates. This choice has a practical advantage: it yields consistent bound estimators without requiring ; that is, cluster sizes may remain bounded while the number of clusters grow.
Additionally, dropping the negative term in (6), we could obtain a conservative part of the exact variance, denoted as :
If treatment effects are homogeneous across clusters, then , this conservative variance part is exact. An estimator for the above conservative part of the exact variance is
Proposition 3.
. If the fourth moment of the potential outcomes is bounded, and , , then is an unbiased and consistent estimator of .
6 Simulation Results
This section investigates the performance of our proposed bound estimators through Monte Carlo simulations. We compare it against the widely used Liang-Zeger cluster-robust standard error estimator.
For each unit in cluster , we generate two independent covariates , and let . We consider four distinct data generating processes that vary in how heterogeneity and clustering enter the model. They are presented in Table 1. All noise terms are Guassian with variance .
| DGP 1 | ||
|---|---|---|
| DGP 2 | ||
| DGP 3 | ||
| DGP 4 |
DGP 1 is the benchmark case, corresponding to the constant treatment effect, with all units sharing the same treatment effect . DGP 2 corresponds to the case of heterogeneous treatment effects across individuals without clustering. Treatment effects vary randomly across individuals, , but there are no cluster-level shocks. This mimics a standard i.i.d. setting. DGP 3 depicts cluster-level heterogeneity. Each cluster has its own random intercept , and cluster-specific treatment effect . Units in the same cluster share the same treatment effect, but treatment effects differ across clusters. DGP 4 reflects both clustered and within-cluster heterogeneity. Clusters differ in their mean effect , and within each cluster, individuals differ further via an idiosyncratic spread , and thus have effect . This is the most complex and realistic design, reflecting cases such as school-level interventions with student-specific idiosyncrasies.
To mimic a two-stage sampling clustered experiment, we consider the following sampling and treatment assignment mechanisms. We consider the population has total clusters . We randomly sample clusters without replacement. Among these, we randomly assign clusters to treatment, and the remaining clusters to control.
In the second stage sampling, we consider two regimes to determine the number of sampled units per sampled cluster. The first regime (R1) samples units from each cluster, with identical across clusters. Every sampled cluster contributes the same fraction of its members. The second regime (R2) samples the same number of units, i.e., , from each cluster. Each cluster contributes the same number of sampled units, regardless of its true size.
We conduct 1,000 Monte Carlo replications for each DGP and sampling regime. In each replication, we draw samples, estimate the population ATE using the HT estimator , compute the Monte Carlo standard deviation , the proposed upper bound SE, , and the Liang-Zeger cluster-robust SE, , construct confidence intervals based on each SE and record the coverage and power. All the results are collected in Table 2.
| Sampling | Upper Bound Estimator | Liang-Zeger Estimator | |||||||||
|---|---|---|---|---|---|---|---|---|---|---|---|
| Regime | coverage | power | coverage | power | |||||||
| DGP1 | R1 | 50.380 | 0.412 | 0.465 | 97.4% | 100.0% | 0.457 | 97.1% | 100.0% | ||
| R2 | 50.353 | 0.483 | 0.562 | 97.8% | 100.0% | 0.535 | 96.8% | 100.0% | |||
| DGP2 | R1 | 49,430 | 1.249 | 1.322 | 96.5% | 100.0% | 1.404 | 97.0% | 100.0% | ||
| R2 | 49.426 | 1.461 | 1.580 | 96.7% | 100.0% | 1.613 | 97.1% | 100.0% | |||
| DGP3 | R1 | 48.790 | 14.325 | 16.083 | 96.6% | 89.5% | 17.976 | 98.5% | 82.6% | ||
| R2 | 47.515 | 14.168 | 16.073 | 96.8% | 87.2% | 17.924 | 98.1% | 81.9% | |||
| DGP4 | R1 | 56.303 | 13.273 | 14.642 | 96.7% | 97.7% | 16.277 | 98.0% | 96.1% | ||
| R2 | 55.904 | 13.239 | 14.716 | 96.8% | 97.7% | 16.331 | 98.3% | 95.4% |
When treatment effects are constant (DGP 1) or vary independently across individuals (DGP 2), both the upper-bound and Liang–Zeger estimators perform almost identically: coverage rates are near the nominal , estimated SEs are close to the empirical standard deviation, power is essentially because the true signal is large relative to the sampling noise. This confirms that our estimator remains valid and does not over- or under-estimate variance in simple, without-clustering settings.
When treatment effects vary across clusters (DGP 3) or both within and across clusters (DGP 4), differences emerge: the Liang–Zeger SEs are consistently larger, leading to over-coverage () and lower power (); the upper-bound SEs remain close to the empirical dispersion, yielding coverage near the nominal and higher power (up to +7 percentage points). This behavior aligns with theory: the LZ estimator is known to be conservative under design-based randomization, while our bound-based estimator incorporates the attainable negative covariance term, reducing unnecessary conservatism. Comparing R1 (proportional sampling) and R2 (fixed size): Proportional sampling typically yields smaller standard errors and more stable estimates because sampling intensity scales with cluster size. Fixed-size sampling oversamples smaller clusters relative to their size, which slightly inflates the total variance. The performance patterns of both variance estimators are similar across regimes, but proportional sampling tends to produce marginally more precise inference.
In summary, when treatment effects are clustered or heterogeneous, the proposed bound-based estimator yields tighter and more informative inference than conventional Liang–Zeger cluster-robust standard errors.
7 Application
Cluster randomized controlled trials in development economics often share a common structure: researchers enumerate all clusters in a defined area, randomly assign treatment at the cluster level, and then sample a subset of individuals within each cluster to measure outcomes. Our empirical illustration applies the proposed variance estimators to a school-based cluster randomized trial in India, studied by dhar2022reshaping.
The experiment was implemented in collaboration with the NGO Breakthrough and evaluated by dhar2022reshaping. The program sought to change adolescents’ attitudes toward gender equality by integrating discussions on gender norms into the regular school curriculum. Over roughly two and a half school years, the treatment consisted of a series of 45–minute classroom sessions, accompanied by teacher training and school-wide activities. The intervention was delivered to students in grades 7 through 10 in government secondary schools in the state of Haryana.
The study involved 314 schools across four districts, which serve as the natural “clusters” in our framework. Randomization took place at the school level: 150 schools were assigned to the treatment group and 164 to the control group. Within each school, the research team then selected a subsample of students for baseline and follow-up surveys. Among the students whose parents returned consent forms, approximately 45 students per school were randomly chosen. This procedure represents the second stage of sampling in our setup, with cluster-specific inclusion probabilities . Crucially, recruitment and baseline data collection were conducted blind to treatment status, ensuring that the sampling probabilities are independent of assignment.
The study collected outcome data at two points in time: the first endline survey occurred approximately three and a half months after the program ended, and the second endline survey was fielded about two to two and a half years later, when most students had completed grade 11 or 12. For illustration, we focus on several of the key attitudinal outcomes reported in the original paper. These include a gender-attitudes index, a self-reported behavior index, and individual survey items capturing attitudes toward women’s education and employment—whether women should be allowed to work outside the home, whether they should go to college, and whether respondents would oppose women attending college. Each outcome is coded so that higher values correspond to more gender-equitable attitudes.
This experimental design maps directly onto our two-stage sampling framework. At the first stage, schools are randomized into treatment and control groups. At the second stage, students are sampled within schools with known inclusion probabilities. The setting is therefore an ideal case for applying our variance estimators, which are specifically designed for experiments with cluster-level treatment assignment and within-cluster sampling. We treat the 314 schools as the finite population of interest, so and . Within each school, we compute Horvitz–Thompson totals using the known sampling rates , and we compare our upper-bound (sharp-bound) variance estimator with the standard Liang–Zeger (LZ) cluster-robust estimator.
Table 3 presents the results. For each outcome, we report the estimated average treatment effect , the estimated standard error based on bound variance estimator , the corresponding Liang–Zeger standard error , their respective 95% confidence intervals, and the percentage reduction in confidence-interval width achieved by our estimator.
| Dependent Variable | 95% CI | 95% CI | Reduction in CI | |||
|---|---|---|---|---|---|---|
| Endline 1 Outcomes | ||||||
| Gender attitudes index | 0.310 | 0.061 | [0.191,0.429] | 0.066 | [0.181,0.439] | 7.2% |
| Self-reported behave index | 0.317 | 0.077 | [0.167,0.467] | 0.084 | [0.153,0.481] | 8.3% |
| Women be allowed to work | 0.058 | 0.027 | [0.006,0.112] | 0.029 | [0.000,0.117] | 8.0% |
| Women should go to college | 0.070 | 0.035 | [0.001,0.140] | 0.039 | [-0.006,0.147] | 8.9% |
| Not oppose women in college | 0.092 | 0.031 | [0.032,0.152] | 0.036 | [0.022,0.162] | 13.8% |
| Endline 2 Outcomes | ||||||
| Women be allowed to work | 0.093 | 0.038 | [0.018,0.169] | 0.043 | [0.009.0.177] | 10.3% |
| Not oppose women in college | 0.105 | 0.033 | [0.040,0.170] | 0.037 | [0.032,0.178] | 11.0% |
The results show that our bound-based estimator and the Liang–Zeger estimator produce very similar point estimates and standard errors in this setting, but confidence intervals constructed with the bound estimator are consistently narrower. For the outcomes considered, the reduction in interval width ranges from about 7% to 14%. The differences are modest but systematic, reflecting the conservative nature of cluster-robust standard errors.
Substantively, these results highlight two points. First, the design of this study—cluster-level assignment combined with within-cluster sampling—is exactly the type of setting where our method is most relevant. Second, even in large, well-balanced experiments with hundreds of clusters, traditional Liang–Zeger standard errors can remain slightly over-conservative because they do not exploit the structure of the design-based variance. Our estimator achieves modest efficiency gains while maintaining valid coverage.
Overall, this empirical application complements the simulation evidence: when treatment is assigned at the cluster level and sampling occurs within clusters, the proposed upper-bound variance estimator provides tighter inference than standard clustered standard errors, without relying on additional modeling assumptions.
8 Conclusion
This paper develops a design-based framework for analyzing cluster randomized experiments (cluster randomized controlled trials, CRCTs) when both cluster-level assignment and two-stage sampling—of clusters and, within them, individual units—are present. We derive the exact finite-population variance of the Horvitz–Thompson estimator for the average treatment effect (ATE) and establish its asymptotic normality under mild assumptions on outcome moments, sampling probabilities, and cluster-size heterogeneity.
Building on these foundations, we extend the aronow2014sharp sharp variance bounds to the cluster setting with within-cluster sampling. This yields a sharp, attainable upper bound for the design variance and a consistent estimator for that bound, along with a conservative variance estimator that becomes exact when cluster-level treatment effects are homogeneous. Together, these results bridge classic survey-sampling theory and modern finite-population causal inference, providing a unified design-based approach to inference in cluster experiments.
Our simulations and empirical application, using the school-based CRCT of dhar2022reshaping, show that the proposed upper-bound variance estimator delivers tighter confidence intervals than the conventional Liang–Zeger cluster-robust estimator, while maintaining correct coverage. The improvement is most pronounced when treatment effects are heterogeneous and clustered, where standard clustered standard errors tend to be conservative. In such cases, our estimator captures the attainable negative covariance components that standard approaches ignore, leading to more efficient inference and higher power.
In summary, the paper contributes (i) exact design-based variance formulas for cluster-level treatment with two-stage sampling, (ii) sharp and conservative variance estimators that are consistent under the design, and (iii) asymptotic results linking finite-population theory to practical inference. These tools enable researchers to conduct design-faithful and efficient uncertainty quantification in cluster randomized experiments.
APPENDIX
Appendix A Proof
Proof of Proposition 1.
We begin with the mean and variance of the infeasible estimator . By (2) and (4),
| (A.1) |
Only and are random and , , so that .
Next
We have
with
Note that
if .
The variance is
| (A.2) | ||||
Now consider . We have
Also
In the same way
Therefore .
For the variance, we use
with
| (A.3) | ||||
By iterated expectations
Further
| (A.4) |
By iterated expectations
and
| (A.5) |
By iterated expectations
and
In the same way
Putting these together,
| (A.6) | ||||
Finally, for the cross-product
Now by iterated expectations and because ,
Summarizing all the above terms, we could obtain
∎
Proof of Lemma 1.
Under the assumption that and , we could have :
Assuming that , we could obtain
Therefore, .
If and , where ; or if and , where , we could have , then the asymptotic normality of follows from Lemma A.2., Theorem 1 in su2021model and Theorem 1 in li2017general. ∎
Proof of Theorem 1.
Theorem 1 follows from Theorem 2.1 and Remark 2.4 in ohlsson1989asymptotic if we could prove conditions (C1), (C2), and (2.8). With our notations, these conditions can be written respectively as
| (A.7) | |||
| (A.8) | |||
| (A.9) |
where , , and , .
Condition A.7 has been verified in Lemma 1, and Condition A.9 is satisfied naturally in this design of simple sampling and random assignment without replacement. It remains to prove Condition A.8. Let , , , .
where
where , , and .
Then
under the assumption that .
And since is bounded away from 0, we have
All three conditions are verified. ∎
Proof of Proposition 2.
Under the notation
we would like to prove that
Conditional on the second-stage sampling,
For simplicity, denote ,
Notice that Therefore,
Summing the above, we have , therefore, in probability. Similarly, it can be shown that in probability. ∎
Proof of Proposition 3.
For the treated part, we have
Similarly, for the control part, we have
∎
Appendix B Auxiliary Lemmas
Lemma B.1 (Lemma 1 in aronow2014sharp (Hoeffding)).
Given only and and no other information on the joint distribution of , the bound
is sharp. The upper bound is attained if and are comonotonic. The lower bound is attained if and are countermonotonic.
Optimally-Transported
Generalized Method of Moments
Abstract
We propose a novel optimal transport-based version of the Generalized Method of Moment (GMM). Instead of handling overidentification by reweighting the data to satisfy the moment conditions (as in Generalized Empirical Likelihood methods), this method proceeds by allowing for errors in the variables of the least mean-square magnitude necessary to simultaneously satisfy all moment conditions. This approach, based on the notions of optimal transport and Wasserstein metric, aims to address the problem of assigning a logical interpretation to GMM results even when overidentification tests reject the null, a situation that cannot always be avoided in applications. We illustrate the method by revisiting Duranton, Morrow and Turner’s (2014) study of the relationship between a city’s exports and the extent of its transportation infrastructure. Our results corroborate theirs under weaker assumptions and provide insight into the error structure of the variables.
Keywords: Wasserstein metric, GMM, overidentification, misspecification.
1 Introduction
The Generalized Method of Moment (GMM) (hansen:gmm) has long been the workhorse of statistical modeling in economics and the social sciences. Its key distinguishing feature, relative to the basic method of moments, is the presence of overidentifying restrictions that enable the model’s validity to be tested (Newey:HB). With this ability to test comes the obvious practical question of what one should do if an overidentified GMM model fails overidentification tests, a situation that is not uncommon (as noted in hall:missgmm, hansen:miss, poirier:salvaging, conley:plausexo, kwon:ineqmiss), even for perfectly reasonable, economically grounded, models.
A popular approach has been to find the “pseudo-true” value of the model parameter (sawa:pseudo, white:miss) that minimizes the “distance” or discrepancy between the data and the moment constraints implied by the model. This approach has gained further support since the introduction of Generalized Empirical Likelihoods (GEL) and Minimum Discrepancy estimators (owen:el1, qinlawless, newey:gel), all of which provide more readily interpretable pseudo-true values (imbens:res, kitamura:itgmm, schennach:etel).
GEL implicitly attributes the mismatch in the moment conditions solely to a biased sampling of the population. While this is a possible explanation, it is not the only reason a valid model would fail overidentification tests, when taken to the data. Another natural possibility is the presence of errors in the variables (aguiar:revpref, doraszelski:megmm, schennach:HB). In this work, we develop an alternative to GMM that ensures, by construction, that overidentifying restrictions are satisfied by allowing for possible errors in the variables instead of sampling bias. We employ the generic term error to include, not only measurement error, but anything that could cause the recorded data to differ from the value they should have if the model were fully correct, i.e., this could include some model errors. More generally, we allow distortions in the data generating process whose magnitude is quantified by a Wasserstein-type metric (villani:new), in the spirit of distributionally robust methods (connault:sens, blanchet:distrob). In analogy with GEL, which does not require the form of the sampling bias to be explicitly specified, the error process does not need to be explicitly specified in our approach, but is instead inferred from the requirement of satisfying the overidentifying constraints imposed by the GMM model. Of course, the accuracy of the resulting estimated parameters will typically improve with the degree of overidentification.111In the case where the statistical properties of the errors are in fact known a priori, other methods may be more appropriate (e.g. schennach:nlme, schennach:elvis), schennach:annrev, schennach:HB and references therein.
A fruitful way to accomplish this is to employ concepts from the general area of optimal transport problems (e.g., galichon:optran, villani:new, carlier:vectq, galichon:vectquan, cherno:breniercons, schennach:nlfact). The idea is to find the parameter value that minimizes cost of “transporting” the observed distribution of the data onto another distribution that satisfies the moment conditions exactly. Formally, the true iid data is assumed to satisfy , where is the expectation operator, for a parameter value in some set and some given -dimensional vector of moment functions. However, we instead observe an error-contaminated counterpart of the true vector (both taking value in ). We seek to exploit the model’s over-identification to gain information regarding the error in . The Euclidean norm is chosen here for computational convenience, although one could imagine a whole class of related estimators obtained with different choices of metric. Our focus on Euclidean norms parallels the choice made in common estimators (e.g. least squares regressions, classical minimum distance and even GMM). Considering a weighted Euclidean norm can be useful to indicate the relative expected error magnitudes along different dimensions of .
Given a probability measure for the random variable , this setup suggests solving the following population optimization problem, for a given :
| (1) |
subject to , supported on , having marginal and , where denotes an expectation under the measure . (This problem is guaranteed to have a solution if there exists at least one measure such that .) This setup covers the most general case, including both discrete and continuous variables, and can be handled using linear programming techniques (e.g., santambrogio:optran, Section 6.4.1), after observing that the moment constraint is easy to incorporate since it is linear in the probability measure. However, we shall focus on the purely continuous case in the remainder of this paper, because it enables us to express the main ideas more transparently. The fully continuous case indeed admits a convenient treatment, under the following regularity condition:
Assumption 1.1
The marginals (arising from the solution at each ) and have finite variance and is absolutely continuous with respect to the Lebesgue measure.
Under this condition, by Theorem 1.22 in santambrogio:optran, there exists a unique implied by a deterministic transport map that solves the constrained optimization problem (1) and yielding a transport cost . Since determining the function amounts to finding which value each point should be mapped to, the sample version of this problem can be stated as
| (2) |
subject to:
| (3) |
where denotes sample averages (i.e. , where is sample size and a given function). This optimization problem is then nested into an optimization over , which delivers the estimated parameter value . We call this estimator an Optimally-Transported GMM (OTGMM) estimator.
Our approach is conceptually similar to GEL, in that it minimizes some concept of distributional distance under moment constraints. Yet, the notion of distance used differs significantly. As shown in Figure 1, the distance here is measured along the “observation values” axis rather than the “observation weights” axis (as it would be in GEL). This feature arguably makes the method a hybrid between GEL and optimal transport, since GEL’s goal of satisfying all the moment conditions is achieved through optimal transport instead of optimal reweighting. (The distinction from GEL applies to the discrete case as well, since the OTGMM objective function depends on both the amount and location of probability mass transfers, while the GEL objective function is only sensitive on the amount, but not on the specific locations, of the probability mass transfers.) Most of our regularity conditions will parallel those of optimal GMM and GEL, but some will not (they are tied to the optimal transport nature of the problem and involve assumptions regarding higher order derivatives). In analogy with the behavior of GEL estimators, the OTGMM estimator will be shown to be root- consistent and asymptotically normal, despite involving an optimization problem having an infinite-dimensional nuisance parameter (the ). In general, however, OTGMM’s asymptotic variance does not coincide with that of GEL or efficiently weighted GMM under correct specification. Hence, OTGMM is most useful when errors in the variables or Wasserstein-type deviations in the data distribution constitute a primary concern.
The remainder of the paper is organized as follows. We first formally define and solve the optimization problem defining our estimator, before considering the limit of small errors (in the spirit of chesher:1991) to gain some intuition. We then derive the resulting estimator’s asymptotic properties for the general, large error, case. We show asymptotic normality and root consistency in both cases. We then discuss related approaches and some extensions. The method’s practical usefulness is illustrated by revisiting an influential study of the relationship between a city’s exports and the extent of its transportation infrastructure (duranton2014roads). Our results corroborate that study under weaker assumptions and provide insight into the error structure of the variables.
2 The estimator
2.1 Definition
The Lagrangian associated with the constrained optimization problem defined in Equations (2) and (3) is
where denotes a sample average and where is a Lagrange multiplier. The dual problem’s first-order conditions with respect to , and , respectively, are then
| (4) | |||||
| (5) | |||||
| (6) |
where we let denote a partial derivative with respect to argument . We shall use to denote a matrix of partial derivatives with respect to a transposed variable (e.g., ). This formulation of the problem assumes differentiability of to a sufficiently high order, as shall be formalized in our asymptotic analysis.
2.2 Implementation
The nonlinear system (4)-(6) of equations can be solved numerically. To this effect, we propose an iterative procedure to determine the , for a given . This yields an objective function that can be minimized to estimate . Let and denote the approximations obtained after steps. As shown in Supplement S.1.1, given tolerances and a given , the objective function can be determined as follows:
Algorithm 2.1
-
1.
Start the iterations with and .
-
2.
Let and , where .
-
3.
Increment by ; repeat from step 2 until and .
-
4.
The objective function is then:
This algorithm is obtained by substituting obtained from Equation (6) into Equation (5) and expanding the resulting expression to linear order in . This linearized expression provides an improved approximation to the Lagrange multiplier which can, in turn, yield an improved approximation . The process is then iterated to convergence. The expression for is obtained by re-expressing using Equation (6). Formal sufficient conditions for the convergence of this iterative procedure can be found in Supplement S.1.2. In cases where this simple approach fails to converge, one can employ robust schemes based on a combination of discretization and linear programming (see Section 6.4.1 in santambrogio:optran).
To gain some intuition regarding the estimator, it is useful to consider the limit of small errors when solving Equations (2)-(3), in the spirit of chesher:1991). This limit corresponds to assuming that higher-order powers of are negligible relative to itself. In this limit, the estimator admits a closed form with an intuitive interpretation, as shown by the following result, shown in Appendix A.1.
Proposition 2.2
To the first order in () the estimator is equivalent to minimizing a GMM-like objective function with respect to with a specific choice of weighting matrix:
| (7) |
From this expression, it is clear that the estimator downweights the moments that are the most sensitive to errors in , as measured by . This accomplishes the desired goal of minimizing the effect of the errors when the properties of the error process are unknown.
Although this weighting matrix appears suboptimal (relative to a correctly specified optimally weighted GMM estimator), one should realize that the notion of optimality depends on what class of data generating processes the “true model” encompasses. Optimal GMM and GEL can be seen as Maximum Likelihood estimators (chamberlain:gmmeff, newey:gel) under moment conditions expressed in terms of the observed . In contrast, OTGMM can be interpreted as a Maximum Likelihood estimator for homoskedastic and normally distributed errors () under moment constraints on the unobserved . There is therefore a clear efficiency-robustness trade-off: OTGMM is less efficient if the observed satisfy the moment constraints, but allows for additional error terms that maintain the model’s correct specification even if the observed do not satisfy the overidentified moment constraints, a more general setting where optimally weighted GMM or GEL offers no efficiency guarantees.
3 Asymptotics
In this section, we show that, despite the estimator’s roots in the theory of optimal transport, its large sample behavior remains amenable to standard asymptotic tools since our focus is on an estimator of the parameter rather than on an estimator of a distribution. We first consider the case of small errors, a limiting case that may be especially important in the relatively common case of applications where overidentifying restrictions tests are near the rejection region boundary. This limit also parallels the approach taken in the GEL literature, where asymptotic properties are often derived in the case where the overidentifying restrictions hold (e.g., newey:gel).
3.1 Small errors limit
Our small error results enable us to illustrate that there is little risk in using our estimator instead of efficient GMM when one is concerned about overidentification test failure. If the data were to, in fact, satisfy the moment conditions, using our approach does not sacrifice consistency, root convergence or asymptotic normality. The only possible drawback would be a suboptimal weighting of overidentifying moment conditions, potentially leading to an increase in variance if the model happened to be correctly specified. Conversely, if the model is misspecified, e.g., because the data is error-contaminated, the optimal weighting of efficient GMM is no longer the optimal weighting (since random deviations due to sampling variability are not the main reason for the failure to simultaneously satisfy all moment conditions). For instance, if the errors are such that there is an unknown bias in the moment conditions that decays to zero asymptotically but at a rate possibly slower than , then the model is still correctly specified asymptotically but the bias dominates the random sampling error. Then, the optimal weighting should seek to minimize the effect of error-induced bias, which our approach seeks to accomplish by weighting based on the effect of errors in the variables on the moment conditions. Hence, in that sense, the method provides a complementary alternative to standard GMM estimation offering a different trade-off between efficiency and robustness to misspecification.
Our consistency result requires a number of fairly standard primitive assumptions.
Assumption 3.1
The random variables are iid and take value in .
Assumption 3.2
, and for other , a compact set.
In other words, Assumption 3.2 indicates that we consider here the case where GMM would be consistent, in analogy with the setup traditionally considered in the GEL literature (e.g. newey:gel).
Assumption 3.3
, where denotes the variance operator.
Assumption 3.4
is almost surely continuous and for any and for some function satisfying .
While Assumptions 3.1, 3.2, 3.3 and 3.4 directly parallel those needed to establish the asymptotic properties of a standard GMM estimator (e.g. Theorems 2.6 and 3.2 in Newey:HB), our estimator requires a few more low-level regularity conditions. Given that our estimator, in the small error limit (Equation (7)), involves a sample average involving derivative , we need to place some constraints on the behavior of that quantity as well. Below, we let for a matrix .
Assumption 3.5
is differentiable in its first argument and the derivative satisfies . Moreover, almost surely for all .
Assumption 3.6
is Hölder continuous in .
Assumption 3.7
exists and is of full rank.
These assumptions ensure that the minimization problem defined by (2) and (3) is well-behaved, i.e., small changes in the values of do not lead to jumps in the solution to the optimization problem (aside from zero-probability events). It is likely that these assumptions can be relaxed using empirical processes techniques. However, here we favor simply imposing more smoothness (compared to the standard GMM assumptions), because this leads to more transparent assumptions. They can all be stated in terms of the basic function that defines the moment condition model, making them fairly primitive. We can then state our first consistency result.
As a by-product, this theorem also secures a convergence rate on the Lagrange multiplier which proves useful for establishing our distributional results. The conditions needed to show asymptotic normality also closely mimic those of standard GMM estimators (e.g. Theorem 3.2 in Newey:HB):
Assumption 3.9
, the interior of .
Assumption 3.10
where is a neighborhood of .
Assumption 3.11
is invertible.
We can then provide an explicit expression of the estimator’s asymptotic variance.
Theorem 3.12
This simple normal limiting distribution with root convergent behavior is somewhat unexpected from an estimator that involves a high-dimensional optimization over latent variables. As in GEL, this is made possible thanks to the existence of an equivalent low-dimensional dual optimization problem, which is, in turn, equivalent to a simple GMM estimator, albeit with a nonstandard weighting matrix. Thus, the variance has the expected “sandwich” form, since the reciprocal weights differs from the moment variance . For comparison, an optimally weighted GMM estimator would have an asymptotic variance of .
It is difficult to formulate a simple expression for the difference between the asymptotic variance of OTGMM and that of optimal GMM, but a simple example suffices to illustrate that this difference could be any non-negative-definite matrix.
Let take value in () with and when and zero otherwise, for . For , let the moment functions be given by . We then have
Then, by the harmonic-arithmetic mean inequality, with equality when for all . If leaving all the other finite, while remains finite. (General non-negative-definite differences can be obtained by stacking such moment conditions for different parameters () and possibly linearly transforming the resulting parameter vector .) Generally, we expect the difference to be large when some moments have a disproportionally large variance while not being disproportionally sensitive to changes in the underlying variables.
3.2 Asymptotics under large errors
In some applications, there may be considerable misspecification or its magnitude may be a priori unknown. It thus proves useful to relax the assumption of small errors in deriving the estimator’s asymptotic properties. To handle this more general setup, we employ the following equivalence, demonstrated in Appendix A.2.
Theorem 3.13
If is differentiable in its arguments, the OTGMM estimator is equivalent to a just-identified GMM estimator expressed in terms of the modified moment function
| (8) |
that is a function of the observed data and the augmented parameter vector and where
| (9) |
Note that is essentially the inverse of the mapping (from Equation (6)), augmented with a rule to select the appropriate branch in case the inverse is multivalued.
The equivalence result of Theorem 3.13 implies that many of the asymptotic technical tools used in GMM-type estimators can be adapted to our setup, with the distinction that the function is defined only implicitly. Hence, many of our efforts below seek to recast necessary conditions on in terms of more primitive conditions on the moment function whenever possible or in terms of regularity conditions drawn from optimal transport theory.
We start with a standard GMM-like identification condition:
Assumption 3.14
For some compact sets and , there exists a unique solving for defined in Theorem 3.13.
This condition is implied by a natural uniqueness and regularity condition on the solution to the primal optimal transportation problem (Equations (1)):
Assumption 3.15
Let denote the solution to Problem (1) for a given . (i) is uniquely minimized at (ii) The corresponding marginals and are absolutely continuous with respect to the Lebesgue measure with a density that is finite, nonvanishing and Hölder continuous on their convex support.
Indeed, by Theorem [C3], part b) and d), in caffarelli:regconv, Assumption 3.15(ii) implies that, at each , there exists a unique invertible transport map from to and both and its inverse are equal to the gradient of a twice differentiable strictly convex function. The fact that is the gradient of a twice differentiable strictly convex function ensures that the first-order condition in Assumption 3.14 has a unique solution (making a rule to handle multivalued inverses unnecessary).
Next, we consider standard continuity and dominance conditions that are used to establish uniform convergence of the GMM objective function. These assumptions constitute a superset of those needed for standard GMM because the modified moment conditions include the additional parameter and higher-order derivatives of the original moment conditions. In a high-level form, these conditions read:
Assumption 3.16
(i) is continuous in and for with probability one and (ii) .
Alternatively, Assumption 3.16 can be replaced by more primitive conditions on instead, as given below in Assumptions 3.17, 3.18 and 3.20.
Assumption 3.17
(i) and are differentiable in and (ii) is continuous in both arguments.
This assumption parallels continuity assumptions typically made for GMM, but higher order derivatives of are needed, because they enter the moment condition either directly or indirectly via the function . The next condition ensures that the function is well behaved.
Assumption 3.18
where and , in which exists for and where for some matrix denotes the set of its eigenvalues.
Once again, this condition can be alternatively phrased in terms of optimal transport concepts. The first order condition which implicitly defines can be written in terms of the derivative of a potential function : . With the help of Theorem [C3], part b) and d), in caffarelli:regconv, Assumption 3.15(ii) implies that, at each , the above potential has a positive-definite Hessian (with respect to ), which implies Assumption 3.18.
In order to state our remaining regularity conditions, it is useful to introduce a notion of (nonuniform) Lipschitz continuity, combined with dominance conditions.
Definition 3.19
Let be the set of functions such that (i) and (ii) there exists a function satisfying
| (10) | |||||
| (11) |
for all and , and where is as in the moment conditions.
This Lipschitz continuity-type assumption has no parallel in conventional GMM. It is made here because it ensures that the behavior of the observed and the underlying unobserved will not differ to such an extent that moments of unobserved variables would be infinite, while the corresponding observed moments are finite. Clearly, without such an assumption, observable moments would be essentially uninformative. The idea underlying Definition 3.19 is that we want to define a property that is akin to Lipschitz continuity but that allows for some heterogeneity (through the function in Equation (11)). This heterogeneity proves particularly useful in the case where is not compact (for compact , one can take to be constant in with little loss of generality). For a given function that is finite for finite , membership in is easy to check by inspecting the tail behavior (in ) of the given function . Polynomial tails will suggest a polynomial form for , for instance. Equation (10) strengthens the dominance condition 3.19(i) to ensure that functions in also satisfy a dominance condition when interacted with other quantities entering the optimization problem, i.e. ).
With this definition in hand, we can succinctly state a sufficient condition for to satisfy a dominance condition:
Assumption 3.20
and each element of belong to .
We are now ready to state our general consistency result.
Theorem 3.21
We now turn to asymptotic normality. We first need a conventional “interior solution” assumption.
Assumption 3.22
from Assumption 3.14 lies in the interior of .
Next, as in any GMM estimator, we need finite variance of the moment functions and their differentiability:
Assumption 3.23
(i) exists and (ii) exists and is nonsingular.
Assumption 3.23(ii) can be expressed in a more primitive fashion using the explicit form for provided in Theorem 3.26 below.
Next, we first state a high-level dominance condition that ensures uniform convergence of the Jacobian term .
Assumption 3.24
(i) is continuously differentiable in ;(ii) .
This assumption is implied by the following, more primitive, condition:
Assumption 3.25
We can now state our general asymptotic normality and root- consistency result, shown in Appendix A.2.
Theorem 3.26
The asymptotic variance stated in Theorem 3.26 takes the familiar form expected from a just-identified GMM estimator: . The relatively lengthy expressions merely come from explicitly computing the first derivative matrix in terms of its constituents. This is accomplished by differentiating with respect to all parameters using the chain rule and calculating the derivative of using the implicit function theorem.
We thus have now completely characterized the first-order asymptotic properties of our estimator in the most general settings of large (i.e., non-local) misspecification. This result thus allows researcher to directly replace their GMM estimator which may happen to fail overidentification tests by another, logically consistent and easy-to-interpret, estimator where the overidentification failure is naturally accounted for by errors in the variables. In addition, researchers can further document the presence of errors via Theorem 3.26, as it enables, as a by-product, a formal test of the absence of error. Under this null hypothesis, which can be stated as , we have
| (14) |
where the above expression can be straightforwardly derived from the partitioned inverse formula applied to the sub-block of the asymptotic variance.
4 Discussion and extensions
On a conceptual level, our use of a so-called Wasserstein metric to measure distance between distributions does provide some desirable theoretical properties. For instance, the Wasserstein metric metrizes convergence in distribution (see Theorem 6.9 in villani:new) under some simple bounded moment assumptions. In contrast, the discrepancies which generate GEL estimators do not admit such an interpretation. In fact, most discrepancies are not metrics, as they lack symmetry. The Kullback-Leibler discrepancy, which is perhaps the best known among them, does not allow comparison between distributions that are not absolutely continuous with respect to one another, whereas the Wassertein metric does. (Of course, leveraging this advantage requires considering the most general transport problem of Equation (1).) Finally, it is arguably logical to penalize probability transfer over larger distances more than the same probability transfer over smaller distances, as the Wasserstein metric does, while none of GEL-related discrepancies do.
An interesting extension of our approach would be a hybrid method in which (i) the possibility of general forms of errors is accounted for with the current method by constructing the equivalent GMM formulation of the model via Theorem 3.13 and (ii) additional restrictions on the form of the errors are imposed via additional moment conditions involving some elements of and . This could prove a useful middle ground when a priori information regarding the errors is available for some, but not all, variables.
As shown in Supplement S.2, it is straightforward to extend our approach to allow error in some, but not all variables. When our method is used while allowing for errors in only a few variables, it may not be possible to simultaneously satisfy all moment conditions for any . In such cases, it could make sense to consider a hybrid method where both errors in the variables, handled via our approach, and re-weighting of the sample, handled via GEL, are simultaneously allowed.
Finally, we should mention other approaches aimed at handling violations of overidentifying restrictions, which include the use of set identification combined with relaxed moment constraints (poirier:salvaging), placing a Bayesian prior on the magnitude of the deviations from correct specification of the moments (conley:plausexo), distributionally robust approaches that allow for deviations from the data generating process up to a given bound (connault:sens, blanchet:distrob), sensitivity analysis (shapiro:sens, bonhomme:sens) and misspecified moment inequality models (e.g. kwon:ineqmiss).
5 Application
We revisit the study of duranton2014roads, who documented evidence that the quantity of goods a city exports is strongly related to the extent of interstate highways present in that city. Due to simultaneity concerns, the authors adopt an instrumental variable approach to recover the causal effect of building highways. Although the authors mitigate instrument validity concerns with controls, instrument exogeneity or exclusion could remain a concern (as noted by poirier:salvaging) and this problem would manifest itself by specification test failure.
Main results from Table 5 in duranton2014roads. Original GMM estimates and OTGMM estimates. Heteroskedasticity-robust standard errors (GMM) and small-error asymptotic standard error (OTGMM) in parentheses.
We apply our method to further assess the robustness of the results to potential model misspecification. We seek to recover point estimates that remain interpretable under potential misspecification and account for misspecification by viewing the model’s variables as potentially measured with error. Not only do we consider errors in the regressors, but we also think of potentially invalid instruments as simply mismeasured versions of an underlying valid instrument that is unfortunately not available. Alternatively, one can think of the underlying valid instrument as the counterfactual value of the instrument in a world where the mechanism causing this instrument to be invalid would be absent. This broader interpretation of what could constitute an “error” under our framework considerably expands the scope of models that are conceptually consistent with our approach.
duranton2014roads consider three instruments: (log) kilometers of railroads in 1898, quantity of historical exploration routes, and planned (log) highway kilometers according to a 1947 construction map. The validity of these instruments could be criticized, for instance, in a situation where some cities are in proximity to key natural resources, which could cause higher exports and, at the same time, more transportation routes (or plans to build them). If this mechanism is active both in the present and in the past, the causal effect of highways on exports would be overestimated. In our framework, the true but unavailable instrument could represent a measure of past transportation routes in a counterfactual world where natural resources would be evenly distributed among cities. The actual available instruments represent an approximation to this ideal instrument, a situation which we represent as a potentially non-classical errors-in-variables model.
The model’s moment conditions are written in terms of , where the vector of observables contains the dependent variable , the vector of regressors and the vector of instruments . In duranton2014roads, the dependent variable measures “propensity to export” and is constructed from an auxiliary panel data model, which regresses volume of exports between given cities on distance and trading partner characteristics, modeled as fixed effects. It is the value of these fixed effects that is used as . Following poirier:salvaging, we take this construction as given and focus on export volume measured by weight. Explicitly accounting for errors in is superfluous since, in a regression setting, errors in the dependent variable are already allowed for.
In the analysis below, always includes the regressor of main interest: the (logarithm of) the number of kilometers of highway. It also contains a number of controls, which may differ in the different models considered. These controls include: log employment, log market access, log population in 1920, 1950, and 2000, and log manufacturing share in 2003, average distance to the nearest body of water, land gradient, dummy variables for census regions, log share of the fraction of adult population with a college degree or more, log average income per capita, log share of employment in wholesale trade, and log average daily traffic on the interstate highways in 2005. We allow for errors in all of these variables, except for the constant term and dummies.
We consider two of the specifications employed by duranton2014roads: The specification with many covariates that most obviously passes the overidentifying test and the specification with few covariates that most clearly fails this test. (The other specifications fall in between these extreme cases and are thus not reported here, for conciseness.) The results from GMM, replicated from the original study, and from OTGMM (allowing for large errors) are reported in Tables 1 and 2.
Specification from column 2, Table 5, in duranton2014roads. Replicated estimates from the original paper and OTGMM estimates. Heteroskedasticity-robust standard errors (GMM) and small-error asymptotic standard error (OTGMM) in parentheses.
The OTGMM estimates are similar to those of duranton2014roads. Although some coefficients of the controls are larger in magnitude, the main coefficient — the elasticity of export weight relative to kilometers of highway — is almost unchanged and remains statistically significant. As in the original study, the 95% confidence intervals of the main elasticity of interest obtained with different sets of controls overlap significantly.
It is also instructive to look at the correction of the underlying variables implied by OTGMM. Table 3, columns 1 and 2, documents this by looking at the standard deviation of different elements of the correction . The corresponding quantities for the models of Tables 1 and 2 are reported in columns 1 and 2, respectively. To get an idea of the scale, the last column reports the standard deviations of the observed variables. As expected, the errors in column 1 are exceedingly small, reflecting the fact that the model passes the overidentification test. In contrast, column 2 is particularly interesting for our purposes because it corresponds to a specification that fails the test of over-identifying restrictions. While this may, at first, cast doubt regarding the GMM estimates, OTGMM shows that errors of the order of only 10% of the observed regressors’ magnitude are sufficient to eliminate the mispecification. Such small error magnitudes are highly plausible empirically, thus supporting the plausibility of the OTGMM estimate. As the GMM and OTGMM estimates of the main elasticity of interest are very close in Table 2, this also corroborates the authors’ GMM estimates. Overall, our approach strongly supports the conclusions of the original study and thus provides an effective robustness test.
The fact that the model with more controls leads to smaller error magnitudes is very consistent with our interpretation: to the extent that including more controls reduces the magnitude of the potentially endogenous residuals, one would expect that our estimator has to perform smaller alterations to the variables to arrive at valid instruments and/or non-endogeneous regressors. More quantitatively, suppose that the instrument and the error term can be decomposed into separate components, say and , where and are correlated, but the latter is well explained by additional controls. In a model with fewer controls, our method has to identify and/or as error components to obtain a correctly specified model. However, if including controls already accounts for the term, then our estimator no longer needs to correct the corresponding components and thus achieves orthogonality with smaller errors.
In contrast, using GEL to address misspecification would effectively assume that the misspecification originates from some form of selection bias. The fact that adding more control eliminates the misspecification indicates that the controls incorporate the variables that explain selection bias. Yet, these controls are not included in the model with few controls, precisely the model where the largest amount of sample reweighting would take place when using GEL. This paradox makes the use of GEL as a remedy difficult to rationalize in a setting where misspecification arises primarily from the unavailability of adequate control variables.
In Supplement S.4.1, we perform a number of robustness tests along the lines suggested by poirier:salvaging. We further replicate specifications with many more controls and find that the GMM and OTGMM are still in close agreement. In Supplement S.4.2, we also consider alternative specifications that allow for the possibility that some instrumental variables should be included as regressors. The performance of OTGMM in other simple models is also investigated via simulations in Supplement S.5.
6 Conclusion
We have proposed a novel optimal transport-based version of the Generalized Method of Moment (GMM) that fulfills, by construction, the overidentifying moment restrictions by introducing the smallest possible amount of error in the variables or, equivalently, by allowing for the smallest possible Wasserstein metric distortions in the data distribution. This approach conceptually merges the Generalized Empirical Likelihood (GEL) and optimal transport methodologies. It provides a theoretically motivated interpretation to GMM results when standard overidentification tests reject the null. GEL approaches handle model misspecification by re-weighting the data, which would be appropriate when misspecification arise from improper sampling of the population. In contrast, our optimal transport approach is appropriate when the sources of misspecification are errors or, more generally, Wasserstein-type distortions in the data distribution, which is arguably a common situation in applications.
Appendix A Proofs
A.1 Linearized estimator
Proof of proposition 2.2. In the following, the approximation denoted by “” are exact to first order in . In that limit, . Therefore:
| (15) |
Substituting into the constraint yields: , while using a Taylor expansion gives: , or:
where , thus implying:
| (16) |
Substituting (15) and (16) back into the objective function (2) yields (with ) :
A.2 Asymptotics
Proof of Theorem 3.8. We minimize subject to . First-order conditions for read
| (17) |
It is first shown that there exists a sequence that matches the moment condition and converges uniformly to the ’s, implying convergence of the ’s by their definition in the optimization problem.
We now discuss how to eliminate observations that are too close to a zero gradient. For some and let be the set of all such that . We must have for some because otherwise has probability 0 for all , thus has probability 0 for all by continuity from below, contradicting assumption 3.5 with continuity of .
We now consider such a pair , fix the resulting set , and let be the observations in sample that fall in it. In order to get enough degrees of freedom to offset deviations of sample averages from 0, we make group of observations. Let , and assume for convenience it is an integer that divides 222If not, it suffices to set the remaining (components of) to and re-scale appropriately in what follows.. Without loss, let the in constitute the first observations and let for all . Then, for all (0 included) let and solve wpa1 for in . By the LLN and so that a sequence with will exist by continuity.
We also get because . By definition of and the previous result, we have . By properties of norms, assumptions 3.5-3.6 with Hölder continuity exponent , Cauchy-Schwartz, the LLN, and the previous convergence result
Furthermore, proceeding component-wise with denoting the row of a matrix and using assumptions 3.5-3.6 together with previous results and proceeding as above for the term , we have
and thus, using assumption 3.7, there exists such that
Now we derive a precise rate of convergence and the resulting asymptotic distribution for . Solving for in equation (17) yields , which can be plugged in the second equation to obtain By a Taylor expansion and assumption 3.7, we get
| (18) |
By assumptions 3.1, 3.2, 3.3 and the central limit theorem, the first term is .
Under assumptions 3.1 and 3.7, we have by the LLN and thus the second term is . It follows that with an asymptotically normal distribution.
Finally, we turn to the situation where . By the uniform Law of Large Numbers, using assumption 3.4, .
For any we have by identification for some (otherwise, we can find a sequence whose mapping converges to 0 and by compactness there would be a convergent subsequence, implying existence of some that satisfies ).
With probability approaching one, we have by the mean value theorem and Cauchy-Schwartz . As a result, (or a subsequence) would imply as before and thus , which is impossible. Therefore, with probability approaching one, and the probability that lives outside any neighborhood of decreases to 0.
Eventually, the first-order conditions read and and the linearized version is asymptotically justified.
Proof of Theorem 3.12. Using first-order conditions, previous results, and equation (18), we have
Ignoring lower order terms, we can eventually reframe the problem as minimizing standard GMM: to get the first-order conditions
which are satisfied with probability approaching 1. By an expansion around ,
so that the estimator takes the form
Noting that under assumptions 3.1, 3.2, and 3.3 converges in distribution to a normal random variables by the central limit theorem and that assumptions 3.9-3.11 together with consistency ensure convergence of sample averages to expectations, we obtain the asymptotic normality of by Slutsky with asymptotic variance given in the theorem.
Proof of Theorem 3.13. The first-order conditions with respect to the (Equation (6)) can be written as
| (19) |
Under our assumptions, Equation (19) defines a direct relationship between and , and therefore an implicit reverse relationship between and . Since the latter may not be unique, we observe that our original optimization problem seeks to minimize the distance between and . Hence, in cases where (19) admits multiple solutions for a given , we identify the unique (with probability one) solution that minimizes . This is accomplished by defining the mapping (9). With this definition, the first-order conditions (4) and (5) of the Lagrangian optimization problem for and yield the just-identified GMM estimator stated in the Theorem.
The following Lemmas are shown in Supplement S.3.
Lemma A.1
Proof of Theorem 3.21. Assumptions 3.1-3.16 directly imply consistency of our GMM estimator, by Theorem 2.6 in Newey:HB. There remains to show that Assumption 3.16 is implied by Assumptions 3.17,3.18,3.20.
We first establish Assumption 3.16 (i): Continuity of in . To show that is continuous in , we can invoke Lemma A.1 for , under Assumptions 3.17(i) and 3.18. To show that is continuous in , we can similarly invoke Lemma A.1 for , where is continuous in both arguments by Assumption 3.17(ii).
We now establish Assumption 3.16 (ii): . Since by Assumption 3.20, it follows that , by Lemma A.2. Next, we have, for , by Assumption 3.18 and compactness of . By Assumption 3.20 and Lemma A.2 we then also have that.
Proof of Theorem 3.26. Theorem 3.21 implies consistency . This, in addition to Assumptions 3.22, 3.23 and 3.24 directly implies the stated asymptotic normality result, by Theorem 3.2 and Lemma 2.4 in Newey:HB and the Lindeberg-Levy Central Limit Theorem. There remains to show that Assumption 3.24 is implied by Assumption 3.25.
By Lemma A.1, Assumptions 3.25(i) and (iii) imply that both and are continuously differentiable in , thus establishing Assumption 3.24(i). By Lemma A.2, Assumptions 3.14, 3.25(ii) and (iii) imply Assumption 3.24(ii).
The asymptotic variance of the just-identified GMM estimator defined in Theorem 3.13 is then given by for and as defined in the Theorem statement.
Finally, the explicit expressions for the derivatives of the function follow from the implicit function theorem after noting that is the inverse of the mapping . This can also be shown through an explicit calculation: To first order, (19) implies, for a small change in , a corresponding change in while keeping and fixed, that:
Thus, and we have:
evaluated at . A similar reasoning for , exploiting the fact that , yields:
Online Supplement for:
Optimally-Transported Generalized Method of Moments
by Susanne Schennach and Vincent Starck
Appendix S.1 Algorithms
S.1.1 Iterative solution
The first order condition (6) can be re-written as
| (20) |
We seek to construct a sequence () that converges to , starting with . From the moment conditions and (20), we have:
Adding and subtracting yields
where the expansion is justified from the fact that as .
In the same limit, , so
Isolating gives the approximation to the Lagrange multiplier at step :
| (21) |
From this, we can improve the approximation to to go to the next step, using (20):
| (22) |
It can be directly verified that the values of and that satisfy the first order conditions are indeed a fixed point of this iterative rule. In the next subsection we shall provide conditions under which this fixed point is also attractive.
After iteration to convergence, the objective function can be written in term of the converged values of and :
S.1.2 Iterative procedure convergence
Substituting (21) into (22) yields an iterative rule expressed solely in terms of :
| (23) |
This is an iterative rule of the form , for with fixed point denoted . We then have the following result.
Assumption S.1.1
(i) is twice continuously differentiable in and(ii) is nonsingular for in the closure of an open neighborhood of the fixed point .
Theorem S.1.2
The condition that the initial point should lie in a neighborhood of the solution is standard — most Newton-Raphson-type iterative refinements have a similar requirement. If necessary, this requirement can be met by simply attempting many different starting points in search for one that yields a convergent sequence. The condition that be small intuitively means that the errors should not be too large. This is a purely numerical condition which has nothing to do with sample size, statistical significance of specification tests. In particular, it does not mean that the error magnitude must decreases with sample size or that the effect of the errors should be small relative to the estimator’s standard deviation. Typically, the constraint on is relaxed as the starting point is chosen closer to the solution .
Proof of Theorem S.1.2. For a rule of the form , Banach’s Fixed Point Theorem applied to a neighborhood of provides a simple sufficient condition for convergence: (i) must be continuously differentiable in a neighborhood of and (ii) all eigenvalues of the matrix must have a modulus strictly less than .
The smoothness condition (i) is trivially satisfied under Assumption S.1.1. Next, letting denote one element of the vector , and denote the column of , we can express all blocks of the matrix of partial derivatives of
where the two terms cancel each other. At , and and we have:
This expression (once all derivatives of products are expanded) has the general form of a product of with functions of that contain terms of the form , which is nonsingular for by Assumption S.1.1(ii), and derivatives of up to order 2, which are bounded for in the compact set by Assumption S.1.1(i). Hence the elements are bounded by a constant times . It follows that the eigenvalues of the matrix of partial derivatives of can be made strictly less than 1 for sufficiently small.
Appendix S.2 Constrained estimator
In some applications, it is useful to constrain the error, for instance to enforce the known fact that some variables are measured without error. The optimization problem then becomes to minimize subject to
| (24) | |||||
| (25) |
for some known rectangular full row rank matrix that selects the dimensions of the error vector that should be constrained to be zero. Note that error constraint is imposed for each observation, not in an average sense. Naturally, we assume that the number of constraints imposed is not so large that it is no longer possible to satisfy all moment conditions simultaneously.
Proposition S.2.1
Thanks to linearity, the Lagrange multipliers of the error constraints can be explicitly solved for and the dimensionality of the problem is not increased. The only effect of the constraints is to “project out”, through the matrix , the dimensions where there are no errors.
Proof. The Lagrangian for this problem is
| (27) |
where and are Lagrange multipliers. The first order conditions of the Lagrangian (27) with respect to is
| (28) |
Re-arranging and pre-multiplying both sides by the full column rank matrix yields:
thus allowing us to solve for :
Upon substitution of into (28) and simple re-arrangements, we obtain
where and .
The iterative Algorithm 2.1 can easily be adapted by replacing every instance of by . Similarly the linearized estimator of Equation (7) becomes:
Note that this expression assumes that the matrix being inverted remains full rank, a condition that can be interpreted as requiring the constraints to not be so strong as to make impossible to simultaneously satisfy all the moment conditions.
Asymptotic results can also be straightforwardly adapted.
Appendix S.3 Asymptotics: Proofs of Lemmas
Proof of Lemma A.1. Since is assumed to be continuous in all of its arguments, there only remains to show that is continuous in . In fact, we can establish the stronger statement that is differentiable in . Differentiability in can be shown by the implicit function theorem
since is the inverse of . By the definition of , ,
at , where by Assumption 3.18 and where exists by Assumption 3.17. Thus is continuous in .
By a similar reasoning, we can show that is continuous in if we can show that exists:
Proof of Lemma A.2. By the triangle inequality and Definition 3.19, there exists such that:
| (29) |
for . Next, using the first order conditions (Equation (6)), we have, by a mean value argument, the triangle inequality and the definitions of and from Assumption 3.18, for some mean value ,
| (30) | |||||
Re-arranging and using the fact that by Assumption 3.18 and by compactness of ,
| (31) |
Combining (29) and (31) and noting that applying the operator does not alter the inequalities, we have
where the right-hand side quantities are finite by construction since .
Appendix S.4 Application: Additional results
S.4.1 Regression with all controls
In this section, we estimate a regression with controls, which now also includes average distance to the nearest body of water; land gradient; dummy variables for census regions; log share of the fraction of adult population with a college degree or more; log average income per capita; log share of employment in wholesale trade; and log average daily traffic on the interstate highways in 2005. The results, reported in Table S.1, are similar to those of the main regression shown in Table 1. In Table S.2, we also show how the main coefficient of interest changes when including various subsets of controls.
Results with all controls. Replicated estimates from the original paper and OTGMM estimates. Heteroskedasticity-robust standard errors (GMM) and small-error asymptotic standard error (OTGMM) in parentheses. The regression also include an intercept and census region dummies.
Coefficient on log highway kilometers for various sets of controls, numbered as follows: employment (1), market access (2), population 1920 (3), population 1950 (4), population 2000 (5), log % manuf employment (6), water (7), land gradient (8), college (9), price (10), wholesale (11), traffic (12), region census dummies (13-20). All regressions include an intercept.
S.4.2 Relaxing exclusion restrictions
As poirier:salvaging, we consider the possibility that some instrumental variables have non-zero regression coefficient, i.e., have a direct impact on the outcome. Since our estimator relies on over-identifying restrictions to recover meaningful variable corrections, we only relax one of the exclusion-restrictions at a time. This leads to the following results.
Including log 1947 highway kilometers as a regressor. Replicated estimates from the original paper and OTGMM estimates. Heteroskedasticity-robust standard errors (GMM) and small-error asymptotic standard error (OTGMM) in parentheses
Including log exploration as a regressor. Replicated estimates from the original paper and OTGMM estimates. Heteroskedasticity-robust standard errors (GMM) and small-error asymptotic standard error (OTGMM) in parentheses
Including log 1998 railroad as a regressor. Replicated estimates from the original paper and OTGMM estimates. Heteroskedasticity-robust standard errors (GMM) and small-error asymptotic standard error (OTGMM) in parentheses
As in poirier:salvaging, we found that removing railroads has the largest impact on coefficients and is likely an improper instrument.
Appendix S.5 Simulations
We conduct simulations to assess the performance of our estimator and compare it to efficient GMM. We consider both the OTGMM estimator (Equations (2) and (3)) and the GMM estimator obtained under the assumption of small errors (Equation (7)).
Given the nonlinear nature of our estimator, we deliberately select a small sample size () to explore the regime where asymptotic results do not trivially hold. We consider various moment conditions, underlying distributions and signal-to-noise ratios. There is an underlying random variable which satisfies the moment conditions, but the researcher observes with . We consider different values for the error scale in order to assess the impact of magnitude of the errors on the performance of estimators that only use the observed .
Specifically, we consider the following distributions for : (uniform) (binomial) and . The true parameter value is , as obtained by the following moment conditions:
| (32) | |||||
| (33) | |||||
| (34) |
Finally, we consider the process: with the moment conditions
| (35) |
In all cases, the model is correctly specified in the absence of errors () but starts to violate the overidentifying restrictions when there are errors ( so that ).
| Bias | ||||||||||||||||||
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | 0.00 | -0.01 | -0.05 | -0.16 | -0.49 | -1.61 | 0.00 | -0.01 | -0.03 | -0.04 | -0.05 | -0.04 | -0.02 | -0.03 | -0.09 | -0.17 | -0.23 | -0.25 |
| Uniform | 0.00 | -0.23 | -1.12 | -3.59 | -10.93 | -36.42 | 0.00 | -0.09 | -0.12 | -0.11 | -0.08 | -0.06 | 0.00 | -0.13 | -0.22 | -0.27 | -0.29 | -0.29 |
| Binomial | 0.00 | -0.02 | -0.10 | -0.32 | -0.99 | -3.28 | 0.00 | -0.01 | -0.04 | -0.06 | -0.06 | -0.05 | 0.00 | -0.04 | -0.14 | -0.21 | -0.25 | -0.26 |
| Standard deviation | ||||||||||||||||||
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | 0.14 | 0.15 | 0.16 | 0.22 | 0.76 | 4.43 | 0.14 | 0.15 | 0.17 | 0.21 | 0.25 | 0.29 | 0.16 | 0.16 | 0.16 | 0.19 | 0.24 | 0.29 |
| Uniform | 0.01 | 0.05 | 0.27 | 1.41 | 8.09 | 57.82 | 0.01 | 0.04 | 0.10 | 0.15 | 0.20 | 0.25 | 0.01 | 0.04 | 0.09 | 0.14 | 0.20 | 0.26 |
| Binomial | 0.10 | 0.10 | 0.13 | 0.25 | 1.18 | 7.23 | 0.10 | 0.11 | 0.14 | 0.18 | 0.23 | 0.27 | 0.10 | 0.10 | 0.13 | 0.17 | 0.22 | 0.28 |
| RMSE | ||||||||||||||||||
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | 0.14 | 0.15 | 0.17 | 0.27 | 0.90 | 4.71 | 0.14 | 0.15 | 0.17 | 0.21 | 0.25 | 0.29 | 0.17 | 0.16 | 0.19 | 0.26 | 0.33 | 0.39 |
| Uniform | 0.01 | 0.24 | 1.15 | 3.86 | 13.60 | 68.33 | 0.01 | 0.10 | 0.15 | 0.18 | 0.22 | 0.26 | 0.01 | 0.14 | 0.24 | 0.31 | 0.35 | 0.39 |
| Binomial | 0.10 | 0.11 | 0.16 | 0.41 | 1.54 | 7.94 | 0.10 | 0.11 | 0.14 | 0.19 | 0.23 | 0.28 | 0.10 | 0.11 | 0.19 | 0.27 | 0.33 | 0.38 |
| Bias | ||||||||||||||||||
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| Uniform | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |
| Binomial | 0.00 | 0.00 | 0.01 | 0.01 | 0.02 | 0.02 | 0.00 | 0.01 | 0.02 | 0.03 | 0.03 | 0.04 | -0.01 | 0.01 | 0.04 | 0.05 | 0.06 | 0.05 |
| Standard deviation | ||||||||||||||||||
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | 0.11 | 0.12 | 0.12 | 0.13 | 0.15 | 0.16 | 0.11 | 0.11 | 0.12 | 0.12 | 0.13 | 0.13 | 0.10 | 0.10 | 0.09 | 0.09 | 0.09 | 0.09 |
| Uniform | 0.00 | 0.04 | 0.15 | 0.29 | 0.45 | 0.61 | 0.00 | 0.05 | 0.10 | 0.12 | 0.12 | 0.13 | 0.00 | 0.04 | 0.10 | 0.10 | 0.09 | 0.09 |
| Binomial | 0.10 | 0.11 | 0.13 | 0.15 | 0.18 | 0.20 | 0.10 | 0.11 | 0.12 | 0.13 | 0.13 | 0.14 | 0.10 | 0.10 | 0.10 | 0.10 | 0.10 | 0.10 |
| RMSE | ||||||||||||||||||
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | 0.11 | 0.12 | 0.12 | 0.13 | 0.15 | 0.16 | 0.11 | 0.11 | 0.12 | 0.12 | 0.13 | 0.13 | 0.10 | 0.10 | 0.09 | 0.09 | 0.09 | 0.09 |
| Uniform | 0.00 | 0.04 | 0.15 | 0.29 | 0.45 | 0.61 | 0.00 | 0.05 | 0.10 | 0.12 | 0.12 | 0.13 | 0.00 | 0.04 | 0.10 | 0.10 | 0.09 | 0.09 |
| Binomial | 0.10 | 0.11 | 0.13 | 0.15 | 0.18 | 0.21 | 0.10 | 0.11 | 0.12 | 0.13 | 0.14 | 0.14 | 0.10 | 0.10 | 0.11 | 0.11 | 0.11 | 0.11 |
| Bias | ||||||||||||||||||
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | -0.01 | 0.00 | 0.02 | 0.09 | 0.29 | 1.00 | -0.01 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | -0.01 | -0.01 | -0.04 | -0.07 | -0.07 | -0.07 |
| Uniform | 0.00 | -0.13 | -0.63 | -2.02 | -6.17 | -20.84 | 0.00 | -0.05 | -0.04 | -0.02 | -0.01 | 0.00 | 0.00 | -0.12 | -0.16 | -0.15 | -0.12 | -0.09 |
| Binomial | 0.00 | 0.01 | 0.04 | 0.09 | 0.24 | 0.76 | 0.00 | 0.01 | 0.02 | 0.03 | 0.03 | 0.04 | 0.00 | -0.01 | -0.05 | -0.06 | -0.06 | -0.04 |
| Standard deviation | ||||||||||||||||||
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | 0.11 | 0.11 | 0.12 | 0.15 | 0.49 | 2.77 | 0.11 | 0.11 | 0.11 | 0.12 | 0.12 | 0.13 | 0.12 | 0.11 | 0.11 | 0.12 | 0.13 | 0.13 |
| Uniform | 0.04 | 0.06 | 0.27 | 1.13 | 5.89 | 44.86 | 0.04 | 0.06 | 0.09 | 0.11 | 0.12 | 0.12 | 0.04 | 0.06 | 0.09 | 0.11 | 0.12 | 0.13 |
| Binomial | 0.10 | 0.10 | 0.11 | 0.15 | 0.72 | 6.05 | 0.10 | 0.10 | 0.11 | 0.12 | 0.12 | 0.13 | 0.10 | 0.10 | 0.11 | 0.12 | 0.13 | 0.14 |
| RMSE | ||||||||||||||||||
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Normal | 0.11 | 0.11 | 0.12 | 0.18 | 0.58 | 2.95 | 0.11 | 0.11 | 0.11 | 0.12 | 0.12 | 0.13 | 0.12 | 0.11 | 0.12 | 0.14 | 0.15 | 0.15 |
| Uniform | 0.04 | 0.14 | 0.69 | 2.31 | 8.53 | 49.47 | 0.04 | 0.08 | 0.10 | 0.11 | 0.12 | 0.12 | 0.04 | 0.13 | 0.19 | 0.19 | 0.17 | 0.16 |
| Binomial | 0.10 | 0.10 | 0.11 | 0.17 | 0.76 | 6.09 | 0.10 | 0.10 | 0.11 | 0.12 | 0.13 | 0.13 | 0.10 | 0.10 | 0.12 | 0.14 | 0.14 | 0.14 |
| Bias | ||||||||||||||||||
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Exponential | -0.01 | 0.03 | 0.16 | 0.38 | 0.67 | 0.99 | -0.01 | 0.03 | 0.13 | 0.27 | 0.42 | 0.58 | -0.04 | -0.02 | 0.10 | 0.29 | 0.50 | 0.71 |
| Standard deviation | ||||||||||||||||||
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Exponential | 0.17 | 0.16 | 0.16 | 0.16 | 0.17 | 0.18 | 0.17 | 0.16 | 0.16 | 0.17 | 0.20 | 0.23 | 0.16 | 0.16 | 0.17 | 0.20 | 0.23 | 0.27 |
| RMSE | ||||||||||||||||||
| Linearized OTGMM | OTGMM | Efficient GMM | ||||||||||||||||
| error scale | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 | 0 | 0.5 | 1 | 1.5 | 2 | 2.5 |
| Exponential | 0.17 | 0.17 | 0.23 | 0.42 | 0.69 | 1.00 | 0.17 | 0.17 | 0.21 | 0.32 | 0.47 | 0.62 | 0.16 | 0.16 | 0.20 | 0.35 | 0.55 | 0.76 |
In Tables S.6-S.9, we report the estimation error and decompose it into its bias, standard deviation and the root mean square error (RMSE). These quantities are evaluated using averages over 5000 replications. We consider various estimators : the linear approximation to OTGMM in the small-error limit (leftmost columns, indicated by “Linearized OTGMM”), OTGMM in the general large-error case (middle columns) and efficient GMM ignoring the presence of errors (rightmost columns).
While these tables consider a large number of specifications and data generating processes, we here highlight a few values that illustrate the typical trends present throughout the simulations. First, it is clear that OTGMM is, in general, preferable to linearized OTGMM in terms of bias, and hence we focus our discussion on the former. As an example, let us compare the OTGMM and GMM results in Table S.6, for the column that corresponds to an error scale of for the normal model. The OTGMM estimator reduces the bias magnitude to only 0.05, as compared to 0.18 for GMM. At the same time, the variance only increases from 0.19 (for GMM) to 0.20 (for OTGMM), which is a negligible increase, thus supporting the idea that GMM’s use of the optimal weighting matrix is not particularly beneficial in this context. As a result, the overall root mean square error (RMSE) is 0.21 for OTGMM down from 0.26 for GMM.
The key take-away from these simulations is that the OTGMM estimator exhibits the ability to substantially reduce bias while not substantially increasing the variance relative to efficient GMM. As a result, the overall RMSE criterion points in favor of OTGMM. This is exactly the type of behavior one would expect for an effective measurement error-correcting method. The reduction in bias is especially important for inference and testing, as it significantly reduces size distortion. In contrast, a small increase in variance does not affect inference validity, as this variance can be straightforwardly accounted for in the asymptotics, unlike the bias, which is generally unknown.
A sensitivity analysis for the average derivative effect
Abstract
In observational studies, exposures are often continuous rather than binary or discrete. At the same time, sensitivity analysis is an important tool that can help determine the robustness of a causal conclusion to a certain level of unmeasured confounding, which can never be ruled out in an observational study. Sensitivity analysis approaches for continuous exposures have now been proposed for several causal estimands. In this article, we focus on the average derivative effect (ADE). We obtain closed-form bounds for the ADE under a sensitivity model that constrains the odds ratio (at any two dose levels) between the latent and observed generalized propensity score. We propose flexible, efficient estimators for the bounds, as well as point-wise and simultaneous (over the sensitivity parameter) confidence intervals. We examine the finite sample performance of the methods through simulations and illustrate the methods on a study assessing the effect of parental income on educational attainment an a study assessing the price elasticity of petrol.
1 Introduction
Drawing causal inferences from observational studies is challenging for a variety of reasons. Two prominent challenges are a) the treatment (or exposure) of interest is continuous rather than binary or discrete, and b) the treatment/exposure assignment is not random, and so treated and control groups may substantially differ on the basis of unmeasured confounders. We are motivated by the intersection of these two challenges.
In observational studies, continuous exposures are widespread. In epidemiology, researchers are often interested in quantifying the effect of lead exposure or air pollution on health outcomes. In the social sciences, researchers may be interested in quantifying the effect of household’s income on a child’s later life educational or economic outcomes (Lundberg2023). The vast majority of causal inference methods focus on treatments that are binary or take on a finite number of values. A popular approach when the exposure of interest is continuous is to simply dichotomize the exposure at some threshold. However, this is undesirable because dichotomizing discards potentially useful statistical information from the exposure level and can invalidate the downstream inference (VanderWeele2013), or at least muddies its interpretation (Lee2024). When the exposure of interest is continuous, some popular causal estimands include the dose-response curve (also known as the exposure response function) (kennedy_dose_response), a (projection) parameter of the dose-response curve (Bonvini2022SensitivityModels), average derivative effects (Newey1993EfficiencyModels; Klyne2023AverageLearning), and stochastic intervention effects (Schindl2024), among others.
In this paper, we focus on the average derivative effect (ADE), which has been extensively studied in economics (Hardle1989; Newey1993EfficiencyModels), with recent work in statistics clarifying its causal interpretation (Rothenhausler2019; Hines2021ParameterisingEffects). It has many other names, and has been referred to as an incremental effect (Rothenhausler2019), average partial effect (Klyne2023AverageLearning), or an average causal derivative (Chernozhukov2022LongLearning). Roughly speaking, the ADE measures the average effect of infinitesimally increasing the exposure level for all units in the population. The ADE corresponds to well-known quantities in economic applications. For example, when one is interested in the effect of the price of a good on demand, the ADE corresponds to the average price elasticity of demand. In cases where one is interested in the effect of increased disposable income on consumption, the ADE corresponds to the average marginal propensity to consume (bruns2025two). As elucidated in Hines2021ParameterisingEffects, the ADE estimand has some attractive properties, relative to the widely studied dose-response curve. For example, the ADE is a single number summary of the causal effect; it is difficult to summarize the dose-response curve by a single number. Relatedly, estimation of the ADE, a scalar, is a simpler statistical task than estimating the dose-response curve, which is infinite-dimensional. The ADE also relies on a weaker version of the so-called overlap/positivity assumption than that required for identification of the dose-response curve. Finally, as Hines2021ParameterisingEffects explain, it may be difficult or unrealistic to envision an intervention that sets exposure to the same level for everyone, which is what the dose-response curve measures. Of course, there are settings where the ADE is less appropriate. Notably, if the causal effect is not monotonic, an average of positive and negative derivatives could cancel and result in a near zero ADE, rendering the ADE an inadequate summary of the causal effect. Data where the treatment is continuous almost exclusively comes from observational studies, as continuous treatments are rare in randomized experiments. As a result, the ADE is typically only a relevant estimand in observational studies, where unmeasured confounding can never be ruled out. It is therefore of interest to develop methods for ADE estimation that take into account potential unmeasured confounding.
A popular way to alleviate concerns about unmeasured confounding in an observational study is to perform a sensitivity analysis. Dating back to Cornfield2009, a sensitivity analysis acknowledges the existence of unmeasured confounding, but asks how strong it must be to overturn a qualitative causal conclusion. There are now a wide array of sensitivity analysis methods developed, especially for binary exposures. Some sensitivity models constrain the effect of the unmeasured confounders affect on the treatment assignment (rosenbaum_obs; Tan2006AScores). Others constrain how far the potential outcome distribution can be from the observed outcome distribution (Robins2000; Diaz2013; Nabi2024). Some focus on worst-case departures from the no unmeasured confounding assumption (Yadlowsky2022; Dorn2023; Zhang2022a), and others on average-case departures (Huang2024; Zhang2022a). Some are parametric in nature (Frank2020; Imbens2003; Oster2019; Cinelli2020; Zhang2022) while others are nonparametric. Researchers have also proposed methods that constrain the proportion of unmeasured confounding (Bonvini2022).
For the ADE, we introduce a new sensitivity analysis model that bounds worst-case departures of the treatment assignment from no unmeasured confounding. The model can be thought of as a generalization of the marginal sensitivity model for binary treatments due to Tan2006AScores, and is closely related to the model of (rosenbaum1989sensitivity), which itself is a generalization of Rosenbaum’s sensitivity model for binary treatments (rosenbaum_obs). Under the sensitivity model, for a fixed value of the sensitivity parameter, we derive closed-form upper and lower bounds for the ADE. We consider both binary and continuous outcomes, which lead to different forms for the bounds. We introduce nonparametric, robust estimators for the closed-form bounds, as well as corresponding confidence intervals. These estimators can leverage data-adaptive nuisance estimators that may converge slower than the parametric rate (though not too slowly). One attractive property of our approach is that the closed-form bounds lend themselves to conducting simultaneous inference over an interval of sensitivity parameters in a particularly simple way.
1.1 Related work in sensitivity analysis beyond binary treatments
There is a growing body of literature on sensitivity analyses for non-binary treatments. For the multi-valued treatment case, Basit2023SensitivityTreatments generalize the model and approach of Zhao2019SensitivityBootstrap to derive bounds on linear combinations of potential outcome means. For the continuous treatment case, most sensitivity analysis methods have been aimed at bounds on (aspects of) the dose-response curve/exposure response function. For example, Bonvini2022SensitivityModels propose sensitivity analyses for parameters of marginal structural models for any type of treatment and an array of estimands, including the dose-response curve. Jesson2022, Marmarelis2023 Frauen2023, and Baitairian2024 all propose sensitivity analyses for the dose-response curve, under distinct sensitivity models and derive distinct bounds. One the models considered in dalal2025partial is closely related to ours, but the estimand of interest is the dose-response curve. The recent work of Levis2024 proposes a suite of sensitivity analysis methods for stochastic intervention estimands. Meanwhile, Zhang2024a and Zhang2024b propose sensitivity analysis methods for matched studies with continuous treatments under a related sensitivity model.
Chernozhukov2022LongLearning study omitted variable bias (analogous to sensitivity analysis) within a general class of estimands that are continuous, linear functionals of a conditional expectation. Their results apply to a general class of estimands, with the ADE being one example. They assume bounds on the distance between the “short” and “long” Riesz representers and conditional expectation functions. We take a different approach by appealing to a sensitivity model that imposes a bound on the distance between the odds ratio of the observed and unobserved generalized propensity score at any pair of points in the support of the exposure. This gives rise to different interpretations of the sensitivity parameters and also different bounds on the ADE. We give a more detailed comparison to previous sensitivity models in Remarks 1 and 2 in Section 3.
1.2 Outline of the paper
The paper is organized as follows. We formally introduce notation, assumptions, and the causal estimands in Section 2. In Section 3, we introduce the sensitivity model and discuss its relation to previous models. In Section 4, we compute the worst-case bounds on the ADE under the sensitivity model. We then propose efficient and robust estimators for the bounds in Section 5. We then apply the methods in a simulation study (Section 6) and two real data applications (Section 7).
2 Preliminaries
2.1 Notation
The underlying data are assumed to be a vector of independent and identically distributed samples from some unknown distribution , where is a continuous exposure (also referred to as a treatment or dose), are observed pre-exposure confounders, some unobserved pre-exposure confounders, are potential outcomes, and is the support of the exposure. The potential outcomes represent the outcome quantity that would have been observed if the exposure had been externally set to . In the observed data, we only see one of the potential outcomes. Specifically, the observed outcome satisfies when (Assumption 1). Thus, the observed data consists of i.i.d. samples of the data vector . Throughout the paper, we will consider the two scenarios where is continuous and is binary separately. We will use ′ and to denote taking a partial derivative with respect to exposure , which will be clear from the context. represents the norm, denotes statistical independence, and denotes the indicator function.
2.2 Estimand and assumptions
We now formally introduce the causal estimand, which is well-defined for both binary and continuous (potential) outcomes. Let , which is assumed to exist. Then the ADE is defined as
| (1) |
where the expectation is taken over the joint distribution of , , and . It is useful to unpack the meaning of the estimand . One interpretation is as follows (Hines2021ParameterisingEffects). Consider a very small perturbation . Then compare the difference in average outcomes (scaled by ) between two worlds. In the first world, the exposure is distributed according to the observed distribution, except that the exposure is increased by for all units. In the second, the exposure is simply distributed according to the observed distribution. In fact, one can show that equals , the limiting difference between average outcomes under a small shift intervention and no intervention, scaled by the shift (hines2025learning). When is continuous, also matches the estimand introduced in Rothenhausler2019 under additional smoothness conditions that ensure is well-defined. These connections and the required regularity conditions are made explicit in Appendix A. We now introduce causal assumptions that allow the ADE to be written as a statistical functional of the full data, some components of which are not observed.
Assumption 1 (Consistency/SUTVA).
when .
Consistency requires that the potential outcome corresponding to an exposure matches the observed outcome when the observed exposure matches . It also prohibits interference, where the exposure of one individual can affect the outcome of another.
Assumption 2 (Latent Ignorability).
.
Latent ignorability essentially requires that the vector contains all common causes of the exposure and outcome . This assumption is notably weaker than the typical ignorability or no unmeasured confounding assumption, which requires . Moreover, it is a relatively mild assumption since is unobserved, and there are no restrictions placed on .
Assumption 3 (Local Overlap).
The conditional densities and are continuous in for all .
In contrast to a typical overlap assumption that would require for all (this would be needed for the dose-response curve), the local overlap assumption only requires that at any level of confounders for which , the conditional density must be positive in a small neighborhood around as well. We only require this weaker notion of overlap as we focus on the ADE. We will also refer to the conditional density as the generalized propensity score throughout the remainder of the paper (Imbens2000). The next assumption collects additional regularity conditions on the conditional density functions and potential outcomes.
Assumption 4 (Regularity).
-
(i)
The conditional densities and are differentiable in . Also, and for all fixed as .
-
(ii)
The derivative is continuous.
These regularity assumptions ensure that certain statistical objects that we introduce are well-defined. In essence, they place smoothness restrictions on the potential outcomes and conditional densities.
Here, the score function is the Riesz representer for the statistical functional (Powell1989). Two statistical examples where the interpretation of is fairly simple are the partially linear model and the single index model.
Example 1 (Partially linear model).
Consider the case where , for an arbitrary function and a scalar. Then .
Example 2 (Single index model (Stoker1986)).
Suppose , for a monotone, differentiable function. Then . So the estimand is proportional to the regression coefficient .
For the remainder of the paper, we focus on the statistical functional in the nonparametric model. In Appendix F, we briefly outline how our results could be extended to weighted average derivative effects, i.e. , for weights that are nonnegative and such that . Hines2021ParameterisingEffects and Hines2023OptimallyEffects discuss the interpretation of such weighted average derivative effects.
3 Sensitivity model
Based on the previous section, we can write down the causal estimand in terms of the statistical functional . However, is a function of , where is unobserved. Thus, cannot be estimated from observed data without further assumptions. In this section, we introduce a sensitivity analysis model that, through a parameter , limits the strength of association between the unmeasured confounder and the exposure conditional on . The sensitivity model then facilitates deriving upper and lower bounds on the estimand of interest based on the allowable amount of unmeasured confounding . Recall can be expressed by or as a function of its Riesz representer, . The sensitivity model we consider is a generalization of the model of Tan2006AScores to the continuous exposure case and is related to Rosenbaum’s semiparametric model for continuous doses (rosenbaum1989sensitivity). The model restricts the odds ratio of the generalized propensity score (including ) vs. the generalized propensity score (marginalizing over ) at any two dose levels:
Assumption 5 (Marginal sensitivity model).
| (2) |
This model generalizes Tan’s marginal sensitivity model for binary treatments, as it exactly reduces to the model of Tan2006AScores when and are replaced by 0 and 1, and the from Tan2006AScores is set to . The Tan model has been studied extensively (Zhao2019SensitivityBootstrap; Dorn2023), and has recently been generalized to longitudinal settings (bruns2023robust; Tan2025). dalal2025partial consider a more general formulation to bound dose-response curves. The connection between the Tan and Rosenbaum models has been previously discussed when is binary and continuous (Zhao2019SensitivityBootstrap; dalal2025partial). We examine the connection between model (2) and Rosenbaum’s model for continuous doses specific to our context in Appendix G. In Appendix H, we discuss implications of instead imposing for a smooth, nonnegative, strictly increasing function such that . We now comment on the relationship between the model we consider in relation to other sensitivity models for continuous exposures.
Remark 1.
Chernozhukov2022LongLearning establishes bounds on the bias of estimators that ignore unmeasured confounding for a large class of functionals that can be expressed as a continuous linear function of the conditional expectation . Their class includes the ADE from (1), or what they refer to as the average causal derivative. Their omitted variable bias model imposes type bounds on both the “long” (including ) Riesz representer and the conditional expectation relative to their “short” (excluding ) counterparts. The model we consider differs in several aspects. First, model (2) imposes bounds on worst-case rather than average case departures from no unmeasured confounding. Therefore, the models and their interpretations are fundamentally different. Second, in contrast to Chernozhukov2022LongLearning, we do not directly impose restrictions on the “long” Riesz representer in relation to its “short” counterpart . Rather, we impose restrictions on the generalized propensity score directly, which may be a more familiar statistical object than , and may aid in the interpretation of the sensitivity parameter. Nevertheless, as will be shown in Lemma 2, our model (2) implies constraints on . Moreover, the connection between (2) and derived in Lemma 2 could be useful in interpreting the bound imposed on in Chernozhukov2022LongLearning. We also point out that Proposition 2 from the earlier work of Rothenhausler2019 derives a similar bound on the omitted variable bias as Chernozhukov2022LongLearning, specifically for the ADE.
Remark 2.
In relation to other sensitivity models for other causal estimands for continuous exposures, ours model differs in that we consider bounds on the odds deviation of generalized propensity scores, and the deviation depends on the difference between dose levels and . In contrast, Bonvini2022SensitivityModels, Jesson2022, and Baitairian2024 consider models that place the restriction . This model does not involve odds at different dose levels, and so the bounds do not depend on the dose difference. Marmarelis2023 and dalal2025partial consider closely related models to ours, but do not study the ADE. One of the models considered by Levis2024 is similar to ours in that it considers restrictions on odds, but does not involve the dose difference, and they consider a different estimand.
We next establish an implication of the sensitivity model that will facilitate reducing the problem to an optimization problem with a mathematically tractable form.
Lemma 2.
Under sensitivity model (2),
| (3) |
This result demonstrates that the sensitivity models constrain the Riesz representer to be within of , on the additive scale and in a symmetric fashion. Next, we introduce a statistical restriction (entirely separate from the sensitivity model) on . The restriction is a well-known property of score functions.
Lemma 3.
Suppose Assumption 4 holds, and that is continuous and bounded for all . Then
| (4) |
Thus, combining the previous two lemmas with the fact that , we can formulate the sensitivity analysis as an optimization problem with (2) and (3) as constraints. As a result, a valid sensitivity analysis under model (2) for the ADE would solve the following optimization problem:
| (5) | ||||||
| subject to | ||||||
| and |
It is straightforward to see that the optimization can be conducted in each stratum separately. Thus, we focus on solving the following formulation:
| (6) | ||||||
| subject to | ||||||
| and |
These optimization problems formulation follow a Lagrangian formulation, and they resemble other optimization problems in the causal inference literature, for example those in Jin2022SensitivityPerspective, Zhang2022a, Dorn2023, among others. Equipped with this formulation of the optimization problem, we will aim to obtain closed-form solutions for the cases where is continuous or binary.
4 Solving the optimization problems
4.1 Continuous Outcome
We first consider the case where the outcome is continuously distributed, i.e. for all , the distribution has no point masses.
Proposition 1.
Suppose the outcome is continuously distributed with no point masses. The solution to the maximization version of (6) is
and the opposite (swap the and signs in the piecewise function) for the minimization, where is the conditional median (1/2 quantile) of given . Moreover, the maximum and minimum objective values of the optimization programs are
| (7) | |||
respectively.
One might observe that the solution has a Neyman-Pearson flavor. This flavor of solution in sensitivity analysis has been observed before, for example for the average treatment effect in the binary treatment case (Dorn2023; Zhang2022a). In addition, the optimal values of the optimization problem equal (what one would estimate if the unmeasured confounding due to is ignored), plus or minus times a nonnegative correction term. Thus, the bounds are symmetric around . It is clear that the correction term is indeed nonnegative, since it is exactly the average difference between outcomes above and below the conditional median.
4.2 Binary Outcome
The above formulation and closed form solution required the outcome to be continuous. This can be seen from the fact that the optimal choices for depend on being above or below some median cutoff point. In the binary case, for strata of where , one cannot simply take to match the form in Proposition 1, replacing with 1/2, as this will lead to a violation of the constraint on the score in Equation (6) unless exactly. Of course, the bound obtained by replacing in the solution from Proposition 1 with 1/2 would still be valid, but potentially conservative. Instead, we can show the following result for the binary outcome case:
Proposition 2.
Suppose the outcome is binary. A solution to the maximization version of (6) is
if and
if . The optimal solution for the minimum is the opposite (swap the and solutions). Moreover, the maximum and minimum values of the optimization program are
| (8) | |||
respectively.
Again, the solution has a Neyman-Pearson flavor and the optimal value takes the form of (what one would estimate if they ignored ), plus or minus times a nonnegative correction term, which increases as approaches 1/2. In the binary case, however, the correction term is not as simple to estimate. We observe that the optimal value for the binary outcome involves an absolute value (or maximum), because of the term . Such a term is not smooth when the probability that is not zero. An approach that is popular when trying to estimate such non-smooth quantities is to instead target a smooth approximation that bounds the true quantity of interest, which we describe in detail in Section 5.
Remark 3.
A natural question to ask is whether the bounds derived in the previous two sections are “sharp” in some sense. For example, in the binary treatment case, for the marginal model from Tan2006AScores, Dorn2023 exhibited sharpness of their bounds (and looseness of the original bounds of Zhao2019SensitivityBootstrap) by constructing potential outcome and distributions that can achieve their bounds simultaneously for the treated and control potential outcomes while producing an identical observed data distribution. It is difficult to find an analogous construction in our more complicated setting with a continuous exposure and a sensitivity model that simultaneously places bounds on quantities at any pair of dose levels . Moreover, the optimization problem in (6) is solved separately for each stratum , so we do not impose the mean zero (averaging over ) restriction on score functions. Nevertheless, we do impose the constraint implied by Lemma 3, which is in some sense analogous to the additional constraint imposed by Dorn2023 to sharpen the original conservative bounds of Zhao2019SensitivityBootstrap. In addition, the simulations and real data applications suggest that our bounds can be informative.
5 Estimation and inference
For estimation and inference for the closed-form bounds, we appeal to semiparametric efficiency theory (Tsiatis2006). As alluded to previously, the bounds for a binary outcome can be non-smooth, so we instead target a smooth approximation. The central object in semiparametric efficiency theory is the efficient influence function, whose variance equals the semiparametric efficiency bound, and is unique in a completely nonparametric model. The rest of this section is devoted to characterizing the efficient influence functions of the (smoothed) bounds for continuous and binary outcomes in the nonparametric model, which will motivate construction of estimators. For convenience, we will at times refer to the efficient influence function even when the precise terminology would be the uncentered efficient influence function. For a recent review of semiparametric theory, we refer the reader to Kennedy2022.
5.1 Continuous outcome
In this subsection, we will derive the efficient influence function for the bounds on the ADE under the sensitivity model with continuous outcomes. Recall that these were and . The efficient influence function for the functional was derived in Newey1993EfficiencyModels. Thus, by a linearity property of efficient influence functions (Kennedy2022), it only remains to find the efficient influence function of .
Proposition 3.
The efficient influence function of is
The efficient influence function of derived in (Newey1993EfficiencyModels) takes the following form:
where , and . We then get the immediate corollary:
Corollary 1.
The efficient influence functions of and under the nonparametric model are
| (9) | |||
respectively.
Equipped with the efficient influence functions, it is straightforward to propose estimators with desirable properties. The estimator will require estimating the unknown nuisance functions . To ease the notational burden for the theoretical analysis, we simply analyze a sample split estimator where the nuisance functions are estimated on one split of the data, and on the second split, those estimates are plugged in to the efficient influence function at each data point, i.e.
| (10) | ||||
To make use of all of the data, one can employ the now commonly utilized cross-fitting technique (Chernozhukov2018b), where the data is randomly split into roughly equally sized folds . For each , we compute nuisance estimates for on all folds except , and plug in these estimates on fold (as in (10)). The result of Theorem 3, which outlines conditions under which the sample split estimator achieves asymptotic normality, will also apply to an analogous cross-fitted estimator.
Theorem 3.
Suppose nuisance estimates are estimated from an independent sample of data and the conditional densities are uniformly bounded with no point masses. Also, assume that are bounded almost surely and that . Then
By virtue of using an estimator based on the efficient influence function, the bias of the estimator only involves second-order nuisance estimation errors. Thus, consistency is possible even if the nuisance functions can be estimated at the (slower than parametric) rate. This makes it possible to conduct valid inference even when using nonparametric or data adaptive estimates of the nuisance functions, provided they are not converging too slowly to the truth. Based on the asymptotic normality of the estimators, it is straightforward to construct Wald-style confidence intervals. Since we are bounding upper and lower bounds, it is reasonable to construct one-sided confidence intervals. Explicitly,
| (11) | |||
are asymptotically valid confidence upper and lower bounds for and , provided the variance estimates and converges to the true variances. Here, plug-in variance estimates can be used:
These plug-in estimates are consistent under the assumptions of Theorem 3, and so the confidence bounds from Equation (11) will be asymptotically valid.
5.2 Binary outcome
As derived in the previous section, the bounds in the binary outcome case involve the term . The presence of the minimum makes this quantity potentially non-smooth, so we instead rely on a smooth approximation. Specifically, when minima or maxima are involved, the LogSumExp (LSE) function is a popular choice (see Levis2023 for a recent example in causal inference). For a minimum of quantities, and any fixed ,
| (12) |
Specialized to our setting, where we take a minimum of and , we define
| (13) |
Thus, we will instead estimate (for a fixed )
| (14) | |||
From Equation (12), it is immediate that and for any , and the inequality gap shrinks as increases. At the same time, becomes less smooth as increases, and consequently, becomes harder to estimate. Thus, in choosing , there is a trade-off between approximation error and statistical estimation. A rigorously justified “optimal” choice for is outside the scope of this paper, but we refer the reader to Levis2023 for some additional discussion.
In the remainder of this section, we will derive the efficient influence function for the smoothed lower and upper bounds for the ADE under the sensitivity model with binary outcomes. Recall that these were and . As in the continuous outcome case, the efficient influence function for the functional was derived in Newey1993EfficiencyModels. Thus, it only remains to find the efficient influence function of .
Proposition 4.
The efficient influence function of for any univariate, continuously differentiable function is given by
As before, we get the immediate corollary:
Corollary 2.
The efficient influence functions for and are
respectively.
Again, we propose estimators based on the efficient influence function, and present sample-split versions that estimate the nuisances . They are as follows: (since is binary, and are equivalent):
| (15) | ||||
Theorem 4 establishes asymptotic normality of the estimators.
Theorem 4.
Suppose nuisance estimates are estimated from an independent sample of data. Also, assume that are bounded almost surely and that . Then
Similar to the continuous outcome case, we can construct asymptotically valid Wald-style confidence intervals for and (rather than the smoothed and ), if we account for the approximation error from the LSE function. The respective upper and lower bounds for the confidence intervals are
| (16) | |||
Again, plug-in variance estimates can be used:
These estimates will be consistent under the assumptions of Theorem 4, and so the confidence bounds from Equation (16) will be asymptotically valid.
5.3 Simultaneous confidence bands
The previous subsections introduced estimators and pointwise confidence intervals for a fixed value of . In this subsection, we briefly outline how to conduct simultaneous inference when we wish to conduct the sensitivity analysis over a bounded interval of values, i.e. . Conveniently, the nature of the resulting estimands take the form , and we can easily construct Wald confidence intervals for and under the same assumptions as in the previous subsections. Thus, by the union bound, a uniform % confidence band can be straightforwardly constructed for by constructing % confidence intervals for and and concatenating accordingly. In our setting, corresponds to and corresponds to either (continuous ) or (binary ). Wald-style % confidence intervals for and can be constructed using the respective efficient influence functions. Alternatively, a multiplier bootstrap approach could be implemented (see Kennedy2019a and Zhang2022a for recent applications in causal inference). However, we do not pursue that direction as the proposed approach is much simpler.
6 Simulations
We now evaluate the finite sample performance of the proposed methods through a simulation study. In the simulation, we focus on assessing the coverage of confidence intervals of the true ADE when there is unmeasured confounding based on sensitivity analyses at different choices of .
6.1 Simulation setup
We conduct simulations corresponding to two different dose distributions, two outcome types (binary and continuous), and three different strengths of unmeasured confounders, yielding different settings. For all 12 settings, we draw the confounders , , and draw . We draw the dose from a conditional density that is Gaussian or Gamma. For the Gaussian case, . For the Gamma case (shape and rate parametrization), . is set to for all simulations, and here are randomly drawn coefficients from , redrawn at each iteration. For the outcome model, we consider different settings for a binary outcome and for a continuous outcome. For the continuous outcome, we draw . For the binary outcome, we use a probit model and draw . is varied in . For both outcome models, the coefficients are randomly drawn from , redrawn at each iteration. The interaction coefficients are randomly drawn from , redrawn at each iteration. In the simulation, a higher leads to a higher chance of both a higher dose and outcome. The derivatives of the conditional expectations are available in closed form, and so the “ground truth” average derivative effects are approximated by drawing Monte-Carlo samples from the joint distribution of and computing the sample average of the derivative of .
6.2 Estimators
The nuisance estimates required to compute the estimator include the score function , the conditional mean and its derivative , and the conditional median for the continuous outcome case. For the binary outcome case, we set for computing the LSE function approximation. We use the R packages drape and xgboost for nuisance estimation. Specifically, we estimate scores and conditional mean derivatives using adaptations of the methods from the drape package (Klyne2023AverageLearning). The methods proposed by Klyne2023AverageLearning can re-smooth any first-stage regression estimator to produce a differentiable version to obtain an estimate , and model the conditional distribution through a location-scale model to estimate . Hyperparameters for these methods were chosen in the same manner as the simulations in Klyne2023AverageLearning. To fit conditional means and medians , we use gradient boosted trees as implemented in the xgboost package with default hyperparameters and the appropriate loss function – squared error for estimating the conditional mean of a continuous variable, logistic loss for estimating the conditional mean of a binary variable, and absolute error for estimating the conditional median of a continuous variable. To make use of the full data sample, we implement 5-fold cross-fitting.
6.3 Simulation results
Table 1 collects coverage results for pointwise 95% confidence intervals of the sensitivity analysis procedures at varying levels of . One can verify that the at which the sensitivity analysis model (2) holds (and thus the procedure will be valid) is between and . Therefore, it is not surprising to see in Table 1 that the 95% sensitivity analysis confidence intervals can severely undercover when is taken to be (no unmeasured confounding) or , as both of these are less than the lower bound . This also gives some reassurance that although we have not established sharpness of the analytic bounds, the bounds can still be informative. In addition, one may notice that as the strength of the unmeasured confounders impact on the outcome, measured through , increases, the sensitivity analysis intervals cover less. This is expected, as the sensitivity model we consider only restricts ’s impact on the treatment. Thus, the sensitivity analysis must protect against arbitrary dependence between and the (potential) outcomes, i.e. arbitrarily large values of . Consequently, it is reasonable to expect that if were to be increased further, the coverage rate of the sensitivity analysis bounds for would move closer towards but not necessarily reach the nominal level.
| Simulation parameters | value of the sensitivity analysis | ||||||
|---|---|---|---|---|---|---|---|
| dose | outcome | ||||||
| Gaussian | binary | 2.00 | 0.68 | 0.95 | 0.99 | 1.00 | 1.00 |
| Gaussian | binary | 3.00 | 0.51 | 0.89 | 0.99 | 1.00 | 1.00 |
| Gaussian | binary | 4.00 | 0.26 | 0.76 | 0.99 | 1.00 | 1.00 |
| Gamma | binary | 2.00 | 0.88 | 0.95 | 0.98 | 0.99 | 0.99 |
| Gamma | binary | 3.00 | 0.80 | 0.93 | 0.98 | 0.99 | 1.00 |
| Gamma | binary | 4.00 | 0.70 | 0.88 | 0.97 | 0.99 | 1.00 |
| Gaussian | continuous | 2.00 | 0.24 | 0.95 | 1.00 | 1.00 | 1.00 |
| Gaussian | continuous | 3.00 | 0.07 | 0.86 | 1.00 | 1.00 | 1.00 |
| Gaussian | continuous | 4.00 | 0.02 | 0.73 | 1.00 | 1.00 | 1.00 |
| Gamma | continuous | 2.00 | 0.75 | 0.97 | 1.00 | 1.00 | 1.00 |
| Gamma | continuous | 3.00 | 0.72 | 0.98 | 1.00 | 1.00 | 1.00 |
| Gamma | continuous | 4.00 | 0.69 | 0.96 | 1.00 | 1.00 | 1.00 |
7 Applications
7.1 The effect of parental income on child’s education
We illustrate the methodology for binary outcomes using an empirical example studying the extent to which household income affects a child’s educational attainment (Lundberg2023). The data we use comes from the National Longitudinal Survey of Youth 1997 cohort (NLSY97). The NLSY97 is a dataset consisting of a probability sample of U.S. youths ages 12–17, starting in 1997, who were followed up through 2019. We largely follow Lundberg2023 in pre-processing the data. The treatment variable of interest is reported total gross household income in 1996, when the respondents were age 12-17. Lundberg2023 logged and adjusted these measures to 2022 dollars using the Consumer Price Index. Those without income measurements are dropped, as are households coded as the maximum and minimum income values, as these represent upper and lower cutoffs, not actual incomes. The outcome of interest is a report of enrollment in any college up to age 21, which is binary. Those that did not complete a survey at ages 19–21 are omitted. Following Lundberg2023, four measured confounding variables are adjusted for: race, gender, parents’ education, and wealth. The racial categories from 1997 were Hispanic, Non-Hispanic Black, and Non-Hispanic white or other. Parents’ education is categorical, with the 3 categories no parent completed college, one parent completed college, or two parents completed college. Wealth is the log of household net worth reported by the parent in 1997, also adjusted to 2022 dollars. There are 5219 individuals in the final, processed dataset. Unfortunately, there may be confounders beyond race, gender, parents’ education, and wealth that are not measured that affect both household income and propensity to pursue higher education. These might include things like innate ability or geographic location, both of which could be strongly related to household income and propensity to attend college. Thus, we implement our sensitivity analysis to assess the impact of hypothetical unmeasured confounders on the statistical conclusions. We use the same nuisance estimators as in the simulations for a binary outcome. The results of the sensitivity analysis are reported in Figure 1. Each black dot represents a point estimate of an upper or lower bound at some value of . The shaded regions depict 95% confidence intervals. Assuming no unmeasured confounding, the point estimate for the ADE is , and the 95% confidence interval . Recall that the unit of the outcome is a percentage, and the treatment is the log of household income. In words, this means that on average, an increase of income of on the log scale might be expected to increase the propensity of attending any college by percentage points, assuming no unmeasured confounding. As increases, lower point estimates and confidence bounds decrease. The point estimate ultimately crosses 0 at , the 95% pointwise confidence interval crosses 0 at , and the 95% uniform confidence interval crosses 0 at .
7.2 The effect of price on petrol consumption
We now illustrate the methodology for continuous outcomes using an empirical example studying the extent to which petrol prices affect the demand for petrol, which was also studied in Chernozhukov2022a. The data come from the Canadian National Private Vehicle Use Survey. We preprocessed the data in an identical fashion to Chernozhukov2022a, leaving households, each of which has an outcome – log of the petrol consumption, covariates – log age, log income, log distance, and other time, geographical, household indicators, and treatment – log of petrol price per liter. In this example, the ADE measures the average price elasticity of petrol demand. Instead of constructing pointwise confidence intervals using different estimators, we concatenated the point estimates and standard errors for as estimated in Chernozhukov2022a with estimates and standard errors for the correction term from Equation (7) to produce simultaneous confidence bands. This exercise demonstrates the ease in which the sensitivity analysis can be conducted after (and completely separate from) a primary analysis assuming no unmeasured confounding has been completed. For estimation of the correction term, the only nuisance function is the conditional median. As in the simulation, we used the xgboost package with absolute error loss to fit the conditional median , in conjunction with 5-fold cross-fitting. The results are displayed in Figure 2 when the generalized Dantzig selector (GDS) and Lasso estimators are used for estimating as described in Chernozhukov2022a. Each black dot represents a point estimate of an upper or lower bound at some value of . The shaded region depicts the 95% simultaneous confidence intervals. Assuming no unmeasured confounding, the point estimates for the ADE are and for GDS and the Lasso, respectively. As increases, upper point estimates and confidence bounds increase. The point estimates ultimately cross 0 at and and the 95% simultaneous confidence bounds cross 0 at and for GDS and Lasso, respectively.
8 Discussion
In this paper, we have proposed a new sensitivity model for the ADE estimand, along with valid closed-form bounds, and estimators and confidence intervals for said bounds. The form of the bounds differ for continuous and binary outcomes and are particularly convenient, allowing easy construction of uniform confidence intervals. The extent to which the bounds introduced in this paper are sharp, in the sense of Dorn2023, is unclear and is a promising direction for future research. To the best of our knowledge, sharp bounds for causal estimands under sensitivity models resembling (2) do not exist beyond the binary treatment case. Another promising direction for further inquiry might be a calibration procedure, in the vein of Hsu2013 and McClean2024a. Calibrating the sensitivity analysis to observed confounders, for example, could potentially help researchers gauge whether a certain magnitude of is plausible, though such a practice has limitations. Finally, it may be of interest to study ordinal rather than continuous exposures. Such exposures may arise, for example, when doses of drugs are prescribed at a finite number of ordered levels.
Acknowledgements
We thank Abhinandan Dalal, Zhihan Huang, Ziang Niu, Zhimei Ren, Dylan Small, Eric Tchetgen Tchetgen, and participants at ACIC 2025 for helpful discussions and comments.
Appendix A Proof of results in Section 2
Proof of Lemma 1.
A.1 Equivalent forms for
First, consider the estimand . The difference is exactly the difference between the outcome in the population where treatment is shifted by above the natural treatment value , and the outcome under no intervention. We now show that equals .
Proof.
Rothenhausler2019 introduce a causal estimand for continuous outcomes called the incremental effect. They introduce the following assumption on the potential outcomes.
Assumption 6 (Regularity - potential outcomes).
The potential outcomes are bounded and the derivative is continuous and bounded.
The estimand of interest is the incremental effect, , where the average is taken over potential outcomes, treatments, and confounders. Combining the arguments in Proposition 1 of Rothenhausler2019 and the derivation of Lemma 1, it is straightforward to check that as defined in (1) is equal to . Thus, when the outcome is continuous and one is willing to invoke Assumption 6, our methods are directly applicable to .
Appendix B Proof of results in Section 3
Proof of Lemma 2.
Consider model (2) first. First, taking logs in (2), we get
Next, recall that . By the above equation, plugging in for and for , adding the term everywhere in the inequality and dividing by , we get
which immediately implies the result after taking the limit everywhere. We can also show the implication under the Rosenbaum-style sensitivity model introduced later in (22). Taking logs, we get
Using an analogous argument, we can deduce that
The result follows from an application of the upcoming Lemma 3 combined with the following fact: For a random variable with the property that any two points in its support lie within of each other, it must be the case that any point of its support must lie within of the mean of the random variable. ∎
Proof of Lemma 3.
A similar argument appears in Proposition 2 of Rothenhausler2019, but we provide one for completeness. By definition, we have
The second equality is by Bayes. The fourth equality holds by invoking the dominated convergence theorem (which is possible by the boundedness and continuity assumption), which allows for interchange of limit and integration. The remaining equalities are algebraic or by definition. ∎
Appendix C Proof of results in Section 4
Proof of Proposition 1.
We first characterize the Lagrangian of the minimization program. It is as follows:
Taking the derivative of the Lagrangian with respect to , we get that
By complementary slackness, we know
We also know that at the optimum, . If , then it must be the case that , in which case and . Similarly, if , then it must be the case that , in which case and . In the primal problem, we also have the constraint
Let . It then follows that
Solving for , we get that . Thus, is simply the median of . This means that the (minimization) optimization in (2) is solved by
| (17) |
It is the opposite for the maximization, i.e.
| (18) |
This result can be derived by replacing in the Lagrangian with . Next, the optimal (maximum) value achieved by the solution is the following:
| (19) |
where and , where and . A simple application of iterated expectation yields . For the minimum, it is easy to see that , as desired. ∎
Proof of Proposition 2.
Since is binary, there must exist an optimal solution that sets to distinct values according to whether or . Since the optimization is conditional on , we may rewrite the optimization as
| (20) | ||||||
| subject to | ||||||
| and | ||||||
| and | ||||||
The objective function can be written as . To maximize, we simply take to be the smallest possible it can be while still satisfying all three constraints in (20). It is clear that if , we can take , and since . It is easy to check that the third constraint holds. Conversely, if , we can only take as making it any smaller will cause violation of one of the second or third constraints in (20). To allow to be that small, we must take to be the maximum. It is easy to check that the third constraint holds under this choice. It follows that the maximum value takes the form
We can simplify this to
The derivation for the minimum follows analogously. ∎
Appendix D Deriving the efficient influence functions
D.1 Continuous outcome
We derive the efficient influence function of the minimal value of the ADE under the sensitivity model at a fixed . In some parts, the steps resemble an argument from Zhang2022a. We consider a parametric submodel indexed by that passes through the truth at . We define score functions with respect to the parametric submodel so that , where and . First, we prove a useful lemma.
Lemma 5.
Proof.
We first show the first equality. By definition of as the conditional median of , and then taking derivatives using Leibniz rule,
Again using Leibniz rule and the above equation,
The second equality follows from a similar argument. By definition of as the conditional median of , and then taking derivatives using Leibniz rule,
Again using Leibniz rule and the above equation,
∎
Building off of the previous lemma, the following result derives the efficient influence functions of two quantities whose difference is exactly the functional introduced in Proposition 3. Proposition 3 is then an immediate corollary after taking the difference of the two efficient influence functions.
Lemma 6.
The efficient influence functions of and
in the nonparametric model are
respectively.
Proof.
We start with . The goal is to find a random variable that is a function of the data such that , where . We start by taking derivatives:
We deal with each of the terms separately.
Term I:
The second equality holds because by a property of conditional scores, is mean zero conditional on .
Term II:
where the second equality follows from the above lemma. Next, observe that and again because of the property that is mean zero given . Thus,
where the second to last equality holds because is mean zero given . To see why, observe that by definition, and by definition of the conditional median .
Putting Term I and Term II together, we get the uncentered EIF of as
The steps for are virtually identical to those for , and thus the argument is omitted. ∎
D.2 Binary outcome - smooth functional
For this proof, we introduce some additional notation. Let . In addition, compared to the previous section, the score function changes slightly. Namely, since is binary, we have that
| (21) |
where . We also introduce the following lemma:
Lemma 7.
Let be a differentiable, scalar-valued function. Then
Proof.
By the chain rule and evaluating at ,
Rearranging Equation (21) and substituting into the previous equation yields the result. ∎
We can now prove Proposition 4.
Proof.
Again, the goal is to find a function of the data such that . We start by taking derivatives:
We will deal with the terms separately.
For Term I:
Now, we compute the conditional mean on of
Since is binary with success probability , this quantity is
We continue where we left off,
The second equality is by the fact that is mean zero given . The third equality is due to the fact that for binary, . The fourth equality is by . The fifth is by algebraic simplification. The sixth is by being mean zero given .
For Term II:
The second equality is due to being mean zero given .
Summing the two terms, it follows that the uncentered efficient influence function must be
∎
Appendix E Asymptotic normality
E.1 Helper lemmas
Lemma 8.
Let and be random variables and suppose the conditional density is bounded above by for all in the respective supports with no point masses. Then for functions ,
Proof.
We can write
The first inequality is by the bounded density and the second is by Cauchy-Schwarz. ∎
Lemma 9.
Let and be random variables and suppose the conditional density is bounded above by for all in the respective supports with no point masses. Then for functions ,
Proof.
Observe that by definition,
Taking the difference yields
Taking the norm on both sides of the above inequality yields
as desired.
∎
E.2 Continuous outcome
Proof of Theorem 3.
We only demonstrate the steps for , as the steps for are nearly identical. We can decompose
By the central limit theorem, after the scaling by , the first term converges to a normal distribution with mean zero and variance matching that in the statement of the theorem. By Lemma 1 from Kennedy2022, the second term is since we have assumed that the nuisance estimates are consistent for the true nuisances , sample/cross-fitting is employed, and and are uniformly bounded. This brings us to the final term, which is the bias. We can break this into two parts:
We can rearrange to be
The last equality is by iterated expectation and integration by parts. Assuming and converge at least as fast as , by the Cauchy-Schwarz inequality, we get that . For , we can simplify it as
The term, by iterated expectation, can be rewritten as
This term will by the Cauchy-Schwarz inequality, the assumed rate , and the fact that the rate of convergence of and inherit the rate of convergence of , by Lemma 9.
For the term, note that for to be non-zero, must be greater than and less than , or less than and greater than . The probability of such an event (call it ) converges to zero at the same rate as the convergence of , by Lemma 8. Moreover, on the event , lies between and , so on the event. Then an application of the law of total expectation implies that the term can be expressed as
by the rate assumptions. In summary, the entire bias term is second order in the estimation error , and consequently is . Thus, the central limit theorem holds as stated. ∎
E.3 Binary outcome - smoothed functional
Proof of Theorem 4.
As in the proof of Theorem 3, we only demonstrate the steps for , as the steps for are nearly identical. Similar to the continuous outcome case, we can decompose our estimator as follows:
By the central limit theorem, after the scaling by , the first term converges to a normal distribution with mean zero and variance matching that in the statement of the theorem. By Lemma 1 from Kennedy2022, the second term is since we have assumed that the nuisance estimates are consistent for the true nuisances , sample/cross-fitting is employed, and and are uniformly bounded. This brings us to the final term, which is the bias. We can break this into two parts:
The last equality is because is mean zero by iterated expectation. The decomposition of was already done in the proof of Theorem 3, and is under our assumptions. For , we drop the constant term , as it does not affect the analysis and simplifies exposition. Next, observe that a simple Taylor expansion of the function around the point is as follows:
Applying the Taylor expansion taking to be the ,
The second to last equality is because is mean zero by iterated expectation. The last inequality is due to and being uniformly bounded. is by assumption, so is as well. ∎
Appendix F Weighted average derivative effects
We can also adapt the results to the case where the estimand of interest is a weighted average of the derivative effects. Specifically, consider a known weight function , where almost surely, and . Suppose the estimand of interest is . This quantity will be identified similarly to its unweighted counterpart, under some additional regularity conditions.
Assumption 7 (Regularity - weights).
The derivative of with respect to exists, and implies .
As Hines2023OptimallyEffects discuss, under Assumption 4, and 7, Powell1989 showed that
Note that the first component, , is completely identified and is easily estimable when is known. The second component, is simply . Since is known and is nonnegative, the optimization problems defined in (6) for both continuous and binary outcomes would have the same optimal solutions for maximizing and minimizing , since the optimization problems were solved separately within each strata. The closed-form bounds would be essentially the same, modulo an additional term inside the expectations. Explicitly, these would be
for a continuous outcome and
for a binary outcome. Estimation and inference would follow from the same strategy as presented for the unweighted case. The case where the weights are unknown is an interesting direction for future research.
Appendix G Examining the relationship between model 2 and Rosenbaum’s semiparametric model
An analogous model for the continuous treatment of Rosenbaum’s sensitivity model for binary treatments would look as follows:
Assumption 8 ( sensitivity model).
| (22) |
The model restricts the odds ratio of the generalized propensity scores at any two dose levels and any two values of the unmeasured confounder. This model directly generalizes Rosenbaum’s sensitivity model for binary treatments, i.e. when and are replaced by 0 and 1 (rosenbaum_obs). The key difference between models (2) and (22) is that the latter model compares quantities conditioning on any two different values of the unmeasured confounder, and , whereas the former compares a quantity conditioning on any with an analogous quantity but with marginalized out. We now demonstrate that the semiparametric model introduced in (rosenbaum1989sensitivity) implies model (22).
Example 5 (Rosenbaum model for continuous doses).
Suppose that
where is an arbitrary function, and is a normalizing constant that ensures integrates to 1.
We will now show that if , then sensitivity model (22) holds at . To see this, directly plugging into Equation (22), we get
The model from Example 5 covers a range of familiar settings. For example, the situation where , where is an arbitrary function, falls within the model. Another is when , for a fixed constant and an arbitrary function . There are many other exponential family models for which Example 5 applies. It is also compatible with binary and ordinal treatments.
The next result elucidates a connection between models (22) and (2); a similar result for the binary case was derived in Zhao2019SensitivityBootstrap. The following lemma is a slight generalization of Proposition 3 of Zhao2019SensitivityBootstrap.
Lemma 10.
Proof of Lemma 10.
Fix any , , and and suppose model (2) holds at . Then taking logs, we get
and
Adding these inequality chains, we get
After taking exponents, we get that model (22) holds at . For the second implication, suppose that model (22) holds at . Then
We can simply integrate the chain of inequalities with respect to over the support of , to get
which is equivalent to model (2).
∎
The second half of the result demonstrates that assuming (22) holds at is stronger than assuming (2) holds at . Meanwhile, the first half of the result implies that the interpretation of the ’s from the differing models does not exceed a factor of 2. We refer the reader to dalal2025partial for more detailed comparisons between the Tan (marginal) and Rosenbaum sensitivity models.
Appendix H Alternative transforms in the sensitivity model
Recall that in model (2), the odds ratio is bounded between and . One might instead consider a model that uses a function other than exponential.
Assumption 9 ( marginal sensitivity model).
| (23) |
for some smooth, nonnegative, strictly increasing function such that .
Under such a model, we can follow the logic of Lemma 2 to derive a restriction on the latent score . In contrast to Lemma 2, the restriction is not symmetric on the additive scale.
Lemma 11.
Under sensitivity model (23),
| (24) |
Proof.
We can apply logs to (23) to get
Next, recall that . By the above equation, plugging in for and for , adding the term everywhere in the inequality and dividing by , we get
which immediately implies the result after taking the limit everywhere. ∎
One could then solve the optimization problem (6), except replacing constraint with . Due to the asymmetry, for the continuous outcome case, one would expect the solution to depend on a quantile rather than the median.
Do Test Scores Help Teachers Give Better Track Advice to Students?
A Principal Stratification Analysis††thanks:
Ichino: European University Institute, U. Bologna and CEPR (andrea.ichino@eui.eu); Mealli: European University Institute and U. Firenze (fabrizia.mealli@eui.eu); Viviens: European University Institute, (javier.viviens@eui.eu). We are grateful to Gustav Axén, Kosuke Imai, Matthijs Osterveen, Lina Segers and Dinand Webbink for insightful discussions.
)
Abstract
We study whether access to standardized test scores improves the quality of teachers’ secondary school track recommendations, using Dutch data and a metric based on Principal Stratification in a quasi-randomized setting. Allowing teachers to revise their recommendations when test results exceed expectations increases the share of students successfully placed in more demanding tracks by at least 6%, but misplaces 7% of weaker students. However, only implausibly high weights on the short-term losses of students who must change track because of misplacement would justify prohibiting test-score-based upgrades. Access to test scores also induces fairer recommendations for immigrant and low-SES students.
JEL-Code: I2
Keywords: principal stratification; secondary school track recommendations.
1 Introduction
Every year, more than one million EU students, upon completing primary education, choose a secondary school track based on recommendations received from their teachers. In some institutional settings—such as the Dutch school system examined in this paper—students may begin secondary education in a more demanding track than the one recommended to them, only under exceptional circumstances. Despite the potentially dramatic consequences that bad track recommendations may have, there is no clear evidence that assigning to teachers alone the role of providing these recommendations is the best solution and generates an optimal track assignment.111There is a wide literature on teachers’ biases that may affect the quality of secondary school track recommendations. Recent examples are: Driessen et al. (2008), Burgess and Greaves (2013), Gerhenson et al. (2016), Osikominu et al. (2021), van Leest et al. (2021), Geven et al. (2021), Carlana and Fort (2022), Carlana et al. (2022a), Carlana et al. (2022b), Ferman and Fontes (2022), Batruch et al. (2023) Bach (2023), Alesina et al. (2024), van Huizen et al. (2024), and Falk et al. (2025). Our goal in this paper is to establish, using a well-defined metric and a quasi-randomized experiment, whether teachers can provide “better” recommendations if they do so after seeing the scores of standardized tests taken by their students.
To the best of our knowledge, there is no consensus on what the goal of secondary school track advice should be or what defines a “better” recommendation. The metric we adopt is the one based on Principal Stratification (Frangakis and Rubin, 2002)222For a discussion on Principal Stratification and how it is rooted in the Instrumental Variable literature, see Mealli and Mattei (2012). that Imai et al. (2023) use to study whether algorithms can help judges in deciding which arrested individuals should be released while waiting for their trial: in this case, a decision is better than another if it avoids releasing subjects at risk of repeating a crime. In the Dutch setting, a reform was introduced that gives teachers the right (but not the obligation) to revise their initial track assignment upwards if the student performs better on a standardized test than the initial recommendation would imply. This reform offers an ideal setting to apply the metric that we adopt333The setting offered by the reform was also used in de Ree et al. (2025). .
The nature of this metric in the context of track advising can be intuitively described with reference to an education system featuring two tracks: low (e.g., vocational) and high (e.g., academically oriented). We make two assumptions that we maintain throughout the analysis: (1) if a student is able to complete a more difficult track, it is better she is allowed to do so for herself (because she can obtain more desirable lifetime outcomes) and for society (inasmuch as there are collective gains from a more educated community); (2) changing between tracks is costly for students and for the education system. Note that given these two assumptions, even in the absence of negative congestion externalities in the educational process and in the labor market444For evidence and a discussion of these externalities see Ichino et al. (2025). it would be sub-optimal to send all students to the high track because it would be costly for some of them not to complete it and have to change to the low track.
Let’s divide the population of students into groups – principal strata (Frangakis and Rubin, 2002) – that differ by how the education outcome depends on track recommendations. The first group in which we are interested comprises students for whom the initial track recommendation determines unequivocally the completed track: if the student is recommended the low track in the first year, she will graduate in the same track at the end of secondary school, while if recommended the high track, this will be her final graduation outcome. These are students for whom the teacher’s recommendation is a self-fulfilling prophecy, thus becoming crucially important. In light of this consideration, we call these subjects “Helpable” (H) to signal that they are the ones who can be helped by a high first-year track recommendation instead of a low one.555They correspond to the group of cases that, in the context of Imai et al. (2023), are labeled as “Preventable” crimes. Similarly, de Ree et al. (2025) label them as “Trapped-in-Track” (TT). Adapting the methodology proposed by Imai et al. (2023) to this context, we can establish whether the information provided by test scores helps teachers give better recommendations by measuring the fraction of students in the H group that are recommended the high track by teachers and see if it increases when teachers can decide based on test score information. In the Principal Stratification framework, this is the Average Principal Causal Effect () of “changing the source” of a recommendation (i.e., giving test score information to the source) within the principal stratum of H students. If this is positive, test scores help teachers provide better track advice by recommending more challenging tracks to students who can effectively complete them.
We are also interested in two other groups of students: those who, independently of the track initially recommended to them, always finish secondary education in the low track, and those who, instead, always graduate from the high track independently of the initial recommendation. Adapting to our setting the notation of de Ree et al. (2025), we call these students Always Low (AL) and Always High (AH), respectively. The common characteristic of these two groups is that the final student’s outcome is not affected by the teacher’s recommendation; however, this outcome can be achieved in different ways, which are more or less costly, depending on the number of track changes they require. In these cases, the goal of a recommendation should be to minimize the number of track changes that these two types of students will experience if they start on a track that differs from the one they will ultimately complete. The best recommendation is then the one that sends all AL students to the low track and all AH students to the high track. Therefore, allowing teachers to upgrade based on test scores produces more desirable outcomes, the larger the is for AH students and the lower it is for their AL peers.666It is conceivable the existence of a fourth group of students who always finish secondary education in the opposite track with respect to the one that was initially recommended to them. This is discussed below.
We estimate an for H students which is not lower than 6%, suggesting that when teachers can upgrade on the basis of test scores, the quality of their advice improves by at least 6% as measured by the fraction of these students that are recommended a more difficult track and can complete it successfully, while in the counterfactual case, they would have remained in a lower level track. For AH students, for whom an upgrade is also desirable to reduce the cost of track changes, the is estimated to have approximately the same lower bound.
In the case of AL students, for whom an upgrade is not desirable, the corresponding estimates are also positive with an of at least 7%. This suggests that a significant fraction of students who are unable to complete the more challenging curriculum are upgraded, and we cannot rule out that this fraction is even greater than that estimated for the H and AH strata. A possible reason for this finding is that primary school teachers do not bear any consequence related to track changes in secondary schools and are, therefore, less sensitive to the goal of reducing them in the case of AL students.
Given the opposite desirability of the findings concerning the effect of test scores on the quality of teacher advice for H and AH students on one hand and AL students on the other, it is natural to seek a method to obtain a comprehensive evaluation. A possible approach is offered by the classification framework proposed by Ben-Michael et al. (2024). This framework enables us to construct a weighted sum of the losses generated by mistakes in recommendations for the various types of students. We can then test whether this sum decreases when teachers see test scores, depending on the relative weight of the short-term losses suffered by students who must change track because of misplacement versus the lifetime losses of students who are not directed to a more challenging track they can eventually complete. We find that the relative weight of short-term losses would have to be unreasonably high to conclude that teachers should not be allowed to upgrade based on test scores.
Finally, building on Imai and Jiang (2022), a corollary contribution of this analysis is that it makes it possible to evaluate whether being allowed to upgrade based on test scores helps teachers improve the fairness of their recommendations with respect to protected attributes such as SES, race, or gender. For example, suppose that gender is a protected attribute with respect to which we want to assess how fair a recommender is. Then, in the stratum of H students, the recommender is fair if the fraction of high recommendations is the same for female and male students. However, the analogous fraction in other strata could be lower or higher as long as it is equal for both genders. Therefore, in the overall population, females may receive a different fraction of high first-year advice from this recommender; however, this would not constitute a violation of fairness, as it would be due to differences in the gender composition of the strata, not to discrimination based on gender within a stratum. We find that the availability of test scores improves the fairness of teachers’ recommendations. Specifically, it raises the chances that immigrants and low-SES students, who are otherwise discriminated against, are recommended a more difficult track that they can complete relative to natives and high-SES peers, respectively.
The paper is organized as follows. In Section 2, we describe the Dutch institutional setting and the quasi-randomized experiment that makes our analysis possible. The statistical framework and its assumptions follow in Section 3. Section 4 presents our results, while Section 5 supports their robustness by showing that they also hold when the Exclusion Restriction assumption is substituted by alternative assumptions. In Section 6 we move to the analysis of the fairness of track recommendations, and Section 7 concludes.
2 The Dutch school system and the quasi-experiment
What needs to be known about the Dutch school system for the purpose of this paper is that it features five main secondary school tracks, of which three are vocational (VMBO-BL, VMBO-KL, VMBO-GT ranked in this order by level of difficulty) and two lead to university studies (HAVO, VWO, similarly ranked). In addition, tracks that are contiguous by level of difficulty can be combined in mixed tracks, so that there are in total nine tracks to which a student can be recommended in the first year of secondary school. These nine tracks are listed from the easiest to the hardest in the row headings of Table 1.
| Track |
|
|
|
|||||||||||
| 1 | 2 | 3 | 4 | 5 | ||||||||||
| V: BL | 9,585 | 6,929 | 519 | 0.45 | 0.46 | |||||||||
| V: BL/KL | 3,781 | 3,828 | 526 | 0.31 | 0.30 | |||||||||
| V: KL | 15,698 | 11,258 | 529 | 0.33 | 0.36 | |||||||||
| V: KL/GT | 3,255 | 4,003 | 529 | 0.49 | 0.54 | |||||||||
| V: GT | 29,117 | 21,154 | 533 | 0.45 | 0.46 | |||||||||
| V: GT/HAVO | 8,540 | 9,620 | 537 | 0.40 | 0.41 | |||||||||
| A: HAVO | 28,226 | 21,465 | 540 | 0.47 | 0.50 | |||||||||
| A: HAVO/VWO | 9,932 | 11,136 | 545 | 0.29 | 0.31 | |||||||||
| A: VWO | 27,750 | 24,469 | n.a | n.a. | n.a. | |||||||||
| Total | 135,884 | 113,862 | 0.42 | 0.43 | ||||||||||
| Cohort | 2015 | 2016 | 2015 & 2016 | 2015 | 2016 |
Notes: The row headings are the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The first six tracks are vocational, while the last three lead to university studies (academic). Columns 1 and 2 report the number of students initially assigned to each track by their primary education teachers, in view of starting secondary school in the academic year 2015-16 or 2016-17. Column 3 reports the CITO test score cutoffs above which a student in the track denoted by the corresponding row may be upgraded to the next level by the primary school teacher; the test score takes on integer values , with and . The upgrade is not mandatory. Columns 4 and 5 report, for each initial track, the fraction of students who, in the years considered, may be upgraded due to a sufficiently high CITO test score.
Using the database of the Dutch National Cohort Study on Education (NCO), we consider the population of students who begin their secondary school in the academic years 2015–16 and 2016–17 (hereafter, 2015 and 2016 cohorts).777This database can be accessed following the instructions provided at this link. For documentation in English, see here). The track assignment regulations that generate the quasi-experiment we exploit became effective for students starting secondary school in the 2014-15 academic year, but the data for this cohort do not contain all the necessary information and thus cannot be used. The timing implied by these regulations is described in Figure 1. Let denote the first year of secondary school. In March of the previous year, , the primary education teachers give their students an initial track recommendation. The first and second column of Table 1 report the number of students initially assigned to each track by their primary education teachers as a result of this provisional recommendation.
In April of the same year, , the students rank the schools offering the track that was recommended to them (or a less difficult one if they prefer) and are assigned to one of them.888The more specific method of student allocation differs by municipality. In Amsterdam, for example, a deferred acceptance algorithm is used, with lottery numbers serving as tiebreakers (Ketel et al., 2023). Then, in the following month, they take their end-of-primary education standardized test (CITO), which is graded by a computer with no intervention from primary education teachers. The scores obtained in this test map into track levels on the basis of nationwide, predetermined test score cutoffs. As in de Ree et al. (2025), we refer to the mapping of a CITO test score to a track level as a “test-based” track assignment. If the test-based track assignment is higher than the teacher’s track assignment recorded in March of year , the teacher is mandated by the law to consider an upward revision of the provisional recommendation. Such revision is only optional and dependent on the teacher’s judgment. Column 3 of Table 1 reports the CITO test score cutoffs above which a student, whose provisional recommendation is the track of the corresponding row, must be re-evaluated by the teacher for a possible upgrade. Columns 4 and 5 report, for each track, the fraction of students who may be upgraded because of a sufficiently high CITO test score.
Let be a dummy variable equal to if the primary education teacher finally recommends the low track to student . is, instead, equal to if the teacher recommends an upgrade to a higher track. Note that the 52,219 students whose provisional recommendation is the highest track (VWO in the last row of Table 1) cannot be treated with because there is no higher track to which they can be upgraded. Our analysis focuses on the students for whom the track provisionally recommended in March of year by the teacher is one of those listed in the first 8 rows of Table 1, totaling 197,527 students.
Denote with the track that student in cohort is provisionally recommended by the teacher; can take on values . Let be the score obtained by student in cohort in the CITO test; can take on integer values , with and . Let the assignment to treatment be , defined as follows:
| (1) |
where is the cutoff score that student in cohort provisionally assigned to track has to reach in order to be re-evaluated by her primary school teacher for a possible upgrade; “else” denotes students who do not score close to their respective cutoff score. If we condition on the students in a particular cohort , with a specific value of and with a value of , we can assume that only randomness, possibly conditional on some covariates to capture the heterogeneity of primary schools as illustrated below, determines whether or , that is, whether or .
This institutional setting generates the quasi-experiment we exploit to answer our research questions. Among students with a realized value of , randomly assigns similar subjects in each of the eight initial tracks to two different “sources” of first-year track recommendation: one is the “teacher alone”, while the other is the “teacher informed by the CITO test results”, who therefore has the possibility to upgrade students based on such results. This assumption is known in the Regression Discontinuity (RD) literature as “local randomization” (LR). 999Local randomization has been first formalized by Li et al. (2015) and Cattaneo et al. (2015). The LR approach to the analysis of RD designs has been later discussed in Mattei and Mealli (2017); Branson and Mealli (2019); Cattaneo et al. (2020b, a), and extended to allow for randomization conditional on covariates in Forastiere et al. (2025). One of its advantages is that it allows the researcher to easily deal with discrete running variables without having to rely on the usual continuity-type assumption101010See also Eckles et al. (2025) and Li et al. (2020). We formalize the LR assumption for our context in Section 3, where we also provide evidence to assess its plausibility.
Next, we denote with the outcome that we study. We define if student graduates from a higher track than the one provisionally recommended to him/her in March of , before taking the CITO test, and otherwise. We consider a student as graduating in a given track even he/she does so in more years than the required minimum.111111Our conclusions do not change qualitatively if we assume instead that a student graduates in a given track only if he/she does so in the regular possible time (results available from the authors). Four years are typically required for a vocational track, while five and six years are needed for the two academic tracks (HAVO and VWO, respectively). Six years are also needed for students who change from VMBO to HAVO, and seven for those who change from HAVO to VWO. The release of the NCO data at our disposal follows the 2015 cohort for 7 years, and the 2016 one for only 6 years after enrollment. Graduation is not observed for some students in these cohorts. We thus assume that the students of these cohorts who are still enrolled in secondary school in the last year of observation will graduate in the track in which they are enrolled. Our goal is to evaluate which source of track recommendation (“teacher alone” or “teacher informed by the test”) is “preferable” according to two criteria: a recommendation is better if it (i) induces a student to enroll in the most difficult track that she/he is able to complete successfully and (ii) reduces to a minimum the chances of costly track changes.
An institutional complication of this quasi-experimental setting is that, on rare occasions, also students who score below the cutoff () may be recommended for an upgrade by their teacher ().121212As explained by the relevant authorities (Van Po naar Vo, 2025), “[T]here may be situations in which a student does not obtain a higher test recommendation, but a school or teacher still believes that there is more knowledge and skills than expected. In such situations, the school may choose to adjust the provisional school advice. The school’s judgment is central here. … If parents do not agree with the advice, they will consult with the teacher and/or director of the primary school. If they cannot reach an agreement about the level at which the student will enter secondary education, parents can, as a last resort, use the school’s complaints procedure.” Moreover, as already mentioned, students with may have because their primary school teacher does not want to upgrade them even if an upgrade is possible. Therefore, as we will show in Section 3, non-compliance may occur in both assignments to treatment conditions.
3 Statistical framework
Consider a sample of units of cohort whose provisional recommendation is track with , . Let and be the -dimensional vectors of the possible values of and , with generic element and respectively. Denote with the recommendation that student receives from the teacher as a function of the assignment vector . Similarly, denote with the potential track (low or high) completed by student at the end of secondary school as a function of the assignment vector and the vector of recommendations . Since different primary schools may have different benchmarks to which they compare their students and decide their provisional track assignments, we must consider the possibility that students with different abilities are assigned to the same provisional track simply because they have more or less lenient primary teachers. We denote with the benchmark against which student has been evaluated to decide her/his provisional track assignment.
3.1 Assumptions
The first assumption we need is the Stable Unit Treatment Value Assumption (SUTVA, see Rubin, 1980), which restricts the dependence of the potential outcomes for a student only to the assignment and enrolment of that student, ruling out interference:
Assumption 1
Stable Unit Treatment Value Assumption:
.
Assumption 1 postulates the existence of two potential versions of with and four potential versions of with . Given SUTVA, the next assumption is:
Assumption 2
Local Randomization of the source conditional on primary schools’ leniency benchmarks and provisional track assignments: for every cohort we have
for and where and are observable and unobservable covariates, respectively.
This assumption requires some discussion. It formally says that for students assigned to the same track and whose teachers have a similar benchmark criterion to evaluate beliefs about students’ ability, the reasons for them to score just below or at the cutoff are unrelated to either potential outcomes or observed, , or unobserved, , covariates. Hence, in their case, . If is observed, this is a testable implication in that if Assumption 2 holds, we can expect, within each provisional track assignment and for a given , the observed characteristics of students to be well-balanced across the score cutoff (Mattei and Mealli, 2017; Forastiere et al., 2025; Angrist and Rokkanen, 2015). Moreover, if Assumption 2 holds for all tracks, we can view the data as coming from a block-randomized experiment, with blocks defined by the tracks, and analyze them accordingly (Imbens and Rubin, 2015).
The problem is that while we observe the initial track assignment and we can condition on scoring just below or at the cutoff, we do not observe the benchmark of a student’s primary teachers. A solution would be to condition on primary school fixed effects, assuming that teachers in the same school collectively agree on the same benchmark . However, we do not have enough students per primary school to conduct such an analysis. What we can do is to proxy with the fraction of students in the primary school of who score above the cutoffs. This fraction measures the average teacher severity in that school because, as more students score above the cutoffs, the stricter their primary school teachers must have been in their provisional track recommendations. We define this variable as , which takes on the same value for all students of a given school in a given cohort. will thus be included in all the specifications of our analysis.
| Panel A: 2015 cohort | |||||||||||
| Track | Female | Immigrant | College | Missing | Household | Urbanity | Primary | Primary | Student’s | Obs. for | Obs. for |
| origin | mother | college | income | primary | school | school # | age in | ||||
| mother | school | confession | students | months | |||||||
| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | |
| V: BL | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 519 | 511 |
| V: BL/KL | 1 | 0.7517 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 173 | 183 |
| V: KL | 1 | 0.1612 | 1 | 0.2872 | 1 | 0.1856 | 1 | 1 | 1 | 1199 | 860 |
| V: KL/GT | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0.4632 | 274 | 219 |
| V: GT | 1 | 1 | 1 | 1 | 1 | 1 | 0.6102 | 1 | 1 | 1942 | 1948 |
| V: GT/HAVO | 1 | 0.8050 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 614 | 570 |
| A: HAVO | 1 | 1 | 1 | 1 | 1 | 0.4935 | 1 | 1 | 1 | 2149 | 2172 |
| A: HAVO/VWO | 0.0787 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 843 | 769 |
| Panel B: 2016 cohort | |||||||||||
| Track | Female | Immigrant | College | Missing | Household | Urbanity | Primary | Primary | Student’s | Obs. for | Obs. for |
| origin | mother | college | income | primary | school | school # | age in | ||||
| mother | school | confession | students | months | |||||||
| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | |
| V: BL | 1 | 0.7578 | 0.7132 | 1 | 1 | 1 | 1 | 1 | 1 | 396 | 458 |
| V: BL/KL | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 195 | 195 |
| V: KL | 0.4012 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0.5522 | 656 | 630 |
| V: KL/GT | 1 | 1 | 0.8141 | 1 | 1 | 1 | 1 | 1 | 1 | 261 | 284 |
| V: GT | 1 | 1 | 0.0193 | 0.9419 | 1 | 1 | 1 | 0.8404 | 1 | 1386 | 1376 |
| V: GT/HAVO | 1 | 0.0915 | 1 | 1 | 1 | 1 | 1 | 1 | 0.3441 | 923 | 689 |
| A: HAVO | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1643 | 1728 |
| A: HAVO/VWO | 0.0976 | 0.3389 | 1 | 1 | 1 | 0.4697 | 1 | 1 | 0.1691 | 983 | 952 |
Notes: The row headings are the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. Columns 1 to 9 report, for each track and covariate, the Holm-Bonferroni p-value of a test for the equality of means in the two assignment-to-treatment groups defined by . The definition of the covariates whose names are not self-explanatory is as follows. Urbanity of a primary school is a categorical variable indicating the population density at the location of the primary school. Primary school confession is a categorical variable indicating the type of confession in primary schools (Protestant, Catholic, Public, or Other). Age in months refers to the age of the student at the time she/he receive the first recommendation. The last two columns report the number of observations in the two assignment groups. Standard errors are always clustered at the primary school level.
Columns 1 to 9 of Table 2 support the validity of Assumption 2 by showing that nine observed characteristics of students are balanced in the immediate vicinity of the cutoff. These covariates are: gender, migrant origin, mother’s education, missing mother’s education, household income, urban location of the primary school, confession of the primary school, number of students in the primary school, and age in months. Under Assumption 2, our estimates should not be (and in fact are not, as we will show below) affected by the inclusion or exclusion of these balanced covariates because they are not correlated with , conditional on . However, including them may increase efficiency since they are likely to be correlated with and , and for this reason, they will appear in our preferred specification.
It must be noted that the validity of Assumption 2 does not imply that the proportion of students scoring above or below the cutoff is around , as shown in the last two columns of Table 2. The CITO test score is the result of the students responding correctly to a series of questions. To simplify this data-generating process, think about the score being generated (approximately) by a binomial distribution and suppose that units in the sub-sample that we consider have the same probability to respond correctly to a question in the test. If so, these units also have the same probability (propensity score) to score at the cutoff (), yet this probability is not equal to the probability of scoring below () because these probabilities follow a binomial distribution. The difference in these two probabilities will depend on and on the number of questions in the test, but it is by no means an indication of the failure of Assumption 2. In the data, we may also observe differences in this probability from track to track because the cutoff is different or because the difficulty of the test (i.e., the probability of a correct response to single questions) is different. These findings would not represent violations of Assumption 2.131313This binomial example is, of course, over-simplistic; most likely, not all the questions have the same difficulty, and the students may not have the exact same probability of responding correctly to all questions. It is only meant to illustrate why, with a discrete score, the probability of being below or above the cutoff may differ even if the mechanism that generated is random and unrelated to potential outcomes or covariates.
Imai et al. (2023) invoke two additional assumptions that are necessary to answer our research question.
Assumption 3
Exclusion restriction:
.
This assumption requires that the potential outcomes depend only on the recommendation and not on the source of the recommendation so that they can be written as . Intuitively, this means that the track in which the student graduates should only be affected by scoring above or below the cutoff through its effect on the teacher’s recommendation. A violation of this restriction would occur, for example, if scoring above the cutoff increased the probability of graduating in the high track regardless of whether teachers decide to upgrade. In Section 5 we will show that we obtain similar results if we assume a Homogeneity and a Principal Ignorability assumption instead of the Exclusion Restriction (ER) assumption, and therefore that our conclusions are robust and stable across these alternative identifying assumptions.
Assumption 4
Monotonicity of with respect to :
This assumption plausibly excludes the existence of students, whom we call “Rebels”, who would not complete the high track if it were the recommended track, but who would complete it if the low track were recommended. These students would be characterized by the following set of potential outcomes: .141414de Ree et al. (2025) allow for the existence of a similar stratum, which they label “Slow Starters”. When estimating lower and upper bounds for the corresponding proportion, they cannot reject that this proportion is zero.
Finally, a different type of monotonicity assumption needs to be discussed in our setting.
Assumption 5
Monotonicity of with respect to :
This assumption is typically made in Instrumental Variable (IV) settings with non-compliance151515See Imbens and Angrist (1994) and Angrist et al. (1996). and excludes the existence of “Defiers”, that is, students who would be recommended the low track when scoring at or above the cutoff and the high track when scoring below. Imai et al. (2023) do not make this assumption. In their context, a judge who decides with the help of an algorithm may release an arrested subject while the same judge, not helped by the algorithm, keeps the same subject in prison, or vice versa. More generally, every set of counterfactual decisions with and without the help of the algorithm is possible in their context. To put it differently, they do not have (and do not want to have) any prior on which source of a decision is preferable: the fraction of “Preventable” cases kept under arrest may be higher when the judge is helped by the algorithm or when the judge decides alone, and their goal is precisely to assess for which one of the two sources the fraction is higher.
| TRACK | ||||
| 1 | 2 | 3 | 4 | |
| V: BL | .0039 | .0352 | .0025 | .0677 |
| V: BL/KL | .0231 | .1148 | .0205 | .1897 |
| V: KL | .0209 | .1767 | .0122 | .1952 |
| V: KL/GT | .0109 | .1233 | .0153 | .1655 |
| V: GT | .0036 | .056 | .0065 | .0574 |
| V: GT/HAVO | .0163 | .1684 | .0206 | .1858 |
| A: HAVO | .0051 | .0603 | .0085 | .0758 |
| A: HAVO/VWO | .0237 | .1821 | .0193 | .188 |
| COHORT | 2015 | 2015 | 2016 | 2016 |
Notes: The row headings are the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The other columns report the probabilities indicated in the column headings.
In our case, the institutional setting is such that an upgrade to a higher track should be possible only if students score at or above the cutoff. Therefore, Defiers should not exist by construction because no student below the cutoff should be eligible for an upgrade. For the same reasons, also Always Takers should not exist. On the other hand, teachers may decide not to upgrade students who score at or above the cutoff, so Never Takers may exist in our setting.161616Table A1 in the Online Appendix to Section 3.1 shows that correlates positively with the proportion of Never Takers. This finding supports our choice of as a proxy for the leniency of student teachers. As anticipated in Section 2, however, there are rare instances of non-compliance for students scoring below the cutoff. This is shown in Table 3: in the first and third columns, the probability that a student is upgraded () even if scoring below the cutoff () is low but positive in all tracks, ranging between 0.3% for BL in the 2016 cohort to 2.4% for HAVO/VWO in the 2015 cohort, and therefore Always Takers do exist in our setting. In the second and fourth columns, the probability that a student is upgraded () when scoring above the cutoff () is significantly lower than 1 in all tracks, ranging from 3.5% for BL in the 2015 cohort to 19.5% for KL in the 2016 cohort. So, and this is no surprise, Never Takers also exist and are frequent. In general, bridge tracks show a significantly higher probability of upgrading than the other tracks, which makes intuitive sense as is defined to be equal to one if the student is upgraded to the higher component of the mixed track.
This evidence indicates that non-compliance is certainly present in our setting. However, we do not see reasons to think that in the rare cases in which students scoring below the cutoff are upgraded (see footnote 12 for the possible reasons), these same students would not be upgraded if scoring above, which makes them Always Takers but not Defiers. Hence, Assumption 5 of Monotonicity of with respect to , excluding the possibility of Defiers, appears plausible. Even if not required to identify causal effects in our setting, we will refer to it for the interpretation of our results.
| Track | ||||
| 1 | 2 | 3 | 4 | |
| V: BL | .0319 | .0322 | .07 | .0712 |
| (.0093) | (.0092) | (.0147) | (.0146) | |
| V: BL/KL | .0931 | .0872 | .1722 | .1615 |
| (.0265) | (.0276) | (.0308) | (.0304) | |
| V: KL | .1639 | .1607 | .187 | .1892 |
| (.0152) | (.0149) | (.0176) | (.0177) | |
| V: KL/GT | .1152 | .11 | .1535 | .1564 |
| (.0248) | (.0244) | (.0244) | (.0247) | |
| V: GT | .0564 | .0561 | .0537 | .0537 |
| (.0058) | (.0058) | (.0074) | (.0074) | |
| V: GT/HAVO | .1579 | .1533 | .1719 | .1675 |
| (.0186) | (.0182) | (.0162) | (.0162) | |
| A: HAVO | .0581 | .058 | .0707 | .0705 |
| (.0063) | (.0063) | (.0076) | (.0076) | |
| A: HAVO/VWO | .168 | .1667 | .1732 | .1764 |
| (.0167) | (.0166) | (.0158) | (.0155) | |
| COVARIATES | NO | YES | NO | YES |
| COHORT | 2015 | 2015 | 2016 | 2016 |
Notes: The table reports, separately for each track, estimates of parameter in the first stage regression (2): . The row headings are the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The covariates included in columns 2 and 4 are those described in Table 2, respectively, for the two cohorts.
Table 4 reports, separately for each track, estimates of the first stage regression
| (2) |
where is the proxy for on which we need to condition for the validity of Assumption 2 and are the nine balanced covariates described in Table 2. As expected, the estimates of confirm the existence of imperfect compliance. Considering the covariates , the stability of the first-stage estimates with and without them supports Assumption 2 that the assignment to treatment is as good as random.
3.2 Students’ strata
Under Assumption 1 (SUTVA) and 3 (Exclusion restriction), and according to Frangakis and Rubin (2002), students can be classified in four strata based on the joint values of the two binary potential outcomes: .
-
AH:
These are Always High students who always complete the high track independently of the recommendation they receive. -
AL:
These are Always Low students who always complete the low track independently of the recommendation they receive. -
H:
These are Helpable students who complete the high track if this is the track recommended to them by primary school teachers, and who would not do the same otherwise. -
R:
These are Rebel students who complete the high track if recommended for the low track by primary school teachers, and the low track if recommended for the high track.
These principal strata, which we denote with , will play a crucial role in the definition of the metric we propose in Section 3.3 to measure the quality of track recommendations.
| Strata | Strata | Notes | ||||
| Exclusion restriction | Exclusion restriction | |||||
| 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| AL | 0 | 0 | NT | 0 | 0 | |
| AT | 1 | 1 | ||||
| C | 0 | 1 | ||||
| D | 1 | 0 | May w/out Monotonicity | |||
| AH | 1 | 1 | NT | 0 | 0 | |
| AT | 1 | 1 | ||||
| C | 0 | 1 | ||||
| D | 1 | 0 | May w/out Monotonicity | |||
| H | 0 | 1 | NT | 0 | 0 | |
| AT | 1 | 1 | ||||
| C | 0 | 1 | ||||
| D | 1 | 0 | May w/out Monotonicity | |||
| R | 1 | 0 | NT | 0 | 0 | because Monotonicity holds |
| AT | 1 | 1 | ||||
| C | 0 | 1 | ||||
| D | 1 | 0 |
Notes: The four panels refer to the four Principal Strata listed in the first column of the table and defined by the possible values of and : the Always-Low (AL), the Always-High (AH), the Helpable (H) and the Rebels (R). Columns 2 and 3 describe the values of the potential outcomes when the treatment is equal to or , respectively, and the Exclusion Restriction assumption holds. Each one of these Principal Strata is divided into additional strata defined by the possible values of and , as listed in the fourth column: the Never Takers (NT), the Always Takers (AT), the Compliers (C) and the Defiers (D). Columns 5 and 6 describe the values of the potential treatments when the assignment to treatment is equal to or , respectively. The Notes in the first three rows of the last column make clear that Defiers may or may not exist depending on whether the Monotonicity Assumption 5 of with respect to holds. The last note in the fourth row states that given the maintained Monotonicity Assumption 4 of with respect to , the Rebels do not exist.
Under the same assumptions, the more conventional classification in strata that characterizes compliance types is also possible, based on the joint values of the two binary potential treatment values: :
-
AT:
These are Always Takers who are always recommended the high track, regardless of their score around the cutoff. -
NT:
These are Never Takers who are never recommended the high track, independently of where they score around the cutoff. -
C:
These are Compliers who are recommended the high track if they score at the cutoff, and the low track if they score below. -
D:
These are Defiers who are recommended the low track if they score at the cutoff, and the high track if they score below.
We denote with these strata.
Table 5 describes the relationship between these basic principal strata. Under Assumption 4 (Monotonicity of with respect to ), the stratum of the Rebels, in red, does not exist irrespective of the student belonging to any of the strata , while under Assumption 5 (Monotonicity of with respect to ), the stratum of Defiers, in blue, does not exist in any of the strata .
3.3 The metric to compare recommendations
Following Imai et al. (2023), the metric that we propose to compare recommendations and establish which one is preferable relies on the classification of students in Principal Strata described in the previous section, combined with the following criteria:
-
1.
If a student is able to complete a more difficult track, it is better that she is allowed to do it for both
-
1a)
herself, because she can obtain more desirable lifetime outcomes
-
1b)
and for society, if there are collective gains from a more educated community.
-
1a)
-
2.
Changing tracks is costly for students and for the education system.
The first criterion defines what constitutes a better recommendation for Helpable (H) students. In their case, the goal of teachers should be to recommend the high track to the largest fraction of these students because, as a result of this recommendation, they will complete precisely this track, achieving a better outcome for themselves as well as for society. Therefore, we need to estimate the difference between “the fraction of H students who are recommended a high first-year track by teachers who have seen students’ test scores” and “the analogous fraction for teachers who have not seen them”. If this difference is positive, test scores help teachers give students better track advice.
The second criterion defines instead what constitutes a better recommendation for the Always Low (AL) and Always High (AH) students. In their cases, track advising should aim to minimize the number of track changes that these two types of students experience during secondary school, if they start on a track that differs from the one they will ultimately complete. Therefore, the best recommendation is to send all AL students to the low track and all AH students to the high track.
To formalize this intuitive characterization of what constitutes a better recommendation, consider the following Average Principal Causal Effect:
| (3) |
The is the difference between the fraction of students in stratum who are recommended the high track by the teachers informed by test scores () minus the fraction receiving the same recommendation by teachers who have not seen the test scores (). To put it differently, the is the Intention To Treat effect of the source of recommendations on their content in stratum .
Note that each is also equal to the difference between the fractions of Compliers and Defiers:
If Monotonicity of with respect to (Assumption 5) holds, Defiers do not exist, and the three are all non-negative by construction, being equal to the proportion of Compliers. In this case we could conclude171717As already mentioned when we introduced Assumption 5 in the previous section, we emphasize that this assumption allows to interpret as the proportion of Compliers in stratum , but it is not necessary for identification. that test scores help teachers give better recommendations if the and the are positive while the is equal to zero. Having established that the are informative about whether test scores help teachers give better recommendations, in the next section, we show how these population parameters can be identified with data generated by the quasi-experiment that we study.
3.4 Identification of the
Estimands for Helpable (H) students
Adapting Theorem 1 of Imai et al. (2023) to this context, it is possible to show that181818For the derivation of this equation and all the others in this section, see the proofs in the Online Appendix to Section 3.4.
| (4) |
where the terms in the numerator do not depend on missing potential outcomes and can be estimated with their sample analogs. The denominator is instead the proportion of Helpable students, given that Rebels are excluded because of the Monotonicity Assumption 4, and can only be partially identified. In some settings, even if principal strata are latent, their proportions can be point identified under specific assumptions. This is the case, for example, of the proportion of compliers in IV settings. In our case, because the principal strata are defined by the joint values of , the strata proportions are not point identified unless we impose further assumptions on the distribution of potential outcomes and the recommendation .
To address this problem, Imai et al. (2023) propose to derive non-parametric bounds for the by bounding the terms in the denominator of (4). Using Assumption 1 and the Law of Total Probability, it is possible to show that these bounds are:
| (5) | |||||
These results imply that the lower bound of is
| (6) |
while the upper bound is
| (7) |
An alternative identification and estimation strategy that we can adopt is instead based on the assumption of unconfoundedness of with respect to :
Assumption 6
Unconfoundedness of :
and
This assumption, which allows one to point identify the , implies that, conditioning on observable covariates , a teacher’s decision to upgrade is independent of potential outcomes. In other words, covariates contain all the information teachers use when making their decision, and the content of the recommendation given this information is random.191919This assumption would be violated if, for example, teachers also relied on the student’s behavior in class, which we do not observe, when taking their upgrade decisions. As we will see, we achieve similar results under the different identification assumptions we adopt. Identification under this assumption is formally discussed in the Online Appendix to Section 3.4.
Estimands for Always High (AH) students
As for the AH students, we show in the Online Appendix to Section 3.4 that
| (8) |
where, once again, the terms in the numerator do not depend on missing potential outcomes and can be estimated using their sample analogs, whereas further assumptions are needed for the denominator. To bound this denominator, note that:
Therefore the lower bound of is
| (9) |
while the upper bound is
| (10) |
Also, in the case of AH students, as with their H peers, we will compare the bounds obtained under the above identification strategy with point estimates obtained by assuming the unconfoundedness of (Assumption 6), as shown in the Online Appendix Appendix to Section 3.4.
Estimands for Always Low (AL) students
Finally, considering the AL students, the Online Appendix to Section 3.4 shows that
| (11) |
where, also in this case, the terms in the numerator do not depend on missing potential outcomes and can be estimated with their sample analogs, while the estimation of the denominator requires further assumptions. To bound this denominator, note that:
Therefore, the lower bound of is
| (12) |
while the upper bound is
| (13) |
Once again, as for the and the , also the can be point identified under unconfoundedness of (Assumption 6), as shown in the Online Appendix to Section 3.4.
4 Test scores and quality of teachers’ advice: evidence
4.1 Estimates for the three strata
Estimates for Helpable (H) students
In the first column of Table 6, we present estimates of the numerator of the estimand in the RHS of equation (4), separately for each track and combined for the three aggregate tracks: Vocational, Academic and All.202020Here and in what follows, each population estimand has been estimated separately for each track and cohort. The estimates reported in the tables for aggregate tracks have been obtained as weighted averages with weights based on the relative sample sizes of each cell. For this reason, the remaining tables report only one set of results, rather than separate results by cohort. Standard errors (reported in parentheses) are bootstrapped (with 1000 repetitions) separately in each track, and then appropriately aggregated with the same weights. The estimates in the tables reported in the text include the covariates and (see Table 2). The Online Appendix to Section 4.1 reports tables with corresponding estimates obtained without the balanced covariates (but including ). The inclusion or exclusion of the balanced covariates does not significantly alter the estimates, as expected. This evidence supports the validity of Assumption 2 (Randomization of the source). The point estimates of this numerator are strictly positive in all tracks. Students scoring at the cutoff, who can be upgraded by their primary school teachers based on test scores, are 1.8–to–10% more likely to complete a higher track than students who score below the cutoff. These point estimates are statistically different from zero except for the BL track. The estimates for the three aggregate tracks in the last rows of the table are also significant and equal to 4.4%.
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0251 | .0251 | .6534 | .0381 | 1 |
| (.0167) | (.0154) | (.0126) | (.0263) | - | |
| V: BL/KL | .0773 | .0773 | .2877 | .2757 | 1 |
| (.0294) | (.0279) | (.0222) | (.1029) | - | |
| V: KL | .0996 | .0996 | .6984 | .1426 | 1 |
| (.0159) | (.0159) | (.0112) | (.0229) | - | |
| V: KL/GT | .0256 | .0256 | .3967 | .0644 | 1 |
| (.0238) | (.0217) | (.0232) | (.0564) | - | |
| V: GT | .0182 | .0182 | .7607 | .0237 | 1 |
| (.0077) | (.0077) | (.0076) | (.0104) | - | |
| V: GT/HAVO | .0526 | .0526 | .5089 | .1021 | 1 |
| (.0164) | (.0159) | (.0138) | (.0309) | - | |
| A: HAVO | .0349 | .0349 | .8082 | .0432 | 1 |
| (.0076) | (.0076) | (.0065) | (.0096) | - | |
| A: HAVO/VWO | .0642 | .0642 | .5451 | .117 | 1 |
| (.0156) | (.0154) | (.0126) | (.0276) | - | |
| VOCATIONAL | .0445 | .0445 | .6486 | .0686 | 1 |
| (.0058) | (.0059) | (.005) | (.009) | - | |
| ACADEMIC | .0442 | .0442 | .7251 | .0609 | 1 |
| (.0072) | (.0071) | (.0059) | (.0099) | - | |
| ALL | .0444 | .0444 | .6796 | .0653 | 1 |
| (.0045) | (.0045) | (.0038) | (.0066) | - |
Notes: The row headings are the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (4), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower bound of the (in column 4). Column 5 reports the upper bound of the as equal to 1 because it is the ratio between and which are exactly equal under Assumptions 4 and 5. See footnote 21 for the intuition. The definitions of these two last statistics are in equations (6) and (7), respectively. The aggregations for the last three rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors are computed using a block bootstrap at the school level with 1000 repetitions; each quantity in every iteration is calculated in the same way. The specifications in all columns include the covariates and (see Table 2). The Online Appendix to Section 4.1 reports tables with corresponding estimates obtained without the balanced covariates (but including ). The inclusion or exclusion of the balanced covariates does not change the estimates in a relevant way, as expected, supporting the validity of Assumption 2 (Randomization of the source).
Recall that the denominator of the estimand in equation (4) is certainly positive and equal to the proportion of Helpable students, given Assumption 4 of Monotonicity of with respect to . Therefore, a positive numerator supports the conclusion that the information provided by test scores helps recommenders give better advice in the sense that, thanks to this information, they become better at detecting Helpable students and directing them into the most challenging secondary school track that they can successfully complete.
To confirm this conclusion and assess also quantitatively by how much test scores help teachers achieve this goal, estimates of the are needed, but as explained in the previous section, the denominator in the RHS of equation (4), which is the proportion of students, can only be bounded without additional assumptions. Estimates of the lower and upper bounds of this denominator are reported in columns 2 and 3 of Table 6. Focusing on the aggregate tracks in the last three rows, the proportion of ranges between approximately 4% and 73%.
As a consequence of the large gap between the estimated bounds of the denominator, also the bounds of the reported in columns 4 and 5 of Table 6 are substantially different one from the other, ranging between 6% and 100% for the three aggregate tracks.212121Note that the upper bound of the derived in equation (4) and reported in column 5 of Table 6 is 1 by construction because it is the ratio between and , which are exactly equal under the two Monotonicity Assumptions 4 and 5. The intuition is that the intention-to-treat () at the numerator of this ratio identifies the proportion of students who are compliers and are also helpable. Hence, the minimum plausible share of all Helpable students at the denominator of this ratio cannot be smaller than the share of Helpable who are also compliers at the numerator, and the upper bound of the must be 1 by definition. Note also that when the share of Helpable who are also compliers is effectively equal to zero or very small, we may estimate it to be negative because of small sample variability. In tracks where this occurs, we round this proportion to zero. However, even when this proportion is infinitesimally small, it is still the case that the upper bound for the is 1 for the reason explained above. However, what matters most from the viewpoint of our research question is the lower bound of the , which allows us to conclude that the information provided by test scores improves the quality of recommendations by at least 6% as measured by their capacity to push Helpable students into the high track. This is a remarkable finding, particularly if the proportion of Helpable students in the population can be as high as 80%, as suggested by the estimated upper bound of the denominator in column 3 of Table 6.
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0004 | .317 | .3189 | .0011 | .0011 |
| (.0118) | (.0111) | (.0111) | (.036) | (.036) | |
| V: BL/KL | 0227 | .6858 | .705 | .0323 | .0332 |
| (.0243) | (.0209) | (.0214) | (.0336) | (.034) | |
| V: KL | .0083 | .2289 | .2392 | .0368 | .0379 |
| (.0101) | (.0086) | (.0091) | (.0401) | (.0416) | |
| V: KL/GT | .0745 | .5681 | .581 | .1288 | .1319 |
| ( .0297) | (.0206) | (.0198) | (.0499) | (.05) | |
| V: GT | .0097 | .1938 | .1938 | .0482 | .0482 |
| (.006) | (.0055) | (.0055) | (.03) | (.03) | |
| V: GT/HAVO | .0439 | .4169 | .4262 | .1023 | .1041 |
| (.0165) | (.0113) | (.0115) | (.0375) | (.0379) | |
| A: HAVO | 0 | .1283 | .1283 | 0 | 0 |
| (.0006) | (.004) | (.004) | (.0047) | (.0047) | |
| A: HAVO/VWO | .0281 | .3667 | .3777 | .074 | .0762 |
| (.0146) | (.0105) | (.0109) | (.037) | (.0379) | |
| VOCATIONAL | .0188 | .2988 | .3044 | .0618 | .063 |
| (.0049) | (.004) | (.0041) | (.0157) | (.0159) | |
| ACADEMIC | .0089 | .2035 | .207 | .0429 | .0436 |
| (.0046) | (.0042) | (.0043) | (.0219) | (.0223) | |
| ALL | .0148 | .2601 | .2649 | .0558 | .0568 |
| (.0034) | (.0029) | (.003) | (.0126) | (.0128) |
Notes: The row headings are the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (8), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower and upper bounds of the (in columns 4 and 5). The definitions of these two last statistics are in equations (9) and (10), respectively. The aggregations for the last three rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors for each quantity are computed using a block bootstrap at the school level with 1000 repetitions; each quantity in every iteration is calculated in the same way. The specifications in all columns include the covariates and (see Table 2). The Online Appendix to Section 4.1 reports tables with corresponding estimates obtained without the balanced covariates (but including ). The inclusion or exclusion of the balanced covariates does not change the estimates in a relevant way, as expected, supporting the validity of Assumption 2 (Randomization of the source).
Estimates for Always High (AH) students
Moving to the stratum of Always High students, the estimates for them are reported in Table 7. In their case, the best source of advice is the one that recommends the high track to the highest number of them (possibly to all of them) in order to minimize the cost of track changes they would incur if they started secondary school in the low track. Recall, in fact, that these students always complete the high track no matter where they start.
Focusing for brevity just on Columns 4 and 5 of the table, the gap between the lower and the upper bounds of the is remarkably small in almost all tracks, allowing for a very precise estimation of this population parameter.222222In the case of HAVO track, the sample analog of the numerator in the RHS of equation (8) is close to zero but has a negative sign, possibly due to small sample variability. We approximate it with 0. The intuition for why we obtain such a precise result is that we can be confident in the population proportion of AH students (i.e. the denominator) as those with and are for sure AH, and measuring this under the cutoff (i.e. for those with ) gives a precise estimate of the population parameter, showing that the population proportion of AH is approximately 26%.232323More specifically, because it is extremely rare to have and (which can only occur due to the institutional complications explained in footnote 12) we know that among those with , those with and are AH for sure, while those with and are for sure not AH, and these two categories together form virtually the whole population. This means that the fraction of AH students can be estimated very precisely, as is randomized and the same fraction should exist above the threshold. In the eight basic tracks, the point estimates of the range from 0 to 13%. When the tracks are aggregated in the Vocational, Academic, and All tracks (last three rows of the table), the is estimated to be about 6.2%, 4.3%, and 5.6%, respectively, and statistically significant. These results indicate that when teachers receive test score information, they send a significantly higher fraction of AH students to the high track, thereby reducing the number of costly track changes this group would otherwise incur.
Estimates for Always Low (AL) students
In the case of Always Low students, instead, our estimates of the suggest that test score information does not help teachers give better recommendations. These estimates are
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0271 | .028 | .6576 | .0409 | .9765 |
| (.0056) | (.0055) | (.0125) | (.0085) | (.0266) | |
| V: BL/KL | .0239 | .0266 | .2177 | .1126 | .8981 |
| (.0084) | (.008) | (.0208) | (.0404) | .(1399) | |
| V: KL | .0658 | .0727 | .6612 | .0991 | .9026 |
| (.0071) | (.0069) | (.0126) | (.0105) | (.03) | |
| V: KL/GT | .0335 | .0352 | .3934 | .0853 | .9569 |
| (.0084) | (.0083) | (.0194) | (.0212) | (.0599) | |
| V: GT | .0377 | .041 | .7899 | .0477 | .9193 |
| (.0038) | (.0036) | (.0072) | (.0048) | (.0253) | |
| V: GT/HAVO | .0644 | .0742 | .5213 | .1238 | .8682 |
| (.0082) | (.0077) | (.0128) | (.0156) | (.0409) | |
| A: HAVO | .0487 | .0539 | .8464 | .0576 | .9035 |
| (.0042) | (.004) | (.0058) | (.005) | (.0267) | |
| A: HAVO/VWO | .0788 | .0883 | .5581 | .141 | .8928 |
| (.0077) | (.0074) | (.0126) | (.0133) | (.0278) | |
| VOCATIONAL | .0458 | .0506 | .652 | .0703 | .9059 |
| (.0027) | (.0025) | (.005) | (.004) | (.016) | |
| ACADEMIC | .0582 | .0648 | .7554 | .0771 | .8988 |
| (.0037) | (.0035) | (.0055) | .0048 | (.0192) | |
| ALL | .0508 | .0563 | .694 | .0733 | .9026 |
| (.0022) | (.0021) | (.0037) | (.0032) | (.0126) |
Notes: The row headings are the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (11), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower and upper bounds of the (in columns 4 and 5). The definitions of these two last statistics are in equations (12) and (13), respectively. The aggregations for the last three rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors for each quantity are computed using a block bootstrap at the school level with 1000 repetitions; each quantity in every iteration is calculated in the same way. The specifications in all columns include the covariates and (see Table 2). The Online Appendix to Section 4.1 reports tables with corresponding estimates obtained without the balanced covariates (but including ). The inclusion or exclusion of the balanced covariates does not change the estimates in a relevant way, as expected, supporting the validity of Assumption 2 (Randomization of the source).
reported in Table 8. The best source of advice for these students is the one that recommends the high track to the lowest number of them (possibly to none), in order to minimize the cost of the track changes they would incur if they started secondary school in the high track. Recall that these students can only complete the low track, no matter where they start.
Focusing again for brevity just on the last two columns of the table, Column 4 shows that the estimated lower bound of the is positive and statistically different from zero in many tracks, ranging from 4% for BL to 14% for HAVO/VWO. Considering the aggregate tracks in the last three rows of the table, the is at least as high as 7% and significantly different from zero in all of them. Therefore, it appears that the information provided by test scores leads teachers to recommend the high track to too many AL students. The consequence is a considerable deterioration of the quality of their recommendations. A possible reason for this finding is that primary school teachers do not bear any cost deriving from secondary school track changes and are therefore less sensitive to this consequence of the advice they offer to their students.
4.2 A comprehensive evaluation
The evidence in the previous section suggests that, in the case of Helpable and Always High students, test score information helps teachers improve the quality of the recommendations in terms of the two criteria that we have adopted (directing students towards the most difficult track they can successfully complete and reducing track changes). On the contrary, in the case of the Always Low, the positive estimate indicates that the same information reduces this quality because it induces too many AL students to enroll in the high track that they later cannot complete.
The classification framework proposed by Ben-Michael et al. (2024) provides a comprehensive evaluation of whether it is a good idea to offer teachers the possibility to upgrade based on test score information, depending on how much the policy-maker cares about H and AH students compared to their AL peers. Define with the loss deriving from a recommendation given to a student with potential outcome . The four possible values of this loss are represented in the following Confusion Matrix:
Confusion Matrix
| Negative () | Positive () | |||||
| Negative () |
|
|
||||
| Positive () |
|
|
The True Negative is a student whom the teacher correctly identifies as Always Low, while the True Positive is correctly identified as Always High or Helpable. In all these cases, the recommendation of the teacher does not cause any loss: .242424To simplify the analysis, we abstract here from the possibility of negative losses (gains), which may differ between H, AL, and AH students. The False Negative is a Helpable or Always High student who is not recognized as such by the teacher and thus receives a low recommendation. In her/his case, the loss is positive because the student could complete the high track following the opposite advice (in the case of H) and complete the high track regardless, but with a track change (in the case of AH). Similarly positive is the loss for the last type of student, False Positive , who is an Always Low not recognized as such by the teacher. Therefore, she/he receives a high recommendation without being later able to complete the high track.
Let’s normalize to the loss suffered by a False Negative student. It is then reasonable to assume that . This is because is the “short-term” loss generated by the cost of changing track that a False Positive AL student incurs if she/he is recommended a too-difficult track. In light of this, if is the loss of a False Negative AH, then it is also a short-term cost of having to change track, which can be assumed to be comparable for AL and AH students, and therefore the above inequality holds weakly: . If, instead, is the loss of a False Negative H student, he/she suffers a long-term loss extending well beyond the end of secondary school, because she/he earns, in expectation, the lower lifetime income of a low graduate, while with the opposite advice she/he would earn the higher one of a high graduate. Therefore, in the case of this student, we can assume that strictly.
We can then write the overall expected loss of a recommendation given by a source as
| (14) |
where is the probability of a False Negative (H or AH) and is the probability of a False Positive (AL). Our goal is to compare the overall loss when the source is a “teacher alone” or a “teacher informed by test scores”, computed at different values of the relative weight . In light of the above considerations, this weight is at most equal to , when the short-run loss of AL students is considered by the policy-maker as relevant as the long-run loss of H students, so that , and decreases towards as the former loss becomes less relevant .
In the Online Appendix to Section 4.2 we show that
| (15) |
where the RHS is identified and can be estimated thanks to Assumption 2 (Randomization of the source). The left panel of Figure 2 reports for each track the P-values of the tests of the null that this difference is negative for values of the relative weight in the set :
| (16) |
Failure to reject the null hypothesis of this test (high P-value) indicates that we cannot rule out the possibility that informing teachers with test scores is a better (or equally effective) system because we do not have enough evidence in the data to conclude that the provision of test scores leads to larger losses. Conversely, if we can reject (small P-value), we can rule out that teachers informed by test scores generate smaller losses and thus give better recommendations than teachers alone.
With the exception of three tracks (V:BL, V:GT and A:HAVO), we cannot reject that the difference is negative even when the short-run loss of AL students is considered as relevant as the long-run loss suffered by H students and AL students have the maximum relative weight. For the remaining V:BL, V:GT and A:HAVO tracks, we reject that the difference is negative only when the short-term loss suffered by AL students has an (implausibly) high relative weight – higher than about 0.8, 0.5, and 0.3, respectively, at the 5% significance level. Similar evidence is reported in the right panel of the figure for the three aggregate tracks. We never reject the null with the same confidence for the All and Vocational tracks, even if the relative weight on the short-term loss of AL students is the highest. In the case of the Academic track, we reject only when this weight is higher than about 0.75.
In light of this evidence, we conclude that, overall, the possibility of upgrading based on test scores helps primary teachers improve the quality of their secondary school track recommendations. For this not to be the case, the short-term loss of AL students (who have to change track because they are recommended too difficult a track that in the end they cannot complete) would have to be considered implausibly almost as relevant as the life-time loss of H students (who are prevented from completing a high track they would be able to complete).
5 An alternative to assuming the Exclusion Restriction
In this section, we explore the identification and estimation of the Average Principal Causal Effects without relying on the Exclusion Restriction (ER) assumption. This assumption may fail to hold if the causal effect of on is not mediated solely by . Consider, for instance, the Never-Takers stratum. Since these students are never upgraded, under the ER their expected value of should, in principle, be the same on both sides of the cutoff. However, scoring above the cutoff may boost the confidence of these students and encourage their parents to advocate for an upward track change after initial enrollment in secondary school. More generally, it may have a direct positive effect on these students’ performance in secondary school, even in the absence of an upgrade at the moment of the initial track enrollment. In these cases, the Exclusion Restriction would not hold.
We study such a possibility by deriving bounds on the APCEs when a specific form of direct effect of on for given is allowed to play a role. Finding that even in the presence of this effect, our results are unchanged would suggest that the ER assumption is not strictly necessary for our conclusions about the preferability of informing teachers about the test score results of their students, or, in other words, that at least this deviation from the Exclusion Restriction assumption is not changing our results (see also Angrist et al., 1996).
To this end, we first assume that the direct effect of on is additive and equal for all students:
Assumption 7
Homogeneity of the direct effect of on
,
Under Assumption 7, we derive new expressions for the APCEs and their bounds as a function of , as described in the Online Appendix to Section 5.252525Mealli and Pacini (2013) study another example of bounds for principal causal effects when Exclusion Restriction assumptions are relaxed. These expressions generalize the original framework proposed by Imai et al. (2023). Indeed, it can be shown that as goes to 0, i.e. the level at which the Exclusion Restriction holds, the new bounds converge to those derived in Section 3 under the ER. More generally, the bounds of the APCEs that we obtain are functions of the unknown parameter , which serves as a sensitivity parameter (e.g., Imbens, 2003), and as such cannot be identified and estimated without invoking further assumptions. In order to gain some insights into the plausible magnitude of , we can invoke the Principal Ignorability assumption described below that allows to identify and estimate . This enables us to establish not only the extent to which the ER assumption is violated but also the stability of our results if the violation indeed occurs and identification is achieved under an alternative assumption.
To understand the nature of this Principal Ignorability assumption, consider the effect of on for Never-Takers, which under the Homogeneity Assumption 7 is equal to :
| (17) |
The first term on the RHS of equation (17) is identified and can be easily estimated because students who score at the cutoff but are not upgraded by the teacher are certainly Never-Takers and are observed. The second term, however, is not identified because the group of students scoring below the cutoff (and not upgraded) comprises both Compliers and Never Takers, who cannot be distinguished. We solve this problem by invoking the following assumption:
Assumption 8
Principal Ignorability:
This assumption implies that, conditional on covariates, the distribution of potential outcomes below the cutoff is the same for Compliers and Never-Takers.262626This assumption is, of course, as debatable as the ER assumption. Moreover, similarly to the unconfoundedness assumption, its plausibility crucially depends on the information contained in the observed covariates (see Feller et al., 2017, Mattei et al., 2023, and Ding and Lu, 2017) However, our goal here is simply to assess how stable our results are when invoking this assumption instead of the ER. Therefore, the second term on the RHS of equation (17) is identified and can be estimated.272727Using observations above the cutoff, where Never-Takers are identified, we can estimate the probability of being a Never-Taker conditional on covariates (the estimated principal score ). Then, can be estimated by the average for units with and , weighted by the principal score. That is, under Assumption 8, is identified and can be estimated by . See Mattei et al. (2023) for further details.
Table A5 in the Online Appendix to Section 5 reports estimates of obtained with the above procedure. Some point estimates of are quantitatively sizeable. For example, for students in track V:BL/KL in the 2016 cohort is 0.1013 (s.e.: 0.0465), which is larger than the estimated numerator of the for Helpable students in the same track, i.e. 0.0773. In light of this finding, we use the point estimates to estimate the bounds for the APCEs when the Homogeneity Assumption 7 and the Principal Ignorability Assumption 8 hold instead of the ER.
Figure 3 compares, for the three aggregate tracks and for the H, AH and AL strata, the point identified estimates obtained under unconfoundedness (Assumption 6, red dot), that are derived in the Online Appendix to Section 3.4; the bounds of the obtained under the ER (Assumption 3, solid black), that are derived in Section 3; and the bounds of the obtained under PI and Homogeneity (Assumptions 8 and 7, dashed light-blue), that are derived in this section. In the case of H and AL students, unconfoundedness delivers estimates that are located near the lower bounds of the partially identified alternative estimates. For the AH students, both with and without unconfoundedness, the estimates are precise and close one to the other.
This graphical comparison under the different assumptions indicates that the implications in terms of lower bounds of the relevant effects are very similar in all cases. We conclude that our main results are robust with respect to these alternative identification strategies.282828The estimates of all the bounds obtained without the Exclusion Restriction and under the alternative PI and Homogeneity assumptions can be found in Online Appendix to Section 5.
6 Fairness of teachers’ recommendations
Within the same statistical framework described in the previous sections, Imai and Jiang (2022) and Imai et al. (2023) propose the concept of “Principal Fairness,” which can be interestingly used to compare the fairness of two sources of first-year track assignment.
Definition 9
Principal Fairness:
A source of first-year track assignment satisfies Principal Fairness with respect to a protected attribute (e.g., SES, race, gender), if the first-year recommendations deriving from this source are conditionally independent of within each principal stratum
| (18) |
According to this definition, a recommender is fair as long as her recommendations are independent of the protected attribute among students in the same stratum. The analogous fraction in other strata can be lower or higher if it is equal for both genders within each stratum. A test for Principal Fairness of a source of secondary school track recommendations can then be designed as follows. Let the two sources of recommendations that we would like to compare be: “teacher alone” () versus “teachers informed by test scores” (). Given two values and of a protected attribute , the Principal Fairness of source in stratum is given by:
| (19) |
where is the recommendation given by source to student . Note that source is perfectly fair if . Otherwise, source is unfair because its probability of recommending the high track to students in stratum changes with the values of .
It is important to note that, considering the protected attribute without conditioning on the other covariates requires to interpret carefully the reason for a possible lack of fairness. A fairness criterion that does not hold unconditionally may hold conditionally if the covariates correlate with the protected attribute (see Imai and Jiang, 2022, for a discussion). For instance, suppose that teachers do not discriminate based on immigrant status but do discriminate based on SES, and immigrants have, in general, lower SES. In this case, if the protected attribute in equation (19) is immigrant status, this attribute should actually be considered as a proxy for SES. However, from a descriptive viewpoint, it would still be the case that immigrants are treated differently, even if in each stratum defined by SES they are not. As argued by Imai et al. (2023), the choice between marginal principal fairness and conditional principal fairness is not statistical.
The sign of is also important, as it indicates the direction of discrimination. Suppose again that denotes immigrant students. Then finding, for example, that while , would mean that when test score information is provided to recommenders, immigrant students are positively discriminated into the high track, while when the recommenders decide without this information, immigrants are negatively discriminated. Note that in our setting, where teachers are officially allowed to upgrade only students who score above the cutoff, all the should be equal to 0 by definition. Therefore, a cannot be interpreted as evidence of fairness in the provisional track recommendation made by teachers without the test scores. On the other hand, a different from 0 would indicate different rates of non-compliance below the cutoff across protected attributes in stratum .
Under assumptions 1 (SUTVA), 2 (Local Randomization), 3 (Exclusion Restriction), 4 (Monotonicity of with respect to R), and 6 (Unconfoundedness of R with respect to Y), as shown by Imai et al. (2023), we can estimate equation (19). Figure 4 plots, for the three aggregate tracks and for the H, AH, and AL strata, and , considering immigrant status as the protected attribute . In all these cases, is not statistically different from zero and is small in size. is instead clearly positive and significantly larger than in all strata. This evidence suggests that the information provided by test scores considerably increases the chances that immigrant students are upgraded. The interpretation of this finding is that test scores induce a larger revision of the teachers’ posterior beliefs regarding the ability of immigrant students compared to those of native students. Note that this outcome is good news for immigrant students in the H and AH strata but not for immigrant students in the AL stratum, for whom this positive discrimination causes track changes.
Figure 5 reports the analogous quantities considering low socioeconomic status (SES) as the protected attribute, where low SES is defined as having a household income below the median. For the academic tracks, none of the is significantly different from 0. In vocational tracks, however, the are negative and the are positive and economically significant, albeit smaller in size than in the case of immigrant students (the scale of the vertical axis changes between Figures 4 and 5). The fact that all the are negative suggests a higher rate of non-compliance below the cutoff among high-SES students. A plausible explanation is that parents of high-SES students may be more likely to pressure teachers to upgrade their children when they are provisionally recommended for vocational tracks. On the other hand, the positive values indicate that the provision of test scores induces upgrades for low-SES students more often than for high-SES ones. We view this finding as a consequence of test scores inducing a larger shift in teachers’ posterior beliefs for low-SES students.
Finally, the analogous evidence for gender is shown in Figure A1 of the Online Appendix to Section 6. We do not find any significantly different from 0, meaning that teacher recommendations are fair when the protected attribute is gender.
7 Conclusions
Using a quasi-experimental setting offered by Dutch educational institutions, we show that when track recommendations given to students at the end of primary education can be upgraded based on standardized test scores, the quality of advice improves by at least 6% as measured by the fraction of students that are recommended a more challenging track they can complete successfully, while in the counterfactual case, they would have remained in a lower level track. An improvement of about the same size is also estimated for students who should be recommended to more challenging tracks, as they are always able to complete them independently of where they start.
We also find, however, that the possibility of upgrading based on test scores results in a significant number of students who are indeed upgraded but are unable to complete a more challenging track. In the case of these students, the goal should be to direct them immediately toward the low track that they can complete successfully, thus minimizing costly track changes. However, an overall evaluation that combines these opposite findings in a weighted objective function indicates that the relative weight of the short-term losses suffered by students who must change track because of misplacement when teachers are given the opportunity to upgrade based on test scores would have to be unreasonably high to conclude that it is better if teachers do not see these scores.
The implications for policy of these results are particularly relevant in countries where tracking is the established way to organize secondary school studies. It is surprising that no experimental evidence exists to guide educational policymakers of these countries in designing the best system of track advising. We fill in this gap in several ways.
First, we define a reasonable metric to evaluate the quality of teachers’ recommendations. This metric values two objectives: reducing costly track changes and directing towards more challenging tracks students who are able to complete them, but who need to be “pushed” towards these tracks and be convinced that they can complete them successfully. Differently from the case of a weather forecast, predicting the school track in which a student would have the best performance and recommending this track may have an effect on the object of the prediction itself, i.e., the choice of the student and her performance in the chosen track, as well as later in life. In other words, track recommendations may be self-fulfilling prophecies, and it is crucial to consider this feature in the optimal design of their implementation. The metric we propose puts this feature at the center stage.
Second, our results show that information based on standardized test scores can assist teachers in giving better recommendations, and its provision should, therefore, be considered.
Third, a large literature (see footnote 1) has demonstrated, in different contexts, that teachers’ recommendations do not typically reflect only the previous academic achievement and the future ability potential of a student, but are also highly correlated with characteristics like SES, gender, race, and behavior in class. The bias affecting recommendations that are determined by the conscious or unconscious use of these variables by teachers is typically deemed unacceptable and has reinforced the opposition to school tracking in general. Our evidence suggests that test score information also has an impact on the fairness of recommendations with respect to protected attributes. We find that the provision of test scores induces more frequent upgrades of recommendations for immigrant and low SES students. We interpret these results as indicating that the provision of test scores information induces a larger shift in teachers’ posterior beliefs regarding these students. Together with the fact that high-SES students are more likely to be upgraded when originally recommended a vocational track and scoring below the cutoff, we conclude that test scores are a valuable tool for increasing the fairness of track recommendations.
Independent of track-advising, but no less importantly, our results contribute to the recent and growing literature on algorithm-assisted human decisions (see, for example, Imai and Jiang, 2022, Imai et al., 2023, Ben-Michael et al., 2024, and Rambachan, 2024). It is increasingly common for decision-makers in various fields (justice, medicine, finance, politics, and education, to name a few) to use big data processed by multiple algorithms to make high-stakes decisions in highly uncertain contexts. However, it is still unclear how to make the best use of these algorithmic and data-driven tools in human decision-making. Our results contribute to this literature by providing novel quasi-experimental evidence and by improving existing methods to assess algorithm-assisted human decisions.
References
- Alesina et al. (2024) Alesina, A., M. Carlana, E. La Ferrara, and P. Pinotti (2024). Revealing stereotypes: Evidence from immigrants in schools. American Economic Review 114(7), 1916–1948.
- Angrist and Rokkanen (2015) Angrist, D. J. and M. Rokkanen (2015). Wanna get away? regression discontinuity estimation of exam school effects away from the cutoff. Journal of the American Statistical Association 110(512), 1331–1344.
- Angrist et al. (1996) Angrist, J. D., G. Imbens, and D. Rubin (1996). Identification of causal effects using instrumental variables. Journal of the American Statistical Association 91(434), 444–455.
- Bach (2023) Bach, M. (2023). Heterogeneous responses to school track choice: Evidence from the repeal of binding track recommendations. Economics of Education Review 95, 102412.
- Batruch et al. (2023) Batruch, A., S. Geven, E. Kessenich, and H. van de Werfhorst (2023). Are tracking recommendations biased? a review of teachers’ role in the creation of inequalities in tracking decisions. Teaching and Teacher Education 123, 1–18.
- Ben-Michael et al. (2024) Ben-Michael, E., D. J. Greiner, M. Huang, K. Imai, Z. Jiang, and S. Shin (2024). Does ai help humans make better decisions? a methodological framework for experimental evaluation. arXiv preprint.
- Ben-Michael et al. (2024) Ben-Michael, E., K. Imai, and J. Zhichao (2024). Policy learning with asymmetric counterfactual utilities. Journal of the American Statistical Association 0(0), 1–14.
- Branson and Mealli (2019) Branson, Z. and F. Mealli (2019). The local randomization framework for regression discontinuity designs: A review and some extensions. arXiv preprint arXiv:1810.02761.
- Burgess and Greaves (2013) Burgess, S. and E. Greaves (2013). Test scores, subjective assessment, and stereotyping of ethnic minorities. Journal of Labor Economics 31(3), 535–576.
- Carlana and Fort (2022) Carlana, M. and M. Fort (2022). Hacking gender stereotypes: Girls’ participation in coding clubs. AEA Papers and Proceedings (112).
- Carlana et al. (2022a) Carlana, M., E. La Ferrara, and P. Pinotti (2022a). Goals and gaps: Educational careers of immigrant children. Econometrica 90(1), 1–29.
- Carlana et al. (2022b) Carlana, M., E. La Ferrara, and P. Pinotti (2022b). Implicit stereotypes in teachers’ track recommendations. AEA Papers and Proceedings (112), 409–414.
- Cattaneo et al. (2015) Cattaneo, M., B. R. Frandsen, and R. Titiunik (2015). Randomization inference in the regression discontinuity design: An application to party advantages in the u.s. senate. Journal of Causal Inference 3(1), 1–24.
- Cattaneo et al. (2020a) Cattaneo, M. D., N. Idrobo, and R. Titiunik (2020a). A Practical Introduction to Regression Discontinuity Designs: Extensions. Cambridge University Press.
- Cattaneo et al. (2020b) Cattaneo, M. D., N. Idrobo, and R. Titiunik (2020b). A Practical Introduction to Regression Discontinuity Designs: Foundations. Cambridge University Press.
- de Ree et al. (2025) de Ree, J., M. Oosterveen, and D. Webbink (2025). The quality of school track assignment decisions by teachers. https://arxiv.org/abs/2304.10636v2.
- Ding and Lu (2017) Ding, P. and J. Lu (2017). Principal stratification analysis using principal scores. Journal of the Royal Statistical Society Series B: Statistical Methodology 79(3), 757–777.
- Driessen et al. (2008) Driessen, G., P. Sleegers, and F. Smit (2008). The transition from primary to secondary education: Meritocracy and ethnicity. European Sociological Review 24(4), 527–542.
- Eckles et al. (2025) Eckles, D., N. Ignatiadis, S. Wager, and H. Wu (2025). Noise-induced randomization in regression discontinuity designs. Biometrika 112(2).
- Falk et al. (2025) Falk, A., F. Kosse, and P. Pinger (2025). Mentoring and schooling decisions: Causal evidence. Journal of Political Economy. Forthcoming.
- Feller et al. (2017) Feller, A., F. Mealli, and L. Miratrix (2017). Principal score methods: Assumptions, extensions, and practical considerations. Journal of Educational and Behavioral Statistics 42(6), 726–758.
- Ferman and Fontes (2022) Ferman, B. and L. F. Fontes (2022). Assessing knowledge or classroom behavior? evidence of teachers’ grading bias. Journal of Public Economics 216, 104773.
- Forastiere et al. (2025) Forastiere, L., A. Mattei, J. M. Pescarini, M. L. Barreto, and F. Mealli (2025). Selecting subpopulations for causal inference in regression discontinuity designs. Annals of Applied Statistics. Forthcoming.
- Frangakis and Rubin (2002) Frangakis, C. E. and D. B. Rubin (2002). Principal stratification in causal inference. Biometrics 58(1), 21–29.
- Gerhenson et al. (2016) Gerhenson, S., S. B. Holt, and N. Papageorge (2016). Who believes in me? the effect of student-teacher demographic match on teacher expectations. Economics of Education Review (52), 209–224.
- Geven et al. (2021) Geven, S., Øyvind N. Wiborg, R. E. Fish, and H. G. van de Werfhorst (2021). How teachers form educational expectations for students: A comparative factorial survey experiment in three institutional contexts. Social Science Research 100, 102–599.
- Ichino et al. (2025) Ichino, A., A. Rustichini, and A. Zanella (2025). College, cognitive ability, and socioeconomic disadvantage: policy lessons from the uk in 1960-2004. CEPR Discussion Paper 17284.
- Imai and Jiang (2022) Imai, K. and Z. Jiang (2022). Principal fairness for human and algorithmic decision-making. Statistical Science.
- Imai et al. (2023) Imai, K., Z. Jiang, J. Greiner, R. Halen, and S. Shin (2023). Experimental evaluation of algorithm-assisted human decision-making: Application to pretrial public safety assessment. Journal of the Royal Statistical Society.
- Imbens (2003) Imbens, G. W. (2003). Sensitivity to exogeneity assumptions in program evaluation. American Economic Review 93(2), 126–132.
- Imbens and Angrist (1994) Imbens, G. W. and J. D. Angrist (1994). Identification and estimation of local average treatment effects. Econometrica 62(2), 467–475.
- Imbens and Rubin (2015) Imbens, W. and D. Rubin (2015). Causal Inference for Statistics, Social, and Biomedical Sciences. An Introduction. New York, NY, USA: Cambridge University Press.
- Ketel et al. (2023) Ketel, N., H. Oosterbeek, S. Sóvágó, and B. van der Klaauw (2023). The (un)importance of school assignment. Tinbergen Institute Discussion Paper TI 2023-076/V.
- Li et al. (2015) Li, F., A. Mattei, and F. Mealli (2015). Bayesian inference for regression discontinuity designs with application to the evaluation of italian university grants. The Annals of Applied Statistics 9(4), 1906–1931.
- Li et al. (2020) Li, F., A. Mercatanti, T. Mäkinein, and A. Silvestrini (2020). A regression discontinuity design for ordinal running variable: Evaluating central bank purchases of corporate bonds. Annals of Applied Statistics.
- Mattei et al. (2023) Mattei, A., L. Forastiere, and F. Mealli (2023). Assessing principal causal effects using principal score methods. In Handbook of Matching and Weighting Adjustments for Causal Inference, pp. 313–348. Chapman and Hall/CRC.
- Mattei and Mealli (2017) Mattei, A. and F. Mealli (2017). Regression discontinuity designs as local randomized experiments. Observational Studies 3(2), 156–173.
- Mealli and Mattei (2012) Mealli, F. and A. Mattei (2012). A refreshing account of principal stratification. The International Journal of Biostatistics 8(1).
- Mealli and Pacini (2013) Mealli, F. and B. Pacini (2013). Using secondary outcomes to sharpen inference in randomized experiments with noncompliance. Journal of the American Statistical Association 108(503), 1120–1131.
- Osikominu et al. (2021) Osikominu, A., G. Pfeifer, and K. Strohmaier (2021). The effects of free secondary school track choice: A disaggregated synthetic control approach. IZA Discussion Paper Series.
- Rambachan (2024) Rambachan, A. (2024). Identifying prediction mistakes in observational data. The Quarterly Journal of Economics 139(3), 1665–1711.
- Rubin (1980) Rubin, D. B. (1980). Discussion of “randomization analysis of experimental data in the fisher randomization test” by basu. Journal of the American Statistical Association 75, 591–593.
- van Huizen et al. (2024) van Huizen, T., M. Jacobs, and M. Oosterveen (2024). Teacher bias or measurement error? arXiv preprint arXiv:2401.04200.
- van Leest et al. (2021) van Leest, A., L. Hornstra, J. van Tartwijk, and J. van de Pol (2021). Test- or judgement based school track recommendations: Equal opportunities for students with different socio-economic backgrounds? British Journal of Education Psychology 91, 193–216.
- Van Po naar Vo (2025) Van Po naar Vo (2025). Veelgestelde vragen. Accessed: 2025-04-07.
Online Appendix
Do Test Scores Help Teachers Give Better Track Advice to Students
A Principal Stratification Analysis
Andrea Ichino, Fabrizia Mealli, Javier Viviens
November 6, 2025
Appendix to Section 3.1
| Track | ||||
| 1 | 2 | 3 | 4 | |
| V: BL | .014 | -.0187 | .1829 | .1482 |
| (.0535) | (.0527) | (.0702) | (.0734) | |
| V: BL/KL | .1297 | .0148 | .0582 | -.0281 |
| (.1073) | (.1366) | (.1373) | (.1531) | |
| V: KL | .3561 | .2637 | .3123 | .2355 |
| (.0846) | (.0866) | (.0763) | (.0776) | |
| V: KL/GT | .1322 | .172 | .0916 | -.0134 |
| (.121) | (.1195) | (.1359) | (.1507) | |
| V: GT | .1875 | .1448 | .1712 | .1534 |
| (.0341) | (.0338) | (.0397) | (.0405) | |
| V: GT/HAVO | .2028 | .1308 | .3215 | .2633 |
| (.096) | (.0981) | (.0854) | (.0898) | |
| A: HAVO | .142 | .097 | .1365 | .0997 |
| (.0348) | (.0317) | (.0367) | (.0387) | |
| A: HAVO/VWO | .3135 | .2067 | .1839 | .1142 |
| (.0694) | (.0764) | (.072) | (.0794) | |
| COVARIATES | NO | YES | NO | YES |
| COHORT | 2015 | 2015 | 2016 | 2016 |
Notes: the table reports, separately for each track and cohort, estimates of the correlation of the proxy for school leniency with the probability of being a Never-Taker. Specifically, it reports the coefficient from running the following regression using units with : . The row headings report the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The covariates included in the second and fourth columns are those described in Table 2.
Appendix to Section 3.4
Proof of Theorem 1 of Imai et al. (2023) in our context.
Average Principal Causal Effects (APCEs, equations 4, 11 and 8)
Derivation of the bounds of the ; equation (5)
Upper and Lower Bounds of the ; equations (6), (7), (9), (10) (12), and (13)
Lower bound of :
Upper bound of :
Lower bound of :
Upper bound of :
Lower bound of :
Upper bound of :
Identification and Estimation of and Principal Fairness under Unconfoundedness
Because the goal of fairness analysis is really to understand and compare the size of the ’s, it is crucial to be able to point-identify and estimate these quantities for fairness comparisons. A natural and useful starting point, that would be more or less plausible depending on the information cointained in the baseline covariates, is assuming Assumption 6 Unconfoundedness.
Consider the following principal scores, which represents the population proportions for each of the strata in each cell defined by the covariates:
Under assumptions 1, 2, 3, 4 and 6, we can identify these principal strata proportions as:
Theorem 10
Similarly, consider the following principal scores for a specific value of a protected attribute with ( is in general one of the covariates in ):
which can be still identified under Assumption 1, 2, 3, 4 and 7.
Appendix to Section 4.1
Estimates obtained without covariates
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0238 | .0238 | .651 | .036 | 1 |
| (.0171) | (.0162) | (.0137) | (.027) | - | |
| V: BL/KL | .0737 | .0737 | .2872 | .2643 | 1 |
| (.0298) | (.0288) | (.0226) | (.1019) | - | |
| V: KL | .1049 | .1049 | .7016 | .1496 | 1 |
| (.0159) | (.0159) | (.0105) | (.0229) | - | |
| V: KL/GT | .0221 | .0221 | .3946 | .0566 | 1 |
| (.0229) | (.0207) | (.0221) | (.0553) | - | |
| V: GT | .0216 | .0216 | .7624 | .0284 | 1 |
| (.0089) | (.0087) | (.0075) | (.0118) | - | |
| V: GT/HAVO | .0554 | .0554 | .5051 | .1072 | 1 |
| (.017) | (.0165) | (.0146) | (.0323) | - | |
| A: HAVO | .0376 | .0376 | .8068 | .0466 | 1 |
| (.0079) | (.0079) | (.0067) | (.0101) | - | |
| A: HAVO/VWO | .062 | .062 | .5431 | .1124 | 1 |
| (.016) | (.0156) | (.013) | (.0286) | - | |
| VOCATIONAL | .0469 | .0469 | .6489 | .0723 | 1 |
| (.0063) | (.0061) | (.005) | (.0097) | - | |
| ACADEMIC | .0453 | .0453 | .7236 | .0626 | 1 |
| (.0076) | (.0075) | (.006) | (.0106) | - | |
| ALL | .0463 | .0463 | .6792 | .0681 | 1 |
| (.005) | (.0049) | (.0039) | (.0073) | - |
Notes: The row headings reports the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (4), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower and upper bounds of the (in columns 4 and 5). The definitions of these two last statistics are in equations (6) and (7), respectively. The aggregations for the three last rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors for each quantity are computed using a block bootstrap at the school level with 1000 repetitions; each quantity in every iteration is calculated in the same way.
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0033 | .3188 | .3208 | .0097 | .0098 |
| (.0129) | (.0115) | (.0116) | (.0384) | (.0384) | |
| V: BL/KL | .0331 | .6843 | .7032 | .0469 | .0482 |
| (.0275) | (.0208) | (.0211) | (.0378) | (.0383) | |
| V: KL | .0035 | .2256 | .2367 | .0161 | .0166 |
| (.0091) | (.0086) | (.0092) | (.0364) | (.0379) | |
| V: KL/GT | .0792 | .5698 | .5813 | .1369 | .1399 |
| (.0283) | (.0192) | (.0186) | (.0478) | (.0477) | |
| V: GT | .0049 | .1923 | .193 | .0249 | .0251 |
| (.0054) | (.0057) | (.0057) | (.0272) | (.0272) | |
| V: GT/HAVO | .0433 | .4178 | .4269 | .0997 | .1014 |
| (.017) | (.0113) | (.0115) | (.0385) | (.0389) | |
| A: HAVO | 0 | .1283 | .1283 | 0 | 0 |
| (.0004) | (.004) | (.004) | (.0028) | (.0029) | |
| A: HAVO/VWO | .0293 | .3684 | .3798 | .0759 | .0783 |
| (.0146) | (.0108) | (.0113) | (.0368) | (.0375) | |
| VOCATIONAL | .0169 | .298 | .3038 | .0558 | .0569 |
| (.005) | (.004) | (.0041) | (.016) | (.0162) | |
| ACADEMIC | .0092 | .2041 | .2077 | .0445 | .0453 |
| (.0046) | (.0043) | (.0044) | (.0218) | (.0221) | |
| ALL | .0138 | .2599 | .2648 | .0522 | .0532 |
| (.0035) | (.0029) | (.003) | (.013) | (.0132) |
Notes: The row headings report the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (8), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower and upper bounds of the (in columns 4 and 5). The definitions of these two last statistics are in equations (9) and (10), respectively. The aggregations for the last three rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors for each quantity are computed using a block bootstrap at the school level with 1000 repetitions; each quantity in every iteration is calculated in the same way.
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0267 | .0277 | .6579 | .0403 | .9734 |
| (.0051) | (.005) | (.0137) | (.0078) | (.0319) | |
| V: BL/KL | .0261 | .0285 | .223 | .1212 | .9209 |
| (.0087) | (.0084) | (.0202) | (.0417) | (.1156) | |
| V: KL | .0663 | .0728 | .6584 | .1005 | .91 |
| (.0071) | (.0068) | (.0128) | (.0105) | (.0276) | |
| V: KL/GT | .0341 | .0356 | .3966 | .0861 | .9603 |
| (.0084) | (.0082) | (.0185) | (.0213) | (.0536) | |
| V: GT | .0376 | .0411 | .7896 | .0477 | .9174 |
| (.0038) | (.0037) | (.0069) | (.0048) | (.0261) | |
| V: GT/HAVO | .0677 | .0771 | .5177 | .131 | .8774 |
| (.0086) | (.0082) | (.0136) | (.0161) | (.0367) | |
| A: HAVO | .0486 | .054 | .8449 | .0575 | .8994 |
| (.0041) | (.0039) | (.0061) | (.0048) | (.027) | |
| A: HAVO/VWO | .0792 | .0886 | .5583 | .1416 | .8943 |
| (.0077) | (.0075) | (.0123) | (.0135) | (.0272) | |
| VOCATIONAL | .0466 | .0512 | .6512 | .0715 | .9097 |
| (.0027) | (.0026) | (.0048) | (.0041) | (.0149) | |
| ACADEMIC | .0583 | .0649 | .7544 | .0772 | .8973 |
| (.0037) | (.0036) | (.0057) | (.0049) | (.0188) | |
| ALL | .0513 | .0568 | .6931 | .074 | .9039 |
| (.0022) | (.0021) | (.0037) | (.0031) | (.0118) |
Notes: The row headings report the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (11), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower and upper bounds of the (in columns 4 and 5). The definitions of these two last statistics are in equations (12) and (13), respectively. The aggregations for the last three rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors for each quantity are computed using a block bootstrap at the school level with 1000 repetitions; each quantity in every iteration is calculated in the same way.
Appendix to Section 4.2
Derivation of the Loss Function
Appendix to Section 5
Identification of APCEs Without Assuming Exclusion Restriction
Without the exclusion restriction, we have to index the potential outcomes using both and , . Nevertheless, under Assumptions 7 and 8, we can normalize to and .
From equations (A3) and (A4), and under the exclusion restriction, we know that:
Replacing the exclusion restriction with assumptions 7 and 8, this becomes:
| (A17) |
| (A18) |
| (A19) |
| (A20) |
Similarly, from the proof in Appendix to Section 3.4, we known that
| (A21) | ||||
| (A22) |
Therefore, using the results from (A17-A20), it follows that
| (A23) |
and
| (A24) | ||||
| (A25) |
From equations (A22) and (A14), we have that:
Similarly, from equations (A17) - (A20),
Finally, we derive the bounds on and without the exclusion restriction. Following the same intuition, and using the results from the section with the exclusion restriction, we have that:
All in all, the estimators of the bounds of the APCEs are given by the sample analogs of the following expressions:
Lower bound of :
|
|
(A26) |
Upper bound of :
| (A27) |
Lower bound of :
| (A28) |
Upper bound of :
| (A29) |
Lower bound of :
| (A30) |
Upper bound of :
| (A31) |
Estimates of Scoring Above Cutoff for Never-Takers ()
| Track | ||
| 1 | 2 | |
| V: BL | -.0097 | .0258 |
| (.0299) | (.0332) | |
| V: BL/KL | .0336 | .1013 |
| (.0521) | (.0465) | |
| V: KL | .0492 | .036 |
| (.0216) | (.025) | |
| V: KL/GT | .0269 | -.0338 |
| (.044) | (.0444) | |
| V: GT | .0195 | -.0057 |
| (.0128) | (.0154) | |
| V: GT/HAVO | -.0302 | .0639 |
| (.0305) | (.0265) | |
| A: HAVO | .0242 | .0278 |
| (.0106) | (.0122) | |
| A: HAVO/VWO | .0064 | .0551 |
| (.0268) | (.0226) | |
| VOCATIONAL | .0278 | .0338 |
| (.0092) | (.009) | |
| ACADEMIC | .0196 | -.0215 |
| (.0106) | (.0099) | |
| ALL | .0245 | .0108 |
| (.007) | (.0068) | |
| COHORT | 2015 | 2016 |
| COVARIATES | YES | YES |
Notes: The row headings report the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. Columns 1 and 2 show the estimates of for each track and cohort, as explained in section 5.
Estimates of APCEs Without Assuming Exclusion Restriction
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0187 | .0187 | .6513 | .0289 | 1 |
| (.0094) | (.0076) | (.0164) | (.0147) | (.4627) | |
| V: BL/KL | .0083 | .0083 | .2764 | .0318 | 1 |
| (.019) | (.0128) | (.0239) | (.069) | (.6564) | |
| V: KL | .0555 | .0555 | .6907 | .0803 | 1 |
| (.0088) | (.0088) | (.0112) | (.0125) | (0) | |
| V: KL/GT | .0306 | .0344 | .3982 | .0731 | .8895 |
| (.0163) | (.0135) | (.021) | (.0414) | (.4774) | |
| V: GT | .0064 | .0072 | .7677 | .0082 | .8888 |
| (.0064) | (.0061) | (.009) | (.0124) | (.4177) | |
| V: GT/HAVO | .0285 | .0285 | .5041 | .0566 | 1 |
| (.0098) | (.0091) | (.0135) | (.0191) | (.337) | |
| A: HAVO | .0091 | .0091 | .8266 | .0111 | 1 |
| (.0029) | (.0028) | (.006) | (.0034) | (.1736) | |
| A: HAVO/VWO | .0313 | .0313 | .5387 | .0583 | 1 |
| (.0087) | (.0086) | (.0128) | (.0157) | (.1429) | |
| VOCATIONAL | .0232 | .0237 | .6484 | .0357 | .9772 |
| (.004) | (.0037) | (.0054) | (.0064) | (.0761) | |
| ACADEMIC | .0161 | .0161 | .7357 | .0219 | 1 |
| (.0034) | (.0033) | (.0059) | (.0046) | (.0193) | |
| ALL | .0203 | .0206 | .6838 | .0297 | .9844 |
| (.0027) | (.0025) | (.004) | (.004) | (.0538) | |
| COVARIATES | YES | YES | YES | YES | YES |
Notes: The row headings report the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (4), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower and upper bounds of the (in columns 4 and 5), without assuming the exclusion restriction, as given by equations equations (A26) and (A27), respectively. The aggregations for the three last rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors for each quantity are bootstrapped with 1000 repetitions; each quantity in every iteration is calculated in the same way.
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0028 | .3187 | .3187 | .0125 | .0125 |
| (.0088) | (.0159) | (.0158) | (.0283) | (.0283) | |
| V: BL/KL | .0805 | .6858 | .705 | .1138 | .1167 |
| (.0213) | (.0248) | (.0247) | (.0304) | (.0303) | |
| V: KL | .0433 | .2282 | .2392 | .1827 | .191 |
| (.0055) | (.0101) | (.0101) | (.0216) | (.0224) | |
| V: KL/GT | .0709 | .5681 | .5791 | .1224 | .1248 |
| (.0169) | (.022) | (.0225) | (.0292) | (.0293) | |
| V: GT | .0106 | .1908 | .1916 | .054 | .0542 |
| (.0063) | (.0077) | (.0077) | (.024) | (.024) | |
| V: GT/HAVO | .0632 | .4169 | .4262 | .1486 | .1522 |
| (.0081) | (.0131) | (.0132) | (.0183) | (.0185) | |
| A: HAVO | .0037 | .1176 | .1186 | .0315 | .0318 |
| (.0022) | (.0052) | (.0052) | (.018) | (.0181) | |
| A: HAVO/VWO | .0546 | .3667 | .3777 | .1455 | .1498 |
| (.0076) | (.0116) | (.0117) | (.0192) | (.0196) | |
| VOCATIONAL | .0323 | .2977 | .3034 | .1064 | .1084 |
| (.0036) | (.005) | (.005) | (.0117) | (.0119) | |
| ACADEMIC | .0198 | .1962 | .2004 | .0988 | .1009 |
| (.0028) | (.0052) | (.0052) | (.0136) | (.0138) | |
| ALL | .0272 | .2565 | .2616 | .104 | .1061 |
| (.0024) | (.0036) | (.0037) | (.0091) | (.0092) | |
| COVARIATES | YES | YES | YES | YES | YES |
Notes: The row headings report the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (4), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower and upper bounds of the (in columns 4 and 5), without assuming the exclusion restriction, as given by equations equations (A30) and (A31), respectively. The aggregations for the three last rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors for each quantity are bootstrapped with 1000 repetitions; each quantity in every iteration is calculated in the same way.
| Track | Numerator | Lower | Upper | Lower | Upper |
| bound | bound | bound | bound | ||
| denominator | denominator | ||||
| 1 | 2 | 3 | 4 | 5 | |
| V: BL | .0277 | .0286 | .664 | .0411 | .9776 |
| (.0053) | (.0053) | (.0172) | (.0078) | (.0266) | |
| V: BL/KL | .0351 | .0378 | .2867 | .1237 | .9394 |
| (.0103) | (.0099) | (.0253) | (.0338) | (.0884) | |
| V: KL | .0741 | .0811 | .7053 | .1049 | .9132 |
| (.0078) | (.0076) | (.0131) | (.0105) | (.0266) | |
| V: KL/GT | .0321 | .0337 | .3865 | .0825 | .9603 |
| (.0098) | (.0095) | (.0244) | (.0236) | (.0584) | |
| V: GT | .0382 | .0416 | .8012 | .0477 | .9197 |
| (.0039) | (.0038) | (.0095) | (.0048) | (.0256) | |
| V: GT/HAVO | .0691 | .0789 | .5453 | .1264 | .8734 |
| (.0091) | (.0086) | (.0157) | (.0152) | (.0395) | |
| A: HAVO | .0506 | .0558 | .8722 | .0579 | .9066 |
| (.0042) | (.004) | (.0059) | (.0047) | (.0249) | |
| A: HAVO/VWO | .0851 | .0946 | .591 | .1442 | .8999 |
| (.0087) | (.0085) | (.0144) | (.0134) | (.0258) | |
| VOCATIONAL | .049 | .0538 | .6731 | .0728 | .9115 |
| (.0029) | (.0027) | (.0062) | (.0041) | (.0148) | |
| ACADEMIC | .0615 | .068 | .7835 | .0785 | .9036 |
| (.004) | (.0039) | (.0063) | (.0049) | (.0176) | |
| ALL | .0541 | .0596 | .7179 | .0753 | .9079 |
| (.0024) | (.0023) | (.0044) | (.0032) | (.0113) | |
| COVARIATES | YES | YES | YES | YES | YES |
Notes: The row headings report the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. With reference to the estimand defined in equation (4), the table reports for the outcome and separately for each track, estimates of the numerator (in column 1), of the lower and upper bounds of the denominator (in columns 2 and 3), and of the lower and upper bounds of the (in columns 4 and 5), without assuming the exclusion restriction, as given by equations equations (A28) and (A29), respectively. The aggregations for the three last rows are performed as follows: point estimates of the numerator and of the lower and upper bounds of the denominator are created by weighing each track in the corresponding aggregate by the relative population in each track. For the lower and upper bounds of the , the aggregated estimates for the lower and upper bounds of the denominators are used directly. Standard errors for each quantity are bootstrapped with 1000 repetitions; each quantity in every iteration is calculated in the same way.
Estimates of the based on unconfoundedness
Table A9 shows the point estimates of from assuming unconfoundedness. As can be seen, the results show that the estimates for the three strata are similar in all tracks.
| Track | |||
| 1 | 2 | 3 | |
| V: BL | 0.0444 | 0.0481 | 0.0508 |
| (0.0071) | (0.0076) | (0.0089) | |
| V: BL/KL | 0.1360 | 0.1357 | 0.1311 |
| (0.0306) | (0.0236) | (0.0207) | |
| V: KL | 0.1560 | 0.1682 | 0.1807 |
| (0.0107) | (0.0108) | (0.0129) | |
| V: KL/GT | 0.1247 | 0.1288 | 0.1379 |
| (0.0206) | (0.0175) | (0.0182) | |
| V: GT | 0.0497 | 0.0556 | 0.0584 |
| (0.0040) | (0.0047) | (0.0057) | |
| V: GT/HAVO | 0.1530 | 0.1607 | 0.1659 |
| (0.0122) | (0.0117) | (0.0126) | |
| A: HAVO | 0.0574 | 0.0700 | 0.0776 |
| (0.0041) | (0.0051) | (0.0063) | |
| A: HAVO/VWO | 0.1531 | 0.1674 | 0.1770 |
| (0.0099) | (0.0103) | (0.0116) | |
| VOCATIONAL | 0.0969 | 0.1037 | 0.1090 |
| (0.0040) | (0.0039) | (0.0045) | |
| ACADEMIC | 0.0876 | 0.1007 | 0.1090 |
| (0.0042) | (0.0048) | (0.0057) | |
| ALL | 0.0931 | 0.1025 | 0.1090 |
| (0.0029) | (0.0030) | (0.0035) |
Notes: The row headings report the names of the secondary school tracks to which a student can be assigned in the Dutch system, ranked by level of difficulty. The last three rows of the table aggregate the Vocational (VMBO*), the Academic (HAVO*), and All tracks, respectively. The columns show point estimates obtained by assuming unconfoundedness. Standard errors for each quantity are bootstrapped with 1000 repetitions; each quantity in every iteration is calculated in the same way.
Training and Testing with Multiple Splits: A
Central Limit Theorem for Split-Sample Estimators
Abstract
As predictive algorithms grow in popularity, using the same dataset to both train and test a new model has become routine across research, policy, and industry. Sample-splitting attains valid inference on model properties by using separate subsamples to estimate the model and to evaluate it. However, this approach has two drawbacks, since each task uses only part of the data, and different splits can lead to widely different estimates. Averaging across multiple splits, I develop an inference approach that uses more data for training, uses the entire sample for testing, and improves reproducibility. I address the statistical dependence from reusing observations across splits by proving a new central limit theorem for a large class of split-sample estimators under arguably mild and general conditions. Importantly, I make no restrictions on model complexity or convergence rates. I show that confidence intervals based on the normal approximation are valid for many applications, but may undercover in important cases of interest, such as comparing the performance between two models. I develop a new inference approach for such cases, explicitly accounting for the dependence across splits. Moreover, I provide a measure of reproducibility for p-values obtained from split-sample estimators. Finally, I apply my results to two important problems in development and public economics: predicting poverty and learning heterogeneous treatment effects in randomized experiments. I show that my inference approach with repeated cross-fitting achieves better power than previous alternatives, often enough to find statistical significance that would otherwise be missed.
1 Introduction
As predictive algorithms become increasingly popular, using the same dataset to train and evaluate a new model has become routine across research, policy, and industry. Applications include estimating heterogeneous treatment effects from policy interventions, predicting future and contemporaneous outcomes, and building models for targeting. For example, the City of Chicago uses a machine learning (ML) algorithm to prioritize restaurant inspections most likely to find critical violations, while industry applications include personalized marketing ads and dynamic pricing. Despite their great potential, such procedures carry risks: algorithms may have low precision or negatively impact disadvantaged groups, for example by disproportionately inspecting restaurants owned by ethnic minorities. Thus, when training a new model, it is essential to evaluate its accuracy and distributional impacts before deploying it to perform or assist decision-making.
I study a setting in which an analyst (a researcher, policymaker, or industry practitioner) wishes to use the same dataset to both:
-
(i)
train a new model, and
-
(ii)
evaluate some of its properties, such as a measure of accuracy or fairness.
In the restaurant inspections example, step (i) consists of training the ML model to predict whether an establishment has a critical violation, and a parameter of interest in step (ii) is the rate of correct classifications or the mean squared error. My setting imposes no complexity or convergence rate restrictions, thus accommodating popular machine learning algorithms such as random forests and neural networks. Specifically, I consider using multiple splits of the sample for conducting both tasks, derive a central limit theorem (CLT) for split-sample estimators, and provide valid confidence intervals for the parameters of interest in step (ii).
Using the same observations for both steps (i) and (ii) creates a form of statistical dependence that makes inference challenging. This difficulty is often overcome by randomly splitting the sample into two, one part to train the model (training sample), and the other to evaluate its properties (evaluation sample). Since each task is conducted with separate data, such statistical dependence is not generated, and one can use standard approaches to inference. This procedure, however, has three drawbacks: it uses only part of the data for training the model, only part of the data for evaluating its properties, and different random splits can lead to widely different estimates and potentially affect statistical significance.
I study inference when averaging estimates across multiple sample splits, improving upon a regular 50/50 split by using more data for training, twice as much data for evaluation, and leading to better reproducibility properties. In empirical applications and Monte Carlo experiments, I show that these improvements often enable statistically significant conclusions that would otherwise be inconclusive. The main challenge of using multiple splits, for example with cross-fitting or repeated sample-splitting, is a new form of statistical dependence that complicates inference, since observations are used in both training and evaluation roles across different splits. I address this challenge by proving a new CLT for a large class of split-sample estimators, from which I develop new approaches to inference and a new reproducibility measure.
To illustrate the technical challenges and empirical implications of my results, consider as a simple running example one of my applications, the problem of predicting poverty. Accurate out-of-sample poverty prediction is central to Development Economics for understanding poverty dynamics and designing targeted policy interventions. In this setting, I focus on assessing predictive accuracy as the natural starting point, though my framework applies more broadly. Consider a sample of households, where are covariates and is an indicator for being below the poverty line measured 13 years after the covariates. The goal is to use the sample to (i) train a model to predict poverty by estimating , for example using a machine learning algorithm, and (ii) evaluate its accuracy, for example by estimating and calculating a confidence interval (CI) for the out-of-sample mean squared error (MSE)
where are out-of-sample observations from the same population as the sample. An alternative interpretation to is given as follows. Suppose a policymaker is given a mandate to use the data to train a new model , then apply it to predict for the entire population. reflects the accuracy that the trained model will have across the population. Note that is data-dependent, and is thus different from targeting a parameter for some fixed . In the policy prediction example, the researcher is not interested in the out-of-sample accuracy of an ideal but unknown model . Instead, they are interested in the accuracy of the actually estimated model .
In this context, CIs are often constructed using sample-splitting. If the entire sample is used for both tasks, standard CLTs do not apply to the average
since the summands are not independent. For example, and are dependent since is estimated with both and . A standard approach to handle this dependence is to impose complexity restrictions on how is estimated, such as Donsker conditions. These restrictions hold for simple procedures like ordinary least squares, but fail for complex machine learning algorithms frequently used in applied problems (chernozhukov2018double). Sample-splitting avoids this dependence without strong assumptions: randomly split into sets and of size for example , use data in to estimate and data in to calculate the average
| (1.1) |
Since the summands in are independent conditional on , standard CLTs apply, and the normal approximation gives a valid CI for . However, this procedure uses only half of the data for each task, and different random splits can lead to widely different estimates and potentially different conclusions about statistical significance.
Using multiple splits can improve upon these drawbacks but introduces a new challenge. Consider, for example, two-fold cross-fitting, where the roles of samples and are reversed and the final estimator averages the split-specific estimates. That is, estimate using and using , then calculate the final estimator
| (1.2) |
where . The estimand in this case is the MSE of an average model, as discussed in the next paragraph. While this estimator averages over all observations, standard CLTs do not apply due to a different form of statistical dependence: the first sum is not independent of the second since both use the entire dataset. My first main contribution is a central limit theorem for a large class of estimators that includes , which I use to construct valid CIs. In addition to using the entire sample for evaluation in 1.2, which reduces the variance of the asymptotic distribution compared to that of 1.1, more data can be used for training by increasing the number of folds. With 3 folds, for example, three models are trained, each using two-thirds of the data, with the remaining third used to evaluate the MSE. Finally, reproducibility is improved by repeating the splitting process multiple times and averaging the estimators over repetitions.
I show that is asymptotically normal under weak conditions, targeting its out-of-sample expectation
In the example above, is mathematically equivalent to the MSE of the average model, that is, , where
This happens anytime the outcome is binary, and holds for the MSE, mean absolute deviation, among others, including when averaging over multiple folds and repetitions. In the poverty prediction example, this means that a researcher or policymaker can use model for out-of-sample predictions, which will have MSE . For continuous outcomes, the researcher has two options. The first is to use a model that predicts a value in at random. This model has an out-of-sample MSE equal to . Alternatively, one could still use , which has the guarantee to perform better or equal than in terms of out-of-sample accuracy due to a risk-contraction property (for details, see Appendix A).
I make three main contributions. First, I prove a new central limit theorem for a large class of split-sample estimators under mild conditions. Specifically, I make no restrictions on the complexity of the models , or on their rates of convergence or algorithmic stability. For sample-average estimators, my CLT follows under a standard moments condition and assuming that converges to an arbitrary limit, at any rate. I show that the normal approximation yields a valid CI in many applications, but may fail to do so in important cases of interest, such as comparing the performance between two models or some instances when converges to zero. My second contribution builds on the CLT to develop a new inference approach that covers such cases, explicitly accounting for the dependence across splits. I focus on the case of comparing the performance between two models, and discuss how the arguments apply more broadly to other cases. Finally, I develop a reproducibility measure for p-values obtained from split-sample estimators. It addresses a common concern: another researcher using the same dataset, but different splits, may reach a different conclusion about statistical significance. For a given (large) number of repetitions of sample-splitting/cross-fitting, my measure quantifies p-value reproducibility, assessing whether the number of repetitions is sufficiently large to ensure reproducible inference.
Other contributions include a central limit theorem for split-sample empirical processes, which I use to prove my main central limit theorem, and may be of independent interest. I also apply this CLT to develop a new ensemble method for learning features of heterogeneous treatment effects in randomized experiments, following the framework of chernozhukov2025generic. The ensemble method improves on previous alternatives by using the entire sample for evaluation, more data for training, and combining multiple machine learning predictors, potentially improving power and avoiding issues of multiple hypothesis testing.
I apply my inference approaches to two important problems in development and public economics: predicting poverty and learning heterogeneous treatment effects in randomized experiments. In the first application, using a panel from Ghana (ghanapaneldataset) and Monte Carlo experiments, I show that repeated cross-fitting outperforms previous alternative approaches in detecting predictive power for being below the poverty line 13 years ahead. In the second application, I revisit the experiment of karlan2007does on charitable giving and conduct Monte Carlo simulations, showing in both cases that my ensemble method achieves improved power for detecting heterogeneous treatment effects compared to previous alternatives.
The rest of the paper is structured as follows. Section 1.1 summarizes related work, and Section 2 establishes the setup and notation. Section 3 establishes a central limit theorem for split-sample Z-estimators, and Section 4 develops inference using the normal approximation and for comparing two models. I introduce my measure of reproducibility in Section 5. Finally, I implement my inference approaches in two empirical applications: predicting poverty in Ghana in Section 6 and heterogeneous treatment effects in charitable giving in Section 7. Section 8 concludes. Proofs are delayed to Appendix B.
1.1 Related Work
I join a large literature on inference using multiple splits of the data. The literature on risk estimation via cross-validation provides related results establishing asymptotic normality for sample-average estimators based on multiple splits. Like my approach, these CLTs target data-dependent parameters, but rely on different types of assumptions, and focus on the specific case of sample-averages. dudoit2005asymptotics consider estimators that average over all possible splits or cross-splits of the sample, assume bounded loss function and require to be loss consistent for a risk minimizing function, whereas I assume that converges to any limit. austern2020asymptotics; bayle2020cross provide CLTs under rate assumptions on the algorithmic stability of . bayle2020cross provides two CLTs using estimators based on a single repetition of cross-fitting, one relying on rate condition for algorithmic stability, and the second requires a “conditional variance convergence” assumption that they verify using rates for loss stability. My result does not require verifying loss stability conditions, which may not be satisfied in some high-dimensional settings (bates2024cross), and my result allows for any ML algorithm as long as converges to any limit at any rate, in the sense established in Section 3. ledell2015computationally provides a CLT for the particular case of estimating the area under the curve (AUC) measure via cross-validation.
A different class of related results show asymptotic normality using cross-fitting when targeting parameters that are not data-dependent. These approaches require stronger conditions on that may not hold in general for nonparametric models with more than a handful of covariates, such as requiring to converge in probability at some specified rate (luedtke2016statistical; belloni2017program; chernozhukov2018double; benkeser2020improved; imai2025statistical). Theorem 3.1 requires no complexity restrictions, and assumes converges in probability to any limit at any rate.
In the context of learning features of heterogeneous treatment effects in randomized trials, chernozhukov2025generic proposed taking the median of estimators, confidence intervals and p-values across splits, similarly focusing on a data-dependent parameter, without relying on complexity or rate assumptions. wager2024sequential proposed a modified, sequential approach based on luedtke2016statistical, and chernozhukov2025reply suggested taking the median over repetitions of the sequential approach. In the same framework, imai2025statistical developed inference using cross-fitting, relying on rate assumptions. My results build on this literature in four main dimensions, relying on the mild assumption that the trained models converge to any limit, at any rate. First, my estimator uses all observations on the role of evaluation sample, leading to a smaller variance of its asymptotic distribution. Second, my approach does not exhibit a tradeoff between training and evaluation sample sizes, allowing for more data to be used to train the models. Third, I provide inference for an interpretable estimand under no rate assumptions on the trained models, while chernozhukov2025generic require a rate of concentration condition for coverage of their median estimand, which requires for example the training sample to be large relative to the evaluation sample. Finally, I introduce a new ensemble method that combines predictions from multiple ML algorithms, potentially improving statistical power for detecting HTE, and avoiding issues of multiple hypothesis testing.
The literature on learning features of heterogeneous treatment effects with multiple splits is a subset of a broader literature on aggregating potentially dependent p-values (ruger1978maximale; ruschendorf1982random; meng1994posterior; meinshausen2009pvalues; gasparin2025combining). These approaches similarly apply to data-dependent parameters under weak conditions, and typically target size control under the worst data generating process, thus being conservative in general. My confidence intervals are asymptotically exact and improve statistical power.
Finally, my work is complementary to ritzwoller2023reproducible. They provide a stopping algorithm for determining how many times to repeat sample-splitting to ensure reproducibility of averages over split-sample statistics, for example, the average point estimate. I take the number of repetitions as given and focus on inference, providing a measure of reproducibility for p-values calculated using multiple splits. While ritzwoller2023reproducible uses an asymptotic framework that takes the sample size fixed and assumes a small threshold for the variability of the average split-sample statistic, my framework uses a growing sample size and number of repetitions.
2 Setup
I consider a setup in which an analyst (a researcher, policymaker, or industry practitioner) wishes to use a dataset to both (i) train a new model and (ii) evaluate some of its properties. This is typically the case when one wants to train a new model to automate or assist decision-making, for example using a machine learning algorithm. Since these algorithms, despite their potential, may perform poorly in practice or have disparate performance across groups, it is often important to assess their accuracy and fairness. I use the term fairness as in the algorithmic fairness literature (chouldechova2018frontiers; cowgill2020algorithmic; barocas2016big) and provide example measures below. I state the analyst’s goals, discuss the parameter of interest with examples, and introduce the split-sample procedures I study.
The first goal of the analyst is to train a model using an algorithm and a dataset , where each is an iid draw from a distribution . I use train to denote the fitting/estimation of using , training algorithm (or just algorithm) for the procedure that maps to , and estimated model (or just model) for the realized function . For example, one can use the Random Forests algorithm to train a new model . The sets and are in principle unconstrained, and depends on the choice of training algorithm. Typically, the analyst will estimate by minimizing some loss function. My setup, however, is agnostic to the choice of training algorithm, and all results hold for any algorithm as long as converges to an arbitrary limit at any rate, in the sense defined in Section 3.
The second goal of the analyst is to use to evaluate some performance property of , denoted . Specifically, the analyst wishes to construct a confidence interval for such that, for ,
| (2.1) |
where the probability accounts for the randomness in both and . can be, for example, a measure of accuracy or fairness of . The parameter of interest, , depends on the data through the estimated model . This differs from the standard semiparametric literature, where the parameter of interest takes the form of for some nuisance function . In the applications I consider, the object of interest is since the analyst/policymaker is interested in the accuracy or fairness of the specific estimated model they can actually implement. This is different from evaluating the performance of an ideal but unknown model .
I provide three examples of such parameters of interest, and then discuss related cases in the literature where the parameter of interest is data-dependent.
Example 1 (Mean Squared Error).
The individual observations are , where is an outcome and is a set of covariates with . is a function that predicts from . In the poverty prediction example discussed in Section 1 and developed in Section 6, is a binary indicator for whether a household is below the poverty line, and is an estimate of . The mean squared error (MSE) of model is
A related estimand, also covered by my framework, is the difference in MSE between two groups, which is a measure of fairness (see, e.g., auerbach2024testing). Let , where indicates group membership (e.g., racial groups). Then,
| (2.2) |
quantifies how much better performs for one group relative to the other, where is the conditional distribution of given . ∎
Example 2 (Classification Rate - Binary Classifiers).
The individual observations are , where is a binary outcome and is a set of covariates, for some . is a function that predicts whether or . The correct classification rate of model is
is a measure of accuracy, corresponding to the probability that classifies an observation correctly.
Similar to 2.2, the difference in classification rate between two groups is a measure of fairness. ∎
Example 3 (Classification Rate - Probabilistic Classifiers).
The previous example can be generalized to accommodate probabilistic classifiers , with being the estimated probability that given . The correct classification rate is given by
This is equivalent to the probability (taking fixed) that , where with probability , independent of . A measure of fairness can be defined similar to 2.2. ∎
There are several examples in the literature where the parameter of interest takes the form of a data-dependent . This occurs anytime the hypothesis of interest is selected only after the data has been used (dawid1994selection). An important case is the approach of chernozhukov2025generic to inference on features of heterogeneous effects in randomized trials, which I revisit in Section 7. Other examples include evaluating the impacts of data-driven algorithms in policy applications (potash2015predictive; kuzmanovic2024causal), measuring welfare gains generated from data-driven rules (kitagawa2018should; ida2024dynamic), and the “inference on winners” framework of andrews2024inference.
My setup also applies to some cases where the parameter of interest is not data dependent, but is estimated using split-sample techniques. For example, in fava2025predicting I develop an approach to inference on points of the distribution of treatment effects. Although the parameter of interest, , is not data dependent, I incorporate covariate-adjustment terms that yield bounds . Inference on can then be derived from the asymptotic distribution of split-sample estimators , centered around the bounds . Other examples where is informative about a parameter include learning the mean outcome under an optimal treatment regime (shi2020breaking; fischer2024bridging), and averages of intersection bounds (ji2024model; semenova2025debiased). Another type of application is when does not depend on , yet estimating leads to some better properties. This is the case of adding a covariate-adjustment term for learning the average treatment effect in a randomized trial, as I discuss in Appendix I.
I consider four split-sample procedures for attaining the analyst’s goals: 1) sample-splitting, 2) cross-fitting, 3) repeated sample-splitting, and 4) repeated cross-fitting. First, I introduce some notation. Let and the dataset be an iid sample of . I denote the training algorithm by , a function that takes a sample of size and returns a value . The dependence on is suppressed in the notation of . For any subsample , .
Sample-splitting consists of taking a random subsample of size , using its complement to train the model , and calculating from for the parameter . Cross-fitting consists of partitioning into roughly equal-sized subsets (folds) , at random. For , train a model , that is, using all observations except those in fold . Each model is trained from observations when is a multiple of . In Section 3, I discuss different ways to aggregate the models into an estimand , where , and the construction of a confidence interval for . I consider fixed as .
Repeated sample-splitting and cross-fitting consist of repeating the procedures above times. That is, for repeated sample-splitting, take independent, random subsamples of of size , , and train models . For repeated cross-fitting, take independent, random partitions of into roughly equal-sized folds, , where each forms a partition of . For each subsample , train a model using all observations except the ones in , giving a total of models. I discuss different ways to aggregate the multiple splits in Section 3.
I give a unified notation to the four split-sample procedures described above. Let denote a collection of random splits of the sample, where each split can be either:
-
•
Sample-splitting: and with of size , or
-
•
-fold cross-fitting: and forms a partition of .
I use to denote sample-splitting for convenience. means that consists of one subsample, of size chosen by the researcher. For cross-fitting, I assume folds are equal-sized if is a multiple of , and have sizes and otherwise, and define . Define , . With this notation, denotes sample-splitting and denotes cross-fitting. I allow to grow as increases, and denote
This notation unifies the four split-sample procedures described previously, as shown in Table 1.
| Number of folds () | ||||
|
1 | |||
| 1 | Sample-splitting | Cross-fitting | ||
| Repeated sample-splitting | Repeated cross-fitting |
I use the term multiple splits to denote any of the three procedures that use more than one split ( and/or ). In all cases, I assume that the splits are taken at random uniformly over all possible splits or cross-splits. Although the number of possible splits is finite for any given , I consider that the repetitions are taken independently, with repetition. This assumption reflects common practice, as the computationally feasible number of repetitions is usually much smaller than the total number of possible splits, so that the probability of taking two identical splits is negligible.
I compare the four split-sample procedures in terms of statistical power, modeling power, and reproducibility properties in Section 3.2.
3 CLT for Split-Sample Z-Estimators
I prove a central limit theorem for split-sample Z-estimators, defined as zeroes of empirical moment equations. Z-estimators are a large class of estimators which include averages, linear regressions, and most M-estimators, since the parameter value that maximizes some objective function is the same that sets its partial derivatives to zero. This CLT can be used off-the-shelf in many applications, including the poverty prediction application in Section 6. First, in Section 3.1, I define split-sample Z-estimators and Z-estimands, introduce the assumptions used, and state the CLT. Finally, in Section 3.2, I compare the four split-sample procedures (sample-splitting, cross-fitting, repeated sample-splitting, and cross-fitting).
I provide a more accessible exposition for the particular case of sample average estimators, such as the MSE (Example 1), in Appendix D. I prove a new CLT for split-sample empirical processes in Appendix E, which I use to prove my CLT for Z-estimators and may be of independent interest.
3.1 Main Result
Since Z-estimators can be nonlinear, unlike the mean squared error (Example 1), different approaches to aggregating multiple splits lead to different estimators and estimands. I discuss three such approaches. Let be the Euclidean norm, be measurable functions for and ( is defined as in Section 2), and . For , let , , and be the Jacobian matrix of . As in Section 2, let denote a collection of splits with repetitions and folds, and let .
The first type of estimand is an average across split-specific estimands:
| (3.1) |
where for is the unique solution for in , i.e.,
3.1 consists of solving the moment condition for each split , and averaging over the split-specific estimands. The Z-estimator for 3.1 is
| (3.2) |
where . This approach is analogous to the DML1 estimator in chernozhukov2018double.
The second type of estimand solves the average of the moment conditions. That is, uniquely solves
| (3.3) |
The associated Z-estimator is given by
| (3.4) |
This approach is analogous to the DML2 estimator in chernozhukov2018double.
Finally, the third type of estimand is a hybrid of the previous two approaches. It solves the moment condition at each cross-split of the sample, and averages across repetitions. That is,
| (3.5) |
where uniquely solves
The associated Z-estimator is given by
| (3.6) |
where
| (3.7) |
In this approach, each uses the whole sample both for calculating and the average in 3.7, and the final estimator is the average of the cross-fitting estimators across repetitions. Note that if , if , and if . The estimators are not assumed to be unique, but I assume the estimands and the limit of the estimators to be unique.
For a concrete example, I consider below the particular case of sample-averages, as in the example of calculating the MSE for poverty prediction (Example 1).
Example 4 (Split-sample averages).
Let for some known . In this case, the three estimators coincide:
for any , and the estimand is
∎
can be interpreted as the value of that solves the moment condition for a randomized function that takes value across uniformly at random. That is, 3.3 is equivalent to
where and takes value in uniformly at random. If, for example, each is a probabilistic classifier as in Example 3, can be interpreted as solving the moment condition for a randomized rule that predicts a positive classification with probability .
I provide a CLT for the three estimators . Below, I establish my main regularity conditions.
Assumption 3.1.
For some , the following conditions hold:
-
(i)
For some ,
-
(ii)
There exists such that for , , and every ,
uniformly in .
∎
Assumption 3.1(i) is a standard moments condition for CLTs. Assumption 3.1(ii) is a mild stability condition on . Importantly, is allowed to converge at any rate and to any limit , which may depend on . It holds, for example, if
pointwise for every and . This condition is more interpretable but stronger than required (see Assumption E.2 in Appendix E). Assumption 3.1(ii) differs from the typical approach in the double machine learning literature where faster convergence rates (often ) are required for nuisance functions (e.g., chernozhukov2018double). The key difference between the two approaches is that I target a different, data-dependent parameter.
My CLT relies on the additional technical regularity conditions Assumption B.1, which I delay to Section B.1. This assumption adapts standard conditions for consistency and asymptotic normality of Z-estimators to the context of split-sample estimators (e.g., van2000asymptotic; van2023weak). This is a weak assumption that holds in many settings, and it mostly concerns the choice of . First, it assumes that the classes are Donsker, which restricts complexity along but does not restrict the complexity of . Second, it requires to nearly solve the moment conditions, and to be unique and well-separated zeroes of the population moment conditions. Finally, it assumes that is differentiable in for , and the Jacobian is continuous in around . Assumption B.1 holds, for example, in the case of sample averages (Example 4), or the “fraction in poverty by tercile” estimator in the poverty prediction application in Section 6.
Theorem 3.1 is the first main result of this paper.
Theorem 3.1.
(CLT for split-sample Z-estimators)
Let Assumptions 3.1 and B.1 hold.
Then, for ,
uniformly in , where
and
∎
The limiting variance is the product of two terms, the scalar and a positive semidefinite matrix. The choice of split-sample procedure only affects through , which acts as a variance-inflating term since . When using a single split (), the asymptotic variance is inflated by , where is the fraction of the sample used to evaluate (as opposed to training ). This occurs because is calculated from only observations. When using repeated sample-splitting ( and ), is an average of and with weights proportional to and . This occurs since each observation is picked a different number of times across splits for calculating . A larger number of repetitions leads to more balance in how often each observation is selected, and decreases with larger . In fact, if , there is perfect balance in large samples and . When using cross-fitting (), all observations are used an equal amount of times, and . For intuition on this result, consider the particular case of sample averages (Example 4). In this case,
is the same for . If , averages over observations. If and , different observations are picked by splits a different, random amount of times, and larger leads to more balance. If , is an average over all observations, the entire sample is used equally, and the variance-inflation term is minimum. Hence, the asymptotic variance is minimized using cross-fitting with any number of folds and repetitions .
Theorem 3.1 appears to be new. The literature on risk estimation via cross-validation provides related results establishing asymptotic normality for sample average estimators based on multiple splits. Like my approach, these CLTs target data-dependent parameters, though they rely on different types of assumptions, and focus on the specific case of sample-averages. dudoit2005asymptotics consider estimators that average over all possible splits or cross-splits of the sample, assume bounded loss function and requires to be loss consistent for a risk minimizing function, whereas I assume converges to any limit. (austern2020asymptotics; bayle2020cross) provide CLTs under rate assumptions on the algorithmic stability of . bayle2020cross provides two CLTs using estimators based on a single repetition of cross-fitting, one relying on rate condition for algorithmic stability, and the second requires a “conditional variance convergence” assumption that they verify using rates for loss stability. My result does not require verifying a loss stability condition, which may not be satisfied in some high-dimensional settings (bates2024cross), and my result allows for any ML algorithm as long as Assumption D.1(ii) holds. ledell2015computationally provides a CLT for the particular case of estimating the area under the curve (AUC) measure via cross-validation.
A different class of related results are CLTs with cross-fitting for parameters that are not data-dependent. These approaches require stronger conditions on , such as requiring to converge in probability at some specified rate, typically (luedtke2016statistical; chernozhukov2018double; benkeser2020improved; imai2025statistical). Theorem 3.1 requires no complexity restrictions, and assumes converges in probability to any limit at any rate.
A central limit theorem for the class of split-sample Z-estimators appears to be new. The characterization of the asymptotic variance, specifically how the variance-inflating term depends on the number of splits when , also appears to be new. The proof uses a new CLT for split-sample empirical stated in Appendix E, which also appears to be new and may be of independent interest.
Remark 3.1.
In the double machine learning context, which targets a different parameter and uses , simulation evidence (chernozhukov2018double) and theoretical results (velez2024asymptotic) suggest using DML2 over DML1. It is unclear whether similar arguments hold for comparing and , and how they compare with . Exploring theoretical and empirical properties of the three methods is an interesting direction for future research. ∎
3.2 Comparison of Split-Sample Procedures
I compare the four split-sample procedures (sample-splitting, cross-fitting, repeated sample-splitting, and repeated cross-fitting) in terms of statistical power, modeling power, reproducibility, and computation time.
Cross-fitting and repeated cross-fitting, as well as repeated sample-splitting with , all exhibit the highest statistical power since they all minimize the variance of the asymptotic distribution in Theorem 3.1. Repeated sample-splitting with comes second, and sample-splitting yields the largest variance.
I say that an estimator has better modeling power than another if the models in are trained using larger datasets. Using more data for training typically leads to models with smaller expected loss, as I formalize in Appendix C. For sample-splitting or repeated sample-splitting, modeling power increases by picking a smaller (and ), so that more data is used to train each . However, if , a smaller leads to smaller statistical power, since fewer data are used as evaluation sample at each split. When using cross-fitting, modeling power increases with , since . In this case, the returns to increasing are diminishing. For example, if , is calculated with % of the sample, and this fraction raises to % with . If , however, the fraction only raises by another %. Although a large value of or small value of (when ) lead to better modelling power, my asymptotic framework takes these quantities as fixed. This means that the quality of the asymptotic approximation may be poor if is large (or small) relative to the sample size. For example, my asymptotic framework does not accommodate for leave-one-out cross-fitting, that is, .
I formalize the fact that increasing leads to better reproducibility properties in Section 5. For example, as increases, it becomes more likely that two researchers using the same dataset but different random splits will reach the same conclusion about statistical significance of . Although I make no formal comparison between the cases and in terms of reproducibility, I note that ritzwoller2023reproducible documented the difference in variance between repeated sample-splitting and cross-fitting in an earlier draft.111This discussion appears in the second version at https://arxiv.org/abs/2311.14204 (dated December 9, 2023). Comparing cross-fitting with repetitions to sample-splitting with repetitions, they argued that in principle it is possible that split-sample estimators have smaller variance conditional on data when instead of , but show empirical evidence that cross-fitting typically leads to better reproducibility.
Table 2 summarizes the comparison of the four procedures.
| Procedure |
|
|
Reproducibility |
|
||||||
| Sample-splitting | Low | Low | Low | Low | ||||||
| Cross-fitting | High | High | Medium | Medium | ||||||
| Repeated sample-splitting | Med/High∗ | Med/High∗ | High∗∗ | High | ||||||
| Repeated cross-fitting | High | High | High∗∗ | High |
-
•
∗High if , medium if .
-
•
∗∗Whether repeated sample-splitting or cross-fitting dominates depends on application.
-
•
Modeling power considers the trade-off with statistical power: for sample-splitting and repeated sample-splitting with , increasing modeling power requires decreasing statistical power. Computation time and reproducibility columns compare repeated cross-fitting with repetitions to repeated sample-splitting with repetitions.
The choices of , , and (when ) involve tradeoffs. Statistical power is maximized when or (Appendix D), and the reproducibility properties improve with larger and are ambiguously affected by , despite empirical evidence that usually leads to better properties (ritzwoller2023reproducible). For and , there is a tradeoff between statistical and modelling powers, unlike with cross-fitting. A larger is always beneficial in terms of reproducibility (and statistical power when ), but this comes at the cost of higher computation time. Hence, I recommend choosing as large as computationally convenient, and but small, since that provides valid asymptotic inference, maximum statistical power, and likely better reproducibility properties. In Section 5, I provide a measure to assess whether a given is sufficiently large to ensure reproducibility of p-values calculated from split-sample Z-estimators.
4 Inference on Split-Sample Estimands
The CLT in Theorem 3.1 can be directly applied to conduct inference on many split-sample estimands. However, confidence intervals based on the normal approximation may fail to cover at the nominal level in some important cases of interest. First, in Section 4.1, I consider inference when the normal approximation is asymptotically exact, and discuss why this approximation may not be precise in some contexts. Then, in Section 4.2, I propose a new approach for the particular cases of inference on comparisons between models, which explicitly accounts for the dependence across splits.
I discuss in Section 4.1 that a typical case when the normal approximation CI may have coverage probability smaller than nominal is when the variance of a moment function is allowed to be zero in the limit. I provide a general method for inference that covers this case in Appendix F, by exploring the faster-than- convergence rate of the empirical moment functions and introducing a tuning parameter. I also discuss in Sections 4.1 and 4.2 that although Section 4.2 considers the specific case of comparing two models, the arguments developed in that section apply more broadly, covering other cases such as the Generic ML approach of chernozhukov2025generic (see Section B.5.5).
4.1 Inference from Normal Approximation
Consider the problem of conducting inference on , where is any of the split-sample Z-estimands in Section 3, and is any scalar differentiable function with row-vector of partial derivatives . This encompasses many cases of interest, for example when is a subset of the vector or a linear combination of its entries, as in the application of Section 6. An application of Theorem 3.1 and the delta-method yields
| (4.1) |
where is a Z-estimator as in 3.4, and
If , one can calculate the plug-in estimator
| (4.2) |
where is given in B.11 in Section B.2, and the confidence interval
| (4.3) |
contains with probability approaching , where is the -th quantile of the standard normal distribution.
Theorem 4.1.
(Asymptotic Exactness of Normal Approximation CI)
Let the conditions of Theorem 3.1 hold, be positive definite, assume there exists an estimator such that
uniformly in , and that . Then, for any sequence ,
∎
Theorem 4.1 assumes the existence of a consistent estimator of . If is differentiable in , this assumption is satisfied by the plug-in estimator defined in B.10 in Section B.2 under a uniform integrability condition on this derivative. Otherwise, consistent estimators of can typically be constructed on a case-by-case basis (hansen2022econometrics). Note that the probability in Theorem 4.1 is taken over both the random estimand and the CI.
Theorem 4.1 implies that 4.3 contains with probability approaching in many settings. However, in some cases, 4.3 may not cover with nominal probability, as illustrated in the two examples below.
Example 5.
Example 6.
Consider a dataset with covariates , outcome , and moment function
that is, for each subsample , is the OLS estimator for in the regression
using observations . Focusing on the slope coefficient, the final estimator can be, for example,
If is positive definite, where , the conditions of Theorem 3.1 are met, and 4.3 contains with probability approaching . However, if is constant in , is not invertible, the conditions of Theorem 3.1 are not met, and 4.3 may contain with probability below the nominal level. ∎
Examples 5 and 6 have two features in common: the normal approximation CI may undercover only when is constant, and one of the empirical moment equations evaluated at the true parameter converges to zero at a rate faster than :
| (4.4) |
where is the -th entry of the vector . In Section 4.2, I develop an approach that can be used to test whether is constant, and although I focus on the particular case of comparing the performance between two models, the arguments apply more broadly and could be used to provide a valid CI for the problems in Examples 5 and 6 under the same conditions of Theorem 4.1. In Appendix F, I establish a general approach to inference on that allows 4.4 to happen. The approach explores the faster-than- convergence rate to provide an asymptotically uniformly valid CI by introducing a tuning parameter.
4.2 Inference on Model Comparisons
In several applications, the goal is not only to create a new model and assess some property , but to compare such properties between two models. For example, if is a measure of accuracy such as the mean squared error (Example 1), one might want to infer if has better performance than a baseline model that predicts the sample mean of for all observations. This is the case in the application of Section 6, where the goal is to assess whether a random forest model has predictive power for poverty, that is, whether it achieves smaller MSE than using the sample average. Alternatively, one might want to compare the performance of using different machine learning algorithms, such as training with neural networks versus random forests. I show that the CLTs of the previous sections give a valid inference approach when both models do not have similar performances in large samples. However, if the models have similar performance, the asymptotic distribution of the difference in performance is degenerate at the rate, and CIs based on the asymptotic approximation may fail to control size. In this section, I build on the CLT of Section 3 to develop an inference method that is valid for both cases. Although this section focuses on the particular case of comparing two models, I discuss in the end of Section 4.1 that the arguments developed in this section apply more broadly.
The setting is as follows. denotes any of the estimators of Section 3, assumed to be a scalar () (alternatively, one could consider a scalar transformation as in Section 4.1). I refer to the parameter (defined analogously) as a performance measure for expositional convenience, though the results apply more generally. I focus on comparing to the performance of a baseline model computed using the entire sample, that is, without forms of sample-splitting. is assumed to come from a parametric model, and it can be, for example, the sample average in Examples 3 and 1. Following the notation of Section 3, is the unique solution for in , i.e.,
Similarly, the estimator is a (near) zero of the empirical moment condition,
In Appendix G, I discuss how to extend the current setting for comparing to the performance of another model computed with the same split-sample approach as . Let
be a collection of splits of the sample, that is, a vectorization of defined in Section 2. Notice that each is associated with a model , as in 3.1.
To see the challenge of conducting inference based on , consider a simplified setting where each (as in 3.2) is a sample average, that is, for some and . The CLT in Theorem 3.1 gives
and the normal approximation gives an asymptotically valid CI for . Similarly, if converges to some model ,
and these two results can be combined to construct a CI for based on a normal approximation. However, if the baseline model is the same as , both estimators have the same limit, the difference
| (4.5) |
has a degenerate limit in probability, and the CLT of Section 3 does not inform how to compute a CI for .
First, I develop a test for whether any of the models perform better than , then show how this test can be used to construct a CI for . Both results build on my CLT for Z-estimators.
4.2.1 A Multivariate One-sided Test for Model Differences
From 4.5, the asymptotic distribution of centered around the parameter of interest is degenerate at the rate if . Yet, for each split ,
| (4.6) |
has a non-degenerate limit since the first average does not include observations . I explore this fact to construct a test of whether any model has better performance than , then develop a CI for in the following subsection.
Consider the hypothesis test
| (4.7) |
If is a measure of performance such as the mean squared error, having means that performs better than . The hypotheses and depend on due to the data-dependent parameter of interest . Testing such hypotheses is analogous to constructing a confidence interval for a data-dependent parameter as in 2.1. Let
and similarly define
An application of Theorem 3.1 gives
for some nonzero that can be consistently estimated with (see equation B.12 in Section B.2). Since splits reuse observations, the off-diagonal terms of explicitly incorporate the dependence across splits.
Denote by the entry of the main diagonal of associated with , that is, with the term . I propose computing the test-statistic
This type of test statistic has been considered for example in chernozhukov2007estimation; romano2008inference; andrews2009validity; romano2010inference in the context of moment inequalities. Critical values can be computed via Monte Carlo: simulate and estimate as the quantile of . I note that, alternatively, one could use the likelihood ratio test statistic.
Asymptotic exactness of this test under the least favorable null follows from similar conditions to Theorem 3.1, established below.
Assumption 4.1.
Assumption B.2 consists of more technical conditions, which are delayed to the appendix for ease of exposition. For example, they extend the Z-estimator assumptions on to . Assumption 4.1(i) requires the limiting variance of to be positive, and Assumption 4.1(ii) defines the requirements on the baseline (parametric) estimator . It holds, for example, if belongs to a Donsker class with probability approaching one, which typically happens for parametric models such as the sample average .
Theorem 4.2.
(Size control of multivariate one-sided test for model differences)
Let Assumption 4.1 hold.
Then, for any ,
For any sequence with ,
∎
Theorem 4.2 appears to be new. It establishes size control: the probability of rejecting the null hypothesis, conditional on it being true, does not exceed in large samples. Note that the probabilities in Theorem 4.2 are not random objects, they integrate over the distribution of the data conditional on the events or . Alternative approaches for testing across multiple splits of the sample typically aggregate p-values or confidence intervals computed separately for each split, without accounting for the dependence structure across splits (see, e.g., chernozhukov2025generic; gasparin2025combining). For example, chernozhukov2025generic propose aggregating the median of p-values or CIs across splits. Because these methods do not incorporate the correlation across splits, they are conservative in most data-generating processes, as they guard against the worst-case dependence structure. In contrast, my approach explicitly accounts for the dependence across splits, which enables the test to achieve exactness under the least favorable null in a uniform sense across DGPs. The proof is made possible by the decomposition in 4.6, which follows from the new CLT in Section 3.
The result above requires the probability of the conditioning event to be bounded away from zero using the constant . This could lead to an apparent uniformity issue for sequences of DGPs with , for example. For such sequences, the probability of rejecting the null conditional on the null being true could be greater than . This is not, however, an issue for empirical practice: for such sequences the probability of being under the null itself converges to zero. Incorrectly rejecting the null is not a concern when the probability of the null being true is zero.
4.2.2 A Confidence Interval for the Average Performance
I construct a new confidence interval for based on two insights from the previous subsections. The first is that a CI based on the normal approximation using Theorem 3.1 is asymptotically exact if converges in probability to a value different from zero, since in this case the terms in 4.5 do not cancel out. The second insight is that the case is closely connected with the null hypothesis of the one-sided test developed in the previous subsection. Hence, my CI consists of using the normal approximation if the one-sided test is rejected, and an extended CI in case it is not.
Define the normal approximation CI
where is a standard error for (see equation B.13 in Section B.2), and an extended CI
where denotes the convex hull, that is, has all the elements in , , and all elements in between. The final CI is given by
is based on a pre-test, using different inference approaches depending on whether the one-sided test is rejected or not. This construction is motivated by the following facts, which are formalized in Theorems 4.3, 4.4 and B.1. If converges in probability to a negative value, , and is used, which is asymptotically exact. If converges in probability to a positive value, , is asymptotically exact but is used, which is valid since it is wider than , although conservative. This asymmetric construction is a choice, which reflects the motivating problem of this section of learning whether the new model performs better (instead of worse) than the baseline model . Finally, if , intuitively should be close to given Theorem 4.2. If that happens, covers with high probability since it includes , the limit of . However, this guarantee depends on additional conditions as I discuss next, since may not converge to one even if .
First, I show that is valid pointwise in , assuming that if , then the parametric model is well-specified in the sense that it minimizes the error in , that is, for all . Then, I establish conditions under which is valid uniformly in .
Theorem 4.3.
(Pointwise Asymptotic Validity of )
Let Assumption 4.1 hold.
Then, for any such that either
-
(i)
, or
-
(ii)
,
∎
Further, I show that is asymptotically valid for most sequences of , and discuss why it may fail for specific sequences. Then, I establish that the additional condition Assumption 4.2 is sufficient for to be asymptotically uniformly valid in . Later, I propose a modification to that gives uniform validity under only Assumption 4.1.
Assumption 4.2.
For any sequence such that ,
∎
Theorem 4.4.
(Uniform Asymptotic Validity of )
Let Assumption 4.1 hold.
For any and , define
Then,
Moreover, if Assumption 4.2 holds,
∎
Under Assumption 4.1, covers when this difference is positive or “sufficiently” negative, with only requiring this event to happen with positive probability. If converges to any negative value, coverage is asymptotically exact (Theorem B.1). If it converges to a positive value, similarly, the normal approximation CI is exact, and the extended is conservative. Failure of coverage may happen only if , that is, it converges to zero “from the left”. For such sequences, the components of , , may be enough negative so that the one-sided test rejects the null with high probability, but since they converge to zero, the terms in 4.5 cancel out, and the normal approximation CI may undercover. Importantly, is valid in the case of interest , that is, when performs equally or worse than the baseline model . This is the case, for example, when the parametric model is well-specified, as in Theorem 4.3, since from Assumption 4.1. Hence, may overstate the advantage of when it slightly outperforms , but not when their performances are equal or when performs worse.
Assumption 4.2 rules out the problematic sequences by ensuring that if , is close enough to in large samples so that the one-sided test does not reject with probability higher than . It is motivated by the fact that machine learning algorithms typically penalize deviations from the mean. If there is little signal to be learned by , that is, is close to , it may be reasonable to expect that regularization will make the estimates closer to than to . For example, in the case of estimating a linear model with the Lasso, if the true coefficients are very small, penalization leads to estimated coefficients exactly equal to with high probability (zhao2006model; zhang2008sparsity; wuthrich2023omitted). However, this assumption may not lead to a good approximation for the behavior of DGPs where is sufficiently distant from and is estimated with no or little regularization.
Next, I provide an alternative, more conservative CI that gives uniform coverage without relying on Assumption 4.2. It deals with sequences with by modifying to be more conservative in the one-sided test. For any , consider the modified version of the test in 4.7:
represents a degree of slackness on how large has to be to reject the null hypothesis. The final CI is given by
and the critical value is the same as before. A large gives more robustness in finite samples in the sense that
is (weakly) increasing in . On the other hand, a large leads to less power. Importantly, this approach is not necessary if the goal is to test the null , since this cased is covered by Theorem 4.4. The modified confidence interval is intended for researchers who may want to be careful not to overestimate the magnitude of when it is small but negative.
Theorem 4.5.
5 Reproducibility
The split-sample estimators and estimands defined in Section 3 depend not only on the algorithm used to estimate , but also on the specific splits of the sample . In applications, this may lead to the undesirable phenomenon that different researchers with the same dataset, using different random splits , may reach different conclusions in terms of statistical significance. Intuitively, by averaging over multiple splits, this phenomenon becomes less likely. In this section, I first formalize this intuition by establishing basic reproducibility properties of split-sample Z-estimators. Then, I develop a measure that quantifies the reproducibility of p-values from hypothesis tests based on Z-estimators for a given number of repetitions .
5.1 Basic Reproducibility Properties
I establish two basic reproducibility properties for the three versions of split-sample Z-estimators defined in Section 3. The two results formalize the notion that, for fixed , choosing to use a larger number of repetitions improves reproducibility of the estimators. The results exploit the fact that and are averages over independent repetitions . For the second estimator, I use the fact that, conditional on the data , is still a Z-estimator where the “observations” are the splits and the target parameter is the value of that solves the moment condition averaged over all possible splits. This allows me to explore large properties of using arguments similar to those applied to Z-estimators (e.g., Theorem 5.9 in van2000asymptotic). For , I require an additional technical condition which I delay to Section B.3.1. This assumption is analogous to standard conditions for proving consistency of Z-estimators, and holds, for example, if is bounded, is Lipschitz in with a Lipschitz constant that does not depend on or , and if the solution to the moment condition averaged over all possible splits is unique.
The first reproducibility property is that, for fixed , the variance of the Z-estimators conditional on the data converge to zero as grows. Conditional on the data, the estimators vary only due to the random partitioning. This approximates the behavior of the estimators when the number of repetitions is chosen to be large. This guarantees that two researchers with the same dataset and different splits will calculate estimators that are arbitrarily close to each other with high probability for large enough .
Proposition 5.1.
For the estimators and , I show that the conditional variance is strictly decreasing in . This establishes a stronger property than the asymptotic result in Proposition 5.1: not only does reproducibility improve as , but every increase in strictly reduces variance and thus improves reproducibility.
Proposition 5.2.
5.2 A Reproducibility Measure
I propose a reproducibility measure for p-values from hypothesis tests based on transformations of split-sample Z-estimators. Specifically, I study reproducibility of the p-value for testing versus (and its one-sided versions) for differentiable. The hypotheses and depend on since the parameter of interest, , depends on . Testing this hypothesis is analogous to constructing a CI for : in fact, inverting this test for all values of at significance level gives the confidence interval of Section 4.1.
I begin by defining the reproducibility measure, then describe the asymptotic framework I use and the technical challenges involved. Finally, I establish the limit distribution of the difference of t-statistics constructed from different random splits, and apply this result to construct the reproducibility measure. As in Section 3, I consider repetitions of sample-splitting with folds ( denotes repeated sample-splitting).
The goal of this section is to construct a measure , for , that satisfies
where and are p-values for calculated with separate, independent splits. This measure provides the following guarantee: if a researcher calculates a p-value using one set of random splits, then a second researcher using the same dataset, but different splits, will obtain a p-value exceeding with probability approximately . This allows researcher 1 to assess whether their result would remain statistically significant without the computational cost of re-running the analysis. For example, if but for some small , the researcher may need to increase to guarantee reproducibility of their finding.
I consider an asymptotic regime where both the number of repetitions and the sample size grow to infinity, which is the main technical challenge for proving validity of my reproducibility measure. An alternative framework is to consider the data fixed, let , and treat each repetition as an independent observation. Although this alternative regime facilitates statistical analysis, it provides asymptotic guarantees only when is large relative to . In practice, choosing much larger than is often computationally intractable. My asymptotic framework better reflects much of empirical practice by allowing to grow slower than , so that can be, for instance, a small fraction of . The proofs of my results under this asymptotic regime rely on the CLT of Section 3.
I focus on the estimator from Section 3, and similar results can be extended to and using similar techniques. The case is much more challenging because, unlike and , is not an average of independent terms conditional on the data.
The setting follows Section 3. Additionally, let and be independent collections of splits of the data with folds (uniformly at random). Let and be calculated with and respectively, which leads to analogous definitions of , , and for . Under the null hypothesis and the conditions of Theorem 3.1, the t-statistic
where is given as in 4.2, is a row vector with the partial derivatives of evaluated at , and is a plug-in estimator for defined in Section B.1. Based on this result, one can calculate p-values
for versus and its one-sided versions.
The asymptotic regime assumes , where (defined in B.17 in the appendix) reflects the variance of conditional on the data. Since and converge to non-random quantities and respectively, . Hence, the asymptotic regime requires at a rate slower than . The rate of convergence of depends on the rate at which converges to , and may be slow especially when is estimated nonparametrically. In Theorem 5.3, I show that a safe guideline for achieving the reproducibility guarantees established below is to choose of comparable magnitude to .
I characterize below a central limit theorem for the difference of t-statistics constructed using different splits, which is the main ingredient for deriving my reproducibility measure in Theorem 5.2. Both results rely on the fairly technical Assumption B.4, stated in Section B.3.2. The key condition is a Donsker-type requirement on and . This condition holds, for example, if and are bounded and the cross products of the entries of are Lipschitz. Importantly, Assumption B.4 does not restrict the complexity of , it only restricts the complexity of the function classes over , and not over .
Theorem 5.1.
(Reproducibility of t-statistics based on Z-estimators)
Let Assumptions 3.1 and B.4 hold.
Then, for any ,
conditional on with probability approaching one. ∎
I introduce my reproducibility measure for each of the three tests (two-sided and both one-sided tests), where is the standard normal cdf, and formalize their guarantees in Theorem 5.2.
Theorem 5.2.
(Reproducibility of p-values based on Z-estimators)
Let Assumptions 3.1 and B.4 hold, and .
For any and
it follows that
| (5.1) |
with equality if . ∎
Theorem 5.2 gives a novel measure of reproducibility for p-values based on split-sample Z-estimators. The guarantee of reproducibility in 5.1 is inspired by the definition of -reproducibility in ritzwoller2023reproducible. They provide an algorithm for deciding how many repetitions of the sample-splitting procedure are necessary to guarantee reproducibility of the average across split-sample statistics. This covers, for example, the estimators and . My approach complements theirs by focusing on reproducibility of inference, examining p-value rather than average statistics. My results hold for , and the arguments can easily be extended to and . ritzwoller2023reproducible’s procedure takes as input the desired level of reproducibility, and outputs the required number of repetitions that guarantees such reproducibility. My approach takes as input (assumed “large”), and outputs a measure of how much reproducibility is guaranteed by such . The asymptotic regimes also differ: ritzwoller2023reproducible takes the data as fixed and considers that the desired threshold for the variability of the average split-sample statistic is small, while my framework considers and large.
The result in Theorem 5.2 relies on choosing such that . In practice, it may be hard to choose that satisfies this condition since the rate at which is in general unknown. I show that if grows too fast, i.e., if , the distribution in Theorem 5.1 collapses and the guarantees in Theorem 5.2 hold conservatively. This gives a safe guideline for empirical implementation: choose to be at least a small fraction of , such as , and the guarantee in Theorem 5.2 will hold conservatively.
Theorem 5.3.
(Reproducibility under )
Let Assumptions 3.1 and B.4 hold, replacing B.4(v) with .
Then, for any ,
For
and ,
∎
6 Application 1: Poverty Prediction in Ghana
Understanding the drivers of poverty is at the root of much of Development Economics. For research, being able to better predict poverty dynamics is of first-order importance to both form hypotheses and then validate theories that explain poverty and poverty dynamics. For policy, accurate predictions of current or future poverty could enable better targeting of interventions (ideally then combined with causal inference on policies and interventions).
Using a sample of 319 households in urban Accra from the ISSER-Northwestern-Yale Long Term Ghana Socioeconomic Panel Survey (GSPS) (ghanapaneldataset), I examine how well I can predict which households will be below the poverty line 13 years ahead. The outcome of interest is an indicator for whether a household is below the poverty line in the fourth wave of GSPS (2022/2023), and I use covariates measured in wave 1 (2009/2010), that is, 13 years before. Of the 319 households, 22 were below the poverty line in wave 4 (around 7%). I use predictive covariates including household demographics, parental education, religion, political and traditional leadership experience, asset holdings, and financial indicators (see Section B.4 for details). Although I focus on the binary indicator of below the poverty line, the approach applies more broadly and could use other outcomes such as level of consumption or assets.
I estimate two quantities: the mean squared error (MSE) and the fraction in poverty by tercile of predicted probability of being below the poverty line. In both cases, I use repeated cross-fitting with and , and fit random forest models using the R package ranger implemented through mlr3. Let , denote the indicator of whether household is below the poverty line in wave 4 of GSPS and the set of covariates measured in wave 1. The estimated MSE is given by
For , let be the first and second terciles of , that is,
and let , . For , the fraction in poverty in tercile of predicted probability of being below the poverty line is given by
I show in Section B.4 that is a Z-estimator.
I also compare the MSE of the models estimated with random forests to the MSE of using the sample average, as described in Section 4.2. In particular, I report p-values for the test of Section 4.2.1.
I calculate the MSE estimators and the one-sided test both in the real data and in two Monte Carlo designs, described in Section B.4. The data generating processes are designed to be similar to the original dataset, preserving the empirical marginals and rank-based dependence structure of the observed data. In the first design, denoted Correlated, the outcome is correlated to the covariates . In the second design, denoted Uncorrelated, the outcome is independent of the covariates. I run around 5,000 Monte Carlo iterations for each of the three designs – real data, “correlated” and “uncorrelated” simulated data –, drawing 200 new random splits of the sample at each Monte Carlo iteration. For the real data, the only source of randomness are the 200 splits, while for the simulation designs I draw a new dataset at each iteration (with 200 splits for each dataset). For each simulated dataset and split, I also calculate the difference between top and bottom terciles, .
I compare the estimates and p-values of using repeated cross-fitting (RCF) with three alternatives. The first is the standard “twice the median” (TTM) rule (ruger1978maximale; gasparin2025combining; chernozhukov2025generic): calculate the p-value (for difference in MSE or “top minus bottom” estimator) separately for each fold, that is, using a third of the data, take the median of the 600 p-values (200 repetitions, 3 folds) and multiply it by 2. The second is the Sequential Aggregation (Seq) approach of luedtke2016statistical and wager2024sequential: train a random forest using only fold 1, compute the t-statistic using fold 2, then train a random forest using folds 1 and 2 and compute the t-statistic in fold 3. The p-value for each repetition of cross-fitting uses as final t-statistic times the average of the two t-statistics. Finally, the final p-value for each Monte Carlo iteration is twice the median over the 200 p-values coming from the 200 repetitions, similar to chernozhukov2025reply. The third method is standard sample-splitting (SS): train a random forest using two thirds of the data, calculate p-value in the excluded third, not aggregating across repetitions.
Figure 1 presents the p-values for whether random forest MSE is lower than sample average MSE, and accuracy () point estimates across Monte Carlo iterations for the two simulation designs as well as for the real data. In the uncorrelated design, all methods exhibit similar accuracy on average, with sample-splitting having larger variance since it does not aggregate across multiple splits. All methods are conservative: the p-values concentrate around 1. For sample-splitting, this happens since the sample average is the best predictor of in this design, and the random forests are noisy estimates that have larger MSE. The other methods are conservative for the same reason, and TTM and Seq are more conservative since they take twice the median p-value, which guards against the worst DGP. For the correlated design, all methods correctly give small p-values, with RCF being more concentrated around zero.
In the real dataset, Seq often has the smallest accuracy, and TTM the highest, while RCF stands in between. Seq has smaller accuracy since one the two models that it averages over is trained with only a third of the data. RCF and TTM, on the other hand, always use two thirds of the data for training. The only difference between the two numbers is that RCF averages model performances over 200 repetitions while TTM takes the median. Hence, the higher accuracy of TTM reflects the distribution of model accuracies being left-skewed. Only RCF manages to consistently reject the null, concluding that poverty can be predicted from the observed covariates using a random forest model. TTM and Seq are more conservative, with Seq having larger p-values than TTM due to its lower accuracy.
| Method | Variable | Estimate | 95% CI | p-value |
| RCF | Bottom tercile | 0.046 | [0.007, 0.085] | |
| Top tercile | 0.122 | [0.059, 0.184] | ||
| Top minus bottom | 0.076 | [0.002, 0.150] | 0.023 | |
| TTM | Bottom tercile | 0.056 | [, 0.133] | |
| Top tercile | 0.114 | [0.006, 0.223] | ||
| Top minus bottom | 0.083 | [, 0.208] | 0.228 | |
| Seq | Top minus bottom | – | – | 0.150 |
Notes: Estimates of fraction below poverty line by tercile of predicted probability of being below the poverty line. Fraction below poverty line in entire sample is around 7%. Bottom tercile corresponds to and top tercile to . RCF, TTM and Seq correspond respectively to repeated cross-fitting, twice-the-median, and sequential aggregation. All estimates aggregate over 7,104,600 splits.
Table 3 shows the point estimates and CIs for the estimators , , and their difference using RCF and TTM in the real dataset, as well as p-values for testing whether the difference between top and bottom groups is positive (that is, top tercile has a larger fraction below the poverty line than the bottom tercile). These final estimates aggregate over all the 7,104,600 splits displayed in Figure 1, averaging for RCF and taking the median for TTM. I do not display the point estimates for Seq since wager2024sequential focuses on testing, but the p-value indicates that the difference between top and bottom groups is not significant. Table 3 shows that the difference between top and bottom terciles is statistically significant only for RCF.
7 Application 2: Heterogeneous Treatment Effects in Charitable Giving
There has been growing interest in the literature for learning features of heterogeneous treatment effects using machine learning (chernozhukov2025generic; wager2024sequential; imai2025statistical; for applications, see, e.g., bryan2024big; athey2025machine; johnson2023improving). I revisit the Generic Machine Learning framework of chernozhukov2025generic (henceforth CDDF), and propose a new ensemble estimator that uses the entire sample for calculating confidence intervals, more data for training machine learning algorithms, and aggregates predictions over multiple ML predictors into an ensemble. I first revisit CDDF’s approach, and second introduce my ensemble estimator. Theoretical properties are delayed to Section B.5. Finally, I compare my estimator to the approaches of CDDF and of wager2024sequential in a Monte Carlo design and in an empirical application using data from karlan2007does. The simulation exercise shows gains in power using the ensemble method, and the ensemble approach is the only to detect statistically significant treatment effect heterogeneity in the empirical application.
7.1 The Generic ML Approach of chernozhukov2025generic
CDDF proposed a method for learning features of treatment effect heterogeneity in randomized trials. In this section, I focus on their Sorted Group Average Treatment Effects (GATES) estimand. This approach consists of using a machine learning (ML) algorithm and pre-treatment covariates to find groups of individuals with larger and smaller average treatment effects (ATEs). If such groups exist, this means that treatment effect is heterogeneous and that this heterogeneity can be explained at least in part by observable characteristics. Moreover, one can explore how these groups differ in terms of these characteristics. They call this last step Classification Analysis (CLAN), and although I focus on GATES to simplify exposition, my results also hold for CLAN.
First, I define some notation. Let denote the data, where is a scalar outcome, is the treatment assignment indicator, and is a vector of pre-treatment covariates. I assume that are drawn i.i.d. from a distribution . Let denote an ML algorithm, a function that takes a dataset as input, and outputs an estimate of the Conditional Average Treatment Effect (CATE) function,
For example, could be Causal Forests (wager2018estimation), or based on Random Forests, Neural Networks, or Gradient Boosting.222For example, one could use any of these three algorithms to estimate separately the functions and , and use the difference of the two estimated functions as an estimate of the CATE. For any subsample , let , , and , that is, is the model trained with algorithm using the subsample .
The procedure is given as follows. First, take random subsets of of size . For each , denote the subsample by , where and . For each repetition , call the main sample, and the auxiliary sample. For , train the model
| (7.1) |
using data from the auxiliary sample. In the main sample, calculate predicted individual treatment effects (ITEs) . Sort into quantile groups , where
| (7.2) |
with , , and are calculated such that the number of observations in is balanced or nearly balanced. For example, with , is a partition of the sample into quartiles of . Calculate the split-specific GATES estimator by running the weighted regression
| (7.3) |
with weights , where is the (known) propensity score. These weights guarantee correct identification of ATEs when the propensity score is not constant, that is, it ensures
Denote the estimates by . A frequent parameter of interest is
the difference in ATEs between the top and bottom groups of predicted ITEs. This parameter can be estimated with the analogue
and a CI can be calculated as usual,
| (7.4) |
where is a heteroscedasticity-robust standard error for calculated as usual from the OLS regression 7.3, and is the quantile of the standard normal distribution. Finally, the final estimators and CIs are given by
and
where denotes the median across repetitions . Conditions for the validity of this CI are established in Theorem 4.3 of CDDF.
This approach carries a tradeoff that’s not present in my method, and it considers a single ML algorithm . The tradeoff regards the choice of : a larger means more data is used to estimate the regression 7.3, leading to narrower CIs in 7.4; but fewer data are used to train the ML model in 7.1, likely yielding a worse estimate of the CATE. Moreover, regularity condition R3 in CDDF requires to be relatively small to guarantee that the CI covers the median of across all possible splits. My ensemble approach presented next avoids this tradeoff since it uses the entire sample for estimation and a larger sample for training. The ensemble estimator also incorporates more than one ML algorithm, which is important if one does not want to commit beforehand to any specific algorithm. Although CDDF’s approach can be repeated with different algorithms, that comes with potential issues of multiple hypothesis testing.
In the next subsection I propose a new GATES estimator that (i) uses the entire sample to calculate in 7.3, and (ii) combines predictions from multiple ML algorithms to form an ensemble, eliminating the need for algorithm selection.
7.2 An Ensemble Estimator
Before defining my ensemble estimator, I introduce some additional notation. Theoretical properties are delayed to Section B.5. Let denote the number of machine learning algorithms that will be used for predicting ITEs. For , let denote an ML algorithm, that is, a function that takes a dataset as input, and outputs an estimate of the CATE. For example, one could choose to use Random Forests, Neural Nets, and Gradient Boosting. For and , let
that is, is the model trained with algorithm using the subsample .
The ensemble approach is summarized in Algorithm 1. The first difference is that instead of splitting the sample into two sets, I split it into roughly equal-sized folds , again repeating the process times. I calculate predicted ITEs for each individual using the ML algorithms, trained using all folds except the one that contains observation . I denote the predicted ITEs by , where is such that . Then, to calibrate the weights for combining the multiple ML predictions into one, I split the sample again into different folds, for each repetition . Let denote the folds ( is not incorporated in the notation to simplify exposition). For , estimate the weighted regression
| (7.5) |
with weights . In 7.5, , is the propensity score, and is a vector of functions of , for example , where is a subset of . The role of is only reducing noise in estimation, so this term can be omitted if desired. Denote the estimates of by . The final predicted ITE is then given by
Repeating this process for gives for every observation. I sort into groups separately by fold. That is, for ,
with , , and are calculated such that the number of observations in is balanced or nearly balanced. Finally, the split-specific GATES estimator uses the whole sample, defining
| (7.6) |
and running the weighted regression
| (7.7) |
with weights .
7.5 is very close to the Best Linear Predictor (BLP) regression of CDDF, except that it uses the predicted ITEs instead of just one. The intuition behind 7.5 is that are the best linear predictor coefficients of a regression where the true CATE is the response variable, and are the independent variables (see Theorem 3.1 of CDDF). Hence, is the best linear approximation of given .
The final estimator averages over repetitions,
where, as before, , with and being the estimates from 7.7. The final standard error is
| (7.8) |
where is a heteroscedasticity-robust standard error for calculated as usual from the OLS regression 7.7. The parameter of interest is
where and are defined in 7.7.
7.3 Application to Charitable Giving and Monte Carlo Experiments
I compare my new ensemble approach to two alternative methods in an empirical application and in Monte Carlo experiments. I revisit karlan2007does, which sent fundraising letters to prior donors of a liberal nonprofit organization in the United States, randomizing the match ratio offered (1:1, 2:1, or 3:1) versus no match for a control group. I pool all match treatments into a single treatment group, focusing on the binary treatment of receiving any match offer versus none. The outcome of interest is the amount donated in dollars. The predictive covariates I use include individual donation history (frequency, recency, amount), gender, state-level political variables (Bush vote share, count of court cases in which the organization was either a party to or filed a brief), and zip code-level demographics and economics (race, age, household size, income, homeownership, education, urbanization) (see Section B.5 for details). I focus on the subset of 6,419 donors who donated within the last two months, as they were more responsive to the solicitation and the smaller sample facilitates computation of the Monte Carlo experiments.
I compare the ensemble with CDDF’s approach, described in Section 7.1, and the sequential aggregation approach of luedtke2016statistical, wager2024sequential, and chernozhukov2025reply. Sequential aggregation (Seq) consists of splitting the sample into folds, for train an ML model using folds through , and compute GATES in the -th fold. The final estimator is the average over the estimates, and the p-value uses the final t-statistic equal to times the average of the fold-specific t-statistics. This approach uses more data for calculating GATES and p-values ( observations), but trains some ML models using fewer data (the first model uses observations). I aggregate the final estimates and p-values taking the median over repetitions as in chernozhukov2025reply.
I compute the three approaches across four designs: (i) using the real data (real), (ii) using the real data but shuffling the treatment assignment indicator at random (so there is no treatment effect heterogeneity) (real-shuffled), (iii) drawing from a DGP where treatment effect is partially predictable using covariates (mc-hte), (iv) drawing from a DGP where treatment effect heterogeneity is independent of covariates (mc-nohte). The two DGPs are meant to be similar to the real data, preserving the marginal distributions of covariates and rank-correlation structure, as described in Section B.5. Across all methods and datasets, at each Monte Carlo iteration I use 100 repetitions of sample-splitting, take random samples (without replacement) of sizes (entire dataset), and compare the number of folds (for CDDF, the ML is trained with observations and GATES calculated in the remaining sample). For Ensemble, I draw at random between 1 and 4 ML algorithms among 10 popular algorithms available in R’s mlr3verse: XGBoost (xgboost), Random Forest (ranger), Neural Networks (nnet), Elastic Net (glmnet), k-Nearest Neighbors (kknn), Linear Regression (lm), Decision Trees (rpart), Fast Nearest Neighbors (fnn), Multivariate Adaptive Regression Splines (earth), and Gradient Boosting (gbm). For CDDF and Seq, I draw one of the same ten algorithms at random, for each Monte Carlo iteration. I show the number of iterations used for each specification in Table 4 in the appendix.
Figure 2 shows the gains in power of using the ensemble method in the real dataset. It displays boxplots of one-sided p-values for testing whether the top tercile of predicted treatment effects has a larger ATE than the bottom tercile. A small p-value means rejecting the null hypothesis of no detectable treatment effect heterogeneity. With (the entire dataset), Ensemble with 4 algorithms detects treatment effect heterogeneity at the 10% level in more than 75% of the iterations. Seq and CDDF give p-values above 10% in most iterations. None of the methods are powered enough to reject the null consistently with .
Figure 3 is similar to Figure 2, except that it uses the synthetic DGP where there is no detectable heterogeneity. It shows that all methods correctly fail to reject the null in most iterations. Similar figures for designs real-shuffled and mc-hte are presented in Section B.5.
Figure 4 shows the rejection probabilities at the 5% significance level, that is, the percentage of iterations with p-value below 5%. For the two datasets with no detectable heterogeneity, real-shuffled and mc-nohte, all methods are conservative when or , they yield rejection probabilities below the nominal level. In the real-shuffle design with and or , the ensemble methods reject the null with probability slightly higher than nominal, but smaller than 10%. With , only Ensemble 4 rejects the null with probability higher than nominal with in the real-shuffled design. In the real dataset, CDDF almost never detects HTE, and Seq detects in less than 20% of iterations with and . The ensemble methods have higher power especially in the specifications using the entire dataset. For example, Ensemble 2 detects heterogeneity in around 50% of iterations with folds. In the synthetic dataset where there is detectable heterogeneity, mc-hte, as well as in the real data, Ensemble 2 and 4 have higher power across all specifications.
As I discuss in Section B.5, the rejection probability under the null of no detectable heterogeneity could in principle be above the nominal level when using the normal approximation CI. In Section B.5.5, I propose an alternative CI that controls size under the null, at the expense of being more conservative and requiring more computational time. However, I note that extensive simulation experiments, including but not limited to the design of Figure 4, suggest that Ensemble 4 is conservative for relatively small values of . Hence, my recommendation for empirical practice is to use the normal approximation CI with Ensemble 4 and .
8 Conclusion
As predictive algorithms become increasingly popular, using the same dataset to both train and test a new model has become routine across research, policy, and industry. I derived a new inference approach on model properties that averages across several splits of the sample, where at each split one part is used to train a model and the remaining to evaluate it. Compared to a standard 50-50 sample-splitting, my approach improves statistical and modeling power by using more data for training and evaluating, and improves reproducibility, so two researchers using different splits are more likely to reach the same conclusion about statistical significance. Although the practice of averaging over multiple splits is not new, the confidence intervals and establishing their validity appears to be new.
I addressed the main technical challenge, the dependence created by reusing observations across splits, by proving a central limit theorem for the large class of split-sample Z-estimators. Leveraging the data-dependent parameter of interest, my CLT does not require restricting the complexity of the model or its convergence rate, unlike in the classic semiparametrics problem that used cross-fitting and focused on a different parameter that is not data-dependent. This generality is important as it allows the model to be learned with potentially complex machine learning algorithms, as is commonly done across research, policy, and industry.
Using the CLT, I constructed CIs based on the normal approximation that are valid in a large class of problems, and documented cases where this approximation may fail to cover the parameter of interest at nominal rate. I provided a new approach to inference for such problems, focusing on the particular case of inference when comparing the performance between two models. The approach builds on my CLT, and I discussed how the arguments can be extended to other problems. I also provided a general approach that allows the moment functions to have zero limit variance in Appendix F, by exploring the faster-than- convergence of the empirical moment equations and a tuning parameter.
In Section 5, I derived a new reproducibility measure for p-values calculated with split-sample Z-estimators. This measure is especially useful when computational resources are limited, quantifying whether a given number of split-sample repetitions suffices for two researchers using different splits to reach similar conclusions about statistical significance with high probability.
Finally, I illustrated the empirical implications of my results by revisiting two important problems in development and public economics: predicting poverty and learning heterogeneous treatment effects in randomized experiments. Using a panel from Ghana (ghanapaneldataset) and Monte Carlo experiments, repeated cross-fitting performed better than previous alternatives in detecting predictive power for being below the poverty line 13 years ahead. For the heterogeneous treatment effects application, I developed a new ensemble method that uses the entire sample for evaluation, more data for training, and combines multiple machine learning predictors. I revisited karlan2007does’s experiment on charitable giving and conducted Monte Carlo simulations. In both cases, my ensemble method achieved improved power for detecting heterogeneous treatment effects compared to previous alternatives.
Appendix A Bounding the Performance of Average Model
Let be a scalar outcome, a set of covariates, and be a collection of models estimated through multiple splits of the sample, where is the complement of , as in Section 2. For example, can be a vectorization of defined in Section 2, . Denote . If is binary, some algebra manipulation gives the following equalities:
Hence, one can use either or a model that takes value in uniformly at random, and both will yield the same out-of-sample mean absolute deviation and mean squared error.
For the general case, if is continuous, an application of the triangle inequality establishes a risk-contraction property for :
Similar results hold for other distance-based functional forms where the triangle inequality applies. Although my framework does not cover the parameters and , it covers and , which are upper bounds on the error rate of using model . Hence, if one uses model for out-of-sample prediction, they have the guarantee that its accuracy will be at least as large (error at least as small) as the error they can estimate, or . Note that the root mean squared error estimand is similar although different from the one discussed in Section 1. In this case, the estimator is also covered by Section 3 and given by
Appendix B Proofs and Extra Definitions
The following notation is used throughout the proofs. If unspecified, denotes convergence in probability uniformly in , that is, for every , . . . means weak convergence uniformly in .
B.1 Proofs and Extra Definitions of Section 3
Define
where is the Jacobian matrix of , its derivative in .
Assumption B.1.
For some , the following conditions hold:
-
(i)
, and the classes are P-Donsker uniformly in and in the sense defined in Assumption E.1 with , where , and is the -th coordinate of ;
-
(ii)
The estimators satisfy
uniformly in ;
-
(iii)
For every ,
-
(iv)
For ,
uniformly in ;
-
(v)
is differentiable at for , and for some ,
∎
Assumption B.1(i) is a Donsker condition for a subset that contains in its interior. Importantly, Assumption E.1, defined in Appendix E, does not restrict the complexity of the class of trained models , and it allows to be estimated with any machine learning algorithm as long as Assumption 3.1(ii) holds. It restricts the complexity of only along , and not along . Assumption B.1(i) holds, for example, if is bounded and is Lipschitz in with a Lipschitz constant that does not depend on or . Assumption B.1(ii) allows for approximate Z-estimators which nearly solve the moment condition, and is immediately satisfied for exact Z-estimators, for example when
in the case of . Assumption B.1(iii) requires to be a unique and well-separated zero of , and can be replaced by the higher-level condition that uniformly in for . Assumption B.1(iv) holds under the condition that is continuous in around . Finally, Assumption B.1(v) requires the absolute determinant of the Jacobian to be bounded away from zero, which guarantees its invertibility in a uniform sense over .
Proof of Lemma B.1.
B.1 and B.2 follow from asymptotic equicontinuity established in Theorem E.1. B.3 follows from asymptotic equicontinuity of (follows from Assumption E.1(v)) and pointwise in convergence (Assumption E.2). ∎
Proof of Lemma B.2.
By Assumption B.1(iii), for any , there is such that
Hence,
since
by Assumption B.1(ii), B.3, and B.1. This implies uniformly in .
The result follows from the triangle inequality. ∎
Proof of Theorem 3.1.
I first show the result for the case of (and ). Differentiability of and Assumption B.1(iv) gives
| (B.4) |
Asymptotic equicontinuity gives
| (B.5) | ||||
| (B.6) | ||||
| (B.7) | ||||
| (B.8) |
where B.5 uses (Assumption B.1(ii)) and , and B.6 uses Assumption B.1(i) and Theorem E.1, and
uniformly in , established in Lemma B.2. Note that Assumption 3.1(ii), used for Theorem E.1, is stronger than Assumption E.2 (see proof of Theorem D.1).
By invertibility of ,
Plugging B.4 in the right-hand side gives
which implies
where the equality follows from B.8 and Assumption B.1(v). As a consequence,
| (B.9) |
Finally, combining B.4 and B.7 gives
Hence,
by applying Theorem E.1, and the result follows for . Note that Assumption 3.1(ii) is stronger than Assumption E.2 (see proof of Theorem D.1).
The results for and follow similarly. For , applying the same arguments above with and gives
for any , and the result follows for by summing over :
Similar holds for applying the arguments above with and and summing over . ∎
B.2 Proofs and Extra Definitions of Section 4
If is differentiable in , let be the Jacobian matrix of , where the derivatives are taken in respect to . In that case, can be given by
| (B.10) |
Define
| (B.11) |
Proof of Theorem 4.1.
Under the conditions of the theorem, for ,
uniformly in , where is the identity matrix. Consistency of the inner term to follows similarly to the proof of Theorem D.2, and the result follows from the continuous mapping theorem, Theorem 3.1 and the delta method. ∎
Assumption B.2.
The following conditions hold:
-
(i)
There exists a consistent estimator uniformly in ;
-
(ii)
uniformly in ;
-
(iii)
uniformly in ;
-
(iv)
.
∎
Item Assumption B.2(i) requires to be consistently estimable, which can typically be verified as in Theorem 4.1. Item Assumption B.2(ii) through Item Assumption B.2(iv) adapt conditions Assumption B.1(iv) through Assumption B.1(v) to instead of .
I give below a formula for for the case of sample averages, that is, . Analogous estimators can be defined for the general case using the fact that is asymptotically linear:
from Theorem 3.1.
| (B.12) |
where for splits with complements ,
where , and for any set , and , with .
Again, I give a standard error for the case of sample averages, and analogous estimators can be constructed for the general case following, e.g., Theorem 4.1.
| (B.13) |
where is defined as in 4.2 and
Proposition B.1.
uniformly in . ∎
Proposition B.2.
uniformly in . ∎
The two propositions above follow from a law of large numbers and Assumption 3.1(i) (assumed in Assumption 4.1).
Coverage of is exact along any sequences where in the limit, without relying on Assumption 4.2.
Theorem B.1.
Proof of Theorem B.1.
Follows from B.16 and Proposition B.2. ∎
Proof of Theorem 4.2.
I first show the result for the case . Let , arbitrary such that . For any and , denote the event
By Theorem 3.1 and Assumption 4.1(ii),
which implies
Hence,
and
conditional on . Together with Proposition B.1 and the continuous mapping theorem, this implies
where . The result follows since the quantiles of converge to those of by the continuous mapping theorem and Proposition B.1.
Similar happens for the case . The inequality comes from the fact that
∎
Proof of Theorem 4.3.
Proof of Theorem 4.4.
For the first result, an argument similar to the proof of Theorem 4.2 conditional on
| (B.14) |
implies
| (B.15) |
and
| (B.16) |
conditional on B.14, uniformly in . B.16 uses Assumption 4.1(ii), Theorem 3.1, and Proposition B.2. If is such that , the result follows Proposition B.2 since B.16 is asymptotically normal (nondegenerate). is ruled out since that implies
If , the result follows from B.15 and Proposition B.1.
Proof of Theorem 4.5.
Follows as in the proof of Theorem 4.4, except for sequences with , where the result follows from using B.15 and Proposition B.1. ∎
B.3 Proofs and Extra Definitions of Section 5
B.3.1 Proofs and Extra Definitions of Section 5.1
Assumption B.3.
The following conditions hold:
- (i)
-
(ii)
For every ,
with probability , where uniquely solves .
∎
Proof of Proposition 5.1.
For , the result follows from a Law of Large Numbers since is an average of iid observations (conditional on data). Note that convergence in probability to a point implies convergence of the variance to zero given uniform square integrability (Assumption E.1(iv)). For , consistency follows from consistency of M-estimators (for example, Theorem 5.9 in van2000asymptotic). ∎
Proof of Proposition 5.2.
Let if and if . Then,
and conditional on for . It follows that
is strictly decreasing in as long as . ∎
B.3.2 Proofs and Extra Definitions of Section 5.2
I first define some objects used in the proofs.
and uniquely solves . Note that uniquely solves .
. Note that
from E.11 (the equality holds without if ).
are the entries of ,
| (B.17) |
Assumption B.4.
The following conditions hold:
-
(i)
For any and ,
uniformly in ;
- (ii)
-
(iii)
There exists an estimator such that
uniformly in ;
-
(iv)
For some ,
-
(v)
.
-
(vi)
Either
or
∎
Assumption B.4(i) is a Donsker condition on conditional on the data. It is similar to Assumption B.1, and can typically be verified using arguments similar to the ones used to verify Assumption E.1(vi). It holds, for example, if is bounded and is Lipschitz in with a Lipschitz constant that does not depend on or (see, e.g., Example 19.7 in van2000asymptotic). Assumption B.4(ii) is a Donsker condition similar to Assumption B.1(i), but in terms of the product instead of . It is used to derive asymptotic normality of the standard errors . If for some , that is, if the functions are uniformly bounded, then Assumption B.4(ii) is implied by Assumption B.1(i) (see, e.g., Example 2.10.10 in van2023weak). Assumption B.4(iii) assumes the existence of a consistent estimator of . If is differentiable in , the plug-in estimator defined in B.10 satisfies this assumption under a uniform integrability condition on this derivative. Otherwise, consistent estimators can typically be constructed on a case-by-case basis (hansen2022econometrics). Assumption B.4(iv) requires the asymptotic variance of to be lower bounded. Assumption B.4(v) establishes the asymptotic regime. Finally, Assumption B.4(vi) restricts a corner case where the variance of the t-statistic is zero because of perfect negative correlation between and . Note that the quantities can all be consistently estimated with defined previously.
Before proving Theorem 5.1, I establish some key intermediary results.
Lemma B.3.
Let the conditions of Theorem 5.1 hold. Then,
Proof.
I show and the second result follows analogously.
Theorem B.2.
Proof of Theorem B.2.
For a random variable and a deterministic (conditional on ) sequence , I use to denote
uniformly in for any , and analogously define similar to the notation.
By differentiability of ,
| (B.18) |
Further,
| (B.19) | ||||
| (B.20) | ||||
| (B.21) | ||||
| (B.22) |
B.19 uses the definitions , and B.20 uses Assumption B.4(i). B.22 follows from the Lindeberg CLT.
since
by Assumption B.1(v), and an argument similar to B.9, exploring B.22, gives
The second result follows since, for any events and ,
Finally,
∎
Proof of Theorem 5.1.
The proof is divided into three main steps. First, I show that
| (B.23) |
Second, I show that
| (B.24) | |||
Finally, I combine the previous steps to reach the result.
Step one.
since
where by Theorem 3.1, the second equality holds from Theorem 3.1, and the last equality from Theorem E.1, using the fact that . Note that from Lemma B.3.
By differentiability of ,
since from Theorem B.2. This implies
Theorem B.2 gives
B.23 follows from combining the two previous displays.
Step two.
since
and
where the second equality follows from Assumption B.4(ii) and Theorem E.1, and the last equality uses .
Finally,
and
using the fact that .
Step three.
conditional on with probability approaching one, by Lindeberg’s CLT, by definition of , and since and conditional on . Note that by Lemma B.3. ∎
Proof of Theorem 5.3.
The first result follows since, from the proof of Theorem 5.1,
For , from the proof of Theorem 5.2,
which converges to zero since
from Theorem 5.1, and
since . Analogous results follow for and . ∎
B.4 Details of Section 6
B.4.1 Covariates Description
The following variables from the Ghana Socioeconomic Panel Survey are used as predictive covariates for poverty prediction in Section 6:
Household Demographics
-
•
children: Number of children in household
-
•
adults: Number of adults in household
-
•
female_head: Indicator for female household head
-
•
married_head: Indicator for married household head
-
•
spouse_in: Indicator for spouse living in the household
Religion
-
•
christian: Proportion Christian
-
•
muslim: Proportion Muslim
-
•
traditional: Proportion traditional religion
Political and Traditional Leadership
-
•
ever_political_office: Indicator for ever holding political office
-
•
today_political_office: Indicator for currently holding political office
-
•
ever_traditional_office: Indicator for ever holding traditional office
-
•
today_traditional_office: Indicator for currently holding traditional office
Parental Education
-
•
father_primary: Indicator for father completed primary education
-
•
father_middle: Indicator for father completed middle school
-
•
father_secondary: Indicator for father completed secondary education
-
•
father_tertiary: Indicator for father completed tertiary education
-
•
mother_primary: Indicator for mother completed primary education
-
•
mother_middle: Indicator for mother completed middle school
-
•
mother_secondary: Indicator for mother completed secondary education
-
•
mother_tertiary: Indicator for mother completed tertiary education
Asset Holdings
-
•
plot_acreage: Total land holdings in acres
-
•
livestock_value: Total value of livestock
-
•
livestock_expenses: Annual livestock maintenance expenses
Financial Resources
-
•
health_insurance: Proportion of household members covered by health insurance
-
•
savings_home: Amount of savings kept at home
-
•
d_saving_bank: Distance to nearest bank (in km)
-
•
savings_bank: Amount of savings in bank account
B.4.2 Fraction Per Tercile as a Z-Estimator
For a given split , the vector
is a Z-estimator with the moment functions
Hence, the final estimators are averages over split-specific estimators as in 3.2.
Note that the conditions in Theorem 3.1 are met whenever is not flat in . This condition is testable, for example using the one-sided test for the accuracy in Figure 1.
B.4.3 Monte Carlo Designs
I simulate outcome and covariates by (i) converting each observed column to rank-based uniforms , (ii) Gaussianizing to and estimating the latent normal correlation , (iii) drawing and mapping back to uniforms , and (iv) inverting each margin with the empirical CDF of the corresponding variable. For the correlated design, I modify by multiplying by 3 the first row/column, the one corresponding to the correlation between outcome and covariates, and use as correlation matrix its nearest positive definite matrix in case the modified is no longer positive definite. For the uncorrelated design, I sample covariates the same way, and the outcome is sampled independently from a binomial distribution with probability .
B.4.4 Comparison of Top-Bottom Estimates
B.5 Details of Section 7
B.5.1 Covariates Description
Donation History Variables:
-
•
hpa: Highest previous contribution
-
•
freq: Number of prior donations
-
•
years: Number of years since initial donation
-
•
mrm2: Number of months since last donation
Individual Demographics:
-
•
female: Female indicator
State-Level Political Variables:
-
•
cases: Count of court cases between 2002 and 2005 in which the organization was either a party to or filed a brief
-
•
perbush: State vote share for Bush
-
•
nonlit: Count of incidences relevant to this organization from each state reported in 2004-5 (values range from zero to six) in the organization’s monthly newsletter to donors
Zip Code Demographics and Economics:
-
•
pwhite: Proportion white within zip code
-
•
pblack: Proportion black within zip code
-
•
page18_39: Proportion age 18-39 within zip code
-
•
ave_hh_sz: Average household size within zip code
-
•
median_hhincome: Median household income within zip code
-
•
powner: Proportion house owner within zip code
-
•
psch_atlstba: Proportion who finished college within zip code
-
•
pop_propurban: Proportion of population urban within zip code
B.5.2 Monte Carlo Designs
The designs are explicitly calibrated to the observed data so that simulated covariates and outcomes are distributionally aligned with the original sample.
Treatment assignment. I draw the treatment assignment indicator from a Bernoulli distribution with mean .
Covariates and potential outcome under control. Starting from the observed outcome and covariate matrix for the control sample, I form pseudo-uniforms for each column by ranking within sample and scaling, . I then Gaussianize to and estimate the latent normal correlation on these (taking the nearest positive definite matrix if needed). To generate synthetic and covariates, I draw , map to uniforms , and invert each margin via the empirical CDF of the corresponding original variable.
Treatment effect. From the original data, I estimate two arm-specific components as functions of treatment and covariates. The first is a logistic regression for whether (no donation), using treatment, covariates and their interactions. The second is a Poisson regression, with amount of donation as outcome and same variables in the model. For generating simulated observations, the treatment effect is zero with probability (rounded to zero or one if necessary), where
with being the covariates, the value of potential outcome under control, the probability that coming from the logit model with coefficients associated with treatment being multiplied by 4, and coming from the Poisson model with mean multiplied by . Conditional on the treatment effect being different from zero, I draw from a truncated Poisson distribution starting at with the same mean coming from the Poisson regression.
Final outcome. For the design where treatment effect heterogeneity is predictable, I generate the observed outcome as if treatment is , and otherwise. For the design where treatment effect heterogeneity is not predictable, I generate the entire dataset exactly the same way, but shuffle the treatment assignment indicator at random as the last step.
B.5.3 Additional Figures and Table
Figure 6 displays results with the real dataset with shuffled treatment indicator (at random, so treatment effect is constant and equal to zero), and Figure 7 displays results for the synthetic DGP where there is explainable treatment effect heterogeneity. Table 4 gives the number of Monte Carlo iterations used for each specification.
| data | n500 | n1000 | n2000 | n6419 | ||||||||
| Method | Data Type | K2 | K3 | K2 | K3 | K2 | K3 | K5 | K2 | K3 | K5 | K10 |
| CDDF | Real (Shuffled) | 26,175 | 6,990 | 27,585 | 20,976 | 16,850 | 17,050 | 17,125 | 13,784 | 14,029 | 13,393 | 11,632 |
| CDDF | MC: No HTE | 26,109 | 26,320 | 26,637 | 26,211 | 16,433 | 15,805 | 18,756 | 12,384 | 12,142 | 12,376 | 11,676 |
| CDDF | Real Data | 23,324 | 4,327 | 27,026 | 12,845 | 17,204 | 17,131 | 8,579 | 13,765 | 14,005 | 13,420 | 9,417 |
| CDDF | MC: With HTE | 21,283 | 18,018 | 27,783 | 25,352 | 17,303 | 17,120 | 18,073 | 13,756 | 13,545 | 13,900 | 11,794 |
| Seq | Real (Shuffled) | 644 | 192 | 670 | 502 | 1,316 | 1,164 | 1,040 | 1,054 | 1,092 | 1,338 | 446 |
| Seq | MC: No HTE | 668 | 700 | 760 | 570 | 1,182 | 1,246 | 1,454 | 1,230 | 1,150 | 1,296 | 1,214 |
| Seq | Real Data | 1,154 | 172 | 1,342 | 650 | 1,134 | 1,380 | 568 | 1,018 | 1,004 | 1,430 | 656 |
| Seq | MC: With HTE | 508 | 464 | 696 | 622 | 1,366 | 1,368 | 1,248 | 1,302 | 1,262 | 1,320 | 1,226 |
| Ens. 1 | Real (Shuffled) | 3,177 | 901 | 3,390 | 2,638 | 1,969 | 2,084 | 1,990 | 1,493 | 1,543 | 1,580 | 1,505 |
| Ens. 1 | MC: No HTE | 3,399 | 3,368 | 3,327 | 3,335 | 2,105 | 2,057 | 2,346 | 1,492 | 1,537 | 1,560 | 1,522 |
| Ens. 1 | Real Data | 2,871 | 491 | 3,372 | 1,572 | 2,038 | 2,014 | 1,032 | 1,493 | 1,614 | 1,530 | 1,268 |
| Ens. 1 | MC: With HTE | 2,744 | 2,229 | 3,206 | 3,132 | 2,004 | 2,040 | 2,091 | 1,549 | 1,569 | 1,563 | 1,549 |
| Ens. 2 | Real (Shuffled) | 3,183 | 841 | 3,433 | 2,664 | 2,096 | 2,078 | 1,974 | 1,543 | 1,552 | 1,571 | 1,485 |
| Ens. 2 | MC: No HTE | 3,370 | 3,409 | 3,417 | 3,340 | 2,124 | 2,003 | 2,374 | 1,582 | 1,538 | 1,544 | 1,455 |
| Ens. 2 | Real Data | 2,865 | 499 | 3,269 | 1,584 | 2,160 | 1,999 | 987 | 1,574 | 1,556 | 1,561 | 1,265 |
| Ens. 2 | MC: With HTE | 2,625 | 2,226 | 3,429 | 3,170 | 2,120 | 2,052 | 2,075 | 1,589 | 1,585 | 1,588 | 1,443 |
| Ens. 4 | Real (Shuffled) | 3,261 | 868 | 3,476 | 2,512 | 2,048 | 2,072 | 2,035 | 1,581 | 1,643 | 1,511 | 1,389 |
| Ens. 4 | MC: No HTE | 3,367 | 3,405 | 3,421 | 3,410 | 2,069 | 1,996 | 2,319 | 1,524 | 1,438 | 1,525 | 1,420 |
| Ens. 4 | Real Data | 2,876 | 546 | 3,375 | 1,614 | 2,052 | 2,089 | 991 | 1,567 | 1,575 | 1,506 | 1,114 |
| Ens. 4 | MC: With HTE | 2,569 | 2,196 | 3,451 | 3,081 | 2,050 | 2,073 | 2,116 | 1,591 | 1,547 | 1,614 | 1,484 |
B.5.4 Theoretical Properties of Ensemble Approach
I establish the theoretical properties of the ensemble estimator using the CLTs proven in this paper. I show that when there is detectable heterogeneity, i.e., when the ensemble weights do not converge to zero, the confidence interval based on the normal approximation is asymptotically exact. If there is no detectable heterogeneity, however, my theoretical result gives no coverage guarantee to the normal approximation CI. Extensive simulation exercises, including but not limited to those of Section 7, suggest that the normal approximation CI is actually conservative under the null hypothesis of no heterogeneity for small values of and such as and . Hence, my recommendation for empirical practice is to use the normal approximation CI with no more than 4 algorithms and 5 folds. I also propose an adaptive approach using ideas developed in Section 4.2 that is valid even when there is no detectable heterogeneity, at the cost of having smaller power.
First, I introduce additional notation. Denote the set of splits
and the set of model . I use to denote the cdf of the random variable and
For some results, I focus on a set such that is equicontinuous at points for . This is a collection of DGPs where the quantiles of the limit predicted ITE are well-defined. This is required so that the groups defined in 7.6 are well-defined in the limit. Note that being continuous implies that the limit predictor is not flat in , so this class essentially excludes DGPs where there is no detectable heterogeneity, that is, where the true CATE is flat in .
My first result is that the normal approximation CI is asymptotically exact when there is detectable heterogeneity. It relies on Assumption B.5, defined in Section B.5.6. It is a mild but technical assumption that requires: (i) the weights have finite limits, (ii) a standard moments condition, (iii) propensity scores are bounded away from and , (iv) the variance-covariance matrix of the regressors is positive definite, and (v) the models estimated with ML converge to any limit at any rate.
Theorem B.3.
Let Assumption B.5 hold, and let be such that is equicontinuous at points for . Then, for any sequence ,
∎
Although Theorem B.3 does not cover cases when there is no detectable heterogeneity, extensive simulation exercises, including but not limited to the ones of Section 7, suggest that the coverage probability is larger than in those cases at least when , , that is, the CI Theorem B.3 is conservative. Next, I consider a test for detectable heterogeneity that can be used, for example, when and/or . If the test rejects no detectable heterogeneity, the normal approximation CI may be used.
B.5.5 A Test for Detectable Heterogeneity
I propose using a version of the test proposed in Section 4.2.1 for testing whether the models have explanatory power for heterogeneous treatment effects. Specifically, I first calculate the mean squared of residuals from the BLP regression
| (B.25) |
with weights , , for , as in 7.5 but at the fold level. Denote it by
I compare with
where and are the estimates from the weighted least squares regression
Let be an estimate of the asymptotic variance of , and are the entries of the main diagonal. I propose calculating the test-statistic
I establish the validity of this test in Theorem B.4, where is calculated as in Section 4.2.1. The result follows from Theorem 4.2.
Theorem B.4.
Denote the normal approximation CI
and the extended CI
where denotes the convex hull, that is, has all the elements in , , and all elements in between. For a given fixed , denote the final CI
Theorem B.4 implies that this CI is asymptotically valid pointwise in for , and uniformly in for any .
B.5.6 Proofs and Extra Definitions
Define as the space that contains the covariates for some integer . Let . For any
and , let
is the n-by-n diagonal matrix of weights:
is the coefficient of the linear projection with weights of on
when that is well-defined, and zero otherwise. Note is the same for all since the limit does not depend on the data. Let be the cdf of the random variable and
Define
Similarly, is the same for all .
Define and column vector such that
| (B.26) |
are the estimates from 7.7, and denotes from the -th repetition.
Assumption B.5.
The following conditions hold:
-
(i)
For some with compact ,
-
(ii)
For some ,
-
(iii)
For some , and all ,
-
(iv)
.
-
(v)
There exists such that
uniformly in , where and .
∎
Theorem B.5.
Proof of Theorem B.5.
First, note that equicontinuity of implies the quantiles groups to be well-defined, which together with Assumption B.5 implies
For each , using B.26 leads to the decomposition
by a uniform law of large numbers. The terms in are given by
These are split-sample empirical processes as in Theorem E.1, with functions
and
Step one of the proof of Theorem E.1 gives
Together with consistency of to , which follows from a uniform law of large numbers, this gives
Finally, asymptotic equicontinuity in gives
Summing over concludes the proof. ∎
Proof of Theorem B.3.
Follows from Theorem B.5, Lyapunov’s CLT and consistency of , which follows by a law of large numbers. ∎
Proof of Theorem B.4.
Follows directly from Theorem 4.2, noting that
always holds when is flat, since in that case the true coefficients in regression B.25 are all zero. ∎
Appendix C Modeling Power
I formalize the notion that using a larger sample for training is desirable by the analyst by introducing the concept of modeling power. This appendix uses notation introduced in Section 2. I say that an estimator has better modeling power than another if its collection of splits has a smaller expected loss. Although my results rely on no assumptions on the training algorithm other than a mild stability condition on , in practice, typically minimizes some loss function. For example, in Example 2, logistic regression minimizes log-likelihood, and neural networks minimize classification error over a class of network architectures. Let , be a loss function,
be the loss value of function , and
Note that is equal to the expected value of over uniformly at random, which is equivalent to the loss value of using a function that takes value in uniformly at random. The expected loss is defined as .
The expected loss, and thus the modeling power of an estimator depends only on the sample size used to estimate the functions in . That is because
where is a random subset of of size , with if , and assuming that is a multiple of for simplicity. If is calculated with the goal of minimizing the loss with respect to , it is reasonable to assume that the expected loss decreases with the sample size used to calculate . If that is the case, the expected loss increases with , since fewer data are used to estimate each . Hence, to increase modeling power when , one can pick a smaller (and ). However, if , a smaller leads to smaller statistical power, since fewer data are used as evaluation sample at each split. When using cross-fitting, modeling power increases with , since . In this case, the returns to increasing are diminishing. For example, if , is calculated with % of the sample, and this fraction raises to % with . If , however, the fraction only raises by another %. Although a large value of or small value of (when ) lead to better modelling power, my asymptotic framework takes these quantities as fixed. This means that the quality of the asymptotic approximation may be poor if is large (or small) relative to the sample size. For example, my asymptotic framework does not accommodate for leave-one-out cross-fitting, that is, .
Appendix D CLT for Split-Sample Averages
I derive a CLT for split-sample estimators based on sample averages. The objective is to expose my main result in an accessible setting, and discuss the main insights of the proof. The result is generalized in Appendix E, where I derive a functional CLT uniformly over a large set of data generating processes, and in Section 3 where I prove a CLT for Z-estimators.
The notation follows Section 2. Additionally, let be measurable functions for , and define
| (D.1) |
that is, is a marginal expectation that takes as fixed. This is typical notation in the empirical process literature.
Example 3 (Revisited).
In the probabilistic classifiers example, , is a function that predicts the probability of given , and
is the correct classification rate of predictor . ∎
In this section, I consider estimators of the form
| (D.2) |
where is a collection of random splits or cross-splits of the sample, is the number of folds ( denotes sample-splitting), is the size of each subsample (either the chosen subsample size when or the approximate fold size when ), and . I show in Theorem D.1 that is -Gaussian when centered around its marginal expectation
In Example 3, is the fraction of individuals correctly classified under a rule that predicts with probability
for an individual with characteristics .
Assumption D.1 establishes sufficient conditions for the CLT in Theorem D.1.
Assumption D.1.
-
(i)
For some ,
-
(ii)
For some and ,
pointwise for every .
∎
Assumption D.1(i) is a standard moments condition for CLTs, uniformly over possible values of . Assumption D.1(ii) is a mild stability condition on . Importantly, is allowed to converge at any rate and to any limit . This condition is more interpretable but stronger than what I use for proving the more general CLTs in Appendices E and 3. Assumption D.1(ii) differs from the typical approach in the double machine learning literature where faster convergence rates (often ) are required for nuisance functions, in a context where the target parameter does not depend on the estimated model (e.g., chernozhukov2018double).
Theorem D.1.
Theorem D.2.
The proof of Theorem D.1 relies on four main insights. I show them for the case of repeated cross-fitting, assuming that is a multiple of for simplicity. I provide a more detailed proof in Section D.1, and a formal proof follows from the more general Theorems E.1 and 3.1. The first insight and main argument of the proof is to show that
| (D.3) |
Once this is established, the result follows from Lyapunov’s CLT, since are iid. The second insight is that an application of Markov and Hölder inequalities gives that a sufficient condition for D.3 is that
| (D.4) |
where is a random subset of of size and is its complement. The third insight is that an application of the Law of Total Variance gives
| (D.5) | |||
| (D.6) |
Since the summands in D.5 are iid conditional on , D.5 equals D.6, which does not rely on the term . This is the crucial step that enables asymptotic normality without requiring an assumption on the rate at which converges to .
The final insight is that Assumption D.1 gives a sufficient condition for D.6 to converge to zero. For any ,
where the first term is bounded by . By Hölder’s inequality, the second term is bounded by
The first term above is bounded by Assumption D.1(i), and the second term can be made arbitrarily small since
converges to zero by the dominated convergence theorem, since
from Assumption D.1(ii) and independence of and . The result follows since can be made arbitrarily small.
D.1 Proofs
Proof of Theorem D.1.
I provide a detailed proof for the repeated cross-fitting case discussed in Appendix D, since that contains the main insights of the proof. A complete and formal proof follows from the more general Theorem E.1.
The argument consists of showing that
and applying Lyapunov’s CLT to the first term on the right side of the equality.
Define and note that
since for cross-fitting. For any , it holds that
| (D.7) | |||
| (D.8) | |||
| (D.9) | |||
| (D.10) |
D.7 follows from Markov’s inequality. D.8 defines as a random subset of of size , and uses the fact that the expected value does not depend on how the sample is (randomly) split. D.9 follows from Hölder’s inequality. D.10 follows since
by definition.
Since is assumed fixed, it is enough to show that
| (D.11) | |||
| (D.12) | |||
| (D.13) |
converges to zero. D.11 follows from the Law of Total Variance, since
D.12 follows since the observations are iid conditional on .
To show convergence to zero of D.13, consider the inequality
For any fixed ,
The first term is bounded by . By Hölder’s inequality,
The first term above is bounded by Assumption D.1(i), and the second term can be made arbitrarily small since
converges to zero by the dominated convergence theorem, since
from Assumption D.1(ii) and independence of and . The result follows since can be made arbitrarily small. ∎
Proof of Theorem D.2.
Note
By a law of large numbers conditional on ,
and similarly
Hence,
Fix and define .
by Hölder’s inequality. The term is bounded by Assumption D.1(i), and I show that converges in probability to zero. In the proof of Theorem D.1, I established that
This implies that since convergence implies convergence in probability. Hence,
which implies
A similar argument gives
Combining results implies
The result follows from Theorem D.1, since . ∎
Appendix E CLT for Split-Sample Empirical Processes
I derive a CLT for empirical processes based on a broad class of split-sample procedures, uniformly over a large class of probability distributions. This section generalizes Appendix D, which gives a more accessible exposition focusing on the particular case of sample averages. The CLT of this section can be used to prove asymptotic normality for a large class of estimators. That is the case for Z-estimators, which I develop in Section 3. Moreover, this CLT can be used to establish asymptotic consistency of the bootstrap in several applications, following, for example, the arguments in Chapter 3.7 of van2023weak.
The notation follows Section 2. Let be a set of probability distributions, and , the dataset, be an iid sample of . I denote the expected value under by , and the variance by . Given a set , let be measurable functions for and , with defined as in Section 2, and let . , , , and denotes all finitely discrete probability distributions. I use to denote cardinality when is a set and absolute value when is scalar. I denote by and respectively the covering and bracketing numbers, as in Definitions 2.1.5 and 2.1.6 of van2023weak. For , define the empirical measure
the marginal expectation
and the empirical process
I establish below sufficient conditions for the CLT for split-sample empirical processes, presented in Theorem E.1.
Assumption E.1.
The following conditions hold:
-
(i)
is totally bounded for some semimetric ;
-
(ii)
For every and , is measurable;
-
(iii)
For all , there exists a measurable envelope function ; That is, is such that for all and ;
-
(iv)
;
-
(v)
For every ,
-
(vi)
One of the following conditions holds for all :
(E.1) or
(E.2)
∎
Assumption E.2.
There exists such that for , , and every ,
uniformly in . ∎
Although technical, Assumption E.1 is a weak condition that is satisfied in many applications. Assumption E.1(i) through E.1(vi) are standard Donsker conditions in the literature of weak convergence of empirical processes (e.g., van2023weak), generalized for the presence of the functions . In fact, if and are singletons, these conditions are implied by the “” moments condition in Assumption D.1(i) (Proposition H.1). These assumptions are standard for proving functional CLTs by limiting the complexity of the sets and . In addition to ensuring that each set is Donsker, Assumption E.1 requires that the inequalities and convergences be uniform in . Importantly, Assumption E.1(vi) does not restrict the complexity of the class , and it does not imply the much stronger condition that is Donsker. In applications, except for the restrictions on , Assumption E.1(i) through Assumption E.1(vi) are verifiable since they depend only on the choices of and , and typically do not depend on how is calculated. The assumptions on involve the mild uniform square integrability condition Assumption E.1(iv), and the smoothness condition Assumption E.1(v).
Assumptions Assumption E.1(i) through Assumption E.1(vi) give standard conditions for a CLT when consists of a single sample split. The proof for the case of multiple splits relies on the additional Assumption E.2. This is a weak stability condition that requires to converge at any rate to any function , which is allowed to depend on . If and are singletons, this is implied by Assumption D.1(ii) (Proposition H.1). Note that the requirement is pointwise in , and it holds, for example, if for almost all .
Theorem E.1.
(CLT for split-sample empirical processes)
Let Assumptions E.1 and E.2 hold.
Then, the sequence is asymptotically -equicontinuous uniformly in and
uniformly in , where
For any sequence such that, for every ,
| (E.3) |
for some ,
in , where is a tight Gaussian process. Moreover, the covariance function of is given by , where
∎
To the best of my knowledge, this appears to be the first central limit theorem for empirical processes that average over multiple splits of the sample. This result enables asymptotic inference for a large class of split-sample estimators. For example, combined with the functional delta method, it immediately implies asymptotic normality of Hadamard differentiable functionals of the split-sample empirical measure
In Section 3, I use Theorem E.1 as a building block to prove asymptotic normality of split-sample Z-estimators, a broad class that cover many if not most estimators used in practice, including the ones in Section 6.
E.1 Proofs
Lemma E.1.
Let Assumptions Assumption E.1(i) through Assumption E.1(vi) hold, be a deterministic sequence, , and be a random (uniformly) subset of such that as . Define
Then, the sequence is asymptotically -equicontinuous.
Proof of Lemma E.1.
The result follows from an application of Theorems 2.11.1 and 2.11.9 in van2023weak, respectively for when conditions E.1 and E.2 hold. Their notation is adapted with , , and , where it is implicit in the notation that (alternatively, one could denote instead of ). The presence of the suprema over and guarantee that the conditions in those theorems hold for any sequences and . ∎
Lemma E.2.
Let Assumptions Assumption E.1(i) through Assumption E.1(vi) hold, and be a random (uniformly) subset such that as . Define
Then, the sequence is asymptotically -equicontinuous uniformly in .
Proof of Lemma E.2.
Proof of Theorem E.1.
The proof is divided into three main steps. First, I show that
uniformly in . Second, I show that is asymptotically -equicontinuous. Finally, I prove the Gaussian limit of for any finite .
Step one.
Let , , and fix . It follows that
| (E.6) | |||
| (E.7) | |||
| (E.8) |
where E.6 follows from Markov’s inequality, and E.7 defines as a random subset of (uniformly over all subsets).
Since , E.8 converges to zero if the term inside the expectation convergences in probability to zero uniformly in , since it is uniformly integrable (by Assumption E.1(iv)). This follows from stochastic equicontinuity of (as a process indexed by ) and pointwise convergence in , by applying Theorem 22.9 in davidson2021stochastic. Stochastic equicontinuity follows since
is a sum of two stochastically equicontinuous processes, respectively by Lemma E.2 and Lemma E.1. For pointwise convergence, I show that the variance converges to zero, and note is mean zero by construction. For an arbitrary ,
| (E.9) | |||
| (E.10) | |||
where E.9 uses the Law of Total Variance and the fact that , and E.10 follows since the summands are iid conditional on . Finally, the last term converges to zero from Assumption E.2. Note that since are uniformly square integrable by Assumption E.1(iv), convergence in probability of the conditional variance implies its convergence in .
Step two.
Let and note that
Let , , , and .
The last term converges to zero from asymptotic equicontinuity of for arbitrary sequences and satisfying for all . Asymptotic equicontinuity can be verified under Assumption E.1, for example, by applying Theorem 2.11.1 (when E.1 holds) and Theorem 2.11.9 (when E.2 holds) of van2023weak. Their notation is adapted with , , and , where it is implicit in the notation that (alternatively, one could denote instead of ). For , note that
and
for any , , and .
Step three.
If , for all , and the Gaussian limit follows from Lindeberg’s CLT and the Cramér-Wold device, using Assumption E.1(iv).
For and , let
so .
Without loss of generality, let .
In Lemma H.1, I show that
Hence,
since the sum in the right is the second moment of a binomial distribution with parameters and . Collecting the results,
The Gaussian limit follows from Lindeberg’s CLT conditional on and the dominated convergence theorem, and the Cramér-Wold device.
Finally, let and . I show that
| (E.11) |
converges to zero in . For the mean,
For the variance,
where the first equality follows since by the Law of Total Variance, and the second equality since the summands are iid conditional on . Since is bounded, if , which follows from
by a law of large numbers, since and splits are independent. Finally, the Gaussian limit follows from Lindeberg’s CLT and the Cramér-Wold device. ∎
Appendix F Inference with Fast Converging Moments
Consider the setting of Section 4.1. The normal approximation CI 4.3 may not cover with nominal probability when the variance of any moment function evaluated at the limit parameter is , that is,
| (F.1) |
for any . If that happens, either , is not invertible, or both. If , 4.1 implies that the centered estimator multiplied by converges in probability to zero, and the normal approximation in 4.3 may not be accurate. Similarly, if is not invertible, is not well-defined, and the normal approximation may be inaccurate. In this subsection, I provide an approach to inference on that is general in considering the class of Z-estimators in Section 3.
I explore the fact that F.1 implies that the empirical moment equation evaluated at converges faster than the typical rate to construct a confidence interval for that is uniformly asymptotically valid regardless of whether F.1 happens or not. The issue discussed in this section is not important for every application. First, I discuss examples of when one may or may not comfortably assume that F.1 does not hold. Then, I propose a confidence interval, prove its uniform asymptotic validity, and characterize its power properties. I focus on the estimator from Section 3, and the results can be extended to and using similar techniques.
F.1 Examples
In many cases, the researcher can safely assume that F.1 does not happen, depending on the setup and definition of . In other cases, as in Section 7, F.1 can happen under one of the main hypotheses of interest. I present examples of both cases below.
Example 2 (Revisited).
In Example 2, , is binary, and is a predictor of using covariates . The parameter of interest is a split-sample Z-estimand with :
The variance
is positive unless always predicts correctly or always incorrectly. In practice, predictive algorithms rarely have a near perfect (or imperfect) performance, and in many cases the researcher can confidently assume . ∎
Example 9.
Consider a dataset with covariates , a mean zero continuous outcome , and the goal of assessing whether a predictor has predictive power for . One way of assessing predictive power for is by conducting inference on the covariance
is a Z-estimand with moment function , and its limit variance is
Let be the limit of . If has no predictive power for , for example because and are independent, , and . Hence, the CI in 4.3 may fail to achieve nominal coverage asymptotically. ∎
Remark F.1.
When , the asymptotic distribution of may depend on the specific structure of how is calculated. Let be a mean zero scalar random variable, , , and be a 2-fold cross-split of the data of equal sizes. Let for some odd positive , where . Then, and , which follows a non-trivial distribution that depends on . If, for example, , is approximately distributed as the cube of a standard normal distribution, and leads to a different distribution. Moreover, the dependence between and is not trivial. ∎
F.2 An Adaptive Confidence Interval
I show how to construct a confidence interval that satisfies
regardless of whether F.1 may hold or not, by introducing a tuning parameter. In Sections 4.2 and B.5, I propose a different approach for the particular cases of inference on comparisons between models and in the Generic ML context of chernozhukov2025generic, which explicitly account for the dependence across splits.
I construct by inverting the test
| (F.2) |
that is, contains all values of for which the null hypothesis is not rejected at significance level . My approach consists of a data-driven procedure to choose one of two p-values for testing F.2: or . is a conservative p-value, meant to be valid when F.1 holds, that is, the p-value a researcher would use if they knew F.1 were true. is an exact p-value, coming from the normal approximation 4.3, as it achieves exact nominal coverage in large samples when F.1 does not hold. Hence, I test F.2 with the p-value when the data suggest that the empirical moment equations are away from zero, and with otherwise. The idea of using different tests based on pre-testing some condition (in this case, whether the empirical moment equations are away from zero), is similar to shi2015model, in the context of moment inequalities. Specifically,
The idea behind is that when . The sequence is a tuning parameter that should ideally be specified before the data analysis. The properties of and are specified in Assumption F.1 below.
Assumption F.1.
The following conditions hold:
-
(i)
;
-
(ii)
;
-
(iii)
The set can be decomposed as , where
-
(a)
For every ,
is differentiable at for , and for some ,
-
(b)
, for some uniformly in , and
-
(a)
∎
Assumption F.1(i) requires the p-value to be valid, even if conservative, including when . Constructing is context-specific, but a conservative, trivially valid option is . Note that this option does not lead to an unbounded CI since with probability approaching one for values of far from . Assumption F.1(ii) requires to converge to zero at the rate. Assumption F.1(iii) substitutes and weakens Assumption B.1(iii) and Assumption B.1(v). It allows to be singular and to have multiple solutions for when the variance of is zero for some . Valid inference is achieved in these cases since with probability approaching one. Note that Assumption B.1(iii) and Assumption B.1(v) imply Assumption F.1(iii) since , and if
Assumption F.1(iii) implies both Assumption B.1(iii) and Assumption B.1(v). I establish the uniform asymptotic validity of , and explore its power properties.
I show that the hypothesis test F.2, where is rejected if , has power approaching for fixed alternatives and non-trivial power for some sequences of local alternative hypotheses. I compare my test with an oracle test that correctly picks or depending on the asymptotic behavior of . In order to study local power, I consider sequences under different regimes for the limit behavior of and the variance of . Let
The oracle test is defined by
This test is infeasible since it depends on the sequence of DGPs . For the different regimes, I compare the limits
Theorem F.2.
Let Assumption B.1(i)-Assumption B.1(iv) and Assumption F.1 hold, , , and be a sequence such that the limits , and exist. Assume is an independent Bernoulli random variable taking value with probability and with probability (that is, it rejects the null with probability ). Then, the relationships in Table 5 hold, where each row defines a separate regime for .
* Assumes ; ** Assumes ; *** Assumes .
∎
F.3 Proofs and Extra Definitions
Proof of Theorem F.1.
Let be such that
exists.
For , note that by Theorem E.1,
and, for any ,
If ,
for some , and hence
As a consequence,
and , which concludes the proof. ∎
Proof of Theorem F.2.
Define
First, let . If ,
since
If ,
is , and . If , , and .
Second, let , and hence . It follows that
is if , and converges to zero if .
Finally, the last row follows since . Note that for the second row. ∎
Appendix G Note on Comparing Two Nonparametric Models
I discuss an extension of the setting of Section 4.2 for comparing to the performance of another model , computed with the same split-sample approach as .
Let
and denote the split-specific models and , where and , that is, the two models are trained using the same sample but different algorithms. For example, could be estimated with random forests while could be estimated with neural networks. Denote
and
can be used for testing whether for all versus the alternative that for at least one , similarly to Section 4.2.1 and Theorem 4.2. Note that the Donsker and rate conditions in Assumption 4.1(ii) are not required for Theorem 4.2. They are used only for the pointwise Theorem 4.3 to cover the case . Similarly, can be used to test whether for all versus the alternative that for at least one .
Appendix H Additional Results
Proposition H.1.
Proof of Proposition H.1.
E.1(i) and E.1(ii) hold trivially. E.1(iii) holds by taking as its own envelope. The uniform integrability condition E.1(iv) is implied by the assumption D.1(i). E.1(v) holds trivially. E.1(vi) holds since both covering and bracketing numbers are equal to with singleton . Finally, E.2 follows since
as established under D.1(ii) in the proof of Theorem D.1 D.13, since convergence in implies convergence in probability. ∎
Lemma H.1.
In the context of Theorem E.1,
Proof.
I show that and as . By definition, . since all are equally distributed for any . The event is equivalent to the event that observation is chosen in exactly of the splits of the sample. Since the splits are independent, follows a binomial distribution with parameters and . Hence, the probability of this event is .
To show that , I use the fact that
Hence, it’s enough to show that
I show that . Note is the number of draws in each split. Using combinatorial arguments, the conditional probability is given by
represents the number of splits that contain both observations 1 and 2. Since observation 2 is chosen in splits, . There are ways of choosing among the splits that contain observation 2, which will also contain observation 1. There are ways of choosing the remaining splits that contain observation 1 but not 2. is the probability of choosing observation 1 in the splits that contain both observations. is the probability of not choosing observation 1 in the remaining splits that contain observation 2. is the probability of choosing observation 1 in the splits that contain observation 1 but not 2. Finally, is the probability of not choosing observation 1 in the remaining splits that contain neither observation.
For large , we can approximate the combinatorial terms:
Similarly,
It follows that
where the third equality uses Vandermonde’s Identity. ∎
Appendix I Covariate Adjustment in Randomized Trials
Let , where is an observed outcome, is a binary (randomized) treatment assignment indicator, and is a set of covariates, for some . Let denote potential outcomes respectively under treatment and control, and . In the simplest form of an RCT, . In this setting, the ATE can be identified from the regression
| (I.1) |
The covariates are not necessary for identification of . However, adding regressors in (I.1) can lead to power improvement by reducing the variance of the error term and thus the asymptotic variance of the least squares estimator of . One approach to incorporating covariates is through a covariate-adjustment term :
| (I.2) |
If , does not depend on . Still, its OLS estimator does depend on . In practice, one needs to estimate with a model . Inference becomes challenging if the same data is used to estimate both and because the observations in I.2 become no longer iid. The asymptotic distribution of can be characterized following Section 3, specifically Theorem 3.1.
Words Matter: Forecasting Economic Downside Risks with Corporate Textual Data
Cansu Isler111Ph.D Candidate, Brandeis University. Contact: cansuisler@brandeis.edu. I am deeply grateful to my advisors, Davide Pettenuzzo and Yeabin Moon, for their invaluable guidance and continuous support. I also thank Catherine L. Mann, Giulia Mantoan, and seminar participants at Brandeis University for their insightful comments and stimulating discussions.
November 7, 2025
Accurate forecasting of downside risks to economic growth has become critically important for policymakers and financial institutions, particularly following recent economic crises. This paper builds upon the Growth-at-Risk (GaR) approach, introducing a novel daily sentiment indicator derived from textual analysis of mandatory corporate disclosures (SEC 10-K and 10-Q reports) to forecast downside risks to economic growth. Utilizing the Loughran–McDonald dictionary and word-count methodology, I calculate firm-level tone growth by measuring the year-over-year difference between positive and negative sentiment expressed in corporate filings. These firm-specific sentiment metrics are aggregated into a weekly tone index, weighted by firms’ market capitalizations to capture broader, economy-wide sentiment dynamics. Integrated into a Mixed Data Sampling (MIDAS) Quantile regression framework, this sentiment-based indicator enhances the prediction of GDP growth downturns, outperforming traditional financial market indicators such as the National Financial Conditions Index (NFCI). The findings underscore corporate textual data as a powerful and timely resource for macroeconomic risk assessment and informed policymaking.
Keywords: Sentiment Analysis, GDP Forecasting, Growth-at-Risk, MIDAS Quantile Regression, Textual Analysis, Corporate Filings
JEL Classification: C53, E37, G17, G32
1 Introduction
Accurately forecasting downside risks to economic growth is a central concern for policymakers and financial institutions. This concern has intensified in the wake of severe downturns such as the 2008 global financial crisis and the 2020 COVID-19 shock. Traditional macroeconomic forecasts, which often focus on the mean or baseline outlook, have been criticized for failing to anticipate such tail-risk events. In response, a growing literature has shifted toward modeling the entire distribution of future GDP growth rather than just point forecasts. In particular, the “Growth-at-Risk” (GaR) framework links current financial and economic conditions to the conditional distribution of future growth, providing estimates of the probability of adverse outcomes in GDP growth (Adrian et al., 2019; Prasad et al., 2019). This approach is analogous to the concept of Value-at-Risk in finance but applied to aggregate economic activity: it quantifies how vulnerabilities in the present can translate into higher likelihood of low (or negative) growth in the future. Empirical studies find that downside risks to GDP growth vary over time and are heavily influenced by prevailing financial conditions (Adrian et al., 2019). Notably, the left tail of the future growth distribution tends to deteriorate markedly when financial markets are under stress, even if the central forecast remains stable. This ability to assess the entire forecast distribution (and especially its lower tail) makes GaR a powerful tool for macro-financial surveillance and has been adopted by institutions like the International Monetary Fund for risk monitoring (Prasad et al., 2019).
However, existing GaR models and related macroeconomic risk forecasts often rely on indicators that may be overly backward-looking or slow-moving. Many studies use aggregate financial conditions indices—such as the National Financial Conditions Index (NFCI)—to proxy for macro-financial conditions. While these indices capture contemporaneous financial tightness or stress, they largely reflect information from recent historical data and traditional economic indicators. As a result, their predictive content for future downturns can be limited. For example, Hatzius et al. (2010) find that several popular financial conditions indexes have little out-of-sample forecasting advantage for growth relative to simpler predictors, except during periods around the crisis they were designed to track. Similarly, Kliesen et al. (2012) document that many financial stress indices are highly persistent and tied to past crisis episodes, raising concerns about their utility in signaling emerging risks.
In this paper, I propose a text-based approach to enhance forecasts of downside macroeconomic risk by extracting forward-looking sentiment from corporate disclosures. Firms’ mandatory reports to investors—especially annual 10-K and quarterly 10-Q filings submitted to the U.S. Securities and Exchange Commission (SEC)—contain extensive qualitative information about business prospects and risks. Senior executives discuss financial results alongside risk factors, uncertainties, and outlooks, making these disclosures timely indicators of corporate sentiment. An increasingly pessimistic tone or heightened discussions of risk across many firms may signal broader impending economic slowdowns. My approach builds on the burgeoning literature in financial textual analysis, which demonstrates that the content and tone of corporate documents can meaningfully predict economic outcomes Loughran and McDonald (2011); Hassan et al. (2019).
Specifically, I construct a new firm-level sentiment indicator using the textual tone of 10-K and 10-Q reports, applying finance-specific sentiment dictionaries and natural language processing techniques (Loughran and McDonald, 2011). The resulting high-frequency (weekly) sentiment indicator aggregates sentiment across numerous corporate filings, capturing management’s private forward-looking information regarding demand conditions, investment plans, and perceived risks before they manifest in traditional macroeconomic indicators. This study complements recent research emphasizing the predictive power of textual analysis, including news-based indexes and corporate reports (Baker et al., 2016; Hassan et al., 2023; Bybee et al., 2021).
A critical methodological challenge in using textual sentiment to forecast macroeconomic outcomes is the mismatch between high-frequency sentiment measures (weekly or daily) and low-frequency GDP data (quarterly). I bridge this gap through Mixed Data Sampling (MIDAS) quantile regression methods introduced by Ghysels et al. (2007). MIDAS regression efficiently incorporates high-frequency predictors into low-frequency regression models without requiring temporal aggregation. I further adapt MIDAS to a quantile regression framework, emphasizing tail-risk forecasting (Prüser and Huber, 2024; Andreou et al., 2013). This approach allows immediate utilization of changes in corporate sentiment and recognizes potentially nonlinear impacts of sentiment on different quantiles of GDP growth, particularly severe downturns.
This paper makes three key contributions:
-
1.
Novel sentiment-based risk indicator: I develop a new forward-looking sentiment indicator derived from corporate disclosures, providing timely signals of macroeconomic vulnerabilities not fully captured by traditional backward-looking financial indicators.
-
2.
Methodological innovation – MIDAS quantile regression: I integrate high-frequency corporate textual sentiment into a quarterly macroeconomic forecasting framework, demonstrating significant advantages over traditional temporal aggregation methods.
-
3.
Practical policy implications: Empirical results indicate that my sentiment-based indicator significantly enhances forecasts of GDP downside risks, offering policymakers an improved tool for early detection and management of economic vulnerabilities.
Overall, my study bridges macro-finance literature on Growth-at-Risk and financial textual analysis literature, providing novel evidence that forward-looking corporate narratives significantly inform macroeconomic risk assessments.
The rest of the paper is organized as follows. Section 2 reviews the related literature. Section 3 describes the data used in the analysis. Section 4 details the construction of the corporate sentiment indicator and outlines the MIDAS quantile regression methodology. Section 5 presents the empirical results, including both in-sample analysis and out-of-sample forecasting performance, and it provides robustness checks. Section 6 concludes.
2 Literature Review
2.1 Textual Sentiment Analysis in Finance and Macroeconomics
"Over the past two decades, researchers have increasingly used textual analysis to extract sentiment and predictive signals from unstructured data in finance and macroeconomics. Early seminal work demonstrated the value of qualitative text for understanding market behavior. For example, Tetlock (2007) quantitatively analyzed a popular Wall Street Journal column and found that high pessimistic tone in media coverage predicts short-term stock price declines followed by reversion to fundamentals. This study gave concrete evidence that investor sentiment, as reflected in news, can influence asset prices. Subsequent research in finance solidified the importance of textual sentiment measures. In particular, Loughran and McDonald (2011) developed a domain-specific sentiment dictionary tailored to financial language, showing that general-purpose sentiment lexicons (e.g. Harvard IV) misclassify common finance terms (such as “liability” or “capital”) as negative, whereas a finance-specific dictionary yields more meaningful tone measures for corporate disclosures. Using their word lists on firms’ 10-K reports, they demonstrated that a negative tone in annual reports is associated with lower future earnings and stock returns, highlighting that qualitative tone in mandatory filings provides information beyond quantitative accounting data. In the macroeconomic realm, textual analysis has been used to gauge broad economic sentiment and uncertainty. Baker et al. (2016), for instance, introduced the Economic Policy Uncertainty index by counting policy-related keywords in newspapers, and showed that spikes in this text-derived uncertainty measure foreshadow declines in investment, output, and employment. This work, published in a top economics journal, underscored that newspaper text can be systematically quantified to capture policy-driven economic sentiment with real effects. More recently, Husted et al. (2020) construct a Monetary Policy Uncertainty index from Federal Reserve documents, illustrating how textual cues from structured central bank communications can be quantified to reflect uncertainty around policy actions. Overall, these studies established that analyzing text—from news media, firm disclosures, or central bank communications—yields sentiment metrics that often have significant predictive power for financial markets and the macroeconomy.
Methodologically, approaches to textual sentiment analysis in finance and economics can be broadly divided into dictionary-based and machine-learning-based techniques. Dictionary (or lexicon) methods remain popular for their simplicity. In this approach, one defines a lexicon of positive, negative, and uncertainty words and measures sentiment by counting their occurrences in a document (possibly scaled by total words or adjusted for context). Influential early studies like Tetlock (2007) and Loughran and McDonald (2011) employed dictionary-based sentiment – the former using a general sentiment word list (Harvard’s negative word list) and the latter creating a finance-specific dictionary – to successfully link textual tone with economic outcomes. The appeal of dictionary methods lies in their interpretability: researchers can easily understand which words drive the sentiment score, making it straightforward to interpret “what the text is saying.” However, a well-known limitation is that fixed dictionaries may fail to capture contextual nuances or evolving language usage. For example, a dictionary cannot on its own distinguish negation (“not good”) or sarcasm, nor can it adapt when new jargon or firm-specific language emerges. These limitations have motivated the use of more complex machine learning (ML) and natural language processing (NLP) techniques. ML-based approaches treat sentiment analysis as a predictive classification or regression problem, using statistical models trained on example texts to infer sentiment. Such models can be supervised (trained on texts labeled with sentiment or with an outcome of interest) or unsupervised (learning latent topics or themes from text). In economics, Gentzkow et al. (2019) survey a wide array of text-as-data methods, highlighting how techniques like topic modeling, clustering, and tree-based classifications have been applied to extract information from text beyond simple word counts. For instance, Jegadeesh and Wu (2013) present a novel data-driven content analysis method: rather than relying on an external dictionary, they infer the tone of corporate reports by statistically learning which words in 10-K filings predict firms’ subsequent stock returns. Their algorithm assigns weights to words based on market reactions, effectively creating a customized sentiment measure with improved predictive power for returns. This approach is akin to a machine-learning model that “trains” on historical text and outcome data to identify which patterns of language are truly meaningful. The advantage of such ML approaches is greater flexibility and potentially higher out-of-sample accuracy – they can capture subtle linguistic cues (tone, context, even syntax) that fixed dictionaries might misclassify. Indeed, applications of deep learning and transformer-based models (fine-tuned for financial text) have shown further improvements in capturing the nuance of sentiment in news and social media. The downside is that these models often act as “black boxes,” making it harder to interpret what drives their sentiment scores; they also require large training corpora and careful tuning to avoid overfitting noise. In practice, both dictionary and ML approaches are viewed as complementary. Many finance studies start with dictionary-based sentiment as a baseline, given its ease of use, and then explore ML enhancements for additional predictive gain. As Gentzkow et al. (2019) note, the choice often depends on the context and available data: if human labels or clear objective outcomes are available to train on, supervised ML can outperform lexicon methods, whereas in data-scarce settings a well-crafted dictionary may suffice and is less prone to spurious patterns.
Another important dimension in this literature is the nature of the textual sources: structured vs. unstructured disclosures, and firm-level vs. media-based text data. Structured disclosures refer to texts that follow a regulated or standardized format, such as corporate filings (annual reports, quarterly reports) or transcripts of earnings conference calls, which tend to have formal language and recurring sections (e.g. discussions of financial performance, risks, outlook). Unstructured texts include news articles, social media posts, or other free-form communications that are not standardized in presentation or content. Each type of source offers distinct advantages and challenges for sentiment analysis. Firm-level disclosures, structured texts produced directly by companies, often contain rich insider information about business conditions and expectations. Managers may reveal optimism or concern about future earnings, project pipelines, or macro conditions in the narrative sections of reports or calls. These texts are usually carefully worded, yet researchers have found they still contain predictive signals. For example, the tone in annual reports and IPO prospectuses has been shown to predict firm performance and risk (Loughran and McDonald, 2011; Jegadeesh and Wu, 2013). Hassan et al. (2019) demonstrate that by aggregating firm-level textual information one can measure economy-wide risks: they analyze thousands of quarterly earnings call transcripts to construct a novel index of firm-level political risk, then show that elevated political risk in these transcripts correlates with reduced corporate investment and hiring at the macro level. This underscores a benefit of firm-level textual data: micro-level sentiments can be aggregated to glean bottom-up insights about the broader economy’s state (e.g. if many firms lament weak demand or policy uncertainty, it likely signals macroeconomic headwinds). However, a challenge with firm disclosures is their lower frequency and coverage – e.g. 10-K reports are annual, and even quarterly calls may be too infrequent to capture rapidly changing conditions. Moreover, private firms or small enterprises (which collectively matter for the economy) do not produce similar publicly available texts, so macro sentiment indices built from firm communications might be biased toward larger public companies.
In contrast, media-based textual data – such as newspapers, financial news wires, and other journalistic sources – provide higher-frequency and broad coverage of information, albeit in a less structured form. Media outlets continuously interpret and synthesize diverse signals (firm news, economic data releases, policy changes, geopolitical events), so the sentiment in media can serve as a near real-time barometer of public and investor mood. Tetlock (2007)’s media sentiment measure is an early example focusing on a daily news column; subsequent studies have extended this to a variety of outlets and time periods. Notably, Manela and Moreira (2017) construct a News Implied Volatility (NVIX) index by parsing historical newspaper archives for terms related to financial volatility and disaster, effectively creating a text-based gauge of financial market fear. NVIX spikes during wars, crises, and recessions, and it predicts increases in stock market volatility and risk premia. This suggests that news content captures risk perceptions that are not fully reflected in contemporaneous market prices. Recent research also leverages news sentiment for macroeconomic forecasting: Ashwin et al. (2024), for example, show that incorporating sentiment extracted from European newspaper articles improves the accuracy of nowcasts of GDP growth, especially during crisis periods when traditional quantitative indicators are slow to reflect abrupt changes. Media-based sentiment indicators (such as those based on newspaper archives or even social media feeds) thus offer a top-down perspective on the economy, complementing the bottom-up signals from firm-level text. The challenge with unstructured media data is the sheer volume and noise: news articles may contain irrelevant information, journalistic bias, or repetitive content, and distinguishing signal from noise requires careful filtering or advanced NLP techniques. There is also the issue of multiple narratives – media might simultaneously carry optimistic and pessimistic takes from different sources, making aggregate sentiment harder to define. Despite these challenges, the benefits of media text data lie in timeliness and scope: when an economic shock occurs (e.g. a pandemic or financial panic), media sentiment can deteriorate almost immediately, whereas firm financials or official statistics reflect the impact only with a lag. Indeed, an emerging consensus in the literature is that no single textual source is definitively superior; rather, each captures a different facet of sentiment. Firm disclosures are closer to fundamental information and may better predict micro outcomes, while media reflect broader perceptions and can diffuse information quickly. A critical takeaway is that textual sentiment adds incremental value over traditional quantitative data: whether it is the guarded optimism of executives in an earnings call or the alarmist tone of a front-page news article, these qualitative signals have been shown to anticipate important movements in asset prices and macro aggregates.
Despite the substantial progress in textual analysis for finance and economics, there remain noteworthy gaps in the literature. One such gap is the relatively limited integration of text-derived sentiment measures into established macroeconomic risk forecasting frameworks. Most existing studies use textual sentiment to predict mean outcomes (like stock returns, earnings, or GDP growth) or volatility/uncertainty measures; far less work has examined whether textual data can improve forecasts of tail risks or extreme events. In particular, the burgeoning research on Growth-at-Risk – which focuses on forecasting the lower tail of the GDP growth distribution – has yet to fully incorporate the advances from textual sentiment analysis. While we now have indices like EPU, NVIX, or central bank communication sentiment that capture aspects of risk and uncertainty, these are only beginning to be linked to formal tail-risk models. This represents an important opportunity for new research. Textual sentiment could plausibly inform downside risk: for example, a sharp rise in negative language in news or corporate commentary might presage an economic downturn’s severity even if traditional financial indicators remain benign. However, integrating text into macroeconomic quantile frameworks is non-trivial, as one must map qualitative information into numerical predictors suitable for tail forecasting. To date, very few studies have attempted this integration, leaving a gap that the present work aims to address. By bringing together the textual sentiment literature and the Growth-at-Risk framework, I contribute to filling this void, examining whether and how qualitative information can enhance the assessment of macroeconomic tail risks.
2.2 Growth-at-Risk and Quantile MIDAS in Economic Forecasting
The concept of Growth-at-Risk (GaR) has emerged as a key paradigm for assessing macroeconomic tail risks, drawing inspiration from risk management approaches in finance (such as Value-at-Risk) but applied to GDP growth. GaR generally refers to a specified lower quantile of the future GDP growth distribution (for example, the 5th percentile), conditional on information available today. Rather than focusing on the most likely outcome or the average forecast, GaR shines light on downside risk – how bad economic growth could get in adverse scenarios – and how this risk fluctuates over time with changing conditions. A seminal contribution is the work of Adrian et al. (2019), who show that U.S. financial conditions have strong predictive power for the left tail of the GDP growth distribution. In their framework, when financial conditions (capturing variables like credit spreads, equity market volatility, leverage, etc.) are loose, the entire distribution of future growth shifts: near-term median growth is higher, but the left tail (worst-case growth) is considerably less fat. Conversely, when financial conditions tighten, GaR deteriorates – the left tail becomes much more negative even if the median forecast only declines modestly. This finding, often summarized as “vulnerable growth,” highlights an intuitive but quantitatively important macro-financial linkage: credit booms or easy financing conditions can portend not just higher expected growth but also increased risk of a sharp downturn in the medium term. The GaR approach thus provides a systematic way to monitor financial stability risks spilling into the real economy. Subsequent studies have extended this insight across countries and time. Adrian et al. (2022) analyze a panel of advanced economies and find a similar pattern internationally: a deterioration in financial conditions disproportionately elevates downside growth risks (the lower 5% quantile) relative to the median. Notably, they document an intertemporal trade-off consistent with leverage cycles: extremely accommodative conditions reduce GaR in the short run (fewer near-term recessions) but lead to a buildup of vulnerabilities that increase GaR at longer horizons (making severe recessions more likely down the line). Their evidence, strengthened by causal identification via instruments, suggests that policymakers should account for dynamic tail-risk trade-offs – policies or shocks that stimulate growth today may sow the seeds of greater tail risk tomorrow. The GaR framework has quickly gained traction in policy institutions for macroprudential surveillance and stress-testing, since it offers a quantitative way to link current macro-financial conditions to the risk of future economic calamity. For instance, the International Monetary Fund’s research and country surveillance have incorporated GaR analyses to evaluate how financial vulnerabilities translate into GDP downside risk (Prasad et al., 2019). There is also a growing literature exploring which factors drive GaR: credit growth, asset price bubbles, leverage, and uncertainty measures have all been examined as potential predictors of the conditional distribution of growth (Castelnuovo and Mori, 2024; Prüser and Huber, 2024). Overall, the consensus is that modeling the full distribution (or at least the lower tail) of GDP growth yields insights that a traditional mean forecast or recession probability model might miss. By focusing on tail outcomes, GaR research directly addresses questions of financial stability and extreme event risk, making it particularly relevant in the aftermath of the 2008–09 crisis and the COVID-19 shock, where managing downside risk is as crucial as managing central forecasts.
The typical empirical approach to Growth-at-Risk employs quantile regression models to link current economic/financial indicators to future lower quantiles of GDP growth. Unlike OLS regression, which models the conditional mean, quantile regression allows the impact of predictors to differ across the distribution of the dependent variable. This is critical because certain variables might have small effects on the expected growth rate but large effects on the left tail. In Adrian et al. (2019)’s GaR model, for example, a financial conditions index had an economically negligible coefficient at the median of the growth distribution but a strong coefficient at the 5th percentile. Quantile regressions thus capture asymmetric predictive relationships. However, one practical challenge arises: GDP growth is typically measured quarterly, whereas many relevant predictors (financial market variables, sentiment indices, etc.) are available at higher frequencies (monthly, weekly, or daily). Using only quarterly averaged data might discard timely information. This is where the MIDAS (Mixed Data Sampling) Quantile approach becomes valuable. MIDAS Quantile is an extension of the MIDAS regression framework – originally developed by Ghysels et al. (2006) for mean predictions – into the realm of quantile forecasting. The idea is to allow high-frequency indicators to enter a quarterly quantile regression through distributed lag polynomials or other parsimonious representations, rather than aggregating them into quarterly averages. By doing so, one exploits the rich information content and timeliness of high-frequency data without running into the curse of dimensionality. Recent research has started to apply MIDAS Quantile in the context of macroeconomic tail-risk forecasting. For instance, Castelnuovo and Mori (2024) employ a mixed-frequency quantile regression to study how financial uncertainty indicators (available monthly) influence U.S. GDP growth’s conditional skewness and downside risk at the quarterly horizon. Their results indicate that increases in uncertainty foreshadow a more pronounced left tail for GDP growth – in other words, uncertainty shocks worsen GaR – and by using the MIDAS structure they capture these effects in real time as monthly data arrive. The MIDAS Quantile framework proved effective in handling the ragged-edge problem (when higher-frequency data extend further than low-frequency data at a given forecast origin) and in improving forecast accuracy for tail outcomes, as it avoids arbitrary aggregation of volatility or credit data. Other studies echo these benefits: high-frequency financial market signals (such as daily credit spreads or stock volatility) can substantially improve nowcasts of tail risks when integrated via mixed-frequency quantile models. Empirically, mixed-frequency tail-risk models tend to pick up nascent stress conditions faster – for example, a sudden spike in corporate bond spreads in mid-quarter would immediately drag down the forecasted 5th percentile of GDP growth for that quarter, even if the average growth forecast moves only slightly. This sensitivity is crucial for early warning systems.
It is worth noting that most of the GaR literature to date has focused on traditional structured data (financial indicators, credit aggregates, etc.) as predictors, with relatively little attention to textual or sentiment-based predictors. Nearly all GaR models include some measure of financial conditions or macro volatility; some include survey-based expectations or uncertainty indices. Yet, as discussed, there are now text-based measures of sentiment and uncertainty (e.g., news sentiment indices, policy uncertainty from text, corporate risk sentiment) that could enrich these models. The integration of textual sentiment into GaR remains limited in the existing literature – a gap that my work aims to fill. Intuitively, textual sentiment indicators may capture dimensions of risk that are not fully reflected in market prices or hard data. For example, consider a period of mounting investor anxiety that has not yet translated into wider credit spreads or falling stock prices; news sentiment might already be sharply negative even while quantitative financial indicators lag. Incorporating such a sentiment variable into a GaR quantile regression could flag rising downside risk earlier than a model based purely on lagging financial data. Conversely, textual data might help distinguish between benign and more dangerous forms of financial easing – e.g., if credit growth is high but media sentiment remains cautious, the increase in tail risk might be less than in a scenario where credit is booming amid euphoric media chatter. These conjectures underscore why merging text analysis with GaR is a promising direction. My paper breaks new ground by doing exactly this: I extend the GaR framework using MIDAS Quantile regressions that incorporate textual sentiment measures alongside traditional predictors. In doing so, I address both a methodological gap (bringing high-frequency textual data into mixed-frequency quantile models) and a substantive gap (studying how sentiment informs macroeconomic tail risk). This contribution builds on the literatures reviewed above, uniting them. It heeds the call from the sentiment analysis side to apply rich textual information to pressing macroeconomic questions, and from the GaR side to broaden the information set used for tail-risk assessment. By clearly demonstrating the incremental value of text-based sentiment in forecasting GDP downside risks – and discussing the conditions under which it matters – I aim to push the frontier of both literatures.
3 Data
3.1 Firm Filings and SEC EDGAR
The U.S. Securities and Exchange Commission’s (SEC) Electronic Data Gathering, Analysis, and Retrieval (EDGAR) system constitutes a comprehensive public repository of financial documents submitted by publicly traded companies in the United States. Within EDGAR, two types of filings, namely the 10-K and 10-Q reports, hold particular significance for academic research and financial market analysis.
Annual 10-K filings provide extensive insights into a firm’s financial condition, encompassing audited financial statements, detailed Management’s Discussion and Analysis (MD&A) of financial results, and explicit disclosures of risks and operational strategies. These filings are essential not only for understanding firms’ historical financial performance but also for capturing management perspectives and strategic outlooks (Loughran and McDonald, 2016; Brown et al., 2020). In contrast, quarterly 10-Q filings deliver interim financial statements, unaudited updates, and timely disclosures regarding corporate performance and evolving risk factors. The more frequent issuance of 10-Q reports allows researchers and analysts to monitor changes in firm-level conditions and managerial sentiment with finer granularity.
Data from 10-K and 10-Q filings are widely used in financial research. In this study, textual data extracted from these filings will be employed to investigate market perceptions of risk. Due to their structured format, comprehensive informational content, and timeliness, 10-K and 10-Q filings constitute rich datasets uniquely suited for systematic textual analysis. To ensure dataset reliability and quality, I implemented careful selection and extraction procedures. Below, I explain the data selection process step by step.
3.1.1 Sample Construction and Filtering
The dataset consists of quarterly and annual financial reports filed with the SEC’s EDGAR system from 1994Q1 through 2024Q4. I obtain these filings from the Loughran-McDonald SEC/EDGAR dataset.222I am grateful to Tim Loughran and Bill McDonald for providing access to their dataset and dictionary for academic purposes. Their dataset, which provides cleaned text and metadata for all 10-X filings33310-X filings include all variants of Forms 10-K and 10-Q, such as: 10-K-A, 10-K405, 10-K405-A, 10-KSB, 10-KSB-A, 10-KT, 10-KT-A, 10-Q-A, 10-QSB, 10-QSB-A, 10-QT, 10-QT-A, 10KSB, 10KSB-A, 10KSB40, 10KSB40-A, 10KT405, 10KT405-A, 10QSB, and 10QSB-A. during this period. Table 1 provides an overview of filing types in the original dataset. The initial dataset comprises approximately 1.2 million filings.
Researchers constructing textual indices from SEC filings have long grappled with inconsistencies in the raw 10-K and 10-Q data. A single fiscal period can generate multiple filings—initial reports, amendments, transition reports, or small-business versions—which, if not handled carefully, could distort textual analyses. Early studies often sidestepped these issues by focusing on select filings (e.g., a single annual report per firm) or smaller samples. However, as EDGAR data became broadly accessible, the need for systematic cleaning rules became apparent. Over time, best practices have emerged, including consolidating legacy form types, removing duplicate or redundant filings, and excluding amended and nonstandard submissions. To reduce noise and accurately capture firm-level sentiment and its implications for macroeconomic and financial risks, I implemented rigorous filtering criteria aligned with established textual analysis conventions (Loughran and McDonald, 2011; Hoberg and Lewis, 2016).
- •
-
•
Transition Reports (10-KT, 10-QT): These reflect irregular intervals due to fiscal year shifts, which complicates consistent temporal analysis (Hoberg and Lewis, 2016).
-
•
Historical and Obsolete Formats (10-K405, 10-KSB, 10-QSB): These formats, phased out by the SEC, were characterized by inconsistent reporting standards, impeding temporal comparability (SEC, 2008).
These filtering procedures ensure a consistent, reliable dataset, substantially reducing redundancies, irregularities, and temporal biases, thereby enhancing the robustness of the ensuing textual sentiment analyses.
3.1.2 Section Extraction and Methodological Rationale
In conducting textual analysis to gauge firm sentiment and assess forward-looking disclosures, I specifically isolate narrative sections within SEC filings known to provide insightful qualitative information regarding firm outlook and inherent risks. The extraction procedure targets three standardized narrative sections: Risk Factors, Management’s Discussion and Analysis of Financial Condition and Results of Operations (MD&A), and Quantitative and Qualitative Disclosures about Market Risk (hereafter referred to as Market Risk). The precise targeting of these sections enhances comparability across firms and over time, thus ensuring consistency and reliability in the analysis of textual sentiment signals.
These specific sections correspond to Item 1A, Item 7, and Item 7A in 10-K filings, and Items 1A, 2, and 3 in 10-Q filings. Existing research underscores these narrative components as particularly valuable due to their inherent forward-looking characteristics and management’s qualitative assessments of financial health, operational conditions, and risk exposures (Li, 2010; Loughran and McDonald, 2011; Brown and Tucker, 2011). Thus, by focusing on these areas, my analysis aligns closely with established methodologies in financial textual analysis literature.
To systematically perform this extraction, I employ regular expressions (regex), a robust computational method frequently used in textual data preprocessing for financial documents (Jegadeesh and Wu, 2013; Huang et al., 2014). The regex patterns are designed to accurately identify and delimit the boundaries of targeted sections within each filing, allowing precise and efficient extraction. This method effectively excludes non-informative boilerplate content such as cover pages, generic financial statements, and standardized footnotes, which generally lack substantive sentiment variation.
To further enhance data quality and ensure analytical robustness, I exclude filings with insufficient narrative content. Specifically, any filing in which the combined word count of the extracted sections—Risk Factors, Management’s Discussion and Analysis (MD&A), and Market Risk—falls below a threshold of 610 words (approximately the 10th percentile) is removed from the analysis. This threshold is consistent with best practices in textual analysis research and helps ensure that retained filings contain enough qualitative content to support meaningful sentiment measurement (Loughran and McDonald, 2011; Huang et al., 2014). A comparison between the full-length original filing and its extracted narrative counterpart is provided at the end. I selected this example somewhat arbitrarily from among filings exhibiting a high proportion of negative language.
3.1.3 Removing Multiple Filings
Additional filtering procedures implemented in the dataset preparation stage include:
-
•
Excluding firms that submitted more than four filings within a single calendar year to mitigate potential bias arising from unusually frequent reporting patterns.
-
•
Removing duplicate filings submitted by the same firm on the same day to eliminate redundant sentiment signals.
However, I retain instances where firms submitted both 10-K and 10-Q reports within the same quarter, as each filing type typically contains distinct and complementary sentiment-related information (Loughran and McDonald, 2016; Brown and Tucker, 2011).
After applying all filtering procedures, the final sample comprises 766,847 filings—approximately 63% of the initial dataset—covering 29,273 unique firms over the period from 1994 through 2024. Table 2 summarizes the number of filings remaining after each stage of the filtering process. Figure 1 illustrates the evolution of the number of SEC filings per year across five stages of dataset construction. The blue line (“All Filings”) reflects the total number of 10-X filings, including amended forms, small business disclosures, and transition reports. This count peaks around 2000 with over 60,000 filings and then declines steadily—likely due to regulatory changes, firm delistings, and form consolidations. The orange line (“Only 10-K and 10-Q”) shows filings retained after filtering for standard annual and quarterly reports. The narrowing gap between the blue and orange lines after 2008 suggests a decline in the use of nonstandard filing types. The green line represents filings from which only narrative sections—MD&A, Risk Factors, and Market Risk—were successfully extracted. The drop in document count here is more pronounced in earlier years, likely reflecting less standardized formatting that hindered regex-based extraction.The red line reflects filings that pass the minimum word count threshold of 610 words, used to exclude short disclosures. The wider gap between the green and red lines prior to 2008 further supports the notion that early filings were less structured, resulting in poorer extraction coverage.Finally, the purple line represents the final dataset, incorporating all filters—document type, successful section extraction, word count threshold, and removal of duplicates or more than four filings per year. Overall, the figure demonstrates that the filtering pipeline successfully reduces noise while preserving a large, diverse, and representative sample suitable for high-quality sentiment analysis and macroeconomic forecasting.
To assess the richness of textual content, I also document the length of filings over time: Figure 2 presents trends in mean word count, while Figure 3 shows median word count by year. Finally, Table 3 reports the word count statistics at each stage of the document filtering process. The mean and median word counts for the full set of SEC filings (All Filings) are approximately 26,356 and 27,470, respectively, reflecting the inclusion of lengthy, often boilerplate-heavy filings across all form types. When restricting the dataset to only 10-K and 10-Q reports (Selected), average word counts increase slightly to over 28,000, indicating that these forms tend to be more content-rich. Following section extraction—targeting only the Risk Factors, MD&A, and Market Risk sections—the average word count drops substantially to around 8,010, highlighting the concentrated nature of sentiment-relevant disclosures. Applying a minimum word count threshold of 610 words (Filtered) removes the shortest and likely least informative filings, slightly raising the average word count to 8,679. This progression illustrates that the filtering process effectively narrows the dataset to filings with more substantive textual content, improving the reliability of sentiment estimation while maintaining sufficient variation in document length for robust analysis.
3.2 Market Capitalization Retrieval and Weighting
To construct sentiment indices, I aggregate firm-level sentiment growth across companies filing within the same week, weighting each firm’s contribution by its market capitalization. This weighting scheme is implemented to ensure that larger firms, which typically hold greater economic significance, have a proportionally greater influence on the aggregate sentiment measure (Kelly and Pruitt, 2014; Baker and Wurgler, 2006).
Market capitalization () for firm at filing time is computed as the product of the closing share price and the number of shares outstanding:
Here, denotes the closing stock price for firm on the filing date , while refers to the total number of shares outstanding at the end of the corresponding fiscal quarter, as reported in the firm’s financial disclosures.
The data required for these calculations are obtained from Wharton Research Data Services (WRDS) through the integration of multiple well-established financial databases: specifically, the Compustat company fundamentals database, the Center for Research in Security Prices (CRSP) stock files, and the Compustat-CRSP Merged (CCM) database444Market capitalization is computed as the product of the closing share price and the number of shares outstanding for each firm on the filing date. Daily share prices and shares outstanding are primarily sourced from CRSP; if unavailable, Compustat quarterly data are used as a fallback. All values are reported in U.S. dollars.. These comprehensive data sources provide high-frequency stock prices, detailed firm-level share information, and robust linking identifiers, allowing for precise alignment of market capitalization with each filing date (Fama and French, 1993; Jegadeesh and Wu, 2013). This rigorous data integration ensures both the accuracy and the consistency of the firm-level weights used in constructing the sentiment indices. Table 4 presents summary statistics for firm-level market capitalization at different stages of the dataset filtering process. Mean, median, and standard deviation values are reported in millions of U.S. dollars, with values rounded for clarity. The maximum and minimum market capitalizations—$3.57 trillion and $34,920, respectively—are constant across all datasets and reported in the table footnote. The mean market capitalization increases gradually from $5.32 billion in the full dataset (“All Filings”) to $5.48 billion in the final analytical sample. Similarly, the median rises from $454 million to $487 million. This upward trend suggests that the filtering steps—particularly the removal of filings with limited textual content—systematically exclude filings from smaller firms. Standard deviations also increase slightly across stages. This reflects not only the increasing presence of larger firms but also the widening spread in firm size as less informative (and generally smaller) filings are removed. Overall, these statistics confirm that while the filtering process reduces the total number of observations, it retains economically significant firms and enhances the quality and relevance of the final dataset for macroeconomic forecasting and sentiment analysis.
3.3 GDP Growth and Weekly NFCI
Real Gross Domestic Product (GDP) data are obtained from the Bureau of Economic Analysis (BEA)555Downloaded from the FRED database (Series: A191RL1Q225SBEA). To forecast quarterly real GDP growth, I employ MIDAS Quantile Regressions, which allow the incorporation of high-frequency predictors—specifically, weekly sentiment indices—into models of lower-frequency outcomes such as quarterly GDP. As a benchmark, I include the Chicago Fed National Financial Conditions Index (NFCI), which is available at a weekly frequency. This represents an improvement over the original implementation by Adrian et al. (2019), who used the NFCI at a quarterly frequency. Figure 4 presents the time series of quarterly real GDP growth alongside the weekly NFCI, highlighting the dynamics and potential lead-lag relationships between financial conditions and macroeconomic performance.
4 Methodology
4.1 Sentiment Index
My analysis begins by constructing a firm-level sentiment metric based on quarterly and annual filings. For each firm and each filing date , I calculate the textual sentiment of its 10-Q or 10-K report. A lexicon-based approach is employed using the Loughran–McDonald (LM) master dictionary, which classifies words commonly found in financial texts into categories such as positive, negative, uncertainty, and litigious, among others. To quantify sentiment, I focus particularly on the difference between the positive and negative word counts, following the methodology of Loughran and McDonald (2011), to derive a measure of overall document tone. The sentiment ratio for a given filing is defined as the number of sentiment words in category divided by the total word count of the document.
Formally, the sentiment ratio for firm at time , and sentiment category (e.g., Negative, Positive, Uncertainty, or Litigious), is calculated as:
Once the sentiment ratio for the current filing is computed, I measure sentiment growth as the year-over-year change in sentiment relative to the same fiscal quarter in the previous year. This approach accounts for potential seasonality in language use across reporting periods. Specifically, for each firm and sentiment category , the sentiment growth rate is defined as:
Here, represents the sentiment ratio for the current filing, and represents the sentiment ratio for the matched filing from the previous year. Note that and need not fall on the same calendar date, but must belong to the same fiscal quarter. Tone Growth is calculated as
When calculating sentiment growth rates, special considerations are needed:
-
•
If the initial quarter for a firm lacks a prior-year counterpart, no year-over-year growth rate is computed. This initial observation is commonly omitted from analyses rather than assigning arbitrary values. Feldman et al. (2010), for example, measure tone change in MD&A sections relative to prior filings, implying that a firm’s first quarter (without previous data) has no associated tone-change metric.
-
•
Similarly, if a firm has an intermediate missing quarter, the corresponding year-over-year growth calculation is omitted to maintain consistency. For instance, if a firm reports data in 2001-Q1 and 2003-Q1 but misses 2002-Q1, no valid growth rate is computed for 2003-Q1 due to the absence of a benchmark sentiment from 2002-Q1.
-
•
Filings matched by type is priority (i.e., 10-K filings matched exclusively with previous 10-K filings, and 10-Q filings matched exclusively with previous 10-Q filings).
-
•
When a firm releases multiple filings of the same type within a quarter in consecutive years, sentiment ratios are matched based on chronological order, using the earliest-to-earliest, latest-to-latest principle.
-
•
If there are unequal numbers of filings between two consecutive years:
-
–
If a firm submits multiple filings within the same quarter of the current year, but only a single filing is available from the same quarter in the previous year, each current-year filing is individually paired with the single available previous-year filing. Consequently, the sentiment growth measure for each current-year filing is calculated using the same baseline previous-year filing. This approach ensures all current filings within the quarter receive a consistent comparative baseline.
-
–
The matching procedure strictly considers the filing type (either 10-K or 10-Q). If a firm submits both 10-K and 10-Q filings in the current year for a particular quarter, but the same quarter of the previous year contains only one filing type (e.g., 10-Q), only the current-year filing of the matching type (the 10-Q) is paired for the sentiment growth calculation. The current-year filing with no matching previous-year filing of the same type (the 10-K in this scenario) is excluded from sentiment growth calculations, as the matching process does not allow cross-type pairing (e.g., pairing a 10-K with a 10-Q).
-
–
Focusing on year-over-year changes mitigates firm-specific reporting styles and highlights shifts in optimism or pessimism. Research indicates that changes in sentiment can predict future firm performance and market reactions (Azimi and Agrawal, 2021). Therefore, sentiment growth serves as my fundamental input.
Using the firm-level sentiment growth measures, I construct a weekly sentiment index by aggregating across firms that filed within the same calendar week. Let denote the filing date within week , and let be the set of firms with filings during that week. For each firm , the sentiment growth measure is weighted by the firm’s market capitalization on its filing date, denoted .
The weekly sentiment index for sentiment category (e.g., Positive, Negative, or Tone) is computed as a market-capitalization-weighted average:
This weighting scheme ensures that larger firms—those with greater macroeconomic relevance—exert proportionally more influence on the aggregate sentiment index.Figure 5 displays the time series of weekly sentiment growth across five categories: Negative, Positive, Uncertainty, Litigious, and overall Tone. While all categories fluctuate around zero, sharp spikes—especially in negative and litigious sentiment—are observable around major economic disruptions such as the 2001 recession, the 2008 financial crisis, and the onset of the COVID-19 pandemic in 2020. These patterns indicate that firm-level textual sentiment responds strongly to macroeconomic stress. Figure 6 provides disaggregated views of each sentiment category, overlaid with shaded areas for U.S. recessions and annotated historical events. Negative and uncertainty sentiment growth visibly surge during recessions and crisis periods, while positive sentiment contracts. The tone index captures these shifts clearly, reinforcing the value of textual sentiment as a forward-looking macroeconomic indicator.
4.2 MIDAS Quantile Regression
4.2.1 Quantile Regression
Quantile regression, introduced by Koenker and Bassett (1978), provides a statistical methodology for modeling conditional quantiles of a dependent variable, thus extending classical regression methods that typically focus on modeling the conditional mean. A quantile represents a specific point in the cumulative distribution function of a random variable, dividing the distribution into intervals with defined probabilities. By focusing on quantiles, researchers can capture the entire distribution of the dependent variable conditional on explanatory variables, thereby providing detailed insights into tail behaviors and distributional heterogeneity.
| (1) |
Unlike Ordinary Least Squares (OLS) regression, which minimizes the sum of squared residuals to estimate conditional means, quantile regression minimizes an asymmetric absolute loss function known as the "check" or "pinball" loss function. Specifically, quantile regression estimates the conditional quantile by minimizing:
| (2) |
where is the dependent variable, is a vector of explanatory variables, and is the quantile-specific loss function defined as
This loss function imposes asymmetric penalties: underestimating and overestimating the dependent variable are weighted differently depending on the chosen quantile. For example, estimating the median yields equal weights to under- and over-predictions, minimizing absolute deviations. Conversely, estimating extreme quantiles, such as the 5th percentile or 95th percentile, places disproportionate emphasis on one side of the distribution, effectively capturing tail risks or opportunities.
The predicted value from this regression is the quantile of conditional on :
| (3) |
Koenker and Bassett (1978) show that is a consistent linear estimator of the conditional quantile function of given .
Based on estimates of the conditional quantile function over a discrete number of quantile levels, it is possible to estimate the full continuous conditional distribution of . As in Adrian et al. (2019), choose to fit a flexible distribution known as the generalized skewed Student’s distribution in order to smooth the estimated conditional quantile function of and recover a probability density function. This specific distribution allows for fat tails and asymmetry and reduces to a normal distribution as a special case. The generalized skewed Student’s distribution has the following density function:
| (4) |
where is a location parameter, is a scale parameter, is a fatness (degrees-of-freedom) parameter, is a shape (skewness) parameter, and and respectively denote the probability density function and the cumulative distribution function of the standard Student’s t-distribution (Azzalini and Capitanio, 2003).
In practice, the four parameters of the generalized skewed Student’s distribution are estimated through a quantile matching approach, aiming at minimizing the squared distance between the estimated conditional quantile functions and the inverse cumulative distribution function of the generalized skewed Student’s distribution:
| (5) |
where is the inverse cumulative distribution function of the generalized skewed Student’s distribution. Following Adrian et al. (2019), I focus specifically on the lower 5th percentile quantile of the predicted distribution, referred to as the Growth-at-Risk at the 5% quantile, GaR(5%) (see also Figueres and Jarociński, 2020), defined formally as:
This GaR measure reflects the expected GDP growth at the 5th percentile of its conditional distribution h quarter ahead, providing direct quantification of downside macroeconomic risk.
Quantile regression offers several distinct advantages for forecasting tail risks compared to traditional mean-based methods. First, it provides robustness against outliers and distributional anomalies, as extreme values influence estimates only proportionally to their occurrence at specific quantiles. Second, it enables a nuanced understanding of how predictors influence different parts of the dependent variable’s conditional distribution. For instance, predictors such as weekly sentiment indicators or financial condition indices may significantly impact lower quantiles—thus revealing crucial insights into downside economic risks—even if their effects on median or average outcomes are limited.
4.2.2 MIDAS Regression
The Mixed Data Sampling (MIDAS) regression, introduced by Ghysels et al. (2004) and further developed by Ghysels et al. (2006), provides a robust econometric framework for integrating variables observed at different frequencies into a unified regression model. Traditional econometric methods often aggregate high-frequency data (such as daily financial indicators) to match a lower frequency target variable (such as quarterly GDP), resulting in a potential loss of information and aggregation bias (Andreou et al., 2013). MIDAS regression addresses this limitation by directly incorporating high-frequency observations without aggregation, preserving crucial information contained within the high-frequency data.
At the core of the MIDAS methodology is the use of distributed lag polynomials, which effectively map a large number of high-frequency observations into a concise set of parameters. Specifically, rather than estimating separate coefficients for each high-frequency lag—which would be computationally infeasible and prone to overfitting—MIDAS regressions employ structured weighting functions to parsimoniously estimate lagged impacts.
Formally, suppose I want to model a low-frequency dependent variable (e.g., quarterly GDP growth) using high-frequency explanatory variables (e.g., weekly financial indicators or weekly sentiment indices). A general MIDAS regression can be expressed as:
| (6) |
where
-
•
is the intercept,
-
•
is the slope coefficient on the aggregated predictor,
-
•
is the high-frequency lag (e.g., weekly),
-
•
is the number of high-frequency observations per low-frequency period (e.g., 13 weeks per quarter),
-
•
is a lag weighting function parameterized by ,
-
•
is the error term.
The lag weights depend on a small set of parameters and are typically constrained to follow smooth parametric functions such as the Almon polynomial (Almon, 1965) or Beta polynomial (Ghysels et al., 2007). These polynomial forms impose smoothness and monotonicity constraints on lagged effects, facilitating efficient estimation and interpretability.
The Almon polynomial lag structure, in particular, defines weights as a polynomial function of lag :
| (7) |
with endpoint restrictions frequently applied to ensure that weights smoothly transition to zero at the boundaries. This flexibility enables MIDAS regressions to accurately capture complex lagged relationships between variables sampled at mixed frequencies.
MIDAS regression has gained widespread popularity in economics and finance due to its versatility and empirical efficacy. It has been successfully applied in areas such as forecasting financial volatility (Ghysels et al., 2007), nowcasting macroeconomic variables like GDP (Andreou et al., 2013), and modeling monetary policy impacts where high-frequency financial market data provide valuable forward-looking signals (Ghysels, 2016).
Since its inception, MIDAS has undergone considerable methodological advancement, demonstrating robust predictive performance and theoretical coherence across various empirical settings. By maintaining high-frequency granularity and efficiently summarizing information through parsimonious lag structures, MIDAS regressions provide researchers and policymakers with timely and precise forecasting capabilities critical for economic decision-making.
4.2.3 MIDAS Quantile Regression
I employ a mixed-data sampling quantile regression (MIDAS-QR) framework to forecast the lower tail (specifically, the 5th percentile, ) of quarterly GDP growth. This methodology combines low-frequency quarterly data (GDP growth) with high-frequency weekly predictors within a unified quantile regression model.
Formally, the MIDAS-QR model at quantile is specified as follows:
| (8) |
where:
-
•
denotes the conditional -quantile of given information set ,
-
•
and are quantile-specific coefficients,
-
•
is the Almon lag weighting polynomial evaluated at lag with quantile-specific parameters .
The weighting function defines how each weekly lag contributes to the forecasted quantile of GDP growth. Unlike traditional MIDAS approaches that use Beta polynomial weighting schemes (e.g., Ghysels, 2016), I employ an unnormalized Almon lag polynomial of degree 3 with two endpoint restrictions, following the approach of Ferrara et al. (2022). The unnormalized Almon lag polynomial is defined as:
| (9) |
where is the vector of polynomial parameters to be estimated. This polynomial representation provides flexibility and parsimony, significantly reducing the number of parameters relative to freely estimating all weekly lags individually.
For estimation purposes, it is convenient to rewrite the model in a compact linear form. Define the vector of weekly observations in quarter as:
and construct the polynomial design matrix , a matrix, whose elements are defined by , for and . Using these definitions, the MIDAS-QR model can be equivalently expressed as:
where is a vector of polynomially weighted weekly observations from quarter . In this specification, the dependent variable represents GDP growth in quarter , and thus clearly denotes a one-quarter ahead forecast horizon.
The primary advantage of using an unnormalized Almon lag polynomial is its interpretability and efficiency. Instead of estimating all weekly parameters individually, this polynomial approach significantly reduces complexity, requiring estimation of only polynomial parameters. This substantially improves model parsimony, mitigating risks of overfitting and multicollinearity. Additionally, economically meaningful constraints can be imposed on the polynomial. For example, endpoint constraints such as setting the polynomial weight at the longest lag to zero () or constraining the slope at this lag to be flat () are practical and economically justified (Ferrara et al., 2022). Imposing these linear restrictions further reduces the number of parameters from to , enhancing interpretability and model robustness.
In summary, the MIDAS-QR approach using an unnormalized Almon lag polynomial offers a flexible yet efficient method to incorporate weekly high-frequency predictors into quarterly GDP tail-risk forecasts. This specification enhances interpretability and robustness, making it particularly suitable for assessing growth-at-risk scenarios.
4.3 Out-of-Sample Forecasting and Evaluation
To robustly evaluate the predictive performance of the Quantile-MIDAS model, I conduct out-of-sample forecasts using a rolling-window strategy with a fixed window size of 80 quarters (equivalent to 20 years). At each forecast origin, the model is re-estimated using the most recent data, and a one-quarter-ahead forecast is generated. Forecast accuracy is then assessed using standard quantile forecasting metrics: the pinball loss function, the Quantile Skill Score (QSS), and the Diebold–Mariano (DM) test.
- Pinball Loss:
-
The pinball loss (also known as the quantile loss or check function) is the standard objective function in quantile regression. For a given quantile level , the loss for a prediction of an actual value is defined as:
where is the indicator function. This piecewise-linear loss penalizes over- and under-predictions asymmetrically, based on the quantile level. For instance, under-predicting the 5th percentile incurs a loss weighted by , while over-predicting incurs a loss weighted by . A forecast that matches the true conditional quantile minimizes the expected pinball loss, making it a proper scoring rule. I report average pinball loss across the out-of-sample forecast horizon; lower values indicate higher forecast accuracy.
- Quantile Skill Score (QSS):
-
The Quantile Skill Score provides a relative measure of forecasting improvement compared to a benchmark model. It is defined as:
where QS refers to the average pinball loss. A positive QSS implies that the model outperforms the benchmark. For example, a QSS of 0.20 indicates a 20% reduction in pinball loss relative to the benchmark, while a QSS below zero implies inferior performance.
- Diebold–Mariano Test:
-
To formally test whether forecast accuracy differences between two models are statistically significant, I use the Diebold–Mariano (DM) test (Diebold and Mariano, 1995). Define the period- loss differential as , where denotes the pinball loss. The test statistic is:
where is the mean loss differential, and is its estimated variance. Under the null hypothesis of equal predictive accuracy (), the statistic asymptotically follows a standard normal distribution. In small samples, a Student- distribution may be used instead, following the adjustment in Harvey et al. (1997). A significantly negative DM statistic (i.e., lower loss from the MIDAS-QR model) indicates superior predictive performance over the benchmark.
Advantages of the MIDAS Quantile Approach: Combining MIDAS with quantile regression offers several key advantages for analyzing GDP tail risks, as outlined below:
-
•
Capturing Nonlinear Tail-Risk Dynamics: Quantile regression allows the influence of predictors to vary across the distribution of GDP growth. This enables the model to capture nonlinear and asymmetric relationships that specifically affect the lower tail—i.e., severe downturns—more than the center of the distribution. For instance, a rise in financial stress or negative sentiment may have a modest impact on average GDP growth but a disproportionately large effect on the 5th percentile. By modeling conditional quantiles directly, the MIDAS-QR approach captures such tail-risk dynamics that would be missed by traditional mean-based regressions.
-
•
Parsimonious Use of High-Frequency Predictors: The MIDAS component enables efficient incorporation of high-frequency data (e.g., weekly tone growth or financial indicators) without introducing a large number of parameters. Rather than estimating dozens of separate lag coefficients, the MIDAS polynomial specification compresses the information into a few smooth weights. This parsimony helps avoid overfitting and allows timely weekly information to improve forecasts while maintaining a relatively simple and interpretable model structure (Ghysels et al., 2007).
-
•
Robustness to Outliers and Distributional Asymmetry: Quantile regression relies on the pinball loss function, which is less sensitive to outliers than squared-error-based methods. As a result, extreme GDP observations (such as during crisis periods) exert limited influence on estimated lower-tail quantiles. Furthermore, because quantile methods do not assume symmetric error distributions, the model can accommodate skewness in GDP growth. If downside risk dominates (i.e., the distribution is left-skewed), the lower quantile forecast naturally reflects this without being distorted by central values. This enhances the credibility of risk estimates in turbulent economic conditions (Koenker and Bassett, 1978).
In summary, the rolling MIDAS-QR out-of-sample forecasting strategy is particularly well-suited for analyzing GDP tail risks. It combines the richness of high-frequency sentiment and financial indicators with the flexibility of quantile methods to generate forecasts that are responsive to rapid changes and robust to extreme events. The model’s structure strikes a balance between flexibility and parsimony, ensuring that observed gains in predictive performance—such as lower pinball losses or higher skill scores—stem from genuine signal extraction rather than overfitting. Overall, this two-step framework provides a powerful and transparent tool for macroeconomic risk forecasting, grounded in well-established econometric principles (Koenker and Bassett, 1978; Ghysels et al., 2007; Diebold and Mariano, 1995).
5 Results
5.1 Quantile Regression and Tail-Risk Effects
The MIDAS-QR regression framework enables an examination of how predictors influence not only the central tendency of GDP growth, but also its distributional tails—especially the downside risk component. Following Adrian et al. (2019), Figures 7 and 8 visualize these effects by plotting one-quarter-ahead GDP growth against two key predictors: the weekly sentiment index (tone growth) and the weekly NFCI, respectively. In both figures, the fitted lines include ordinary least squares (OLS, blue), as well as quantile regressions for the 5th percentile (Q0.05, black), the median (Q0.50, orange dashed), and the 95th percentile (Q0.95, green dashed).
Figure 7, which uses the weekly tone growth as the predictor, shows that the lower quantile line (Q0.05) has a noticeably positive slope. This suggests that low tone is associated with a higher probability of extreme negative GDP outcomes, even though the average (OLS) effect remains close to flat. The upper quantile (Q0.95) is relatively stable across the sentiment distribution, indicating that strong tone growth does not meaningfully raise the likelihood of extreme positive outcomes. This asymmetry is consistent with the idea of sentiment as a leading indicator of downside macroeconomic risk.
Figure 8, which uses the weekly NFCI as the predictor, shows that all fitted lines—OLS, Q0.05, Q0.50, and Q0.95—exhibit a mild downward slope, indicating a general negative relationship between financial conditions and GDP growth. However, the Q0.05 line (black) lies only slightly below the OLS fit, suggesting that tighter financial conditions are associated with lower growth overall, but do not exert a strong disproportionate effect on the lower tail of the GDP distribution.
These results suggest that while financial conditions are relevant for overall GDP dynamics, their incremental value in forecasting downside risk may be weaker than that of textual sentiment indicators. This further highlights the unique role that forward-looking tone measures extracted from corporate filings can play in identifying extreme economic risks.
5.2 Out-of-Sample Forecast Accuracy
To assess the predictive power of the MIDAS-QR model, I conduct out-of-sample forecasts using a rolling-window strategy with a fixed window of 80 quarters (20 years). At each step, the model is re-estimated using the most recent data and used to forecast one-quarter-ahead GDP growth at the quantile. This approach balances estimation reliability with adaptability to structural change, aligning with best practices in real-time macroeconomic forecasting (Pesaran & Timmermann, 2007).
Figure 9 compares actual GDP growth (black line) with the out-of-sample forecasts from the sentiment-based model (blue dashed line) and the wNFCI-based model (red dotted line). Both models broadly capture the cyclical dynamics of GDP, but key differences emerge. The sentiment model is more responsive at economic turning points. For instance, it anticipates the 2020 collapse in GDP earlier than the wNFCI model and rises more sharply in the subsequent recovery. In calmer periods, such as 2016–2019, the sentiment model also tracks observed fluctuations more closely.
Table 5 quantifies these performance differences. The sentiment model achieves a pinball loss of 0.889, substantially lower than the 1.296 from the wNFCI model, resulting in a Quantile Skill Score (QSS) of 0.314—indicating a 31.4% improvement in tail-risk prediction. The Diebold–Mariano test confirms the sentiment model’s superiority with a statistic of (), statistically significant at the 10% level.
Figure 10 plots the forecast errors (actual minus predicted) for both models. A well-calibrated model should exhibit errors close to zero. The sentiment model’s errors (blue line) are consistently smaller in magnitude than those of the wNFCI model (red line), especially during high-volatility periods. For example, during the 2020-Q2 downturn, the sentiment model produced a smaller error, having already predicted a sharper decline in GDP. Similarly, during the rebound, it recovered more quickly than the wNFCI model. Even in more stable periods, the sentiment model tends to be closer to the realized outcomes.
These findings underscore the benefits of incorporating forward-looking sentiment derived from firm disclosures. While financial conditions indexes like the NFCI remain valuable tools for macroeconomic risk assessment (Brave & Butters, 2011; Adrian et al., 2019), textual sentiment captures qualitative information and evolving expectations that may not be fully reflected in financial data. The sentiment-based model offers improved forecast accuracy and better responsiveness to real-time developments—critical attributes for early-warning systems and policy evaluation.
5.3 Performance During Recessions
I now evaluate model performance specifically during economic recessions, which represent particularly challenging periods for forecasting. Recessions involve sharp, unpredictable changes and heightened volatility, making accurate prediction of GDP growth much more difficult.
Table 6 summarizes forecast accuracy for the sentiment-based and wNFCI-based models during NBER-defined recession quarters in the sample, including the 2001 dot-com recession, the 2008–2009 Global Financial Crisis, and the 2020 COVID-19 recession.
As expected, both models experience much higher forecast errors during recessions compared to the full sample. The pinball loss rises to 12.996 for the sentiment model and 14.735 for the wNFCI model. This reflects the inherent difficulty of forecasting during crisis periods. Despite this, the sentiment model still outperforms the wNFCI model. Its quantile loss is about 1.74 points lower, corresponding to an 11.8% improvement (QSS = 0.118). Although this performance gap is smaller than in the full-sample analysis (QSS = 0.314), it still represents a meaningful reduction in forecast error during stressful economic conditions.
However, the Diebold–Mariano test yields a test statistic of with a -value of 0.251, indicating that the difference in accuracy is not statistically significant at conventional levels. In other words, while the sentiment model performs better, the improvement is not strong enough to rule out the possibility of equal predictive ability during recessions.
This result is understandable. During recessions, both financial indicators (like the NFCI) and textual sentiment often signal deteriorating conditions. As a result, their information content may overlap more than in normal times, narrowing the performance gap between models. Moreover, extreme volatility and structural breaks reduce forecasting reliability for any model, and the number of recession quarters in the sample is relatively small, limiting statistical power.
Overall, while not statistically significant, the sentiment model’s relative advantage during recessions reinforces its value as a forward-looking indicator that can complement conventional financial metrics during periods of macroeconomic stress.
5.4 Robustness Checks Across Filtration Stages
To assess the stability and reliability of the sentiment-based MIDAS-QR model, I conduct a series of robustness checks that examine whether the model’s forecasting performance is sensitive to different document filtering strategies. These tests are essential for evaluating whether improvements over the benchmark model (based on the weekly NFCI) are consistent and economically meaningful across varying textual inputs.
Model Parameter Robustness.
First, I explore robustness to key model parameters by varying the lag order (2, 4, 6, and 8) and rolling window size (40, 60, 80, and 100 quarters). These variations test the sensitivity of the forecast to the temporal resolution of the predictors and the length of the estimation window. I find that performance is strongest when using 8 lags and a window size of 80 quarters, which balances responsiveness with stability. This configuration consistently delivers the lowest pinball loss, favorable Diebold–Mariano test statistics, and the highest Quantile Skill Score (QSS).
Document Filtering Robustness.
Next, I examine whether the forecasting advantage of the sentiment model depends on how filings are preprocessed. I construct five increasingly filtered datasets:
-
1.
All: Includes all available SEC filings (regardless of type or content).
-
2.
Selected: Retains only standard 10-K and 10-Q filings.
-
3.
Extracted: Uses only the narrative sections (MD&A, Risk Factors, and Market Risk) extracted from the Selected dataset.
-
4.
Filtered: Further excludes short filings (fewer than 610 words) to reduce noise.
-
5.
Final Sample: The most refined dataset, incorporating all the above filters and used in the main analysis.
Table 7 reports the out-of-sample performance of the sentiment-based model relative to the wNFCI benchmark at , using three metrics: pinball loss, Diebold–Mariano (DM) test statistic, and QSS. Across all datasets, the sentiment model consistently achieves lower pinball loss and positive QSS values. While the magnitude of improvement varies, the trend is clear: as the dataset becomes more targeted and structured, performance improves.
In particular, QSS increases from 0.199 (All) to 0.314 (Final Sample), indicating that removing irrelevant or noisy content enhances the predictive power of the tone signal. The DM statistics are negative across all stages, and near or below the 10% significance threshold, suggesting the sentiment model consistently outperforms the benchmark, especially in more refined samples.
Implications.
These results confirm that the forecasting advantage of the sentiment-based model is robust to different preprocessing choices. However, the gain is more pronounced when the input text is curated to include forward-looking content and exclude boilerplate material. This underscores the importance of careful document filtering—particularly section extraction and length-based screening—in improving the signal quality of text-based indicators in macroeconomic forecasting.
6 Conclusion
This paper presents a novel approach to forecasting U.S. GDP tail risks using a tone-based sentiment index derived from SEC filings. By extracting sentiment measures from the narrative sections of 10-K and 10-Q reports—specifically the MD&A, Risk Factors, and Market Risk disclosures—and applying them in a MIDAS quantile regression framework, I show that text-based sentiment is a powerful predictor of one-quarter-ahead GDP growth, particularly at the lower tail of the distribution. My sentiment-based model outperforms the benchmark Weekly National Financial Conditions Index (wNFCI) both in full-sample accuracy and during economic recessions, with quantile skill score improvements of up to 31.4%. These gains are robust across different document filtering schemes and highlight the importance of carefully curated textual inputs for forecasting macroeconomic outcomes.
The results underscore that qualitative information embedded in forward-looking corporate disclosures carries significant predictive content for macroeconomic risk. Compared to financial market indices, tone extracted from firms’ narratives offers a faster and complementary signal, particularly useful around turning points in the business cycle. This finding strengthens the case for incorporating unstructured text into macroeconomic forecasting models and supports recent literature that leverages high-frequency data to improve real-time monitoring of economic conditions.
Looking forward, the next stage of this research will focus on extending the scope and granularity of the sentiment dataset. The current analysis is limited to 10-K and 10-Q filings contained in the Loughran–McDonald dataset, which excludes other timely sources such as Form 8-Ks. Since 8-K filings often disclose earnings surprises, mergers, operational disruptions, and other market-sensitive news, incorporating them will enhance the immediacy and richness of the sentiment signal. To facilitate this, I plan to develop an automated scraping pipeline using the SEC EDGAR RSS feed and the EDGAR Full Text Search API to extract all relevant filings—including 8-Ks—in real time.
Moreover, while my sentiment index is constructed using dictionary-based methods, future work will explore more advanced NLP techniques such as transformer-based models and context-aware embeddings. These models may capture subtler shifts in managerial tone, detect changes in risk language, and improve the classification of sentiment beyond word counts. By incorporating semantic nuance and sequential context, such methods could refine the sentiment signal and further enhance predictive performance.
Ultimately, this research contributes to a growing field that integrates text analytics with economic forecasting, offering both methodological insights and practical tools for monitoring growth-at-risk. My findings suggest that macroeconomic forecasters and policymakers should consider integrating textual sentiment indicators as a complementary input to traditional financial and macroeconomic variables—especially in periods of heightened uncertainty where timely qualitative information may provide a leading edge.
Tables and Charts
Tables
| Filing Type | Explanation |
|---|---|
| 10-K | Annual comprehensive financial disclosure. |
| 10-K-A | Amendment to 10-K filing. |
| 10-K405 | Annual report with insider disclosure (obsolete). |
| 10-KSB | Small business annual report (obsolete). |
| 10-KT | Transitional annual report. |
| 10-Q | Quarterly financial update. |
| 10-Q-A | Amendment to 10-Q filing. |
| 10-QSB | Small business quarterly report (obsolete). |
| 10-QT | Transitional quarterly report. |
| Filtering Stage | Number of Filings |
|---|---|
| Initial Loughran-McDonald dataset | 1,224,495 |
| Only standard 10-K and 10-Q | 914,925 |
| After extracting MD&A, Risk, and Market Risk sections | 876,410 |
| After removing short documents (less than 610 words) | 788,919 |
| After removing excessive and duplicate filings | 766,847 |
| Dataset | Mean Word Count | Median Word Count | Standard Deviation |
|---|---|---|---|
| All Filings (Initial) | 26,356.44 | 27,469.91 | 8,269.45 |
| Selected (10-K/10-Q) | 28,324.20 | 29,855.64 | 8,358.31 |
| Extracted Sections | 8,009.78 | 8,583.81 | 3,749.61 |
| Filtered (Length 610) | 8,679.16 | 9,063.36 | 3,716.25 |
| Final Sample | 8,784.12 | 9,258.42 | 3,795.83 |
| Dataset | Mean (in millions) | Median (in millions) | Std Dev (in millions) |
|---|---|---|---|
| All Filings | 5,323 | 454 | 36,102 |
| Selected (10-K/10-Q) | 5,382 | 462 | 36,418 |
| Extracted Sections | 5,396 | 466 | 36,864 |
| Filtered ( 610 words) | 5,449 | 483 | 37,928 |
| Final Sample | 5,481 | 487 | 38,111 |
-
•
Note: All values are rounded to the nearest million U.S. dollars for readability. The maximum and minimum market capitalization values across all datasets are $3.57 trillion and $34,920, respectively.
| Metric | Sentiment Model (Tone Growth) | wNFCI Model |
|---|---|---|
| Pinball Loss (Median ) | 0.889 | 1.296 |
| Diebold-Mariano Statistic | () | |
| Quantile Skill Score (QSS) | 0.314 |
| Metric | Sentiment Model (Tone Growth) | wNFCI Model |
|---|---|---|
| Pinball Loss (Median ) | 12.996 | 14.735 |
| Diebold-Mariano Statistic | () | |
| Quantile Skill Score (QSS) | 0.118 |
| Dataset | Pinball Loss (Tone) | DM Statistic | DM P-value | QSS (vs. wNFCI) |
|---|---|---|---|---|
| All | 1.038 | -1.385 | 0.087 | 0.199 |
| Selected | 1.020 | -1.451 | 0.077 | 0.210 |
| Extracted | 1.030 | -1.463 | 0.075 | 0.198 |
| Filtered | 0.947 | -1.466 | 0.075 | 0.269 |
| Final Sample | 0.889 | -1.452 | 0.077 | 0.314 |
Figures
References
- Adrian et al. (2022) Adrian, T., Grinberg, F., Liang, N., Malik, S., & Yu, J. (2022). The term structure of growth-at-risk. American Economic Journal: Macroeconomics, 14(3), 283–323.
- Adrian et al. (2019) Adrian, T., Boyarchenko, N., & Giannone, D. (2019). Vulnerable growth. American Economic Review, 109(4), 1263–1289.
- Almon (1965) Almon, S. (1965). The distributed lag between capital appropriations and expenditures. Econometrica, 33(1), 178–196.
- Andreou et al. (2013) Andreou, E., Ghysels, E., & Kourtellos, A. (2013). Should macroeconomic forecasters use daily financial data and how?. Journal of Business & Economic Statistics, 31(2), 240-251.
- Ashwin et al. (2024) Ashwin, J., Kalamara, E., & Saiz, L. (2024). Nowcasting Euro Area GDP with news sentiment: A tale of two crises. Journal of Applied Econometrics, 39(2), 482–501.
- Azimi and Agrawal (2021) Azimi, M., & Agrawal, A. (2021). Is positive sentiment in corporate annual reports informative? Evidence from deep learning. The Review of Asset Pricing Studies, 11(4), 762-805.
- Azzalini and Capitanio (2003) Azzalini, A., & Capitanio, A. (2003). Distributions generated by perturbation of symmetry with emphasis on a multivariate skew t-distribution. Journal of the Royal Statistical Society: Series B, 65(2), 367–389.
- Baker et al. (2016) Baker, S. R., Bloom, N., & Davis, S. J. (2016). Measuring economic policy uncertainty. Quarterly Journal of Economics, 131(4), 1593–1636.
- Baker and Wurgler (2006) Baker, M., & Wurgler, J. (2006). Investor sentiment and the cross-section of stock returns. Journal of Finance, 61(4), 1645–1680.
- Brave & Butters (2011) Brave, S. A., & Butters, R. A. (2011). Monitoring financial stability: A financial conditions index approach. Chicago Fed Economic Perspectives, 35(1), 22–43.
- Brown and Tucker (2011) Brown, S. V., & Tucker, J. W. (2011). Large-sample evidence on firms’ year-over-year MD&A modifications. Accounting Review, 86(2), 315–344.
- Brown et al. (2020) Brown, S. V., Crowley, R. M., & Elliott, W. B. (2020). What are you saying? Using topic to detect financial misreporting. Journal of Accounting Research, 58(1), 237–291.
- Bybee et al. (2021) Bybee, L., Kelly, B. T., Manela, A., & Xiu, D. (2021). Business news and business cycles. Journal of Finance, 79(5), 3105-3147.
- Castelnuovo and Mori (2024) Castelnuovo, E. and Mori, L. (2024). Uncertainty, Skewness, and the Business Cycle Through the MIDAS Lens. Journal of Applied Econometrics, 40(1), 89-107.
- Cecchini et al. (2010) Cecchini, M., Aytug, H., Koehler, G. J., & Pathak, P. (2010). Detecting management fraud in public companies. Management Science, 56(7), 1146–1160.
- Diebold and Mariano (1995) Diebold, F. X., & Mariano, R. S. (1995). Comparing predictive accuracy. Journal of Business and Economic Statistics, 13(3), 253–263.
- Fama and French (1993) Fama, E. F., & French, K. R. (1993). Common risk factors in the returns on stocks and bonds. Journal of Financial Economics, 33(1), 3–56.
- Ferrara et al. (2022) Ferrara, L., Mogliani, M., & Sahuc, J. G. (2022). High-frequency monitoring of growth-at-risk. Journal of Money, Credit and Banking, 54(4), 1067–1084.
- Figueres and Jarociński (2020) Figueres, J. M., & Jarociński, M. (2020). Vulnerable growth in the euro area: Measuring the financial conditions that matter. European Central Bank Working Paper No. 2406
- Gentzkow et al. (2019) Gentzkow, M., Kelly, B., & Taddy, M. (2019). Text as data. Journal of Economic Literature, 57(3), 535–574.
- Ghysels (2016) Ghysels, E. (2016). Macroeconomics and the reality of mixed frequency data. Journal of Econometrics, 193(2), 294-314.
- Ghysels et al. (2007) Ghysels, E., Sinko, A., & Valkanov, R. (2007). MIDAS regressions: Further results and new directions. Econometric Reviews, 26(1), 53–90.
- Ghysels et al. (2006) Ghysels, E., Santa-Clara, P., & Valkanov, R. (2006). Predicting volatility: Getting the most out of return data sampled at different frequencies. Journal of Econometrics, 131(1–2), 59–95.
- Ghysels et al. (2004) Ghysels, E., Santa-Clara, P., & Valkanov, R. (2004). The MIDAS touch: Mixed data sampling regression models.
- Harvey et al. (1997) Harvey, D. I., Leybourne, S. J., & Newbold, P. (1997). Testing the equality of prediction mean squared errors. International Journal of Forecasting, 13(2), 281–291.
- Hassan et al. (2023) Hassan, T. A., Hollander, S., Kalyani, A., van Lent, L., Schwedeler, M., & Tahoun, A. (2023). Economic surveillance using corporate text. Review of Financial Studies, 36(12), 4919–4971.
- Hassan et al. (2019) Hassan, T. A., Hollander, S., van Lent, L., & Tahoun, A. (2019). Firm-level political risk: Measurement and effects. Quarterly Journal of Economics, 134(4), 2135–2202.
- Hatzius et al. (2010) Hatzius, J., Hooper, P., Mishkin, F. S., Schoenholtz, K. L., & Watson, M. W. (2010). Financial conditions indexes. NBER Working Paper No. 16150.
- Hoberg and Lewis (2016) Hoberg, G., & Lewis, C. M. (2016). Do fraudulent firms produce abnormal disclosures? Journal of Corporate Finance, 43, 58–85.
- Huang et al. (2014) Huang, A. H., Teoh, S. H., & Zhang, Y. (2014). Tone management. Accounting Review, 89(3), 1083–1113.
- Husted et al. (2020) Husted, L. F., Rogers, J. H., & Sun, B. (2020). Monetary policy uncertainty. Journal of Monetary Economics, 115, 20–36
- Jegadeesh and Wu (2013) Jegadeesh, N., & Wu, D. (2013). Word power: A new approach for content analysis. Journal of Financial Economics, 110(3), 712–729.
- Kelly and Pruitt (2014) Kelly, B. T., & Pruitt, S. (2014). Market expectations in the cross-section of present values. Journal of Finance, 69(5), 1455–1489.
- Kliesen et al. (2012) Kliesen, K. L., Owyang, M. T., & Vermann, E. K. (2012). Disentangling diverse measures: A survey of financial stress indexes. Federal Reserve Bank of St. Louis Review, 94(5), 369–397.
- Koenker and Bassett (1978) Koenker, R., & Bassett, G. (1978). Regression quantiles. Econometrica, 46(1), 33–50.
- Li (2010) Li, F. (2010). The information content of forward‐looking statements in corporate filings—A naïve Bayesian machine learning approach. Journal of accounting research, 48(5), 1049-1102.
- Loughran and McDonald (2016) Loughran, T., & McDonald, B. (2016). Textual analysis in accounting and finance: A survey. Journal of Accounting Research, 54(4), 1187–1230.
- Loughran and McDonald (2011) Loughran, T., & McDonald, B. (2011). When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. Journal of Finance, 66(1), 35–65.
- Manela and Moreira (2017) Manela, A., & Moreira, A. (2017). News implied volatility and disaster concerns. Journal of Financial Economics, 123(1), 137–162.
- Pesaran & Timmermann (2007) Pesaran, M. H., & Timmermann, A. (2007). Selection of estimation window in the presence of breaks. Journal of Econometrics, 137(1), 134–161.
- Prasad et al. (2019) Prasad, A., Elekdag, S., Jeasakul, P., Lafarguette, R., Alter, A., Feng, A. X., & Wang, C. (2019). Growth-at-risk: Concept and application in IMF country surveillance. IMF Working Paper No. 19/36.
- Prüser and Huber (2024) Prüser, J., & Huber, F. (2024). Nonlinearities in macroeconomic tail risk. Journal of Applied Econometrics, 39(1), 113–140.
- SEC (2008) U.S. Securities and Exchange Commission. (2008). Final rule: Smaller reporting company regulatory relief and simplification. Federal Register, 73(38), 9346–9391.
- Tetlock (2007) Tetlock, P. C. (2007). Giving content to investor sentiment: The role of media in the stock market. Journal of Finance, 62(3), 1139–1168.
