===== QUERY: What is deep learning? =====
Top 1: Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633–638, 2025. DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Meituan LongCat Team. Longcat-flash technical report, 2025b. URL https://arxiv.org/abs/2509.01322. GLM-4.5 Team. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025. URL https://arxiv.org/ abs/2508.06471. ByteDance Seed. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning, 2025. URL https://arxiv.org/abs/2504.13914. Tencent Hunyuan Team. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought, 2025. URL https://arxiv.org/abs/2505.15431. Kimi Team. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Mislav Balunovi´c, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi´c, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025. 10 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828–3850, 2024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: A universal olympiad level
Top 2: (High) Gemini-2.5-Pro Gemini-2.5-Flash DeepSeek-R1-0528 DeepSeek-V3.1-Thinking Qwen3-235B-Thinking GLM-4.5 DeepSeek-R1 LongCat-Flash-Thinking Claude-Sonnet-4.5 DeepSeek-V3-0324 DeepSeek-V3.1 Kimi-K2 LongCat-Flash GPT-4.1 GPT-4o-20241120 Figure 5: The AVG@32 performance of LLMs vs. the average model output length. and LongCat-Flash. These non-reasoning models even outperforms several reasoning models such as o3-mini (Medium), indicating their significant potential in tackling complex reasoning tasks. Comparison of reasoning efficiency. Figure 5 shows the average output length and the AVG@32 performance of each model. Overall, it demonstrates a clear trend that higher-performing models tend to require more output tokens. The first-tier models that reach higher than 40% AVG@32 scores utilize more than 35K completion tokens. Even among non-reasoning models, those with superior performance are distinguished by their ability to process more tokens, sometimes reaching levels comparable to reasoning models. Additionally, when examining models within the same series, there are notable improvements in reasoning efficiency over time. For example, o4-mini (High) outperforms o3-mini (High) at similar or slightly increased token counts. Likewise, DeepSeek-V3.1-Thinking shows significant gains compared to DeepSeek-R1-0528 with even significantly less output tokens. Beyond the main results outlined above, we also provide further analysis and insights based on the AMO-Bench experimental findings. The model output length could indicate the reasoning challenge of the benchmark. Section 2.2 provides a pre-analysis of benchmark difficulty based on annotated solution lengths. Here, we offer a post-hoc analysis of benchmark difficulty based on the relationship between model performance and model output length. Figure 6 clearly demonstrates that the average output length of each model increases as the reasoning benchmark becomes more challenging. Specifically, across six models, benchmarks with higher accuracy scores (such as MAH500 and AIME24) correspond to shorter average outputs, while those with lower scores (like AMO-Bench) require significantly longer responses. This suggests that harder benchmarks demand more elaborate reasoning steps or explanations from the models, resulting
Top 3: llms on 2025 usa math olympiad, 2025. URL https: //arxiv.org/abs/2503.21934. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. s1: Simple test-time scaling. In Workshop on Reasoning and Planning for Large Language Models, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads, 2025. URL https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME. Jie Zhang, Cezara Petrui, Kristina Nikoli´c, and Florian Tramèr. Realmath: A continuous benchmark for evaluating language models on research-level mathematics. arXiv preprint arXiv:2505.12575, 2025. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: A benchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872, 2024. James V Roggeveen, Erik Y Wang, Will Flintoft, Peter Donets, Lucy S Nathwani, Nickholas Gutierrez, David Ettel, Anton Marius Graf, Siddharth Dandavate, Arjun Nageswaran, et al. Hardmath2: A benchmark for applied mathematics built by students as part of a graduate class. arXiv preprint arXiv:2505.11774, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity’s last exam. arXiv preprint arXiv:2501.14249, 2025. 11 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions A Prompt Templates Query prompt template. In order to guide LLMs in generating answers in a


===== QUERY: Explain attention mechanism =====
Top 1: Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633–638, 2025. DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Meituan LongCat Team. Longcat-flash technical report, 2025b. URL https://arxiv.org/abs/2509.01322. GLM-4.5 Team. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025. URL https://arxiv.org/ abs/2508.06471. ByteDance Seed. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning, 2025. URL https://arxiv.org/abs/2504.13914. Tencent Hunyuan Team. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought, 2025. URL https://arxiv.org/abs/2505.15431. Kimi Team. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Mislav Balunovi´c, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi´c, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025. 10 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828–3850, 2024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: A universal olympiad level
Top 2: (High) Model GPT-5 2048 4,096 8,192 16,384 32,768 Logarithm of Average Output Length 10 15 20 25 30 35 40 AVG@32 (%) o4-mini (Low) o4-mini (Medium) o4-mini (High) Model o4-mini 2048 4,096 8,192 16,384 32,768 Logarithm of Average Output Length 5 10 15 20 25 30 AVG@32 (%) o3-mini (Low) o3-mini (Medium) o3-mini (High) Model o3-mini Figure 7: The model performance and output length under different reasoning effort settings. and AIME24 [Muennighoff et al., 2025]. This indicates that further increasing the inference budget will further drive improvements on AMO-Bench. Top-tier models demonstrate promising potential for improvement on AMO-Bench. Existing work reveals that the pass@k performance of the model can reflect its inherent potential to achieve further improvement through reinforcement learning. Inspired by this, we illustrate the pass@k of evaluated models to indicate their inner potential. As shown in Figure 8, the pass@k metric exhibits rapid growth as k increases from 1 to 8, followed by a sustained but gradual improvement as k continues to rise. Notably, the top-tier reasoning models achieve over 70% performance on the pass@32 metric. These results highlight the significant room for improvement in the reasoning capabilities of LLMs. 4 Related Work Evaluating LLMs on mathematical problem solving has been a critical aspect for assessing advancements in reasoning capabilities. Early datasets such as GSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] provided 8 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 0 5 10 15 20 25 30 K 20 40 60 80 Pass@K (%) GPT-5-Thinking (High) DeepSeek-V3.1-Thinking Qwen3-235B-Thinking LongCat-Flash-Thinking Qwen3-Max-Instruct LongCat-Flash DeepSeek-V3.1 GPT-4.1 52.4 73.6 78.0 80.3 82.0 47.6 71.3 74.9 76.8 78.0 47.8 70.0 72.4 73.4 74.0 43.6 76.6 80.9 83.5 86.0 28.8 53.8 61.0 65.1 68.0 14.6 35.0 43.9 49.6 54.0 9.8 27.1 35.3 40.5 44.0 4.1 14.3
Top 3: (High) Gemini-2.5-Pro Gemini-2.5-Flash DeepSeek-R1-0528 DeepSeek-V3.1-Thinking Qwen3-235B-Thinking GLM-4.5 DeepSeek-R1 LongCat-Flash-Thinking Claude-Sonnet-4.5 DeepSeek-V3-0324 DeepSeek-V3.1 Kimi-K2 LongCat-Flash GPT-4.1 GPT-4o-20241120 Figure 5: The AVG@32 performance of LLMs vs. the average model output length. and LongCat-Flash. These non-reasoning models even outperforms several reasoning models such as o3-mini (Medium), indicating their significant potential in tackling complex reasoning tasks. Comparison of reasoning efficiency. Figure 5 shows the average output length and the AVG@32 performance of each model. Overall, it demonstrates a clear trend that higher-performing models tend to require more output tokens. The first-tier models that reach higher than 40% AVG@32 scores utilize more than 35K completion tokens. Even among non-reasoning models, those with superior performance are distinguished by their ability to process more tokens, sometimes reaching levels comparable to reasoning models. Additionally, when examining models within the same series, there are notable improvements in reasoning efficiency over time. For example, o4-mini (High) outperforms o3-mini (High) at similar or slightly increased token counts. Likewise, DeepSeek-V3.1-Thinking shows significant gains compared to DeepSeek-R1-0528 with even significantly less output tokens. Beyond the main results outlined above, we also provide further analysis and insights based on the AMO-Bench experimental findings. The model output length could indicate the reasoning challenge of the benchmark. Section 2.2 provides a pre-analysis of benchmark difficulty based on annotated solution lengths. Here, we offer a post-hoc analysis of benchmark difficulty based on the relationship between model performance and model output length. Figure 6 clearly demonstrates that the average output length of each model increases as the reasoning benchmark becomes more challenging. Specifically, across six models, benchmarks with higher accuracy scores (such as MAH500 and AIME24) correspond to shorter average outputs, while those with lower scores (like AMO-Bench) require significantly longer responses. This suggests that harder benchmarks demand more elaborate reasoning steps or explanations from the models, resulting


===== QUERY: What are LLMs? =====
Top 1: Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633–638, 2025. DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Meituan LongCat Team. Longcat-flash technical report, 2025b. URL https://arxiv.org/abs/2509.01322. GLM-4.5 Team. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025. URL https://arxiv.org/ abs/2508.06471. ByteDance Seed. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning, 2025. URL https://arxiv.org/abs/2504.13914. Tencent Hunyuan Team. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought, 2025. URL https://arxiv.org/abs/2505.15431. Kimi Team. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Mislav Balunovi´c, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi´c, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025. 10 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828–3850, 2024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: A universal olympiad level
Top 2: parsed; however, its applicability is limited to simple answer formats such as numerical values or sets, making it challenging to assess more complex answers. In contrast, LLM-based grading provides greater flexibility across diverse answer types but may be less efficient and does not consistently guarantee accuracy. To fully leverage the strengths of both grading methods, AMO-Bench employs different grading approaches based on the specific answer type for each problem. Specifically, problems in AMO-Bench are divided into four main answer types: numerical answers (e.g., Example 1), set answers (e.g., Example 2), variable-expression answers (e.g., Example 3 which requires providing the general formula for an arithmetic sequence), and descriptive answers (e.g., Example 4 which involves comprehensively considering multiple scenarios). The prompt templates for used for grading are contained in Appendix A. Example 1: Problem with Numerical Answer Question: Let x1, x2, · · · , x2024 be positive real numbers such that xk + xm ≥km for any 1 ≤k < m ≤2024. Find the minimum value of x1 + x2 + · · · + x2024. Answer: 1382935444 4We use the tokenizer of DeepSeek-V3.1 model to count tokens in solutions. 5https://huggingface.co/datasets/HuggingFaceH4/aime_2024. 6https://huggingface.co/datasets/HuggingFaceH4/MATH-500. 4 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Example 2: Problem with Set Answer Question: Find all positive integers n such that for any: an ≥an−1 ≥an−2 ≥· · · · · · a2 ≥a1 > 0, satisfying nP k=1 ak = nP k=1 1 ak , the inequality nQ k=1 ak k ≥1 holds. Answer: {1, 2, 3} Example 3: Problem with Variable-Expression Answer Question: The sequence {an}∞ n=1 consists of positive terms, with a1 = 7, a2 = 2, and satisfies the recurrence relation 8a4 n+2 = 3 + 4an+1 + an (n ∈N∗). Find the general term formula for this sequence. Answer:
Top 3: ranking order. 3.2 Results and Analysis Main results. Figure 4 presents the AVG@32 performance of various leading LLMs, categorized by proprietary/open- source status and reasoning/non-reasoning properties8. Overall, all these models still struggle with the significant challenges presented by AMO-Bench. Even the highest performing model GPT-5-Thinking (High) reaches just 52.4%, while most others score below 40%. This indicates substantial room for improvement in complex reasoning abilities across all current language models. Moreover, both proprietary and open-source reasoning models occupy top ranks in the leaderboard, indicating that recent open-source advancements are closing the gap with leading commercial models. The best-performing open-source model is only about 5% lower than the top proprietary result. Besides reasoning models, some non-reasoning models demonstrate a performance exceeding expectations, such as Qwen3-Max-Instruct 8To facilitate easier reproduction and utilization of AMO-Bench, you can take a fast try on the AMO-Bench-P subset, which includes only the 39 parser-based grading problems from AMO-Bench. Appendix C presents the AVG@32 performance of LLMs on AMO-Bench-P. 6 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 0 10000 20000 30000 40000 50000 Average Output Tokens 0 10 20 30 40 50 AVG@32 (%) Qwen3-Next-Thinking Qwen3-Max-Instruct Qwen3-Next-Instruct Claude-Opus-4 GPT-5-Thinking (High) o4-mini (High) o3-mini (High) Gemini-2.5-Pro Gemini-2.5-Flash DeepSeek-R1-0528 DeepSeek-V3.1-Thinking Qwen3-235B-Thinking GLM-4.5 DeepSeek-R1 LongCat-Flash-Thinking Claude-Sonnet-4.5 DeepSeek-V3-0324 DeepSeek-V3.1 Kimi-K2 LongCat-Flash GPT-4.1 GPT-4o-20241120 Figure 5: The AVG@32 performance of LLMs vs. the average model output length. and LongCat-Flash. These non-reasoning models even outperforms several reasoning models such as o3-mini (Medium), indicating their significant potential in tackling complex reasoning tasks. Comparison of reasoning efficiency. Figure 5 shows the average output length and the AVG@32 performance of each model. Overall, it demonstrates a clear trend that higher-performing models tend to require more output tokens. The first-tier models that reach higher than 40% AVG@32 scores utilize more than 35K completion


===== QUERY: Why do we need retrieval? =====
Top 1: with MATH500 and AIME24. 2.2 Dataset Statistics Problem categories. Referring several official competition syllabus, we categorize the 50 problems of AMO-Bench into the following five primary categories: Algebraic Equations & Inequalities (11/50), Functions & Sequences (13/50), Geometry (5/50), Number Theory (9/50), and Combinatorics (12/50). Figure 3a show the overall distribution of problem categories in AMO-Bench. Length distribution of human-annotated solutions. Since the problems in our AMO-Bench are equipped with manually annotated solutions, we can preliminarily analyze the reasoning complexity of these problems from the view of solution length. We measure solution length in terms of token count4. Additionally, we compare the distribution of solution lengths with those from AIME245 and MATH5006. Figure 3b illustrates the solution length distributions across these benchmarks. It reveals that solutions in AMO-Bench exhibit significantly higher lengths, indicating that problems in this benchmark are inherently more challenging and require more complex reasoning to arrive at the final answer. We conduct a further analysis of the model solution lengths in Section 3.2. 2.3 Grading Method For evaluating answers generated by LLMs, prior work has primarily utilized two approaches: parser-based grading and LLM-based grading. Parser-based grading offers high efficiency and accuracy when the model’s response can be successfully parsed; however, its applicability is limited to simple answer formats such as numerical values or sets, making it challenging to assess more complex answers. In contrast, LLM-based grading provides greater flexibility across diverse answer types but may be less efficient and does not consistently guarantee accuracy. To fully leverage the strengths of both grading methods, AMO-Bench employs different grading approaches based on the specific answer type for each problem. Specifically, problems in AMO-Bench are divided into four main answer types: numerical answers (e.g., Example 1), set answers (e.g., Example 2), variable-expression answers (e.g., Example 3 which requires providing the general
Top 2: These annotated solutions will be utilized in the subsequent quality review stage and will also aid in assessing the overall difficulty of AMO-Bench (see Section 2.2 for details). Quality review. Each candidate problem undergoes blind review by at least three experts to assess its quality. This quality review stage focuses primarily on two aspects: • Whether the problem statement and solution are semantically unambiguous and logically correct. • Whether the mathematical knowledge required for the problem is within the scope typically covered in MO-level competitions such as IMO. Originality review. The originality review stage aims to ensure that these newly created problems are not mere rewrites of publicly available materials, but demonstrate genuine originality. To this end, we assess the originality of each problem through the following methods: • Compare it against problems in existing datasets (e.g., AIME24/25) with 10-gram matching. • Conduct web searches to identify any similar online content. Additionally, during the quality review stage, experts are also required to indicate whether they have encountered highly similar questions in past competitions. Difficulty review. To ensure that AMO-Bench presents a sufficient challenge to state-of-the-art LLMs, we implement a difficulty review stage to filter out problems lacking adequate complexity (even if they may be suitable for some MO-level competitions, e.g., the first 10 questions in AIME). Specifically, each selected problem must satisfy the following two criteria: • The problem must meet or exceed the IMO difficulty standards, as verified by the human expert. • We employed multiple advanced reasoning models (such as GPT, DeepSeek, and Gemini series models) for prelimi- nary evaluation, requiring that at least two such models fail to correctly and consistently solve the problem3. 3For each model, our preliminary evaluation involves three samples. If all three samples are correct, the model is deemed capable of consistently solving
Top 3: Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633–638, 2025. DeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Meituan LongCat Team. Longcat-flash technical report, 2025b. URL https://arxiv.org/abs/2509.01322. GLM-4.5 Team. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models, 2025. URL https://arxiv.org/ abs/2508.06471. ByteDance Seed. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning, 2025. URL https://arxiv.org/abs/2504.13914. Tencent Hunyuan Team. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought, 2025. URL https://arxiv.org/abs/2505.15431. Kimi Team. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. Mislav Balunovi´c, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi´c, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025. 10 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828–3850, 2024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: A universal olympiad level


===== QUERY: What is FAISS used for? =====
Top 1: math competitions. arXiv preprint arXiv:2505.23281, 2025. 10 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3828–3850, 2024. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: A universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations, 2024. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem- solving skills in large language models using odyssey math data. Scientific Data, 12(1):1392, 2025. Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models. arXiv preprint arXiv:2503.21380, 2025. Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovi´c, Nikola Jovanovi´c, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad, 2025. URL https: //arxiv.org/abs/2503.21934. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. s1: Simple test-time scaling. In Workshop on Reasoning and Planning for Large Language Models, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem
Top 2: Y Wang, Will Flintoft, Peter Donets, Lucy S Nathwani, Nickholas Gutierrez, David Ettel, Anton Marius Graf, Siddharth Dandavate, Arjun Nageswaran, et al. Hardmath2: A benchmark for applied mathematics built by students as part of a graduate class. arXiv preprint arXiv:2505.11774, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanity’s last exam. arXiv preprint arXiv:2501.14249, 2025. 11 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions A Prompt Templates Query prompt template. In order to guide LLMs in generating answers in a parser-readable format, we use the following prompt template guide the model generation. There are mainly three requirements in the instruction: the answer prefix (i.e., ### The final answer is:), the LaTeX box environment (i.e., \boxed{}), and the precision requirement. Example 5: Query Prompt Template ... After solving the above problem, please output your final answer in the following format: ### The final answer is: $\boxed{<your answer>}$ Example: ### The final answer is: $\boxed{123}$ The final answer should be given as precisely as possible (using LaTeX symbols such as \sqrt, \frac, \pi, etc.). If the final answer involves a decimal approximation, it must be accurate to at least four decimal places. Grading prompt template. We employ the LLM-based grading using o4-mini (Low) as the grading model, and use the following grading prompt to verify the equivalence between the LLM output and the reference answer. Example 6: Grading Prompt Template For the following math problem, we have the reference answer and the student’s answer. Determine whether the student’s answer is equivalent to the reference answer. If equivalent, output "Correct". If not equivalent, output "Incorrect". ### Problem ... ### Reference Answer ... ### Student Answer ... Now, please provide your judgment. Please strictly
Top 3: solving has been a critical aspect for assessing advancements in reasoning capabilities. Early datasets such as GSM8K [Cobbe et al., 2021] and MATH [Hendrycks et al., 2021] provided 8 AMO-Bench: Large Language Models Still Struggle in High School Math Competitions 0 5 10 15 20 25 30 K 20 40 60 80 Pass@K (%) GPT-5-Thinking (High) DeepSeek-V3.1-Thinking Qwen3-235B-Thinking LongCat-Flash-Thinking Qwen3-Max-Instruct LongCat-Flash DeepSeek-V3.1 GPT-4.1 52.4 73.6 78.0 80.3 82.0 47.6 71.3 74.9 76.8 78.0 47.8 70.0 72.4 73.4 74.0 43.6 76.6 80.9 83.5 86.0 28.8 53.8 61.0 65.1 68.0 14.6 35.0 43.9 49.6 54.0 9.8 27.1 35.3 40.5 44.0 4.1 14.3 19.3 22.9 26.0 Figure 8: The the pass@k trend of various LLMs with increasing k. initial explorations to evaluate these abilities. However, model performance on these benchmarks has quickly reached saturation. To further advance the study of mathematical proficiency in LLMs, recent work has shifted toward more challenging benchmarks. In terms of increasing difficulty, two primary lines of work have emerged. One line focuses on Mathematical Olympiad (MO)-level problems, which rely on a specific range of math knowledge and require complex and intuitive reasoning skills. For instance, Omni-MATH [Gao et al., 2024] introduces a multi-subject evaluation suite designed to rigorously test mathematical reasoning and generalization; OlympiadBench [He et al., 2024] focuses on evaluating the bilingual and multi-modal reasoning abilities with Olympid-level challenges; OlymMATH [Sun et al., 2025] collects MO-level problems from printed publications and evaluates mathematical reasoning by offering problems of two difficulty levels; MathOdyssey [Fang et al., 2025] broadens the scope to include more complex tasks, with a particular focus on long-range and compositional reasoning; BeyondAIME [ByteDance-Seed, 2025] collects problems similar in style to AIME with increased difficulty and expanded data scale; MathArena [Balunovi´c et al., 2025] rapidly tracks model performance in newly held MO-level competitions and explores evaluation

