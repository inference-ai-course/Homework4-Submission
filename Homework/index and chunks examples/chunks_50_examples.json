{"chunk_index": 0, "chunk_text": "Black-Box On-Policy Distillation of Large Language Models Tianzhu Ye\u2217 Li Dong\u2217 Zewen Chi Xun Wu Shaohan Huang Furu Wei Microsoft Research https://aka.ms/GeneralAI Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model\u2019s text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM\u2019s, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation. Project Page: aka.ms/GAD-project Code: aka.ms/GAD-github 3B 7B 14B # Parameters 46 48 50 52 Average Score GPT-5-Chat (Teacher) LMSYS-Chat Benchmark GAD (ours) SeqKD Qwen2.5-Instruct 3B 7B 14B # Parameters 46 47 48 49 50 51 GPT-5-Chat (Teacher) Out-of-Distribution Generalization (Average over Dolly, SelfInst, and Vicuna) GAD (ours) SeqKD Qwen2.5-Instruct Figure 1: Comparison between GAD and sequence-level knowledge distillation (SeqKD; KR16) trained on LMSYS-Chat [ZCS+24] dataset, evaluated by averaged GPT-4o scores. Left: Results on the LMSYS-Chat test set. Right: Average performance across Dolly [Dat23], SelfInst [WKM+23], and Vicuna [CLL+23] datasets. \u2217Equal contribution. Contact person: fuwei@microsoft.com. arXiv:2511.10643v1 [cs.CL] 13 Nov 2025 1 Introduction Knowledge distillation [HVD15] in large language models (LLMs; Ope23, Ope25, LFX+24, YLY+25) is primarily used to create smaller, more efficient student models that retain much of the performance of a larger, resource-intensive teacher model. The setting in which the student has access to the teacher\u2019s internal probability distribution or hidden states is called white-box dis- tillation. Standard white-box approaches align the teacher and student by matching their output distributions, typically via Kullback-Leibler divergence (KLD) [SST+20, GDWH24], or their in- ner states [JYS+20, SCGL19, WWD+20]. However, white-box access is often impractical when the teacher is a proprietary API model (e.g., GPT-5). In this scenario, only teacher-generated texts are accessible, defining the more challenging black-box distillation setting. The absence of fine-grained probability supervision makes conventional likelihood-based objectives unavail- able. Typical black-box distillation methods simply perform supervised fine-tuning on teacher re- sponses [TGZ+23, CLL+23]. Furthermore, when the student and teacher employ incompatible tokenizers, applying likelihood-based objectives also becomes challenging. This highlights the need for a framework that can effectively extract deeper and richer knowledge from teacher-generated text responses. Recent studies [GDWH24, AVZ+24, LL25, YLY+25] in white-box distillation highlight the im- portance of on-policy learning, where the student learns from its own generated responses rather than solely imitating the teacher\u2019s outputs. These studies show that performing reverse KLD on student-generated text promotes mode-seeking behavior and reduces exposure bias compared to teacher-forced training. However, extending this idea to the black-box setting introduces a major challenge: when the student produces its own responses, there are no"}
{"chunk_index": 1, "chunk_text": "rather than solely imitating the teacher\u2019s outputs. These studies show that performing reverse KLD on student-generated text promotes mode-seeking behavior and reduces exposure bias compared to teacher-forced training. However, extending this idea to the black-box setting introduces a major challenge: when the student produces its own responses, there are no probability-level supervision signals available from the teacher to evaluate or correct them. Without explicit feedback, the student cannot directly gauge the quality of its generations relative to the teacher, making effective on-policy distillation infeasible under the standard likelihood-based framework. To address this limitation, we propose GAD, a Generative Adversarial Distillation framework that enables on-policy learning in the black-box regime. Our key idea is to view the student as a gen- erator that produces responses conditioned on prompts, and to train a discriminator to distinguish between teacher and student outputs. The generator is then optimized to produce responses that the discriminator cannot distinguish from those of the teacher, forming a minimax game similar to generative adversarial networks (GANs; GPAM+14, YZWY17). This adversarial process al- lows the student to receive implicit feedback on the quality of its own generations, even without access to the teacher\u2019s probability space. Besides, from the perspective of reinforcement learn- ing (RL; SB+98, SWD+17, SLA+15), our discriminator can be interpreted as an on-policy re- ward model that evolves jointly with the student policy. Unlike conventional reward models in RLHF [OWJ+22] which are fixed after pretraining and prone to reward hacking [SHKK22], our discriminator continually adapts to the student\u2019s behavior during training. The on-policy reward modeling provides stable and dynamic supervision throughout the distillation process. We validate our approach using GPT-5-Chat [Ope25] as a teacher and a range of open-source mod- els from the Qwen2.5 [YYZ+25] and Llama3 [GDJ+24] families as a student. Experiments are conducted on the a subset of LMSYS-Chat-1M dataset [ZCS+24] and evaluated across multiple domains. Under identical training budgets, GAD consistently outperforms both the instruction models before distillation and the SeqKD [KR16, CLL+23, TGZ+23, PLH+23, ZLX+23] baseline across all datasets and model sizes. Notably, on GPT-4o score, Qwen2.5-3B-Instruct distilled with GAD matches the performance of Qwen2.5-7B-Instruct distilled with SeqKD, while Qwen2.5-14B- Instruct trained with GAD approaches the capability of the GPT-5 teacher itself. Our method also delivers particularly strong improvements in out-of-distribution generalization, where SeqKD yields marginal or negative gains. Human evaluations further confirm performance. GAD can effectively extract high-quality knowledge from black-box LLMs without access to output logits. 2 Student (Generator) Discriminator (\ud835\udc37(\u22c5)) Input Prompt (\ud835\udc65) Teacher Gradient Policy Gradient \u2212log\ud835\udf0e\ud835\udc37\ud835\udc66\ud835\udc61\u2212\ud835\udc37\ud835\udc3a\ud835\udc65 Bradley-Terry Loss max \ud835\udc3a min \ud835\udc37 Figure 2: Training procedure of GAD. The student (generator) learns to generate responses that maximize the score assigned by the discriminator. The discriminator is trained with Bradley-Terry loss to assign a lower score to the student than the teacher, learning to distinguish between them. Together, they form a two-player minimax game in an adversarial learning framework. 2 Method We study conditional text generation of large language models, where a model generates a response y conditioned on a"}
{"chunk_index": 2, "chunk_text": "loss to assign a lower score to the student than the teacher, learning to distinguish between them. Together, they form a two-player minimax game in an adversarial learning framework. 2 Method We study conditional text generation of large language models, where a model generates a response y conditioned on a given prompt x sampled from dataset T . To transfer the capabilities of large models to smaller ones, knowledge distillation (KD) trains a student distribution q\u03b8(y|x) parameterized by \u03b8 to approximate the behavior of a teacher distribution p(y|x). In the white-box distillation setting, the student has access to the teacher\u2019s predictive distribution p(y|x). Approaches such as forward KLD [KR16, SST+20, CLL+23, TGZ+23] or reverse KLD [GDWH24] are designed for this setting. However, these techniques can fail if the teacher is a proprietary model that only returns generated text. We refer to this scenario as black-box distillation, where only textual responses from the teacher are observable. The goal is to learn a student model that imitates the teacher\u2019s generative behavior without access to its internal probability space. 2.1 GAD: Generative Adversarial Distillation We perform black-box distillation with generative adversarial training [GPAM+14, YZWY17] as shown in Figure 2. The training dataset T = {(x, yt)} is constructed by iterating over the prompts x in the original dataset and sampling a teacher response yt for each. Our framework consists of a generator G which is the student model, and a discriminator D that assesses the quality of the student and teacher responses. The generator generates the response G(x) to the prompt x. The discriminator predicts a sequence-level scalar score D([x, y]) given prompt x and response y2. The discriminator is initialized using generator model parameters with an extra prediction head. The head projects the final hidden state to a scalar score, and the score of the last token in the sequence is taken as the sequence-level score. The training objective is formulated as a two-player minimax game with the following value function V(G, D): max G min D V(G, D) = E(x,yt)\u223cT [\u2212log \u03c3 (D(yt) \u2212D(G(x)))] , (1) where \u03c3(\u00b7) denotes the sigmoid function. We use Bradley-Terry model [BT52] to capture pairwise preferences between teacher and student response. The proposed generative adversarial training framework allows the student to learn on-policy from its own generated responses via discriminator feedback, eliminating the need to access the teacher\u2019s internal representations. 2.2 Training We discuss the training algorithm of generator and discriminator respectively. From Equation (1), the generator G is trained with the following objective: (Generator) max G E(x,yt)\u223cT [D(G(x))] , (2) 2 The input prompt x and generated response y are concatenated (i.e., [x, y]) and fed into the discriminator (i.e., D([x, y])). For brevity, we use D(y) below to represent D([x, y]). 3 Since the sampling operation in G(x) is non-differentiable with respect to the student model parame- ters, we treat D(G(x)) as a reward and optimize it using policy gradient [SMSM99] with established reinforcement learning algorithms. We employ GRPO [SWZ+24] to train the student"}
{"chunk_index": 3, "chunk_text": "use D(y) below to represent D([x, y]). 3 Since the sampling operation in G(x) is non-differentiable with respect to the student model parame- ters, we treat D(G(x)) as a reward and optimize it using policy gradient [SMSM99] with established reinforcement learning algorithms. We employ GRPO [SWZ+24] to train the student in our experi- ments, with detailed formulations provided in Appendix A.1. For the discriminator D, we minimize its training loss derived from Equation (1): (Discriminator) min D E(x,yt)\u223cT [\u2212log \u03c3 (D(yt) \u2212D(G(x)))] . (3) The discriminator uses Bradley-Terry loss to capture pairwise preferences, encouraging higher scores for teacher responses over student-generated ones. Warmup Before GAD Training We find that jointly warming up the generator and discrimina- tor before the GAD training stage is crucial for final performance. We fine-tune the student on the teacher\u2019s response, and we minimize the cross-entropy loss as warmup for the generator. In the meanwhile, the discriminator is trained using the same data with the Bradley-Terry loss in Equa- tion (3). We conduct warmup for both models for one epoch before starting GAD training. This step promotes effective adversarial optimization and ensures the balance between the generator and discriminator. Ablation studies on the warmup strategy are presented in Section 3.3. 2.3 Implement GAD with Reinforcement Learning Frameworks In our experiments, we implement GAD using existing reinforcement learning frameworks, such as verl [SZY+24]. GRPO [SWZ+24] is used as the policy gradient algorithm, which is detailed in Appendix A.1. As presented in Table 1, we implement the generator as a policy model and the discriminator as a reward model. The generator produces responses, receives rewards from the discriminator, and is optimized to maximize the expected reward. The reward is defined in Equation (2), i.e., D(G(x)). Unlike vanilla reinforcement learning, GAD also needs to jointly update the discriminator (i.e., reward model). The discriminator is trained with Bradley-Terry loss on preference pairs to score the teacher response higher than the student\u2019s output, similar to the reward model in RLHF [OWJ+22]. While conventional RLHF trains a fixed reward model prior to policy optimization which is prone to reward hacking, our approach updates the reward model (discriminator) online to adapt it to the current policy continually. Reinforcement Learning GAD Term Correspondence Policy Model Generator (i.e.,Student LLM) Reward Model Discriminator Reward D(G(x)) (as in Equation (2)) Difference The reward model is typically trained once on a static dataset and then frozen. The policy is then optimized against this fixed reward function. The discriminator co-evolves with the student LLM (i.e., pol- icy model). It is continually up- dated in a minimax game. Table 1: How to implement GAD within reinforcement learning frameworks. Pseudocode of Training Algorithm Algorithm 1 presents the pseudocode for GAD training. 4 Algorithm 1 GAD: Generative Adversarial Distillation Input: Distillation data T = {(x, yt)}; Student LLM (generator) G; Discriminator D Output: Trained student model G Warmup Stage for each batch (x, yt) \u223cT do Update generator G with cross-entropy loss on yt Update discriminator D with Bradley-Terry loss \u25b7Equation (3)"}
{"chunk_index": 4, "chunk_text": "4 Algorithm 1 GAD: Generative Adversarial Distillation Input: Distillation data T = {(x, yt)}; Student LLM (generator) G; Discriminator D Output: Trained student model G Warmup Stage for each batch (x, yt) \u223cT do Update generator G with cross-entropy loss on yt Update discriminator D with Bradley-Terry loss \u25b7Equation (3) end for GAD Training Stage repeat for each batch (x, yt) \u223cT do Sample student responses G(x) Update generator G using D(G(x)) as reward for reinforcement learning Update discriminator D with Bradley-Terry loss \u25b7Equation (3) end for until convergence return G 3 Experiments 3.1 Setup Dataset Given a dataset of instruction prompts, we collect corresponding responses from a teacher model and use them to distill student models. For the following experiments, we use LMSYS-Chat-1M-Clean3, a clean version of the LMSYS-Chat-1M dataset [ZCS+24]. The dataset is derived from high-quality conversational data collected via the Chatbot Arena4 platform. Teacher and Student Models We adopt GPT-5-Chat [Ope25] as the teacher model. It is a closed-source chat model ranked ninth on the Chatbot Text Arena leaderboard at the time of writ- ing. For student models, we use the instruction-tuned variants of open-source models from the Qwen2.5 [YYZ+25] family (Qwen2.5-3B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct) and the Llama3 [GDJ+24] family (Llama-3.2-3B-Instruct, Llama-3.1-8B-Instruct). Training For training data, we sample 200K samples from LMSYS-Chat-1M-Clean and collect the corresponding GPT-5-Chat responses to the instructions as teacher responses. All models are trained for 3 epochs with a batch size of 256, totaling approximately 2400 optimization steps. The PPO mini-batch size for each policy update is also 256. The maximum context length is set to 2048 tokens for instruction prompts and 1536 for model responses. The training and sampling temperature is set to 0.8. We save checkpoints every 50 steps. More training details can be found in Appendix A.2. Evaluation We reserve 500 samples of LMSYS-Chat-1M-Clean as the primary test set. We also include test datasets consisting of a 500-sample subset split from Dolly [Dat23], the 252-sample SelfInst dataset [WKM+23], and the 80-question Vicuna benchmark [CLL+23] to evaluate out-of- distribution generalization. We report the GPT-4o evaluation scores [ZCS+23, GDWH24], where GPT-4o first generates reference answers and then scores the output of the student model against them. We also conduct human evaluations on the LMSYS-Chat-1M-Clean test set for qualitative assessment. We select the checkpoint that achieved the highest GPT-4o score and whose response length is within an acceptable range for each experiment. Detailed evaluation protocols are described in Appendix A.3. 3.2 Main Results Automatic Evaluation We report the results of automatic evaluation using GPT-4o scores in Figure 1 and Table 2. We compare GAD with the instruct model before distillation and the 3 https://huggingface.co/datasets/OpenLeecher/lmsys_chat_1m_clean 4 https://lmarena.ai 5 Model Method LMSYS Dolly SelfInst Vicuna GPT-5-Chat Teacher 51.7 49.8 49.7 49.9 Qwen2.5-3B-Instruct Before Distill. 45.8 45.1 45.6 47.3 SeqKD 47.5 44.8 45.7 48.0 GAD 48.9 46.7 47.7 49.4 Qwen2.5-7B-Instruct Before Distill. 48.7 47.6 48.3 49.1 SeqKD 49.2 47.2 48.3 49.5 GAD 50.8 48.5 50.1 51.4 Qwen2.5-14B-Instruct Before Distill. 50.0 49.1 49.4 50.0 SeqKD 50.6 48.2 49.4"}
{"chunk_index": 5, "chunk_text": "Teacher 51.7 49.8 49.7 49.9 Qwen2.5-3B-Instruct Before Distill. 45.8 45.1 45.6 47.3 SeqKD 47.5 44.8 45.7 48.0 GAD 48.9 46.7 47.7 49.4 Qwen2.5-7B-Instruct Before Distill. 48.7 47.6 48.3 49.1 SeqKD 49.2 47.2 48.3 49.5 GAD 50.8 48.5 50.1 51.4 Qwen2.5-14B-Instruct Before Distill. 50.0 49.1 49.4 50.0 SeqKD 50.6 48.2 49.4 49.7 GAD 52.1 50.4 51.1 51.6 Llama-3.2-3B-Instruct Before Distill. 44.0 45.8 47.0 46.9 SeqKD 47.6 47.0 47.1 48.1 GAD 48.1 48.5 49.1 48.9 Llama-3.1-8B-Instruct Before Distill. 46.9 46.6 48.4 47.9 SeqKD 49.7 47.7 48.7 48.7 GAD 50.3 48.8 49.5 50.2 Table 2: Automatic evaluation results. We report averaged GPT-4o score on the test datasets. The best results are highlighted in bold. GAD consistently outperforms both the instruct model before distillation and SeqKD across all datasets and model variants, with particularly strong gains in out- of-distribution generalization evaluations. vs. Before Distill. vs. SeqKD 0% 20% 40% 60% 80% 100% 68% 52% 4% 40% 28% 8% Qwen2.5-7B-Instruct vs. Before Distill. vs. SeqKD 56% 68% 28% 8% 16% 24% Qwen2.5-14B-Instruct vs. Before Distill. vs. SeqKD 60% 44% 28% 40% 12% 16% Llama-3.1-8B-Instruct GAD Wins Tie GAD Loses Figure 3: Human evaluation results on the LMSYS-Chat-1M-Clean test set. We compare GAD to the instruct model before distillation and the model fine-tuned with SeqKD. SeqKD baseline. Across all datasets, GAD consistently outperforms the baselines. As shown in Figure 1, on the LMSYS-Chat test set, Qwen2.5-3B-Instruct trained with GAD matches the performance of Qwen2.5-7B-Instruct trained with SeqKD; similarly, Qwen2.5-7B-Instruct with GAD rivals Qwen2.5-14B-Instruct with SeqKD, and Qwen2.5-14B-Instruct with GAD is compa- rable to the GPT-5-Chat teacher. In addition, GAD shows particularly strong gains on out-of- distribution generalization benchmarks. On Dolly, SelfInst, and Vicuna, SeqKD yields marginal or even negative improvements, whereas GAD maintains robust performance gains. We attribute this to the superior generalization ability of reinforcement learning compared to supervised fine- tuning [CZY+25, WZZ+25]. We also provide additional automatic evaluation results in Section B.1. Human Evaluation We conduct human evaluations on Qwen2.5-7B-Instruct, Qwen2.5-14B- Instruct, and Llama-3.1-8B-Instruct, comparing GAD against both the instruct model before dis- tillation and the model fine-tuned with SeqKD. For each prompt, the annotators assess the responses of two models and judge whether GAD wins, ties, or loses. GAD achieves a win rate exceeding 50% and a loss rate below 30% in almost all comparisons. The results indicate that GAD can consistently outperform the baseline models on human evaluation performance. 6 1 2 3 4 5 6 N-gram size (n) 0.0 0.2 0.4 0.6 0.8 1.0 Overlap SeqKD GAD Figure 4: Overlap of local patterns between the student and the teacher. SeqKD tends to overfit to local patterns of the teacher. 0 1 2 3 4 5 6 7 8 9 Class 0.0 0.2 0.4 0.6 Probability Teacher SeqKD GAD Figure 5: Black-box distillation on toy data. GAD learns reachable modes from the teacher while SeqKD aims to cover all the modes. 3.3 Analysis SeqKD Overfits to Local Patterns We evaluate the similarity of local patterns between the stu- dent and teacher on"}
{"chunk_index": 6, "chunk_text": "0.2 0.4 0.6 Probability Teacher SeqKD GAD Figure 5: Black-box distillation on toy data. GAD learns reachable modes from the teacher while SeqKD aims to cover all the modes. 3.3 Analysis SeqKD Overfits to Local Patterns We evaluate the similarity of local patterns between the stu- dent and teacher on the LMSYS-Chat test set in Figure 4, measured by the F1 score of N-gram overlap. The student is trained from Qwen2.5-14B-Instruct, and the teacher is GPT-5-Chat. The SeqKD student exhibits a higher N-gram overlap while a lower GPT-4o evaluation score compared to the GAD student. This suggests that supervised fine-tuning tends to memorize local lexical pat- terns [CZY+25, WZZ+25], whereas our RL-based approach better captures the teacher\u2019s global stylistic characteristics. Experiments on Toy Data We simulate the optimizing patterns of GAD and SeqKD in a toy ex- periment shown in Figure 5. We observe that GAD tends to learn reachable modes of the teacher, whereas SeqKD aims to cover all modes. The setup simulates a black-box distillation scenario. We define a discrete Gaussian mixture distribution as a teacher distribution p, which has categorical out- puts 0, . . . , 9. A student, modeled as a single Gaussian distribution, learns to imitate the teacher us- ing only output samples without access to p. We compare two student training schemes, SeqKD and GAD. The GAD student is optimized using the REINFORCE algorithm [Wil92]. As illustrated in Figure 5, the SeqKD student exhibits a mode-covering behavior, spreading probability mass across all possible outputs [GDWH24]. In contrast, the GAD student focuses on mode-seeking, concentrat- ing probability optimization on reachable regions. We find that such mode-seeking behavior leads to more effective knowledge distillation in LLMs. 800 1000 1200 1400 1600 1800 2000 Step 250 500 750 1000 1250 Response Length Reward Hacking Off-Policy Disc. On-Policy Disc. (Ours) Figure 6: Off-policy discriminator suffers from re- ward hacking, whereas on-policy discriminator re- mains stable over thousands of training steps. Comparison to Off-Policy Discriminator As discussed in Section 2.1, from the view of reinforcement learning, our generator (student) acts as the policy model, while the discrimina- tor acts as the on-policy reward model. Figure 6 compares GAD with the off-policy discrimina- tor approach. In the off-policy setting, the stu- dent is first trained for one warmup epoch us- ing SeqKD. The student is then frozen, and the discriminator is trained for two epochs based on the student\u2019s output. Then the resulting dis- criminator serves as a frozen reward model to train the student using Equation (6). In con- trast, GAD jointly trains the student and dis- criminator for one warmup epoch followed by two GAD training epochs, positioning the discriminator as an on-policy reward model. We ob- serve that the student trained with an off-policy discriminator quickly exhibits reward hacking after around 300 training steps, producing excessively long responses (up to 1300 tokens) that deviate significantly from the teacher\u2019s patterns. In comparison, GAD remains stable through thousands of training steps with no sign of reward hacking. The"}
{"chunk_index": 7, "chunk_text": "serve that the student trained with an off-policy discriminator quickly exhibits reward hacking after around 300 training steps, producing excessively long responses (up to 1300 tokens) that deviate significantly from the teacher\u2019s patterns. In comparison, GAD remains stable through thousands of training steps with no sign of reward hacking. The results establish GAD as a highly reliable and robust on-policy distillation method. 7 LMSYS Others SeqKD 49.2 48.3 GAD 50.8 50.0 w/o Gen. Warmup 49.7 49.7 w/o Disc. Warmup 49.0 47.7 Table 3: Ablation of warmup strategy on Qwen2.5-7B-Instruct. Warmup of the genera- tor and discriminator are removed separately. Warmup Strategy We perform an ablation study of the warmup strategy introduced in Section 2.2. As shown in Table 3, we separately remove the warmup stage for the generator and the discrimina- tor on Qwen2.5-7B-Instruct. When removing the generator warmup, we directly use Qwen2.5-7B- Instruct without SeqKD as initialization for both the generator and discriminator for GAD training. This leads to a performance drop. We attribute this to the discriminator easily distinguishing between the stu- dent and teacher outputs in the early training stage. The large distributional gap between the teacher and the student weakens the effectiveness of GAD training. When removing the discriminator warmup, we use the generator obtained after one epoch of SeqKD and initialize the discriminator with the original Qwen2.5-7B-Instruct. In this setting, the imbalance between the generator and the discriminator prevents the discriminator from providing sufficiently informative feedback. Consequently, the adversarial interaction becomes ineffective, and the generator exhibits little improvement beyond its warmup performance. 4 Related Work White-box Distillation of LLM White-box knowledge distillation of LLM assumes full access to the internal representations or token-level probabilities of a teacher model. Standard white-box approaches align the forward KLD of distribution [LHS+21, SST+20], reverse KLD of distribu- tion [GDWH24], hidden states [JYS+20, SCGL19] or attention scores [WWD+20, WBH+21] be- tween the teacher and the student. Recent work [GDWH24, LL25, AVZ+24] also proves the im- portance of on-policy distillation where the student learns from its own responses. Such approaches effectively compress large models while preserving semantic similarity. Despite their effectiveness, these methods rely on full teacher access, which is impractical for proprietary LLMs and limits their applicability to closed-source or API-only teachers. Black-box Distillation of LLM Black-box distillation trains a student model using only the tex- tual outputs of a teacher, typically obtained by API queries to closed-source models such as GPT-5 and Gemini 2.5 [Ope25, CBS+25]. In this setting, conventional white-box distillation methods be- come infeasible because of the lack of access to the teacher\u2019s logits or hidden representations. The standard approach for this scenario, SeqKD, performs supervised fine-tuning (SFT) on the teacher\u2019s responses [KR16, PLH+23, ZLX+23, TGZ+23, CLL+23] to imitate the teacher\u2019s behaviors. Re- cent work [MYS+25, GMK+25, YHX+25, GYZ+25] extends this paradigm by performing SFT on the teacher\u2019s reasoning traces to improve the student\u2019s reasoning ability. 5 Conclusion We introduce GAD, a generative adversarial framework that effectively addresses key challenges of black-box LLM distillation. GAD enables on-policy"}
{"chunk_index": 8, "chunk_text": "to imitate the teacher\u2019s behaviors. Re- cent work [MYS+25, GMK+25, YHX+25, GYZ+25] extends this paradigm by performing SFT on the teacher\u2019s reasoning traces to improve the student\u2019s reasoning ability. 5 Conclusion We introduce GAD, a generative adversarial framework that effectively addresses key challenges of black-box LLM distillation. GAD enables on-policy learning by training a student model and an adaptive discriminator in a minimax game, eliminating the need for any logit-level supervision. This discriminator provides an implicit, on-policy reward signal that guides the student\u2019s optimization. Experiments across multiple model families and datasets confirm our approach. GAD consistently surpasses standard sequence-level distillation, delivering superior generalization and achieving per- formance that rivals the proprietary teacher. These results validate GAD as an effective and robust solution for black-box LLM distillation. Acknowledgements We are grateful to Yi Zhu for technical support during the development of the RL infrastructure and to Yuxian Gu for insightful discussions. 8 References [AVZ+24] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language mod- els: Learning from self-generated mistakes. In The twelfth international conference on learning representations, 2024. [BT52] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952. [CBS+25] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long con- text, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [CLL+23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lian- min Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. [CZY+25] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schu- urmans, Quoc V Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A com- parative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [Dat23] Databricks. Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm, 2023. [GDJ+24] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [GDWH24] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distilla- tion of large language models. In The Twelfth International Conference on Learning Representations, 2024. [GMK+25] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hri- tik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. [GPAM+14] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Ad- vances in neural information processing systems, 27, 2014. [GYZ+25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qi- hao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reason- ing"}
{"chunk_index": 9, "chunk_text": "Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Ad- vances in neural information processing systems, 27, 2014. [GYZ+25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qi- hao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reason- ing capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [JYS+20] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of EMNLP, 2020. [KR16] Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. In Pro- ceedings of EMNLP, 2016. [LFX+24] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Cheng- gang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 9 [LHS+21] Kevin J Liang, Weituo Hao, Dinghan Shen, Yufan Zhou, Weizhu Chen, Changyou Chen, and Lawrence Carin. Mix{kd}: Towards efficient distillation of large-scale language models. In Proceedings of ICLR, 2021. [LL25] Kevin Lu and Thinking Machines Lab. On-policy distillation. Thinking Machines Lab: Connectionism, 2025. https://thinkingmachines.ai/blog/on-policy-distillation. [MYS+25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Han- naneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand\u00e8s, and Tatsunori B Hashimoto. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 20286\u201320332, 2025. [Ope23] OpenAI. GPT-4 technical report, 2023. [Ope25] OpenAI. Introducing gpt-5, 2025. [OWJ+22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Train- ing language models to follow instructions with human feedback. In Proceedings of NeurIPS, 2022. [PLH+23] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruc- tion tuning with GPT-4. arXiv preprint arXiv:2304.03277, 2023. [SB+98] Richard S Sutton, Andrew G Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [SCGL19] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model compression. In Proceedings EMNLP, 2019. [SHKK22] Joar Max Viktor Skalse, Nikolaus HR Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In Proceedings of NeurIPS, 2022. [SLA+15] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889\u20131897. PMLR, 2015. [SMSM99] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gra- dient methods for reinforcement learning with function approximation. Proceedings of NeurIPS, 1999. [SST+20] Kaitao Song, Hao Sun, Xu Tan, Tao Qin, Jianfeng Lu, Hongzhi Liu, and Tie-Yan Liu. LightPAFF: A two-stage distillation framework for pre-training and fine-tuning. arXiv preprint arXiv:2004.12817, 2020. [SWD+17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [SWZ+24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,"}
{"chunk_index": 10, "chunk_text": "two-stage distillation framework for pre-training and fine-tuning. arXiv preprint arXiv:2004.12817, 2020. [SWD+17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [SWZ+24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, et al. Deepseekmath: Pushing the limits of mathe- matical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [SZY+24] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [TGZ+23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An instruction- following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca, 2023. [WBH+21] Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. MiniLMv2: Multi-head self-attention relation distillation for compressing pretrained transformers. In Findings of ACL, 2021. 10 [Wil92] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. [WKM+23] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of ACL, 2023. [WWD+20] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained trans- formers. In Proceedings of NeurIPS, 2020. [WZZ+25] Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of sft: A reinforcement learning perspective with reward rectification. arXiv preprint arXiv:2508.05629, 2025. [YHX+25] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [YLY+25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [YYZ+25] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Li Chengyuan, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115, 2025. [YZWY17] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative ad- versarial nets with policy gradient. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. [ZCS+23] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yong- hao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of NeurIPS, 2023. [ZCS+24] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. Lmsys-chat-1m: A large- scale real-world llm conversation dataset. In The Twelfth International Conference on Learning Representations, 2024. [ZLX+23] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment. In Proceedings of NeurIPS, 2023. 11 A Experimental Details A.1 Implement GAD with"}
{"chunk_index": 11, "chunk_text": "The Twelfth International Conference on Learning Representations, 2024. [ZLX+23] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. LIMA: Less is more for alignment. In Proceedings of NeurIPS, 2023. 11 A Experimental Details A.1 Implement GAD with GRPO We implement policy optimization of the student with GRPO [SWZ+24]. We use qG to denote the output distribution of student G. For each input prompt x, we sample a group of N student responses {yi s}N i=1, and obtain their corresponding rewards {ri s}N i=1, where ri s = D(yi s). The advantage of the i-th response can be calculated with: ri s = D(yi s) (4) Ai = ri s \u2212mean({rj s}N j=1) std({rj s}N j=1) . (5) The student is trained with the following objective: max G E(x,yt)\u223cT ,{yis}N i=1\u223cqG(\u00b7|x) \" 1 N N X i=1 Ai # , (6) where we omit the KL regularizer and the clip operator in GRPO for brevity. For the discriminator, we pair each student response yi s in the group with the same teacher response yt to form (yt, yi s) preference pairs. The discriminator parameters are optimized by minimizing the Bradley-Terry loss across the group: min D E(x,yt)\u223cT ,{yis}N i=1\u223cqG(\u00b7|x) \" 1 N N X i=1 \u2212log \u03c3(D(yt) \u2212D(yi s)) # , (7) where D(yt) is the teacher score shared within the group. A.2 Training Details We train all models with 3 epochs. For GAD, the training consists of 1 warmup epoch followed by 2 GAD training epochs. The models are trained with a batch size of 256, totaling approximately 2400 optimization steps. The PPO mini-batch size for each policy update is also 256. In the warmup stage of GAD, we train the discriminator for 10 steps before jointly training the generator and discriminator. We search learning rate in [1e-6, 5e-6] for GAD and SeqKD baseline. For SeqKD, we find 5e-6 leads to better results in all experiments. For GAD with GPT-5-Chat teacher, we use 1e-6 for both warmup and GAD training stage, and for GAD with Qwen2.5 teacher as in Table 5, we use 5e-6 for warmup stage and 1e-6 for GAD training stage. The maximum context length is set to 2048 tokens for instruction prompts and 1536 for model responses. The training temperature is set to 0.8. In the GRPO algorithm formulated as Equation (6), we set group size N = 8 and the KL weight \u03b2 = 0.001. Distilling Qwen2.5-14B-Instruct from GPT-5-Chat takes about 30 hours on 16 H100 GPUs. 12 A.3 Automatic Evaluation Details The sampling temperature is set to 0.8 and model response length is set to 1536 tokens, same as in training. We use the prompt wrapper in Figure 7 to construct prompts. We use the prompt in Figure 8 for GPT-4o feedback following [GDWH24]. Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Response: Figure 7: The prompt wrapper for training"}
{"chunk_index": 12, "chunk_text": "use the prompt wrapper in Figure 7 to construct prompts. We use the prompt in Figure 8 for GPT-4o feedback following [GDWH24]. Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {instruction} ### Response: Figure 7: The prompt wrapper for training and evaluation. We would like to request your feedback on the performance of two AI assistants in response to the user instruction and input displayed above. Please rate the helpfulness, relevance, accuracy, and level of detail of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. Please first output a single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were pre- sented does not affect your judgment. Figure 8: GPT-4o evaluation prompt. B Additional Results B.1 Additional Automatic Evaluation Results GPT-5 Teacher We provide additional results of the automatic evaluation. In Table 4, we report GPT-4o score and response lengths of distilled student models trained with the GPT-5-Chat teacher. Across datasets, we observe that SeqKD tends to produce shorter responses that closely follow the teacher\u2019s length distribution whereas GAD maintains the original model\u2019s length distribution while integrating the teacher\u2019s global stylistic characteristics. We attribute this behavior to the on-policy sampling of GAD, which encourages generation patterns aligned with both the student\u2019s prior and the teacher\u2019s guidance. Qwen2.5 Teacher In Table 5, we distill from Qwen2.5-14B-Instruct teacher to student models from the Llama family. Although the teacher is open-source, its tokenizer is incompatible with the students, preventing direct application of white-box distillation methods that align KL divergence between teacher and student logits. In this setting, GAD remains effective, outperforming both the pre-distillation models and the SeqKD baseline in most settings on GPT-4o evaluation score. 13 Model Method LMSYS Dolly SelfInst Vicuna Score Len. Score Len. Score Len. Score Len. GPT-5-Chat Teacher 51.7 329.1 49.8 148.5 49.7 188.5 49.9 378.6 Qwen2.5-3B-I Before Distill. 45.8 338.9 45.1 219.2 45.6 279.3 47.3 520.9 SeqKD 47.5 318.2 44.8 160.6 45.7 207.1 48.0 370.4 GAD 48.9 438.0 46.7 239.5 47.7 281.8 49.4 517.9 Qwen2.5-7B-I Before Distill. 48.7 345.2 47.6 220.0 48.3 259.1 49.1 501.7 SeqKD 49.2 320.2 47.2 152.3 48.3 182.3 49.5 398.1 GAD 50.8 414.0 48.5 225.1 50.1 288.5 51.4 511.9 Qwen2.5-14B-I Before Distill. 50.0 322.1 49.1 201.6 49.4 252.0 50.0 475.4 SeqKD 50.6 319.3 48.2 151.2 49.4 199.8 49.7 402.5 GAD 52.1 438.9 50.4 262.6 51.1 284.1 51.6 499.6 Llama-3.2-3B-I Before Distill. 44.0 334.4 45.8 174.5 47.0 265.6 46.9 437.6 SeqKD 47.6 328.6 47.0 147.4 47.1 214.5 48.1 389.3 GAD 48.1 371.5 48.5 232.3 49.1 275.7 48.9 461.8 Llama-3.1-8B-I Before Distill. 46.9 329.2 46.6 184.7 48.4 276.2 47.9 487.8 SeqKD 49.7 319.6 47.7 148.4 48.7 199.7 48.7 400.3 GAD 50.3 394.6"}
{"chunk_index": 13, "chunk_text": "Distill. 44.0 334.4 45.8 174.5 47.0 265.6 46.9 437.6 SeqKD 47.6 328.6 47.0 147.4 47.1 214.5 48.1 389.3 GAD 48.1 371.5 48.5 232.3 49.1 275.7 48.9 461.8 Llama-3.1-8B-I Before Distill. 46.9 329.2 46.6 184.7 48.4 276.2 47.9 487.8 SeqKD 49.7 319.6 47.7 148.4 48.7 199.7 48.7 400.3 GAD 50.3 394.6 48.8 200.6 49.5 263.8 50.2 504.2 Table 4: Extended automatic evaluation results with GPT-5-Chat teacher. We report averaged GPT- 4o score and token length of response. Model Method LMSYS Dolly SelfInst Vicuna Qwen2.5-14B-I Teacher 50.0 49.1 49.4 50.0 Llama-3.2-3B-I Before Distill. 44.0 45.8 47.0 46.9 SeqKD 46.9 47.6 47.6 48.5 GAD 47.5 47.7 47.3 49.0 Llama-3.1-8B-I Before Distill. 46.9 46.6 48.4 47.9 SeqKD 49.0 48.4 48.6 49.4 GAD 49.6 49.9 50.5 49.7 Table 5: Automatic evaluation results with Qwen2.5-14B-Instruct teacher. We report averaged GPT- 4o score. 14 Instella: Fully Open Language Models with Stellar Performance Jiang Liu, Jialian Wu, Xiaodong Yu, Yusheng Su, Prakamya Mishra, Gowtham Ramesh, Sudhanshu Ranjan, Chaitanya Manem, Ximeng Sun, Ze Wang, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum AMD https://huggingface.co/amd/Instella-3B https://github.com/AMD-AGI/Instella Abstract Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or par- tially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct\u2122MI300X GPUs, Instella is devel- oped through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advanc- ing the goal of open and reproducible language modeling research. 0.42 T 1.5 T 4.0 T 2.0 T 9.0 T 18.0 T Pre-training Tokens (trillion) 45 50 55 60 65 70 Avg. score over 11 benchmarks Instella-3B Pythia-2.8B GPT-Neo-2.7B OpenELM-3B StableLM-3B-4E1T Gemma-2B Llama-3.2-3B Qwen-2.5-3B Base Models 1.5 T 4.0 T 2.0 T 9.0 T 18.0 T Pre-training Tokens (trillion) 10 15 20 25 30 35 40 45 50 Avg. score over 9 benchmarks Instella-3B-Instruct OpenELM-3B-Instruct StableLM-zephyr-3b Gemma-2-2b-it Llama-3.2-3B-instruct Qwen2.5-3B-Instruct Instruction-tuned Models Ours (Fully Open) Fully Open Open Weight Figure 1: Average Score versus Pre-training Tokens for base (left) and instruction-tuned (right) models. Instella surpasses prior fully open models of comparable size and, despite being trained on substantially fewer pre-training tokens, achieves competitive performance with state-of-the-art open-weight models for both (left) base models (Table 4) and (right) instruction-tuned models (Table 6). 1 Introduction The rapid advancement of artificial intelligence, driven in large part by large language models (LLMs) (Gemini Team, 2024; OpenAI, 2023; Dubey et al., 2024; Yang et al., 2025a), has accelerated progress toward artificial general"}
{"chunk_index": 14, "chunk_text": "models for both (left) base models (Table 4) and (right) instruction-tuned models (Table 6). 1 Introduction The rapid advancement of artificial intelligence, driven in large part by large language models (LLMs) (Gemini Team, 2024; OpenAI, 2023; Dubey et al., 2024; Yang et al., 2025a), has accelerated progress toward artificial general intelligence and transformed society at large. However, much of this progress has been led by proprietary releases (e.g., GPT-4 (OpenAI, 2023), Claude (Anthropic, 2025), Gemini (Gemini Team, 2024)), where training data, methods, and evaluation details remain opaque. While these mod- els have set new state-of-the-art performance, their closed nature hinders scientific understanding, repro- ducibility, and equitable access. 1 arXiv:2511.10628v1 [cs.CL] 13 Nov 2025 In response, the research community has placed increasing emphasis on open-weight models, where trained parameters are released. Projects such as LLaMA-3.2-3B (Dubey et al., 2024), Qwen-2.5-3B (Yang et al., 2024), and Gemma-2-2B (Team et al., 2024) have demonstrated competitive capabilities in relatively compact architectures. Yet most of these remain open-weight rather than fully open: their training data, preprocessing, and training recipes are either undisclosed or proprietary. As a result, researchers cannot fully reproduce the results, audit potential data contamination, or study the effects of data and training choices at scale. To bridge this gap, we introduce Instella, a new family of fully open 3B-parameter language models. In- stella makes available not only model weights, but also the complete training pipeline, datasets, and opti- mization details, thereby offering full transparency. Instead of solely relying on general-purpose corpora, Instella is pretrained in two distinct stages: an initial 4T-token general-domain pre-training stage, followed by a 57B-token second-stage emphasizing reasoning-heavy domains. To further enrich this stage, we in- troduce an in-house synthetic dataset for mathematics, constructed by abstracting GSM8K problems into symbolic Python programs and parameterizing them to generate diverse yet solvable variants. This ap- proach expands mathematical coverage while maintaining the correctness of synthesized data, providing a principled way to inject reasoning signals into pre-training. In addition, we leverage weight ensembling across stochastic pre-training seeds by conducting multiple second-stage runs with different random seeds and merging their weights into the final checkpoint, which further enhances model performance. Following pre-training, Instella undergoes supervised fine-tuning (SFT) on a carefully curated mixture of 2.3 million high-quality instruction-response pairs drawn from diverse domains such as mathematics, coding, com- monsense reasoning, and multi-turn dialogue. This step equips the model with the ability to follow user prompts, handle complex instructions, and generalize across a wide range of task formats, and is further refined through direct preference optimization (DPO) (Rafailov et al., 2023), aligning outputs with human expectations for helpfulness, safety, and factuality. Building on this foundation, we extend Instella into the long-context regime with Instella-Long, capable of processing sequences up to 128K tokens. Instella-Long is trained in two stages of continued pre-training on 40B tokens, followed by long-context SFT and short-context DPO. Because of the limited availability of long- context SFT data, we synthesize long-context instruction-following examples directly from pre-training documents. Compared with other open-weight models,"}
{"chunk_index": 15, "chunk_text": "of processing sequences up to 128K tokens. Instella-Long is trained in two stages of continued pre-training on 40B tokens, followed by long-context SFT and short-context DPO. Because of the limited availability of long- context SFT data, we synthesize long-context instruction-following examples directly from pre-training documents. Compared with other open-weight models, Instella-Long delivers competitive performance on the challenging Helmet benchmark (Yen et al., 2024), while fully releasing its training details and data to ensure transparency and reproducibility. Finally, Instella advances reasoning-centric reinforcement learning at small scale through Instella-Math. Using only 3B parameters, Instella-Math is, to our knowledge, the first fully open model of this size to apply multi-stage group relative policy optimization (GRPO) (Shao et al., 2024) entirely on open datasets. By gradually increasing rollout lengths and incorporating Olympiad-level problems from DeepScaleR (Luo et al., 2025), the model demonstrates substantial improvements in mathematical and logical reasoning. Re- markably, Instella-Math performs strongly not only on benchmarks like GSM8K and OlympiadBench (He et al., 2024b) but also on TTT-Bench (Mishra et al., 2025), highlighting that reinforcement learning can mean- ingfully enhance reasoning even for compact models. Despite being trained on significantly fewer tokens compared to some leading models, Instella achieves state-of-the-art results among fully open models and rivals the performance of stronger open-weight mod- els. To summarize, our contributions are threefold: \u2022 Instella. A 3B-parameter language transformer trained with a carefully staged pre-training pro- cess. Instella significantly outperforms prior fully open models of comparable size across diverse benchmarks. \u2022 Instella-Long. A long-context variant extending sequence length to 128K tokens driven by con- tinued pre-training and synthetic QA-based long-context instruction tuning. Instella-Long attains competitive performance on the challenging long-context benchmark Helmet. \u2022 Instella-Math. A reasoning-centric variant fine-tuned with curated math datasets and reinforce- ment learning, delivering strong gains on AIME, OlympiadBench, and GSM8K while achieving the highest reported performance on the strategic reasoning benchmark TTT-Bench among fully open models. Our work demonstrates that openness and competitiveness are not mutually exclusive. By releasing model weights, training code, data recipes, and evaluation protocols, Instella enables transparent benchmarking, reproducibility, and further research into the foundations of language modeling. 2 2 Background 2.1 Open-Weight versus Fully-Open Large Language Models The release of open-weight large language models such as LLaMA (Touvron et al., 2023; Dubey et al., 2024) and Qwen (Bai et al., 2023; Yang et al., 2024; 2025a) series has significantly broadened community access to high-performing models. These systems are compact enough to be fine-tuned on modest hard- ware, enabling academic research and downstream applications. However, most such models are not com- pletely transparent: their pre-training datasets, training pipelines, and optimization hyperparameters re- main undisclosed. This opacity prevents reproducibility, makes data contamination difficult to audit, and constrains the ability to study scaling laws or understand how training data composition affects down- stream performance. In contrast, completely transparent models release not only weights but also data recipes, preprocessing scripts, and training code. Notable examples include OLMo (Groeneveld et al., 2024; OLMo et al., 2024) and SmolLM (Allal et al.,"}
{"chunk_index": 16, "chunk_text": "ability to study scaling laws or understand how training data composition affects down- stream performance. In contrast, completely transparent models release not only weights but also data recipes, preprocessing scripts, and training code. Notable examples include OLMo (Groeneveld et al., 2024; OLMo et al., 2024) and SmolLM (Allal et al., 2025), which provide comprehensive training pipelines and fully specified data mixtures. These initiatives enable researchers to systematically investigate questions such as how data diversity affects generalization, how alignment methods interact with model size, and how pre-training choices influence reasoning capabilities. However, prior fully open 3B models still underperform com- pared to state-of-the-art open-weight systems by a considerable margin on challenging benchmarks such as GSM8K (Cobbe et al., 2021b), BBH (Suzgun et al., 2023), and MMLU (Hendrycks et al., 2021b), motivating further work to bridge the gap between transparency and competitiveness. Instella addresses this gap by offering a fully open 3B-parameter model family with state-of-the-art results. We release not only weights but also training data recipes, preprocessing scripts, optimization settings, and evaluation pipelines, pro- viding a truly reproducible foundation for scientific study. 2.2 Long-context Language Models Many real-world applications demand reasoning over inputs significantly longer than the typical 2K\u20138K context windows used in base large language models. Tasks such as legal document analysis, multi-chapter summarization, and retrieval-augmented generation require context lengths exceeding 100K tokens. Recent advances including efficient attention mechanisms (Dao, 2024; Jacobs et al., 2023; Liu et al., 2023), rotary po- sition embedding (RoPE) scaling (Gradient Team, 2024; emozilla, 2023; Ding et al., 2024), and specialized training strategies for long sequences (Gao et al., 2024) have enabled models to process extended sequences. Despite these developments, few transparent models provide both long-context support and strong perfor- mance. On the other hand, open-weight models such as Qwen2.5-1M (Yang et al., 2025b) offer extended context windows, but their training data remain proprietary, limiting reproducibility. Instella-Long con- tributes to this space by transparently extending the context length to 128K tokens through continued pre- training and post-training on the long-context data we release publicly. It achieves competitive results on the long-context benchmarks while establishing a transparent, reproducible long-context baseline. 2.3 Large Reasoning Models The ability to perform multi-step reasoning represents a central goal for large language model development. Benchmarks such as MMLU, BBH, GSM8K, MATH (Hendrycks et al., 2021d) and AIME (AIME) measure a model\u2019s capacity to perform structured, compositional thinking beyond surface-level pattern matching. Recent research demonstrates that high-quality reasoning data and post-training techniques such as rein- forcement learning can dramatically improve performance. Models like DeepSeek-R1 (DeepSeek-AI et al., 2025) and DeepSeek-Math (Shao et al., 2024) show that incorporating step-by-step solutions and applying alignment methods like group relative policy optimization (GRPO) (Shao et al., 2024) can lead to substantial gains in reasoning capabilities. However, most reasoning-focused models remain only partially open: either the reasoning datasets are proprietary, the reinforcement learning recipes are undisclosed, or the resulting models are released without reproducible training pipelines. This lack of transparency hinders systematic study of reasoning capabilities and"}
{"chunk_index": 17, "chunk_text": "2024) can lead to substantial gains in reasoning capabilities. However, most reasoning-focused models remain only partially open: either the reasoning datasets are proprietary, the reinforcement learning recipes are undisclosed, or the resulting models are released without reproducible training pipelines. This lack of transparency hinders systematic study of reasoning capabilities and prevents independent validation of methodological claims. Instella-Math addresses this limitation by providing the first fully open 3B-parameter model trained with multi-stage reinforcement learning entirely on open data. We release not only the model weights but also 3 the reasoning datasets and training configurations, enabling reproducible research into reasoning emer- gence and reinforcement learning training for small-scale models. 3 Instella 3.1 Model Architecture The Instella models are text-only, autoregressive transformer-based language models (Vaswani et al., 2017) with 3 billion parameters. Architecture-wise, Instella consists of 36 decoder layers, each having 32 attention heads with a hidden dimension of 2,560 and an intermediate dimension of 6,912. We use standard multi- head attention (Vaswani et al., 2017). For layer normalization, we employ RMSNorm (Zhang & Sennrich, 2019), which has been shown to provide better training stability and convergence properties compared to standard LayerNorm (Ba et al., 2016), particularly for large-scale language models (Takase et al., 2025; Touvron et al., 2023; Muennighoff et al., 2025). In addition, we apply QK-Norm (Dehghani et al., 2023; Muennighoff et al., 2025; Naseer et al., 2021), where layer normalization is injected after the query and key projections within each attention head. QK-Norm normalizes the query and key vectors before computing attention scores, helping to maintain more balanced attention distributions throughout training. It has been shown to be effective in improving training stability by preventing attention weights from becoming overly extreme, which can lead to gradient instability and poor convergence. Our model uses a standard causal attention mask. The feed-forward network within each transformer layer follows the standard architecture with SwiGLU activation function, which has demonstrated superior per- formance compared to ReLU-based activations in recent language models. We also employ rotary position embeddings (RoPE) (Su et al., 2024) to encode positional information, which provides better extrapolation to longer sequences compared to absolute positional embeddings. The key hyperparameters of Instella-3B architecture are shown in Table 1. We use the OLMo tok- enizer (Groeneveld et al., 2024) with a vocabulary size of 50,304 tokens. This vocabulary size strikes a balance between computational efficiency and representation capacity, allowing the model to handle di- verse text while maintaining reasonable embedding and output layer sizes. Table 1: Key hyper-parameters of Instella-3B architecture. Number of Hidden Intermediate Number of Number of Sequence Vocabulary transformer layers dimension dimension attention heads KV heads length size 36 2560 6912 32 32 4096 50,304 3.2 Training Setup Our training pipeline is based on the open-sourced OLMo codebase, adapted, and optimized for our hard- ware and model architecture. For pre-training we use a total of 128 Instinct MI300X GPUs distributed across 16 nodes. During both pre-training and post-training, we utilize FlashAttention 2 (Dao, 2024), Torch Compile, and bfloat16 mixed-precision training to"}
{"chunk_index": 18, "chunk_text": "is based on the open-sourced OLMo codebase, adapted, and optimized for our hard- ware and model architecture. For pre-training we use a total of 128 Instinct MI300X GPUs distributed across 16 nodes. During both pre-training and post-training, we utilize FlashAttention 2 (Dao, 2024), Torch Compile, and bfloat16 mixed-precision training to reduce memory usage and speed up training. To balance inter-node memory efficiency and intra-node communication overhead within our cluster, we employ fully sharded data parallelism (FSDP) with hybrid sharding, with model parameters, gradients, and optimizer states sharded within a node and replicated across the nodes. 3.3 Pre-training We pre-train the model using two stages with a sequence length of 4,096 tokens and a global batch size of 1,024. The Instella 3B pretraining pipeline is shown in Fig. 2. In the first pre-training stage, we train the model from scratch on 4.07 trillion tokens sourced from OLMoE-mix-0924 (Muennighoff et al., 2025), which is a diverse mix of two high-quality datasets DCLM-baseline (Li et al., 2024) and Dolma 1.7 (Soldaini et al., 2024) covering domains like coding, academics, mathematics, and general world knowledge from web crawl. This extensive first stage pre-training established a foundational understanding of general language 4 58 Billion Tokens 4 Trillion Tokens Stage 1 Stage 2 Instella-3B-Stage1 Instella-3B 760 Million Tokens 26.7 Billion Tokens Supervised Fine-tuning Direct Preference Optimization Instella-3B-SFT Instella-3B-Instruct Pre-training Post-training Figure 2: Instella-3B model training pipeline. in our Instella model. We use the cosine decay learning rate schedule with a maximum learning rate of 4 \u00d7 10\u22124 and set the global batch size to 1024. For our final pre-trained checkpoint, Instella-3B, we conduct a second stage pre-training on top of the first-stage Instella-3B-Stage1 model to further enhance its capabilities on MMLU (Hendrycks et al., 2021b), BBH (Suzgun et al., 2023), and GSM8K (Cobbe et al., 2021b). The model is trained three times with dif- ferent random seeds, and the resulting weights are ensembled to obtain the final checkpoint. Specifically, the second-stage training uses 58 billion tokens sourced from diverse and high-quality datasets, including Dolmino-Mix-1124 (OLMo et al., 2024), SmolLM-Corpus (python-edu) (Ben Allal et al., 2024), Deepmind Mathematics (Saxton et al., 2019), and conversational datasets such as T\u00a8ulu-3-SFT-Mixture (Lambert et al., 2024), OpenHermes-2.5 (Teknium, 2023), WebInstructSub (Yue et al., 2024), Code-Feedback (Zheng et al., 2024), and Ultrachat 200k (Ding et al., 2023). We use the linear decay learning rate schedule with a maxi- mum learning rate of 4 \u00d7 10\u22125 and set the global batch size to 1024. In addition to the publicly available datasets, 28.5 million tokens in the second-stage pre-training data mixture are derived from our in-house synthetic dataset focused on mathematical problems. This dataset is generated using the training set of GSM8k dataset, where we first use Qwen2.5-72B-Instruct (Yang et al., 2024) to 1) abstract numerical values as function parameters and generate a python program to solve the math question, 2) identify and replace numerical values in the existing question with alternative values that are still answerable with the same python program solution as"}
{"chunk_index": 19, "chunk_text": "we first use Qwen2.5-72B-Instruct (Yang et al., 2024) to 1) abstract numerical values as function parameters and generate a python program to solve the math question, 2) identify and replace numerical values in the existing question with alternative values that are still answerable with the same python program solution as the original question. Next, by assigning different new values to these python parameters and using the abstract solution program to compute the corresponding answers, we expand our synthetic dataset with new and reliable question-answer pairs (Yu et al., 2024). 3.4 Post-training We first perform supervised finetuning (SFT) to enable the pre-trained model to follow instructions and respond effectively to user queries. We train for three epochs on 2.3 millions of high-quality instruc- tion\u2013response pairs, resulting in Instella-3B-SFT. During this phase, we utilize datasets spanning a broad spectrum of tasks and domains to ensure that the model generalizes across diverse instruction types. The mixture is selectively sourced from SmolTalk (Allal et al., 2025), OpenMathInstruct-2 (Toshniwal et al., 2024), T\u00a8ulu-3 Instruction Following (Lambert et al., 2024), MMLU auxiliary train set (Hendrycks et al., 2021b), and o1-journey (Qin et al., 2024). We use the linear decay learning rate schedule with a maximum learning rate of 1 \u00d7 10\u22125 and set the global batch size to 128. In the final training stage, we align Instella-3B-SFT with human preferences to ensure its outputs are helpful, accurate, and safe. Building on Instella-3B-SFT, Instella-3B-Instruct is trained with direct preference opti- mization (DPO) (Rafailov et al., 2023) on 0.76 billion tokens from the OLMo 2 1124 7B Preference Mix (OLMo et al., 2024). This alignment step tailors the model\u2019s responses to better reflect human values and expec- tations, thereby improving the quality and reliability of its outputs. We use the linear decay learning rate schedule with a maximum learning rate of 5 \u00d7 10\u22127 and set the global batch size to 128. 4 Instella-Long In this section, we introduce the long-context model of Instella, namely, Instella-3B-Long-Instruct, sup- porting 128K context length. To extend the context length, we continually train the model from Instella- 5 3B-Instruct through: 1. continued pre-training, 2. supervised finetuning (SFT), and 3. direct preference optimization (DPO), as shown in Fig. 3. We detail the training method and data in the following subsec- tions. 20B Tokens (Up to 256K) 20B Tokens (64K) Stage 1 Stage 2 Continued Pre-training 760M Tokens (2K) 1B Tokens (128K) Supervised Fine-tuning Direct Preference Optimization Instella-3B-Long-Instruct Post-training Instella-3B-Instruct Figure 3: Instella-Long model training pipeline. 4.1 Continued Pre-training The long context training is initialized from the short-context checkpoint, Instella-3B-Instruct, which has a context length of 4K. We conduct a two-stage continued pre-training to gradually increase the context length. Stage 1: We extend the context length from 4K to 64K and train the model using 20B tokens. The batch size is 4M tokens and the training steps are 5,000. We follow the RoPE scaling law (Gradient Team, 2024) to increase the base frequency of RoPE from 10,000 to 514,640. We also experiment with alternative"}
{"chunk_index": 20, "chunk_text": "context length from 4K to 64K and train the model using 20B tokens. The batch size is 4M tokens and the training steps are 5,000. We follow the RoPE scaling law (Gradient Team, 2024) to increase the base frequency of RoPE from 10,000 to 514,640. We also experiment with alternative RoPE scaling methods (emozilla, 2023; Gao et al., 2024) and observe only minor differences in performance. Stage 2: As indicated by (Gao et al., 2024), it is beneficial to train the model with the data whose context length is longer than the target context length. In this stage, we train the model on 20B tokens with a maximum context length of 256K - twice our target context length of 128K. Following the RoPE scaling law, we further increase the RoPE base frequency to 3,691,950. The batch size is 8M tokens and the training steps are 2,500. For both stages, we use the linear decay learning rate schedule and the maximum learning rate is 2 \u00d7 10\u22125. Table 2: Long-context continued pre-training data by source and portion. Each stage consists of 20 billion tokens in total. Training Stage 64K Long Data 256K Long Data Short Data Stage 1 Code repos (30%) Books (30%) Textbooks (3%) \u2013 FineWeb-Edu (10%) FineWeb (10%) Wikipedia (5%) OpenWebMath (5%) StackExchange (4%) ArXiv (3%) Stage 2 Code repos (10%) Books (15%) Code repos (20%) Books (15%) Textbooks (2%) FineWeb-Edu (10%) FineWeb (10%) Wikipedia (5%) OpenWebMath (5%) StackExchange (4%) ArXiv (4%) The continued pre-training data originates from the data mixture created by Prolong (Gao et al., 2024). We use the raw text data curated by Prolong and process the data through tokenization, filtering, and packing. In each stage of the continued pre-training, we train on a 20B-token mixture of short- and long-context data with an approximate ratio of 4 to 6. The detailed data sources and portion are listed in Table 2. Let L be the maximum context length of the training stage. We pack both short- and long-context data into L-length sequences for training. For short-context data, we randomly select multiple documents and concatenate them into an L-length sequence. The extra texts beyond L in the last document are discarded. For long- context data, we filter out the documents that are shorter than L. We observe that the raw text data has some super long documents (>> L). For these documents, we randomly sample a few segments from them to avoid producing an excessive number of training examples from a single document. We mix 64K data into 6 the long-context data in the second stage for improving training throughput, where we pack four different 64K documents into a 256K sequence. During data processing, we ensure that the documents used in the first and second stages are mutually exclusive. In training, we apply document masking so that different documents within the same sequence cannot attend to each other. 4.2 Post-training After continued training on the long-context pre-training data, we perform supervised finetuning on a 1B- token mixture of"}
{"chunk_index": 21, "chunk_text": "documents used in the first and second stages are mutually exclusive. In training, we apply document masking so that different documents within the same sequence cannot attend to each other. 4.2 Post-training After continued training on the long-context pre-training data, we perform supervised finetuning on a 1B- token mixture of short- and long-context instruction data. We use a batch size of 4M tokens and train for 250 steps. A linear decay learning rate schedule is employed, with a maximum learning rate of 4 \u00d7 10\u22125. For the SFT data, we pack multiple samples into a 256K sequence with document masking applied during training. Padding tokens are added in order to reach exactly 256K tokens. Similar to the continued pre-training, we train the model on a mixture of short- and long-context instruc- tions data with a ratio of 4 to 6. For short-context instruction data, we use publicly available instruction- tuning datasets, some of which are also used in the post-training of Instella-3B-Instruct. Specifically, we use Ultrachat 200K (Ding et al., 2023), OpenMathinstruct-2 (Toshniwal et al., 2024), T\u00a8ulu-3 Instruction Follow- ing (Lambert et al., 2024), and MMLU auxiliary train set (Hendrycks et al., 2021b). Due to the lack of long-context SFT data, we construct a long-context instruction-following dataset where the context length is controlled to be between 8K and 128K tokens. Specifically, we make use of the long- context documents of Books from our continued pre-training data corpus. We use the documents that have at least 8K tokens and truncate the document to 128K tokens if it is over 128K. Then, we use Qwen2.5-14B- Instruct-1M (Yang et al., 2025b) as a teacher model to synthetically generate a question and an answer for the document. To speed up this process, we randomly choose a subpart of the document for the QA generation instead of using the whole document. The length of the subpart is randomly set to be between 2K and 8K tokens. We use NLTK Bird & Loper (2004) sentence tokenizer to divide documents into sentences to make sure that the selected subpart has complete sentences. The generated question and answer are appended to the end of the long document, serving as a complete single-round instruction-following data sample. Furthermore, we generate long-context instruction data from short-context documents, thereby enhancing dataset diversity with a broader range of sources. We use ArXiv from our continued pre-training corpus and the DCLM subset from Dolmino-Mix-1124 (OLMo et al., 2024). We first generate QA for each short- context document following the same pipeline aforementioned. Next, we iteratively concatenate different short-context documents into a long sequence until it reaches 128K tokens. Since we do not truncate the last document, the concatenated sequence may exceed 128K tokens. Lastly, we randomly choose one QA corresponding to one of the short-context documents and append it to the end of the concatenated sequence. Contrary to the findings by (Gao et al., 2024), we observe that our synthetic long-context instruction data notably improves performance on long-context tasks. The final SFT data mixture"}
{"chunk_index": 22, "chunk_text": "we randomly choose one QA corresponding to one of the short-context documents and append it to the end of the concatenated sequence. Contrary to the findings by (Gao et al., 2024), we observe that our synthetic long-context instruction data notably improves performance on long-context tasks. The final SFT data mixture is shown in Table 3. Table 3: Long-context supervised finetuning data by source and portion, totaling 1 billion tokens. Short Data Long Data Ultrachat 200K (25%), OpenMathinstruct-2 (10%), MMLU auxiliary train set (3%), T\u00a8ulu-3 Instruction Following (2%) Books (44%), DCLM (10%), ArXiv (6%) In the final training stage, we perform human preference alignment using DPO (Rafailov et al., 2023), em- ploying the same training setting and dataset as Instella-3B-Instruct. Different from the previous long- context training stages, this DPO stage is trained on short-context data only with a maximum context length of 2K. Consistent with the findings of other open-weights models, we observe that applying DPO solely on short-context data continues to improve on long-context tasks. 4.3 Implementation Details Sequence Parallelism. We implement sequence parallelism based on Deepspeed Ulysses (Jacobs et al., 2023), which distributes the attention heads across GPUs during attention computation. Compared to Ring- Attention (Liu et al., 2023), this approach is more communication-efficient. For the second continued pre- training stage and SFT, we employ four GPUs as a sequence parallelism group to handle the long input 7 sequences. Sequence parallelism is not used in other stages, as the memory requirements fit within a single GPU. Document Masking and Data Batching. We apply document masking during the continued pre-training and SFT, as each input sequence may contain multiple documents. Document masking is achieved through variable-length FlashAttention (Dao, 2023), which computes attention within each individual document rather than across the entire sequence. This design can also improve training throughput when combined with sorted data batching. Following Prolong (Gao et al., 2024), we sort microbatches at each training step by the sum of document lengths in the sequence. With gradient accumulation, later microbatches benefit from faster processing when they consist of shorter documents. 5 Instella-Math AM-DeepSeek-R1- Distilled-1.4M Context Length: 32k OpenMathInstruct-2 Context Length: 4k Stage 1 Stage 2 Pre-training Instella-3B-Instruct DeepMath Rollouts: 16 Context Length: 16k Big-Math Rollouts: 8 Context Length: 8k Stage 1 Stage 2 Reinforcement Learning (GRPO) Instella-3B-Math DeepScaleR Rollouts: 16 Context Length: 16k Stage 3 Figure 4: Instella-Math model training pipeline. In this section, we introduce Instella-Math, a reasoning-centric language model trained with long chain-of- thought reinforcement learning. To enhance the model\u2019s mathematical and logical reasoning capabilities, we continually train Instella-3B-Instruct through two stages of supervised finetuning and three stages of reinforcement learning, as shown in Figure 4. We detail the training procedure and datasets below. 5.1 Supervised Finetuning As a cold start, we perform a two-stage supervised finetuning process to enhance the reasoning capabilities of Instella-3B-Instruct: Stage 1: Instruction Tuning with OpenMathInstruct-2 for Mathematical Coverage. In the first SFT stage, we begin with instruction tuning, following instructions or prompts properly, especially in a question- answer or"}
{"chunk_index": 23, "chunk_text": "Supervised Finetuning As a cold start, we perform a two-stage supervised finetuning process to enhance the reasoning capabilities of Instella-3B-Instruct: Stage 1: Instruction Tuning with OpenMathInstruct-2 for Mathematical Coverage. In the first SFT stage, we begin with instruction tuning, following instructions or prompts properly, especially in a question- answer or problem-solution format. Using the OpenMathInstruct-2 dataset (Toshniwal et al., 2024), which consists of 14 million problem-solution pairs generated from the GSM8K (Cobbe et al., 2021b) and MATH (Hendrycks et al., 2021d) training sets, the model is trained to solve mathematical questions cover- ing a diverse range of topics from arithmetic and algebra to probability and calculus. Stage 2: Deep Reasoning with Long-Context Training on AM-DeepSeek-R1-Distilled. In the second SFT stage, we further improve the model\u2019s reasoning capability by training on AM-DeepSeek-R1-Distilled- 1.4M (Zhao et al., 2025), which is a large-scale general reasoning dataset containing high-quality and chal- lenging problems. In this stage, we increase the context length of the model from 4K to 32K to allow the model to learn from the long chain-of-thought responses distilled from large reasoning models such as DeepSeek-R1 (DeepSeek-AI et al., 2025). 5.2 Reinforcement Learning Following supervised finetuning, we apply three stages of reinforcement learning using the group relative policy optimization (GRPO) algorithm (Shao et al., 2024) to further strengthen the model\u2019s mathematical reasoning abilities. Training is orchestrated with verl (Sheng et al., 2024) and vLLM (Kwon et al., 2023) for efficient rollout collection, reward scoring, and policy updates. Stage 1: GRPO on Big-Math-RL-Verified (8 Rollouts @ 8K Tokens). In the first stage of reinforcement learning, we apply the GRPO algorithm to train the model on Big-Math-RL-Verified (Albalak et al., 2025), a collection of curated, complex, multi-step math problems. We generate 8 rollouts per prompt, each with up 8 Table 4: Base model performance. Models ARC-C ARC-E BoolQ HS. PiQA SciQ WG. OBQA MMLU BBH GSM8K Avg. Open Weight Models Gemma2-2B 39.5 59.3 74.5 70.5 76.4 96.6 69.8 44.8 53.3 40.8 27.4 59.3 Llama-3.2-3B 47.2 64.9 74.8 73.1 75.9 95.3 70.3 51.2 57.8 47.0 30.1 62.5 Qwen2.5-3B 51.5 67.2 79.1 72.1 77.4 95.5 69.3 51.4 67.2 56.7 63.8 68.3 Fully Open Models Pythia-2.8B 40.5 60.7 64.8 60.1 72.5 89.7 60.8 42.6 26.1 27.7 2.7 49.8 GPTNeo-2.7B 38.5 54.6 62.7 55.2 70.8 88.0 58.3 40.8 27.8 27.3 3.7 48.0 OpenELM-3B 37.5 58.4 68.6 71.7 75.6 92.5 65.4 46.4 26.7 29.4 3.0 52.3 StableLM-3B 44.8 67.0 75.4 74.2 78.4 93.4 68.4 48.6 45.2 37.3 10.8 58.5 Instella-3B-Stage1 53.9 73.2 78.7 74.2 77.5 94.9 71.2 51.4 54.7 34.3 10.8 61.3 Instella-3B 52.8 70.5 76.5 75.0 77.8 96.4 73.1 52.4 58.3 39.7 59.8 66.6 Table 5: Instella 3B base model performance. We report the model performance after stage 1 and stage 2 pretraining. For stage 2, we run the training for three times with different random seeds and merge model weights to obtain the final stage 2 model. Models ARC-C ARC-E BoolQ HS. PiQA SciQ WG. OBQA MMLU BBH GSM8K Avg. Stage1 53.9 73.2 78.7 74.2 77.5 94.9 71.2"}
{"chunk_index": 24, "chunk_text": "and stage 2 pretraining. For stage 2, we run the training for three times with different random seeds and merge model weights to obtain the final stage 2 model. Models ARC-C ARC-E BoolQ HS. PiQA SciQ WG. OBQA MMLU BBH GSM8K Avg. Stage1 53.9 73.2 78.7 74.2 77.5 94.9 71.2 51.4 54.7 34.3 10.8 61.3 Stage2-seed1 51.2 68.8 76.2 73.8 77.3 96.6 72.1 52.0 57.7 38.5 56.1 65.5 Stage2-seed2 50.8 68.4 77.8 74.3 77.2 96.6 71.8 51.4 58.2 38.5 58.8 65.8 Stage2-seed3 49.8 68.8 73.5 75.6 77.2 96.7 72.8 52.0 58.0 38.6 58.3 65.6 Stage2 52.8 70.5 76.5 75.0 77.8 96.4 73.1 52.4 58.3 39.7 59.8 66.6 to 8K output tokens, to explore diverse reasoning trajectories. The model is trained for 1,200 GRPO steps using rule-based reward signals provided by Prime-RL (Cui et al., 2025), which incentivize correctness and well-structured outputs. Stage 2: GRPO on DeepMath (16 Rollouts @ 16K Tokens). To push the limits of long-form reasoning, we conduct a second GRPO stage on DeepMath (He et al., 2025) using 16 rollouts per prompt with up to 16K output tokens. This stage is designed to maximize the model\u2019s capacity for deep mathematical reasoning, enabling it to solve problems that require extended derivations, multiple nested logical steps, or structured proof-like outputs. In this stage, the model is trained for 600 GRPO steps. Stage 3: GRPO on DeepScaleR (16 Rollouts @ 16K Tokens). In the final GRPO stage, we finetune the model on DeepScaleR (Luo et al., 2025), which includes original Olympiad math problems (e.g., AIME and AMC). Similar to Stage 2, this training uses 16 rollouts and a 16K token limit. We run 740 GRPO steps in this phase to improve performance on competition-style reasoning tasks. 6 Evaluation 6.1 Base Model We evaluate the pre-trained base models on ARC-Challenge (ARC-C) (Clark et al., 2018), ARC-Easy (ARC- E) (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (HS) (Zellers et al., 2019), PiQA (Bisk et al., 2019), SciQ (Welbl et al., 2017), WinoGrande (WG) (Sakaguchi et al., 2019), OpenBookQA (OBQA) (Mihaylov et al., 2018), BBH (Suzgun et al., 2022), MMLU (Hendrycks et al., 2021a), and GSM8k (Cobbe et al., 2021a). All the benchmarks use a zero-shot evaluation setting, except BBH, MMLU, and GSM8k, which are evaluated using 3-shot, 5-shot, and 8-shot prompting, respectively. As shown in Table 4, both Instella-3B-Stage1 and Instella-3B models outperform all the other fully open models over all the benchmarks individually (except PIQA). Our final pre-trained checkpoint Instella-3B outperforms the prior top performant fully open pre-trained models by a lead of 8.1% on average, with 9 Table 6: Instruction-tuned model performance. Models MMLU TQA BBH GPQA GSM8K MATH IFEval AE 2 MT Avg. Open Weight Models Gemma-2-2B-Instruct 58.4 55.8 43.0 25.2 53.5 22.5 55.6 29.4 8.1 39.0 Llama-3.2-3B-Instruct 61.5 50.2 61.5 29.7 77.0 46.0 75.4 19.3 7.1 47.5 Qwen-2.5-3B-Instruct 66.9 57.2 57.3 28.1 76.0 60.4 62.5 22.1 8.0 48.7 Fully Open Models StableLM-zephyr-3B 45.1 47.9 39.3 25.7 58.4 10.4 34.2 7.5 6.0 30.5 OpenELM-3B-Instruct 27.4 38.1"}
{"chunk_index": 25, "chunk_text": "Gemma-2-2B-Instruct 58.4 55.8 43.0 25.2 53.5 22.5 55.6 29.4 8.1 39.0 Llama-3.2-3B-Instruct 61.5 50.2 61.5 29.7 77.0 46.0 75.4 19.3 7.1 47.5 Qwen-2.5-3B-Instruct 66.9 57.2 57.3 28.1 76.0 60.4 62.5 22.1 8.0 48.7 Fully Open Models StableLM-zephyr-3B 45.1 47.9 39.3 25.7 58.4 10.4 34.2 7.5 6.0 30.5 OpenELM-3B-Instruct 27.4 38.1 24.2 18.1 1.6 0.4 16.1 0.2 1.0 14.1 Instella-3B-SFT 58.8 52.5 46.0 28.1 71.7 40.5 66.2 7.6 7.1 42.1 Instella-3B-Instruct 58.9 55.5 46.8 30.1 73.9 42.5 71.4 17.6 7.2 44.9 significant improvements in ARC Challenge (+8%), ARC Easy (+3.5%), Winnograde (+4.7%), OpenBookQA (+3.9%), MMLU (+13.1%) and GSM8K (+49%). Second stage pre-training elevates the overall average performance relative to stage-1 by 5.3%, substantially narrowing the performance gap between Instella-3B model and the prior open weight models, and outper- forming Llama-3.2-3B by 4.1% on average (+5.7% ARC-Challenge, +5.6% ARC-Easy, and +29.7% GSM8k), Gemma-2-2B by 7.3% on average (+13.4% ARC-Challenge, +11.2% ARC-Easy, +4.5% HellaSwag, +7.6% OpenBookQA, +5.0% MMLU, and +32.5% GSM8k), and is competitive with Qwen-2.5-3B on the majority of the benchmarks. As shown in Table 5, the Instella-3B checkpoint, obtained by merging the weights of three independently trained models with different random seeds during second stage pretraining, achieves an average performance of 66.6%, surpassing all individual seed runs. The multi-stage pre-training with diverse and high-quality data mixture significantly enhances Instella- 3B\u2019s capabilities, establishing it as a competitive and open alternative in the landscape of comparable size language models. 6.2 Instruction-tuned Model The instruction-tuned models are evaluated on MMLU (Hendrycks et al., 2021a), TruthfulQA (TQA) (Lin et al., 2022), BBH (Suzgun et al., 2022), GPQA (Rein et al., 2023), GSM8K (Cobbe et al., 2021a), Minerva Math (Lewkowycz et al., 2022) (MATH), IFEval (Zhou et al., 2023), Alpaca Eval V2 (AE2) (Dubois et al., 2025), and MT-Bench (MT) (Zheng et al., 2023). Here, GPQA, Minerva Math, IFEval, and Alpaca V2 use a zero-shot evaluation setting, whereas MMLU, TQA, BBH, and GSM8k use few-shot prompting using 5-shots, 6-shots, 3-shots, and 8-shots, respectively. Instella-3B-Instruct model consistently outperforms other fully open models across all evaluated bench- marks with a significant average score lead of 14.37% with respect to the next top performing fully open instruction-tuned models (Table 6). With substantial margins across all the chat benchmarks (+13% MMLU, +7.57% TruthfulQA, +7.43% BBH, +4.46% GPQA, +37.15% IFEval, +10.08% Alpaca 2, and +1.2% MT-Bench). Instella-3B-Instruct narrows the performance gap with leading open-weight models. Instella-3B-Instruct performs on par with or slightly surpasses existing state-of-the-art open weight instruction-tuned models such as Llama-3.2-3B-Instruct (+5.24% TruthfulQA, +0.45% GPQA, and +0.1% MT-Bench), and Qwen2.5-3B- Instruct (+2.01% GPQA and +8.87% IFEval), while significantly outperforming Gemma-2-2B-Instruct with an average score lead of +5.83% (+0.55% MMLU, +3.79% BBH, +4.91% GPQA, +20.47% GSM8k, +19.98% Minerva MATH, and +15.17% IFEval). Overall, Instella-3B-Instruct excels in instruction following tasks and multi-turn QA tasks like TruthfulQA, GPQA, IFEval and MT-Bench, while being highly competitive compared to existing state-of-the-art open weight models on other knowledge recall and math benchmarks, while being trained on significantly fewer training tokens. 10 Table 7: Long-context evaluation on"}
{"chunk_index": 26, "chunk_text": "IFEval). Overall, Instella-3B-Instruct excels in instruction following tasks and multi-turn QA tasks like TruthfulQA, GPQA, IFEval and MT-Bench, while being highly competitive compared to existing state-of-the-art open weight models on other knowledge recall and math benchmarks, while being trained on significantly fewer training tokens. 10 Table 7: Long-context evaluation on the Helmet benchmark. NQ: Natural Question. Inf: InfiniteBench. NarrQA: NarrativeQA. The NIAH-MV task and RAG task (NQ, TriviaQA, and HotpotQA) are evaluated at five context lengths: 8K, 16K, 32K, 64K, and 128K, and the number is reported by averaging across the five context lengths. The InfQA, InfMC, and NarrQA are evaluated at 128K context length. Models NQ TriviaQA HotpotQA InfQA InfMC NarrQA NIAH-MV Avg. Open Weight Models Llama-3.2-3B-Instruct 51.8 86.2 56.4 38.7 56.0 26.0 99.2 59.2 Phi-3.5-Mini-Instruct 41.2 78.6 48.6 24.0 55.0 27.7 87.0 51.7 Gemma-3-4B-it 47.2 76.8 45.2 21.0 49.0 20.7 74.0 47.7 Qwen-2.5-3B-Instruct 34.6 65.8 41.8 14.7 35.0 21.0 80.4 41.9 MiniCPM-2B-128k 28.4 61.6 30.8 3.7 22.0 3.3 46.6 28.1 Fully Open Models Instella-3B-Long-Instruct 43.6 73.0 51.6 30.7 54.0 32.3 84.0 52.7 6.3 Instella-Long We evaluate the long-context performance on Helmet (Yen et al., 2024), a recent comprehensive long- context evaluation benchmark encompassing diverse categories. Helmet demonstrates more consistent alignment with human judgment. We evaluate three main tasks across seven datasets: multi-value needle-in-a-haystack (NIAH-MV), retrieval augmented generation (Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018)), and long-document QA (InfiniteBench MC/QA (Zhang et al., 2024), NarrativeQA (Ko\u02c7cisk`y et al., 2018)). We use substring exact match (SubEM) for the RAG task, recall for NIAH-MV, and exact match for InfiniteBench MC. For InfiniteBench QA and Nar- rativeQA, which involve open-ended answers, we rely on gpt-4o-mini to evaluate model responses against the ground truth, following the prompt and metric provided by Helmet. As shown in Table 7, Instella- 3B-Long-Instruct outperforms open weights models including Phi-3.5-mini-instruct (Abdin et al., 2024), Gemma-3-4B-it (Gemma Team, 2025), Qwen2.5-3B-Instruct (Yang et al., 2024), and MiniCPM-2B-128k (Hu et al., 2024) on most tasks of the Helmet benchmark. Since the context length of Qwen2.5-3B-Instruct is 32K, we also conduct a side-by-side comparison at 8K, 16K, and 32K context lengths, as shown in Table 8. Instella-3B-Long-Instruct outperforms Qwen2.5-3B-Instruct by 2.8% on average. Table 8: Comparison with Qwen2.5-3B-Instruct at 8K, 16K, 32K context lengths. Model NIAH-MV NQ TriviaQA HotpotQA Avg. 8K 16K 32K 8K 16K 32K 8K 16K 32K 8K 16K 32K Qwen2.5-3B-Instruct 95 94 95 48 42 39 77 78 74 51 50 48 65.9 Instella-3B-Long-Instruct 98 95 87 53 49 46 79 73 75 59 59 51 68.7 We also evaluate the short-context performance as shown in Table 9. We observe performance drops on some short-context benchmarks compared to Instella-3B-Instruct. Interestingly, TruthfulQA remains stable, Crows-Pairs shows a slight improvement, and the reduction in Toxigen (57.02 \u219242.34, lower is better) suggests improved toxicity avoidance, together indicating potential gains in responsible AI benchmarks. We hypothesize that these results reflect a trade-off between optimizing for longer context lengths and retaining short-context performance, which may"}
{"chunk_index": 27, "chunk_text": "TruthfulQA remains stable, Crows-Pairs shows a slight improvement, and the reduction in Toxigen (57.02 \u219242.34, lower is better) suggests improved toxicity avoidance, together indicating potential gains in responsible AI benchmarks. We hypothesize that these results reflect a trade-off between optimizing for longer context lengths and retaining short-context performance, which may be more pronounced at the 3B parameter scale compared to larger models. Table 9: Evaluation of Instella-Long on general benchmarks. Models MMLU IFEval MT-Bench TruthfulQA Toxigen (\u2193) Crows-Pair Instella-3B-Instruct 58.9 71.4 7.2 55.5 57.0 58.9 Instella-3B-Long-Instruct 57.4 68.8 6.8 55.5 42.3 60.1 11 6.4 Instella-Math Following the same evaluation settings as DeepScaleR-1.5B (Luo et al., 2025), we report Pass@1 accuracy over AIME 2024/25 (AIME), MATH500 (Hendrycks et al., 2021c), AMC (AMC), Mnerva MATH (Lewkowycz et al., 2022), OlympiadBench (He et al., 2024a), GSM8k (Cobbe et al., 2021b), and GPQA-Diamond (Rein et al., 2023). Table 10 reports the Pass@1 rate for the above benchmarks, calcu- lated based on 16 responses per question. Instella-Math delivers competitive performance when com- pared to leading small-scale open-weight models such as Deepseek-R1-Distilled-Qwen-1.5B, Still-3-1.5B, DeepScaleR-1.5B and SmolLM3-3B. In addition to achieving competitive average performance across all benchmarks, Instella-Math demonstrates the effectiveness of our RL training recipe\u2014improving over its supervised finetuned variant (Instella-Math-SFT) by 10.81 points, compared to a 6.22-point improvement seen in DeepScaleR over its base model (Deepseek-R1-Distilled-Qwen-1.5B). Table 10: Evaluation of Instella-Math on Reasoning Benchmarks Models AIME 2024 AIME 2025 MATH500 AMC Minerva OlympiadBench GSM8K GPQA-D Avg. Pass@1 Open-Weight Models Qwen2.5-Math-1.5B 7.7 4.0 57.8 35.8 15.7 26.0 66.3 15.4 28.6 DeepSeek-R1-Distill-Qwen-1.5B 27.5 22.5 82.6 63.5 26.5 43.0 84.1 16.5 45.8 STILL-3-1.5B-preview 30.6 25.2 84.6 66.7 28.6 45.3 86.6 19.5 48.4 DeepScaleR-1.5B-Preview 40.6 30.8 87.4 73.2 30.1 49.9 87.3 16.5 52.0 Fully-Open Models OLMo-2-1124-7B-Instruct 1.3 0.2 32.6 12.3 10.3 8.5 80.9 11.1 19.6 SmolLM3-3B 52.5 35.8 90.2 78.7 31.8 55.4 92.3 44.9 60.2 Instella-Math SFT 20.0 19.0 77.6 53.9 18.8 43.3 88.0 23.4 43.0 Instella-Math RL Stage 1 27.9 22.5 82.2 58.8 25.1 49.2 90.9 34.2 48.8 Instella-Math RL Stage 2 29.6 22.9 85.8 66.7 27.5 52.7 91.7 37.4 51.8 Instella-Math RL Stage 3 35.6 27.7 86.5 69.7 27.7 53.1 92.5 37.6 53.8 Pass@16 Open-Weight Models Qwen2.5-Math-1.5B 36.7 20.0 87.6 71.1 48.5 53.8 96.0 71.7 60.7 DeepSeek-R1-Distill-Qwen-1.5B 73.3 46.7 95.0 89.2 54.4 63.9 97.0 46.5 70.7 STILL-3-1.5B-preview 70.0 46.7 95.8 89.2 56.6 65.2 96.7 45.5 70.7 DeepScaleR-1.5B-Preview 70.0 53.3 95.2 91.6 54.0 66.2 96.5 39.9 70.9 Fully-Open Models OLMo-2-1124-7B-Instruct 13.3 3.3 66.6 50.6 35.1 23.2 97.3 49.0 42.3 SmolLM3-3B 76.7 77.3 96.6 94.0 54.4 72.4 98.1 90.9 82.1 Instella-Math SFT 50.0 40.0 94.8 89.2 44.9 64.0 97.7 83.8 70.6 Instella-Math RL Stage 1 53.3 43.3 94.6 88.0 51.5 68.6 97.6 90.9 73.5 Instella-Math RL Stage 2 46.7 43.3 95.6 89.2 51.1 68.3 97.7 89.4 72.7 Instella-Math RL Stage 3 63.3 50.0 95.8 86.8 50.4 68.2 97.4 88.9 75.1 Table 11: Evaluation of Instella-Math on TTT-Bench Models oTTT dTTT cTTT sTTT Avg. Open-weight models Qwen2.5-Math-1.5B 12.5 10.0 18.9 7.5 12.2 DeepSeek-R1-Distill-Qwen-1.5B 22.9 10.1 18.2 3.5 13.7 STILL-3-1.5B-preview 24.5"}
{"chunk_index": 28, "chunk_text": "43.3 95.6 89.2 51.1 68.3 97.7 89.4 72.7 Instella-Math RL Stage 3 63.3 50.0 95.8 86.8 50.4 68.2 97.4 88.9 75.1 Table 11: Evaluation of Instella-Math on TTT-Bench Models oTTT dTTT cTTT sTTT Avg. Open-weight models Qwen2.5-Math-1.5B 12.5 10.0 18.9 7.5 12.2 DeepSeek-R1-Distill-Qwen-1.5B 22.9 10.1 18.2 3.5 13.7 STILL-3-1.5B-preview 24.5 12.3 19.8 3.2 14.9 DeepScaleR-1.5B-Preview 23.0 16.5 23.0 8.2 17.7 Fully-open models SmolLM3-3B 51.2 40.1 41.3 42.3 43.7 Instella-Math RL Stage 1 56.3 31.4 39.7 41.9 42.3 Instella-Math RL Stage 2 66.2 37.3 39.2 44.5 46.8 Instella-Math RL Stage 3 70.3 39.6 40.3 49.0 49.8 Additionally, we test Instella-Math on TTT-Bench (Mishra et al., 2025), a new benchmark targeting strategic, spatial, and logical reasoning. Remarkably, without any exposure to TTT-Bench\u2013style or similar strategic 12 gaming data during any stage of training, Instella-Math achieves the best performance among all evaluated models (as shown in Table 11). More importantly, like OLMo2 and SmolLM-3B, Instella-Math is a fully-open language model, with fully- open training data for the base model (Instella-3B), reasoning SFT, and reinforcement learning stages. In contrast, many competing models are only open-weight releases; their base model training (e.g., Qwen- 1.5B) and reasoning distillation processes (e.g., DeepSeek-R1) remain closed. 7 Conclusion We present Instella, a family of fully open three billion parameter language models that are trained entirely on openly available data and codebase. The Instella model family consists of a strong base pre-trained model, a supervised finetuned instruct model, an 128k token context length long-context model, and a reasoning-centric model. Powered by AMD Instinct\u2122MI300X GPUs, Instella models attain state-of-the-art performance among fully open models of similar scale and remains competitive with leading open-weight systems despite using notably fewer pre-training tokens. Instella-Long demonstrates strong long-context capabilities, and Instella-Math delivers impressive gains on mathematical and strategic reasoning bench- marks. Alongside model weights, we release the training code, data recipes, and evaluation protocols to support complete reproducibility and transparent benchmarking to foster open-source innovation. Instella models offers a transparent, performant, and extensible foundation for research and application, support- ing the community in building more capable and reproducible language models. References Marah Abdin, Jyoti Aneja, Harkirat Behl, S\u00b4ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Har- rison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. AIME. Aime problems and solutions, 2025. URL https://artofproblemsolving.com/wiki/index.php/ American Invitational Mathematics Examination. Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, and Nick Haber. Big-math: A large-scale, high-quality math dataset for reinforcement learning in language models, 2025. URL https://arxiv.org/abs/2502.17387. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Mart\u00b4\u0131n Bl\u00b4azquez, Guilherme Penedo, Lewis Tun- stall, Andr\u00b4es Marafioti, Hynek Kydl\u00b4\u0131\u02c7cek, Agust\u00b4\u0131n Piqueres Lajar\u00b4\u0131n, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Cl\u00b4ementine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big \u2013 data-centric training of a small language model, 2025. URL https://arxiv.org/abs/2502. 02737. AMC. American mathematics contest 12 (amc"}
{"chunk_index": 29, "chunk_text": "Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Cl\u00b4ementine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big \u2013 data-centric training of a small language model, 2025. URL https://arxiv.org/abs/2502. 02737. AMC. American mathematics contest 12 (amc 12), 2022. URL https://artofproblemsolving.com/wiki/ index.php/AMC 12. Anthropic. System card: Claude opus 4 & claude sonnet 4. Technical report, Anthropic, AI, 2025. URL https://www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm- corpus, July 2024. URL https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus. Steven Bird and Edward Loper. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pp. 214\u2013217, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/P04-3031/. 13 Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. URL https://arxiv.org/abs/1911.11641. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa- pers), pp. 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300/. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021a. URL https://arxiv.org/abs/2110.14168. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021b. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li,"}
{"chunk_index": 30, "chunk_text": "F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, An- dreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Gritsenko, Joan Puigcerver, Matthias Minderer, Filip Pavetic, Francesco Lo- catello, Thomas Kipf, Sylvain Gelly, Andrew Brock, Alec Radford, Mario Lucic, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. 14 Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In EMNLP, 2023. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang"}
{"chunk_index": 31, "chunk_text": "to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. 14 Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In EMNLP, 2023. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv\u20132407, 2024. Yann Dubois, Bal\u00b4azs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators, 2025. URL https://arxiv.org/abs/2404.04475. emozilla. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically scaled rope further increases/. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Technical report, Google, 2024. URL https://storage.googleapis.com/deepmind-media/gemini/gemini v1 5 report.pdf. Gemma Team. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Gradient Team. Scaling rotational embeddings for long-context language models, 2024. URL https://www. gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models. Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, and Hannaneh Hajishirzi. OLMo: Accelerating the science of language models. In ACL, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024a. URL https://arxiv.org/abs/2402.14008. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024b. Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deepmath- 103k: A large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. 2025. URL https://arxiv.org/abs/2504.11456. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein- hardt. Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009. 03300. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein- hardt. Measuring massive multitask language understanding. In ICLR, 2021b. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric"}
{"chunk_index": 32, "chunk_text": "Dawn Song, and Jacob Stein- hardt. Measuring massive multitask language understanding, 2021a. URL https://arxiv.org/abs/2009. 03300. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein- hardt. Measuring massive multitask language understanding. In ICLR, 2021b. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021c. URL https: //arxiv.org/abs/2103.03874. 15 Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021d. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Ra- jbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly super- vised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Tom\u00b4a\u02c7s Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328, 2018. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonza- lez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\u00a8ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning prob- lems with language models. Advances in neural information processing systems, 35:3843\u20133857, 2022. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Kumar Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee F Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Kamal Mohamed Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Joshua P Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander T Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev,"}
{"chunk_index": 33, "chunk_text": "Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander T Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alex Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-LM: In search of the next generation of training sets for lan- guage models. In NeurIPS Datasets and Benchmarks Track, 2024. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human false- hoods, 2022. URL https://arxiv.org/abs/2109.07958. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpass- ing o1-preview with a 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/ DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering, 2018. URL https://arxiv.org/abs/1809.02789. Prakamya Mishra, Jiang Liu, Jialian Wu, Xiaodong Yu, Zicheng Liu, and Emad Barsoum. Ttt-bench: A benchmark for evaluating reasoning ability with simple and novel tic-tac-toe-style games, 2025. URL https://arxiv.org/abs/2506.10209. 16 Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. OLMoE: Open mixture-of- experts language models. In ICLR, 2025. Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Khan, and Ming-Hsuan Yang. Intriguing properties of vision transformers. In NeurIPS, 2021. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious, 2024. URL https://arxiv.org/abs/2501. 00656. OpenAI. GPT4 technical report. CoRR, abs/2303.08774, 2023. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Zhengzhong Liu, Yuanzhi Li, and Pengfei Liu. O1 replication journey: A strategic progress report \u2013 part 1. https://github.com/GAIR-NLP/O1-Journey, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In NeurIPS, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Keisuke Sakaguchi, Ronan Le Bras,"}
{"chunk_index": 34, "chunk_text": "Finn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In NeurIPS, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. URL https://arxiv.org/abs/1907.10641. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In ICLR, 2019. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. In ACL, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Mirac Suzgun, Nathan Scales, Nathanael Sch\u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/abs/2210.09261. Mirac Suzgun, Nathan Scales, Nathanael Sch\u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In ACL Findings, 2023. 17 Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-training of large language models. In COLM, 2025. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u00b4eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00b4e, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024. Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention"}
{"chunk_index": 35, "chunk_text": "Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions, 2017. URL https://arxiv.org/abs/1707.06209. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chen- gen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv prgeprint arXiv:2505.09388, 2025a. An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report. arXiv preprint arXiv:2501.15383, 2025b. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly. arXiv preprint arXiv:2410.02694, 2024. Xiaodong Yu, Ben Zhou, Hao Cheng, and Dan Roth. Reasonagain: Using extractable symbolic programs to evaluate mathematical reasoning. arXiv preprint arXiv:2410.19056, 2024. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. MAmmoTH2: Scaling Instructions from the Web. NeurIPS, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Anna Korhonen, David Traum, and Llu\u00b4\u0131s M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791\u20134800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology. org/P19-1472/. Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In NeurIPS, 2019. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, et al. \u221ebench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15262\u2013 15277, 2024. 18 Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language model training, 2025. URL https://arxiv.org/abs/2503.19633. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a- judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu"}
{"chunk_index": 36, "chunk_text": "Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a- judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/abs/2306.05685. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. OpenCodeInterpreter: Integrating code generation with execution and refinement. In ACL Findings, 2024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/ abs/2311.07911. 19 Querying Labeled Time Series Data with Scenario Programs Edward Kim*\u2020, Devan Shanker*\u2020, Varun Bharadwaj\u2020, Hongbeen Park\u2020\u2020, Jinkyu Kim\u2020\u2020, Hazem Torfah\u2021, Daniel J. Fremont\u2021\u2021, Sanjit A. Seshia\u2020 \u2020 University of California, Berkeley, \u2020\u2020 Korea University, South Korea, \u2021 Chalmers University of Technology and University of Gothenburg, Sweden, \u2021\u2021 University of California, Santa Cruz, (* first co-authors contributed equally) Abstract. Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber-physical systems (CPS). As a result, sig- nificant research efforts have been directed toward identifying failure scenar- ios within simulation environments. However, a critical question remains: are the AV failure scenarios discovered in simulation relevant to real-world sys- tems\u2014specifically, are they reproducible on actual systems? The sim-to-real gap caused by differences between simulated and real sensor data means that fail- ure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an ef- fective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure per- sists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the SCENIC probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identi- fies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data. Keywords: cyber-physical systems, formal methods, probabilistic programming languages, sensor data retrieval, sim-to-real validation 1 Introduction Simulation-based testing and verification has become an integral part of the design of safety-critical artificial intelligence (AI)-enabled cyber-physical systems (CPS) such as self-driving cars. There is a growing body of literature proposing algorithms which au- tomate search for system failure inducing scenarios in simulation [1\u20135]. These simulation- based testing techniques contribute to scalable assessments of autonomous systems without the risk of injuring other people who may interact with these systems. However, given that the primary objective of simulation-based testing is to improve the performance and safety of CPS in reality (not in simulation), it is pertinent to ask: arXiv:2511.10627v1 [cs.AI] 13 Nov 2025 \u201care failure scenarios identified in simulation reproducible in"}
{"chunk_index": 37, "chunk_text": "injuring other people who may interact with these systems. However, given that the primary objective of simulation-based testing is to improve the performance and safety of CPS in reality (not in simulation), it is pertinent to ask: arXiv:2511.10627v1 [cs.AI] 13 Nov 2025 \u201care failure scenarios identified in simulation reproducible in reality?\u201d Due to dis- crepancies between synthetic versus real sensor data, the systems under evaluation may behave differently. Mathematically characterizing such discrepancies is very difficult, and it is not well understood how these discrepancies affect the behaviors of AI systems comprising deep neural networks. This problem is often referred to as the sim-to-real validation problem. As the result of this problem, the status quo is to physically recon- struct failure scenarios and test the systems in reality, e.g., track testing for self-driving (e.g., [6]), which is labor intensive and not a scalable validation method. In this paper, we propose a novel approach for sim-to-real validation that automat- ically validates real data against formal models. Formal approaches to world (envi- ronment) modeling are crucial to certifying the safety of AI-based autonomy [7]. Our work leverages current trends where (1) large amounts of real data [8\u201310] are being collected and labeled to train systems, and (2) formal scenario modeling languages are increasingly adopted by the research communities working on autonomous CPS and robotics [11\u201313]. Based on these trends, we propose to address the problem by querying the dataset to find real-world instances of the scenario. Specifically, given (i) a formal model of a candidate failure scenario and (ii) a labeled time series dataset, we develop a query algorithm that outputs the subset of the labels (and, thus, the real sensor data) that matches the scenario. Then, the system can be evaluated on the matching real data for validation. If no data is returned from the query, this provides a valuable indication that the failure scenario is not represented in the training dataset. To precisely define the query problem and prove a correctness guarantee for our query algorithm, we formally define what it means for labeled time series data, or label trace, to match a scenario modeled in the probabilistic programming language, SCENIC [11, 12]. A SCENIC program represents a distribution over the initial condi- tions and behaviors of objects. Given an input trace of observations of the world, the program defines a distribution over traces of discrete actions for each object. Given a label trace providing an observation sequence and the corresponding actions of objects, we define a label trace to match a scenario if (1) the initial scene, i.e. the first element of the observation trace, is in the initial state distribution of the program, and (2) the action trace is in the distribution of the output traces generated by the program given the input trace. Related Work. In database systems, numerous video retrieval approaches have been proposed such as BlazeIt [14], VisualWorldDB [15], SVQL [16], and ExSam- ple [17]. These are built using an extension of structured query language (SQL)"}
{"chunk_index": 38, "chunk_text": "in the distribution of the output traces generated by the program given the input trace. Related Work. In database systems, numerous video retrieval approaches have been proposed such as BlazeIt [14], VisualWorldDB [15], SVQL [16], and ExSam- ple [17]. These are built using an extension of structured query language (SQL) [18], which severely limits the expressiveness of queries specifying, for instance, only the presence of particular classes of objects, e.g. retrieve videos which contain a motor- cycle and a truck. Spatialyze [19] and our prior work [20] provide expressive query formalisms, yet can only specify static conditions involving objects positions and orien- tations with respect to road structure; they cannot specify temporal behaviors of objects. Conversely, while not as optimized for querying videos, the field of video understand- ing in computer vision backed by vision language models (VLMs) [21] can take very expressive queries in natural language. However, natural language can be an ambigu- ous way to model scenarios, and VLMs currently have limited performance and no guarantees on accuracy. By contrast, we use SCENIC [12] as a query formalism to ex- pressively model scenarios with precise semantics, and provide a query algorithm with a correctness guarantee. In summary, in this paper, we contribute: 1. A novel problem formulation of querying a labeled time series dataset against a formal scenario model, which is applicable to sensor data of any type (e.g., RGB, LiDAR, radar, etc.) assuming the data is appropriately labeled (Sec. 3). 2. A sound query algorithm for a fragment of SCENIC which can be used to validate candidate failure scenarios for diverse perception, behavior prediction, and plan- ning tasks (Sec. 4). 3. Experiments showing that the algorithm is more accurate and orders of magni- tude faster than the state-of-the-art VLMs and can scale linearly to video duration (Sec. 5). 2 Background and Overview In this section, we provide context and examples to facilitate understanding the query problem formulation in Section 3. Given a scenario description and labeled time series sensor data, our query problem is to determine whether the data matches the scenario description. For brevity, and since video is the main type of time series data we use in this paper, we refer to time series sensor data as a video from now on. Query Language. We adopt SCENIC [11, 12] as our query language to formally model scenarios of interest. SCENIC is a probabilistic programming language designed to model and generate scenarios in simulation. A SCENIC program defines a set of objects, a distribution over their initial configuration, or scene (i.e., position, orientation, etc.), and their probabilistic behaviors. The SCENIC program in Fig. 1 models a scenario where ego car follows a lane and makes a lane change to avoid otherCar that is stationary in front. Lines 10 and 13 define a distribution of initial conditions, ego and otherCar. For instance, in line 13, otherCar is uniformly randomly sampled on the lane of ego, which it intersects with ego\u2019s view cone. Furthermore, ego is assigned a"}
{"chunk_index": 39, "chunk_text": "lane change to avoid otherCar that is stationary in front. Lines 10 and 13 define a distribution of initial conditions, ego and otherCar. For instance, in line 13, otherCar is uniformly randomly sampled on the lane of ego, which it intersects with ego\u2019s view cone. Furthermore, ego is assigned a probabilistic behavior called egoBehavior which is defined on lines 1-5 using try/interrupt statement. By default, ego follows the lane it is on, but changes lane if its distance to the other car is within a distance uniformly randomly sampled from a range of 1 to 15 meters. Note that a behavior can be hierarchically defined using a set of primitive behaviors, which in this case include \u201cfollow lane\u201d and \u201clane change.\u201d Semantics of the Query Language. To define what it means for a video to match a query expressed in SCENIC, we need to understand its semantics. A SCENIC program first generates a scene defining the initial configuration of objects (line 10 and 13); the behaviors in the program then define how the objects behave as a function of the state of the world (e.g., their own positions and those of other objects). In particular, we view a behavior as taking as input a sequence of observations of the environment, forming an input trace, and outputting a series of primitive behaviors, the corresponding output trace. Each element of an input trace is an assignment of concrete values to semantic features, such as objects\u2019 positions, orientations, and lanes occupied by objects. For example, given a finite input trace of three timesteps consisting of se- mantic feature values, e.g. positions and orientations of objects, the egoBehavior Fig. 1. A SCENIC program modeling an ego car making a lane change to avoid a car in front. in Figure 1 can generate a set of output traces, e.g. {\u27e8FL,FL,FL\u27e9, \u27e8FL,LC,FL\u27e9} where FL and LC are abbreviations for the primitive behaviors FollowLane and LaneChange respectively. Note that, due to the probabilistic aspect of the SCENIC be- havior, the ego behavior outputs a set of possible primitive behaviors at each timestep. For example, in line 4 of Figure 1, the interrupt condition for triggering a lane change is defined over a uniform random distribution over an interval of 1 to 15 meters, i.e. Range(1,15). When ego and otherCar are between 1 and 15 meters apart, ei- ther FollowLane or LaneChange could occur depending on the random distance threshold chosen. In such a case, both primitive behaviors are feasible. This semantics of a single behavior extends to the semantics of an entire SCENIC program through synchronous composition: the behaviors of all objects in the program run in parallel, with each one choosing a set of primitive behaviors in each time step. In our example, the set of output traces for the entire program corresponding to the 3-step trace above would be:{{ego : \u27e8FL,FL,FL\u27e9, otherCar : \u27e8St,St,St\u27e9}, {ego : \u27e8FL,LC,FL\u27e9}, otherCar : \u27e8St,St,St\u27e9}}, where each element contains the set of primitive behaviors of each object at each time"}
{"chunk_index": 40, "chunk_text": "in each time step. In our example, the set of output traces for the entire program corresponding to the 3-step trace above would be:{{ego : \u27e8FL,FL,FL\u27e9, otherCar : \u27e8St,St,St\u27e9}, {ego : \u27e8FL,LC,FL\u27e9}, otherCar : \u27e8St,St,St\u27e9}}, where each element contains the set of primitive behaviors of each object at each time step and St abbreviates the Stationary. Label Trace. In our query problem, it is important to note that we query the time series labels of the video, not the raw sensor data. We refer to time series labels of the video as a label trace. We assume the labels contain information that, as we will see in the problem description below, serves as the input and the output traces of the SCENIC program. Specifically, we assume that the labels include observations (such as positions, orientations, occupied lanes), and primitive behaviors of objects. These ob- servations can be computed using localization algorithms like SLAM [22] and behavior prediction algorithms [23], as we demonstrate in our experiments. To account for un- certainty in the classification of primitive behaviors, we allow the label trace to specify a set of possible behaviors at each time step: for example, when using a neural network to predict primitive behaviors, one could include in the trace all behaviors whose pre- dicted confidence is above a threshold. Query Problem. Suppose a SCENIC program P and a label trace \u2113are given. The problem is to determine whether the observations in the label trace match the scenario modeled in the program. Intuitively, we first check if the initial scene, the first element of the input trace of the label trace, is consistent with the scenario, i.e. the initial scene is in the support of the initial distribution of the program. Then, we check if all the objects in the label trace behave consistently with the program, i.e., whether there is some output trace of the program which agrees with the label output trace. We formally define the problem in the next section. Query Examples. We provide a few examples based on querying with the SCENIC program P in Figure 1. Suppose a (simplified) input trace \u03c3\u2113 in = {ego : [(0, 0), (0, 1), (1, 2), (1, 3)], otherCar : [(0, 7), (0, 7), (0, 7), (0, 7)]} from the label trace \u2113, consist- ing of the positions of observed vehicles are given. Let the set of output traces gener- ated by the SCENIC program is \u03c3P OUT = {{ego : \u27e8FL,FL,LC,FL\u27e9, otherCar : \u27e8St,St,St,St\u27e9}, {ego : \u27e8FL,LC,LC,FL\u27e9, otherCar : \u27e8St,St, St,St\u27e9}}. Assume that the initial scene of the input trace is in the support of the initial distribu- tion of the program. Then, the set of output traces of the label trace, \u03c3\u2113 OUT = {{ego : \u27e8FL,FL,LC,FL\u27e9, otherCar : \u27e8St,St,St,St\u27e9}, {ego : \u27e8LC,LC,LC,LC\u27e9, otherCar : \u27e8St,St, St,St\u27e9}} matches the program because both ego and otherCar have at least one shared output trace with the program\u2019s. In contrast, the output trace \u03c3\u2113 OUT = {{ego : \u27e8TL,FL,LC,TL\u27e9, otherCar :"}
{"chunk_index": 41, "chunk_text": "the label trace, \u03c3\u2113 OUT = {{ego : \u27e8FL,FL,LC,FL\u27e9, otherCar : \u27e8St,St,St,St\u27e9}, {ego : \u27e8LC,LC,LC,LC\u27e9, otherCar : \u27e8St,St, St,St\u27e9}} matches the program because both ego and otherCar have at least one shared output trace with the program\u2019s. In contrast, the output trace \u03c3\u2113 OUT = {{ego : \u27e8TL,FL,LC,TL\u27e9, otherCar : \u27e8St,St,St,St\u27e9}, {ego : \u27e8LC,LC,LC,LC\u27e9, otherCar : \u27e8St,St, St,St\u27e9}} where TL represents turn left, does not match. This is because ego does not have any shared output trace. This means that, given the input trace, the observed output trace cannot be generated by the program. Thus, this is not a match. 3 Problem Formulation We formalize the querying problem as a membership problem, where a trace, defined over a predetermined set of labels, is queried against a scenario modeled by a SCENIC program. We begin by introducing some necessary notation. Notation. We write |X| for the cardinality of set X and X \u02d9\u222aY for the disjoint union. If V is a set of variables that are defined over domain D, we define a valuation of V as a function \u03bd : V \u2192D, and write the set of valuations of V as DV . Dist(D) is the set of distributions over D, D\u2217is the set of tuples whose elements are in D, and P(D) is the power set of D. For a sequence \u03c3 = \u03b10, . . . , \u03b1n, we define \u03c3(j) = \u03b1j for \u2200j \u2264n. Finally, we call a sequence \u03c3[i, j] = \u03b1i, . . . , \u03b1j for 0 \u2264i \u2264j \u2264n a window of \u03c3. SCENIC Programs. For purposes of this paper, we formally define a SCENIC pro- gram as a tuple P = (Obj, I, O, Init, B, \u0393). Obj = {obj1, . . . , objn} is a finite set of objects. I and O are disjoint finite sets of input and output variables, defined over a domain D; the input variables I encode the state of the world modeled by the SCENIC program (comprising semantic features such as positions of objects), while the output variables O = O1 \u02d9\u222a. . . \u02d9\u222aOn represent a set of names of primitive behaviors for each object (see Sec. 2). Init \u2208Dist(DI) is an initial distribution over the valuations of input variables. Finally, B is a set of behaviors and \u0393 : Obj \u2192B \u222a{\u22a5} maps each object to its behavior (or \u22a5if it has none). Behaviors. Each object o = oi \u2208Obj has an associated (general) behavior bo \u2208B which is a function bo : (DI) \u2217\u2192P(DOi) defining, given an input sequence, the set of possible primitive behaviors for that object at the current time step. We use a set rather than a distribution of outputs as we are only concerned with membership, so we may abstract the randomness of SCENIC behaviors into nondeterminism. Let a trace be a sequence \u03c3 = \u27e8(i0, o0), . . . , (im, om)\u27e9for some m \u2208N, where ij and oj are valuations of the input and output"}
{"chunk_index": 42, "chunk_text": "of outputs as we are only concerned with membership, so we may abstract the randomness of SCENIC behaviors into nondeterminism. Let a trace be a sequence \u03c3 = \u27e8(i0, o0), . . . , (im, om)\u27e9for some m \u2208N, where ij and oj are valuations of the input and output variables at time step j. A behavior bo induces a set of traces To = {\u27e8(i0, o0), . . . , (im, om)\u27e9| \u2200j \u2264m. ij \u2208DI, oj \u2208bo(i0, . . . , ij)}, which we call the set of traces of bo. For a trace \u03c3, we write \u03c3in for the projection of \u03c3 to its sequence of input valuations, i.e., \u03c3in = \u27e8i0, . . . , im\u27e9. Similarly, we write \u03c3out for the sequence of output valuations, i.e., \u03c3out = \u27e8o0, . . . , om\u27e9. The Semantics of a Program. Given a SCENIC program P as above, we define the behavior of P as the combined synchronous behaviors of its objects, i.e., the function bP : (DI) \u2217\u2192P(DO1) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 P(DOn), mapping \u03c3in \u2208(DI) \u2217to b1(\u03c3in) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 bn(\u03c3in). Given the input trace \u03c3in = \u27e8i0, . . . , im\u27e9, we then define the set of traces of P as TP = {\u27e8(i0, o0), . . . , (im, om)\u27e9| i0 \u2208Supp(Init) and \u2200j \u2264m. ij \u2208DI, oj \u2208 b1(i0, . . . , ij)\u00d7\u00b7 \u00b7 \u00b7\u00d7bn(i0, . . . , ij)}, where Supp(.) defines the support of a distribution. We say that a trace \u03c3 matches a SCENIC program P if \u03c3 \u2208TP . We further say that a set of traces T matches the program P if T \u2229TP \u0338= \u2205. Recall in Sec. 2 that a label trace, which we formalize below, can define a set of feasible observation traces. Thus, as long as there exists a feasible observation trace that can also be generated by the program, it is a match. Lastly, given \u03c3\u2032 \u2208(DI)\u2217, we define T \u03c3\u2032 P = {\u03c3 \u2208TP | \u03c3in = \u03c3\u2032} which represent the set of program traces generated with the input trace \u03c3\u2032. Label Trace. In our problem formulation, we query the labels of time series sen- sor data, not the data itself. For brevity, we refer to a sequence of labels of each frame of sensor data (e.g. RGB image, LiDAR 3D point cloud) as a label trace. Formally, a label trace is defined as a tuple, \u2113= (Obj, I, O, \u03c3in, \u03a3out). Similar to the defini- tion of a SCENIC program, Obj is a set of objects, I and O are sets of input and output variables, respectively, both of which are semantic variables of objects and I \u2229O = \u2205. The label trace also consists of an input trace \u03c3in \u2208(DI)\u2217and a set of output traces \u03a3out \u2286(DO)\u2217. Lastly, we define the set of traces induced by \u2113by T\u2113= {\u27e8(i0, o0), . . . , (im, om)\u27e9| \u2200j \u2264m. \u03c3in(j) = ij \u2227\u2203\u03c3out \u2208\u03a3out. \u2200j"}
{"chunk_index": 43, "chunk_text": "and I \u2229O = \u2205. The label trace also consists of an input trace \u03c3in \u2208(DI)\u2217and a set of output traces \u03a3out \u2286(DO)\u2217. Lastly, we define the set of traces induced by \u2113by T\u2113= {\u27e8(i0, o0), . . . , (im, om)\u27e9| \u2200j \u2264m. \u03c3in(j) = ij \u2227\u2203\u03c3out \u2208\u03a3out. \u2200j \u2264 m. \u03c3out(j) = oj}. Problem Statement. Let a SCENIC program, P = (Obj P, IP , OP , Init, B, \u0393), and a label trace, \u2113= (Obj \u2113, I\u2113, O\u2113, \u03c3in, \u03a3out), be given. It is possible that the label trace can contain additional objects or observations compared to P. For example, a pro- gram modeling a left-turn scenario with two cars should match a trace containing two such cars, even if there is also an unrelated pedestrian in the trace and even if the trace continues after the left turn is complete. We allow the label trace to contain additional information, but do not allow the program to contain additional information. We for- malize this by using the following notion of the object correspondence. To formalize the notion of a match between P and l, we need to check whether the objects of l behave as specified in P. This requires a mapping between objects in P and l. Thus, we define an object correspondence as an injective function C : Obj P \u2192Obj \u2113, mapping the objects in the program, ObjP , to those in the label trace, Obj\u2113(later in our methodology we show how such a mapping can be computed). Given a trace \u03c3, let C\u22121(\u03c3) (slightly abusing notation) denote the trace obtained by mapping each object in \u03c3 to its corresponding object in the program according to C, if there is one, and Supported Fragment Type Syntax Distribution Uniform, Range, Normal, TruncatedNormal Statements require boolean, do behavior, do behavior until, try / interrupt Position Specifier at, in, on, offset by, beyond by, visible from, ahead of, behind by, following for Orientation Specifier facing orientation, facing toward / away from, apparently facing Scalar Operators relative heading of, apparent heading of, distance to, angle to Boolean Operators can see, in Orientation Operators deg, relative to Vector Operators offset by, offset along by Region Operators visible, not visible, visible from, not visible from Table 1. Supported SCENIC Fragment for Querying Label Traces dropping it otherwise. We assume P and \u2113share the same input and output variables (by restricting the sets to the common variables). Problem: For a SCENIC program P, a label trace \u2113, and an integer m \u2208N, check whether there is a correspondence C, trace \u03c3 \u2208T\u2113, and length-m window \u03c3\u2032 of C\u22121(\u03c3) such that \u03c3\u2032 \u2208T \u03c3\u2032 in P . 4 Methodology Given a SCENIC program and a label trace, our key idea is to translate the program to a synchronous composition of hierarchical finite state machines (HFSMs) (see [24]), where each HFSM represents a behavior of an object. The query problem then reduces to checking if the HFSMs accept some trace consistent with the"}
{"chunk_index": 44, "chunk_text": "program and a label trace, our key idea is to translate the program to a synchronous composition of hierarchical finite state machines (HFSMs) (see [24]), where each HFSM represents a behavior of an object. The query problem then reduces to checking if the HFSMs accept some trace consistent with the label trace [25]. We solve this problem by extending the classical non-deterministic finite automata simula- tion algorithm to our symbolic HFSMs. 4.1 Supported SCENIC Fragment We first specify the fragment of SCENIC [12] syntax that is supported in our method- ology as it affects the definition of the HFSM. We support all of SCENIC\u2019s operators for scalar, boolean, orientation, vector, and region, as well as specifiers for position and orientation, but we restrict the types of statements as shown in Table 1. We visual- ize our fragment related to modeling behaviors in Fig. 2. This fragment allows one to flexibly specify any sequential or interrupt-driven behaviors, which can flexibly model diverse interactions among objects. On the other hand, our supported fragment permits variable assignments. This restriction simplifies behaviors so that they become memo- ryless, meaning a behavior maps an input (not a history of inputs) to a set of outputs. 4.2 Input-output hierarchical finite state machines Each behavior defined in the SCENIC program is abstracted into a hierarchical finite state machine (HFSM) [26]. Formally, an HFSM is a tuple, M = (I, O, S, S0, \u00b5, l, G, \u03c4). The sets, I and O, are the sets of input and output variables, respectively. S is a finite set of states and S0 \u2286S is a set of initial states. The function \u00b5: S \u2192M \u222a{\u22a5} is a mapping from states to HFSMs where M represents the set of all HFSMs, and \u22a5means that a state is non-hierarchical. This \u00b5 defines the refinements of each state of the HFSM M. The labeling function, l: S \u2192V O, maps states to concrete output values. G is a set of guards, where a guard is a Boolean predicate defined over I. Finally, \u03c4 \u2286S \u00d7 G \u00d7 S is the transition relation over states. Returning to our running example in Sec. 2, recall that a behavior is hierarchically defined with a pre-defined set of discrete primitive behaviors in SCENIC, e.g. follow lane, lane change, etc. The output variables, O, of the HFSM specify each object\u2019s fea- sible output behaviors. The input variables, I, specify the remaining observations of the world, such as positions, of all objects. As we explained in Sec. 4.1, our behaviors are memoryless. This means that guards are not dependent on the past input. For example, in the SCENIC program in Figure 1, the guard of the condition in line 4 only depends on the current input. The guard predicates of HFSMs are encoded as a satisfiability mod- ulo theories (SMT) [27] formula with a non-linear real arithmetic theory. We provide context for the rest of the HFSM constructs in the following section. Note that, in Figure 1, Range(1,15) in"}
{"chunk_index": 45, "chunk_text": "4 only depends on the current input. The guard predicates of HFSMs are encoded as a satisfiability mod- ulo theories (SMT) [27] formula with a non-linear real arithmetic theory. We provide context for the rest of the HFSM constructs in the following section. Note that, in Figure 1, Range(1,15) in line 4 is not a semantic feature variable in I. It is a variable instantiated by the program and its value is not provided for in the label trace. We refer to such variable as an unobserved variable. In such a case, we encode the domain of such variable to the SMT formula of the guard, such that it evaluates to true if there exists a value in the domain that satisfies the formula. Assuming that the positions of ego and otherCar are observed at the current timestep, their distance on the left side of the inequality can be evaluated. If their distance is in between 1 and 15 meters, then the guard can be evaluated to be true; otherwise, false. Likewise, the guard condition for FollowLane is the negation of the condition in line 4, which also can be evaluated to true if their distance is in between 1 and 15 meters. Thus, in such a case, non-deterministic transitions are executed such that ego can be in either FollowLane or LaneChange states at the current timestep. Thus, note that each object\u2019s behavior can have more than one feasible primitive behavior output due to the non-determinism. The semantics of the HFSMs for the supported fragment follow the semantics of a SCENIC program as defined in the problem formulation. The overall behavior of the program is defined as the combined synchronous behaviors of all its objects. At the initial timestep, each HFSM initializes to its initial states. Given an observed input at each timestep, each HFSM executes to transition its states. Starting from the topmost current states, the HFSM iteratively evaluates the guards of the current states with the input and non-deterministically transitions if they evaluate to true. Then, the HFSM re- cursively traverses down to its current child states repeating the same procedure until a non-hierarchical state, i.e. a state with no child, is reached [25]. For brevity, we define a base state to be a state with no child state. Prior to reaching any base states, if a hi- erarchical state reaches its terminate state (as shown in Figure 2) then its parent state transitions and then resumes the recursive procedure. Once the base states are reached, the HFSM outputs using the label function for the base state. If the topmost hierarchical state reaches a terminate state, then the whole HFSM terminates. In our running exam- ple, the base states are the discrete primitive behaviors, and the labeling functions for the base states return the string name of the base states, e.g. \u201cFollowLane.\u201d Fig. 2. Translation of SCENIC syntax fragment to hierarchical FSM. The green boxes abstractly represent finite state machines, while the yellow boxes are states. The state"}
{"chunk_index": 46, "chunk_text": "states are the discrete primitive behaviors, and the labeling functions for the base states return the string name of the base states, e.g. \u201cFollowLane.\u201d Fig. 2. Translation of SCENIC syntax fragment to hierarchical FSM. The green boxes abstractly represent finite state machines, while the yellow boxes are states. The state with the black dot represents a termination state. The variables with T subscript are Boolean predicates which indi- cates whether the corresponding machine terminated. 4.3 Syntax-Directed Translation to HFSMs SCENIC provides syntax to hierarchically model behaviors using a set of primitive be- haviors as shown in Figure 1. We translate the behaviors in the SCENIC program to HF- SMs, maintaining the hierarchical structure. Hence, we refer to it as a syntax-directed translation to HFSMs. Given a SCENIC program, we convert it into an abstract syntax tree and then translate it to a set of HFSMs. Figure 2 shows the translation each behavior in a SCENIC program to a HFSM. On the left column, a construct of our supported SCENIC fragment for modeling behaviors are listed. On the right column, the corresponding HFSM translation is shown, using the traditional statechart [28] convention to visualize HFSMs. Each box represents a state named after the SCENIC syntax it represents. If a box contains another box, this repre- sents that the box containing the other is the parent. The black double circle represents a termination state. The symbols on the arrows, i.e. transitions, represent the guards. The first row of Figure 2 shows the translation of do X until C syntax which is used to invoke behavior X until a condition C is satisfied. On the right, the DoUntil state has a child called X, which recursively executes upon the execution of its par- ent state. A Boolean predicate, XT, indicates whether the behavior X is completed. The black circle represents a termination state. A do statement, as shown in Figure 1, is equivalent to case where the condition C is assigned false. In the second row, a sequen- tial statement is shown where S and R can be any statement shown in Figure 2. On the right, S and R represent HFSM that represents the corresponding statements. Finally, the HFSM translation of the try/interrupt statement is shown on the third row, left. Again, S and R are statements of any type in the figure. The Try and Interrupt states contain the HFSMs of the corresponding S and R statements. In our methodology, primitive behaviors are compiled to base states. For example, in Fig. 1, FollowLane, Algorithm 1 Determine if a SCENIC program matches a label trace Input: SCENIC program P, a label trace l, and a minimum time duration m Output: Does l match P? (True / False) 1: AST \u2190Parse(P) // get abstract syntax tree (AST) 2: M \u2190Translate(AST) // M = {key:object name, value:HFSM of object\u2019s behavior} 3: \u03d5 \u2190InitializeCorrespondenceSMTConstraints(l, AST, m) 4: while \u03d5 is satisfiable, with solution corr do 5: if Query(AST, M, l, corr, m) then // Algorithm"}
{"chunk_index": 47, "chunk_text": "match P? (True / False) 1: AST \u2190Parse(P) // get abstract syntax tree (AST) 2: M \u2190Translate(AST) // M = {key:object name, value:HFSM of object\u2019s behavior} 3: \u03d5 \u2190InitializeCorrespondenceSMTConstraints(l, AST, m) 4: while \u03d5 is satisfiable, with solution corr do 5: if Query(AST, M, l, corr, m) then // Algorithm 2 6: return True 7: \u03d5 \u2190\u03d5 \u2227\u03d5\u2032 // \u03d5\u2032 is a new constraint to block already checked object assignment 8: return False LaneChange, and Stationary are base states. The behaviors, X, S, and R, in Fig- ure 2 can be either a hierarchical state, i.e. HFSM, or a base state. 4.4 Query Algorithm Our top level algorithm is given in Algorithm 1. As explained in Sec. 4.3, the program P is converted into an abstract syntax tree (AST) by Parse function (line 1), and the AST is translated to HFSMs by Translate function (line 2). Because the object correspondence injectively mapping objects in the SCENIC program to those in the trace is unknown a priori, we begin by searching for this mapping. Correspondence Search. The algorithm analyzes the abstract syntax tree (AST) of the program to identify all the objects and their types (e.g. pedestrian, car). Likewise, from the label trace, it extracts all the observed objects, their types, and the duration for which they have been observed. Then, for each SCENIC object, the algorithm identifies a set of objects in the label trace that share the same object type and are observed for at least the provided minimum time duration m. This way, the algorithm aims to prune out infeasible correspondences, thereby reducing the number of combinatorial searches. These procedures are executed by the function in line 3, which encodes these constraints as a satisfiability modulo theory (SMT) formula, \u03d5, with the linear integer arithmetic logic. Returning to our running example, the SMT formula may encode that ego, otherCar \u2208{car1, car5, car10, car21} and that ego \u0338= otherCar, where car# refers to a unique integer id# in the label trace. If the SMT solver returns unsat, meaning there is no solution, then there is no fea- sible correspondence. Thus, the algorithm returns False, i.e. not a match. If there exists a correspondence, then we use the correspondence to determine a match (line 5). If the label trace is not a match, then it is possible that there still may be another correspon- dence which may result in a match. Thus, as shown in line 7, the algorithm conjoins an additional constraint \u03d5\u2032 which encodes that the current correspondence is not true to the SMT formula \u03d5, e.g. \u03d5\u2032 = \u00ac(ego = car5 \u2227otherCar = car3). Then, the algorithm searches for another correspondence using the SMT solver until either a match is found (returning True) or until no more correspondence can be found (returning False). Query Procedure The pseudocode for determining a match is shown in Algo- rithm 2. Because the problem is to find a match for m consecutive timesteps, we use a Algorithm 2 Query Algorithm Input:"}
{"chunk_index": 48, "chunk_text": "a match is found (returning True) or until no more correspondence can be found (returning False). Query Procedure The pseudocode for determining a match is shown in Algo- rithm 2. Because the problem is to find a match for m consecutive timesteps, we use a Algorithm 2 Query Algorithm Input: Abstract Syntax Tree of the SCENIC program AST, Compiled HFSMs M, label trace l, object correspondence corr, and time duration m Output: l matches given corr 1: for timestep i from 0 to len(l) \u2212m do 2: if InitialInputMatch(AST, l, t) is False then 3: continue // initial scene does not match; try next window 4: currentBaseStates \u2190initial base states of M 5: // dictionary whose key is obj name and value is current base states of the obj\u2019s HFSM 6: for timestep t from i to i + m \u22121 do 7: currentBaseStates \u2190ValidStep(currentBaseStates, M, l[t], corr) 8: if currentBaseStates[obj] is empty for any obj then 9: mismatch \u2190True 10: break // mismatch detected, break out of the inner for-loop 11: if not mismatch then 12: return True 13: return False sliding window of length m across the label trace (line 1 and 6). For the label trace to match the program, its initial input must be in the support of the program\u2019s initial dis- tribution. The InitialInputMatch function on line 2 checks this condition using the algorithm from our prior work [20]. If the check fails, then the algorithm moves on to the next sliding window (line 3). Otherwise, the algorithm proceeds to compare the output traces of the label trace and the SCENIC program. The line 6-9 of the algorithm checks if the HFSM can simulate the label trace for a sliding window of length m. At each timestep of the sliding window, the algorithm exe- cutes ValidStep function which returns the set of possible outputs from each HFSM that are consistent with observed outputs from the label trace, i.e. the returned set is the intersection of the set of outputs from each SCENIC object\u2019s HFSM and the set of outputs from the corresponding object in the label trace. We will describe ValidStep procedure in the next section and move on to explain the rest of Algorithm 2. For all timesteps of the sliding window of length m, if there exists a consistent output between the HFSMs and the output traces of the label trace, then the algorithm returns True, i.e. a match; otherwise, the algorithm moves on to check the next sliding window. If all possible sliding windows are checked but none of them results in a match, then it outputs False. Note that m is a parameter which needs to be carefully chosen by a user. If m is too small, e.g. m = 1, then it may likely return many label traces which may match for a single timestep but does not match for the most part of the traces. ValidStep Procedure The ValidStep function steps, or transitions, the HFSMs such that their outputs"}
{"chunk_index": 49, "chunk_text": "user. If m is too small, e.g. m = 1, then it may likely return many label traces which may match for a single timestep but does not match for the most part of the traces. ValidStep Procedure The ValidStep function steps, or transitions, the HFSMs such that their outputs are consistent with the observed outputs in the label trace. The function takes as an input argument a dictionary called currentBaseStates whose key is an object name and its value is a set of feasible base states. The current base states refer to base states that are running at the given timestep. Recall that the output val- ues of each HFSM are determined by the current base states (refer to the semantics of HFSMs in Sec. 4.3). In line 4, prior to invoking the ValidStep, all the HFSMs are initialized such that their current states are set to their initial states. Then, the current base states of each HFSM is computed by simply traversing down from the topmost current hierarchical states to the base states, without evaluating guards or transitioning states. Thus, the current base states of the HFSMs at the initial timestep of the current sliding window is computed. The ValidStep procedures are as follows. First, given the current base states of each HFSM, the function reconstructs the current states of the HFSMs by recursively traversing up each HFSM from its current base states. Then, starting from the topmost current states of each HFSM, the function recursively computes the following down their current child states. It encodes as SMT formula, with non-linear real arithmetic theory, each current state\u2019s guard conditions using the given input values at the current timestep, l[t], uses a SMT solver to evaluate the guard, and transitions the states if any guard evaluates to true. Finally, once it transitions the HFSMs, the ValidStep function compares the set of outputs of each object\u2019s HFSM to the observed outputs of the corresponding object in the label trace at the current timestep. Then, for each HFSM, the function prunes out, i.e. deletes, its current base states whose outputs are not in the set of observed outputs. Thus, the function returns the pruned set of current base states of each HFSM, which are consistent with the observed outputs. The returned set for a HFSM can be an empty set, which means that the HFSM\u2019s outputs are not consistent with the observed outputs, thereby not a match (line 8-10). 4.5 Correctness of the Query Algorithm Theorem 1. Given a SCENIC program, a label trace, and an integer m \u2208N, our algo- rithm outputs True if and only if the label trace matches the program for a window of length m; otherwise, the algorithm outputs False. Proof Sketch. Alg. 1 checks all possible object correspondences and windows of length m. For each correspondence, Alg. 2, in line 2, correctly checks whether the initial input is in the support of the initial distribution. With a synchronous composition of HFSMs as a formal representation"}
