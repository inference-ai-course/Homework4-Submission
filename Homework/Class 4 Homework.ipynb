{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Retrieval-Augmented Generation (RAG) with arXiv Papers\n",
    "This week marks a major shift in your AI agent's capabilities: youâ€™ll build the foundation for a Retrieval-Augmented Generation (RAG) system tailored to scientific research. Rather than relying on an LLMâ€™s memory alone, RAG architectures allow your agent to search a structured knowledge base and generate grounded, document-aware answers.\n",
    "\n",
    "Your task is to create a RAG pipeline using recent arXiv cs.CL papers, converting them into searchable chunks, embedding them, and indexing them with FAISS. Youâ€™ll then implement a simple query interface that takes a user question, retrieves the top relevant chunks, and displays them for further processing.\n",
    "\n",
    "This week marks the beginning of building your agentâ€™s private research knowledge baseâ€”a semantic index that youâ€™ll evolve into a full-featured hybrid database in Week 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "* Understand the components of a Retriever-Reader QA pipeline.\n",
    "* Explore document chunking strategies (e.g., sections vs. sliding windows) and their impact on retrieval performance.\n",
    "* Index scientific text using vector embeddings and FAISS.\n",
    "* Build and query a semantic index via a FastAPI endpoint that returns relevant passages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Design\n",
    "\n",
    "The project will guide you through building a RAG pipeline on arXiv cs.CL papers:\n",
    "\n",
    "1. **Data Collection:** Obtain 50 arXiv cs.CL PDFs (you can scrape via the arXiv API or use a provided sample set).\n",
    "2. **Text Extraction:** Extract raw text from each PDF (for example, using PyMuPDF's `get_text()` on each page). Clean and concatenate the page text into full-document strings.\n",
    "3. **Text Chunking:** Split each paper into chunks (â‰¤ 512 tokens each). You might split at section boundaries or use a sliding-window approach (e.g., 500-token windows with overlap). Chunking into smaller, meaningful segments (around 250â€“512 tokens) often yields better retrieval precision.\n",
    "4. **Embedding Generation:** Compute dense vector embeddings for each chunk. For instance, using the `sentence-transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(list_of_chunks)  # embeds each text chunk into a 384-d vecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (Alternatively, you can use a Hugging Face Transformer model and apply pooling manually to get chunk embeddings.)\n",
    "5. **Indexing with FAISS:** Build a FAISS index of the chunk embeddings. For example, use a simple index like `IndexFlatL2` with the same dimensionality as your embeddings. Add all chunk vectors to the index (e.g., `index.add(np.array(embeddings))`).\n",
    "6. **Notebook Demo:** Create a notebook where a user query is embedded and passed to the index (`index.search(query_embedding, k)`) to retrieve the top-3 matching chunks. Display the original chunk text for these results.\n",
    "7. **FastAPI Service:** Build a simple FastAPI app. Define an endpoint (e.g. `@app.get(\"/search\")`) that accepts a query parameter `q`. In the handler, embed `q`, perform the FAISS search, and return the top passages as JSON. (For example, a FastAPI endpoint can accept a question and return relevant documents.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Starter Code Snippets\n",
    "\n",
    "Below are skeleton code templates. Fill in the details (indicated by comments or ellipses).\n",
    "\n",
    "**Data Extraction (PDF â†’ Text):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Open a PDF and extract all text as a single string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()  # get raw text from page\n",
    "        # (Optional) clean page_text here (remove headers/footers)\n",
    "        pages.append(page_text)\n",
    "    full_text = \"\\n\".join(pages)\n",
    "    return full_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf('python-developer-resume-example.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking Logic (Sliding Window):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Tuple, Callable\n",
    "def chunk_text(text: str, max_tokens: int = 100, overlap: int = 10) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Embedding Generation (Sentence-Transformers):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(chunks) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**FAISS Indexing and Search:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Assume embeddings is a 2D numpy array of shape (num_chunks, dim)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # using a simple L2 index\n",
    "index.add(np.array(embeddings))  # add all chunk vectors\n",
    "\n",
    "# Example: search for a query embedding\n",
    "query_embedding = model.encode([\"What are the key skills of the candidate?\"])[0]  # get embedding for the query (shape: [1, dim])\n",
    "k = 3\n",
    "distances, indices = index.search(np.array([query_embedding]), k)\n",
    "# indices[0] holds the top-k chunk indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**FastAPI Route Skeleton:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['/Users/yunfeibai/Desktop/GenAI MLE Course/MLE_in_GenAI-Course/class4']\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [36875] using WatchFiles\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "ERROR:    Error loading ASGI app. Could not import module \"main\".\n",
      "INFO:     Stopping reloader process [36875]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(q: str):\n",
    "    \"\"\"\n",
    "    Receive a query 'q', embed it, retrieve top-3 passages, and return them.\n",
    "    \"\"\"\n",
    "    # TODO: Embed the query 'q' using your embedding model\n",
    "    query_vector = model.encode([q])[0]   # e.g., model.encode([q])[0]\n",
    "    # Perform FAISS search\n",
    "    k = 3\n",
    "    distances, indices = index.search(np.array([query_vector]), k)\n",
    "    # Retrieve the corresponding chunks (assuming 'chunks' list and 'indices' shape [1, k])\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append(chunks[idx])\n",
    "    return {\"query\": q, \"results\": results}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    \n",
    "    host = os.getenv(\"HOST\", \"0.0.0.0\")\n",
    "    port = int(os.getenv(\"PORT\", 8000))\n",
    "    debug = os.getenv(\"DEBUG\", \"True\").lower() == \"true\"\n",
    "    \n",
    "    uvicorn.run(\n",
    "        \"main:app\",\n",
    "        host=host,\n",
    "        port=port,\n",
    "        reload=debug,\n",
    "        log_level=\"info\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Code Notebook / Script:** Complete code for the RAG pipeline (PDF extraction, chunking, embedding, indexing, retrieval).\n",
    "* **Data & Index:** The FAISS index file and the set of 50 processed paper chunks (e.g., as JSON or pickled objects).\n",
    "* **Retrieval Report:** A brief report showing at least 5 example queries and the top-3 retrieved passages for each, to demonstrate system performance.\n",
    "* **FastAPI Service:** The FastAPI app code (e.g. `main.py`) and instructions on how to run it. The `/search` endpoint should be demonstrable (e.g. returning top-3 passages in JSON for sample queries).\n",
    "\n",
    "## Student Exploration Tips\n",
    "\n",
    "* Experiment with different chunk sizes and overlaps. Smaller chunks (âˆ¼250 tokens) often give more precise retrieval, while larger chunks include more context.\n",
    "* Try different embedding models (e.g. using `'all-mpnet-base-v2'` or `'paraphrase-MiniLM-L6-v2'`) to see how retrieval results change.\n",
    "* Implement a simple reranking step: for example, after retrieving candidates with FAISS, re-score them with a cross-encoder model for finer ranking.\n",
    "* Use metadata: consider filtering or weighting chunks by paper metadata (e.g. year, authors, keywords) to improve relevance if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below Codes for generate deliverables files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Codes for generate deliverables files\n",
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Open a PDF and extract all text as a single string.\n",
    "    \"\"\"\n",
    "    if pdf_path.startswith(\"http://\") or pdf_path.startswith(\"https://\"):\n",
    "        response = requests.get(pdf_path)\n",
    "        doc = fitz.open(stream=response.content, filetype=\"pdf\")\n",
    "    else:\n",
    "        doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()  # get raw text from page\n",
    "        # (Optional) clean page_text here (remove headers/footers)\n",
    "        pages.append(page_text)\n",
    "    full_text = \"\\n\".join(pages)\n",
    "    return full_text\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 500, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n",
    "\n",
    "# load pdfs and extract text\n",
    "def query_arxiv(category, max_results=50):  # Limited to 50 for size; change to 200 if needed\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=cat:{category}&sortBy=submittedDate&sortOrder=descending&max_results={max_results}\"\n",
    "    response = requests.get(url)\n",
    "    root = ET.fromstring(response.content)\n",
    "    entries = []\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        pdf_url = entry.find('{http://www.w3.org/2005/Atom}id').text.replace('abs', 'pdf') + '.pdf'\n",
    "        paper = {\n",
    "            'pdf_url': pdf_url,\n",
    "            'title': entry.find('{http://www.w3.org/2005/Atom}title').text.strip(),\n",
    "        }\n",
    "        entries.append(paper)\n",
    "    return entries\n",
    "\n",
    "# Embed chunks using SentenceTransformer\n",
    "def embed_chunks(chunks: List[str], model: SentenceTransformer) -> List[List[float]]:\n",
    "     \n",
    "     embeddings = model.encode(chunks)\n",
    "     return embeddings\n",
    "\n",
    "# Assume embeddings is a 2D numpy array of shape (num_chunks, dim)\n",
    "def create_faiss_index(embeddings: List[float]) -> faiss.Index: \n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)  # using a simple L2 index\n",
    "    index.add(np.array(embeddings))  # add all chunk vectors\n",
    "    return index\n",
    "\n",
    "# Initialize model, chunks, and index at module level\n",
    "# process multiple arxiv papers\n",
    "arxiv_papers = query_arxiv('cs.AI', max_results=10)  # e.g., most 10 papers from AI research\n",
    "texts = []\n",
    "for paper in arxiv_papers:\n",
    "    text = extract_text_from_pdf(paper['pdf_url'])\n",
    "    texts.append(text)\n",
    "full_text = \"\\n\".join(texts)\n",
    "\n",
    "# setting up embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# chunking and embedding\n",
    "chunks = chunk_text(full_text)\n",
    "embeddings = embed_chunks(chunks, model)\n",
    "index = create_faiss_index(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverables 2: Data and Index\n",
    "\n",
    "# Save FAISS index (binary format)\n",
    "faiss.write_index(index, \"index and chunks examples/faiss_index.bin\")\n",
    "# Load FAISS index\n",
    "# index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "# Save chunks as JSONL with 50 examples\n",
    "import json\n",
    "chunks_count = len(chunks)\n",
    "if chunks_count < 50:\n",
    "    raise ValueError(f\"Not enough chunks to save 50 examples, only found {chunks_count} chunks.\")\n",
    "else:\n",
    "    with open(\"index and chunks examples/chunks_50_examples.json\", \"w\") as f:\n",
    "        for i in range(50):\n",
    "            chunk = {\"chunk_index\": i, \"chunk_text\": chunks[i]}\n",
    "            json.dump(chunk, f)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliverables 3: Retrieval report\n",
    "\"\"\"\n",
    "Receive a query 'q', embed it, retrieve top-3 passages, and return them.\n",
    "\"\"\"\n",
    "querys =    [\"What are the latest advancements in AI research?\",\n",
    "            \"Summarize the key techniques used in machine learning.\",\n",
    "            \"Explain the concept of reinforcement learning.\",\n",
    "            \"What are the challenges in natural language processing?\",\n",
    "            \"Describe the applications of computer vision.\"]\n",
    "results = []\n",
    "for query in querys:\n",
    "    query_vector = model.encode([query])[0]   # e.g., model.encode([query])[0]\n",
    "    # Perform FAISS search\n",
    "    k = 3  # top-k results\n",
    "    distances, indices = index.search(np.array([query_vector]), k)\n",
    "    # Retrieve the corresponding chunks (assuming 'chunks' list and 'indices' shape [1, k])\n",
    "    results.append(f\"Query: {query}\\n\")\n",
    "    for i,idx in enumerate(indices[0]):\n",
    "        results.append(f\"top-{i+1} chunk: {chunks[idx]}\\n\\n\")\n",
    "with open(\"retrieval_report.json\", \"w\") as f:\n",
    "    json.dump({\"retrieval_report\": results}, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
