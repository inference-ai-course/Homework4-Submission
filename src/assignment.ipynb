{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "549a7bc1",
   "metadata": {},
   "source": [
    "# Assignment for week 4\n",
    "\n",
    "Lets generate the data needed:\n",
    "\n",
    "- Step 1: Data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf8ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 14:14:13,940 [DEBUG] Url: https://arxiv.org/search/cs?query=deep+learning.&searchtype=all&abstracts=show&order=-announced_date_first&size=25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from utils.arxiv_search import build_arxiv_search_url\n",
    "\n",
    "url = build_arxiv_search_url(query=\"Deep learning.\", size=20, )\n",
    "logging.debug(f\"Url: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2015bb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 14:14:13,956 [DEBUG] Scraping started in background. Results will be saved to data/scrapped/research_paper.json.\n"
     ]
    }
   ],
   "source": [
    "from utils.arxiv_search import run_scraper_in_background\n",
    "\n",
    "# target scrapped dir\n",
    "scrapped_data_dir = \"data/scrapped\"\n",
    "os.makedirs(scrapped_data_dir, exist_ok=True)\n",
    "\n",
    "# file name\n",
    "research_papers = \"research_paper.json\"\n",
    "\n",
    "target_file_path = f\"{scrapped_data_dir}/{research_papers}\"\n",
    "run_scraper_in_background(url=url, output_file=target_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb521b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 14:14:17,426 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:18,030 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:18,165 [DEBUG] https://arxiv.org:443 \"GET /search/cs?query=deep+learning.&searchtype=all&abstracts=show&order=-announced_date_first&size=25 HTTP/1.1\" 200 133979\n",
      "2025-11-10 14:14:18,194 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05462 HTTP/1.1\" 200 45222\n",
      "2025-11-10 14:14:18,351 [DEBUG] Saved 25 papers to data/scrapped/research_paper.json\n",
      "2025-11-10 14:14:18,354 [DEBUG] Type of output file:<class 'str'>\n",
      "2025-11-10 14:14:19,287 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:19,455 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05483 HTTP/1.1\" 200 47392\n",
      "2025-11-10 14:14:20,669 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:20,808 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05471 HTTP/1.1\" 200 47671\n",
      "2025-11-10 14:14:23,958 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:24,082 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:24,109 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05421 HTTP/1.1\" 200 47459\n",
      "2025-11-10 14:14:24,247 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05452 HTTP/1.1\" 200 46208\n",
      "2025-11-10 14:14:28,079 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:28,184 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05360 HTTP/1.1\" 200 46195\n",
      "2025-11-10 14:14:28,734 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:28,863 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05394 HTTP/1.1\" 200 45555\n",
      "2025-11-10 14:14:28,867 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:28,986 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05302 HTTP/1.1\" 200 45774\n",
      "2025-11-10 14:14:31,662 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:31,805 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05265 HTTP/1.1\" 200 48749\n",
      "2025-11-10 14:14:36,798 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:36,906 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05236 HTTP/1.1\" 200 48141\n",
      "2025-11-10 14:14:38,031 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:38,159 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05250 HTTP/1.1\" 200 46258\n",
      "2025-11-10 14:14:40,362 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:40,626 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05169 HTTP/1.1\" 200 51129\n",
      "2025-11-10 14:14:43,675 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:43,776 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05106 HTTP/1.1\" 200 47378\n",
      "2025-11-10 14:14:44,171 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:44,320 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05073 HTTP/1.1\" 200 45730\n",
      "2025-11-10 14:14:44,647 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:44,759 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05131 HTTP/1.1\" 200 45741\n",
      "2025-11-10 14:14:52,974 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:53,098 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:53,313 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04998 HTTP/1.1\" 200 49180\n",
      "2025-11-10 14:14:53,314 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04988 HTTP/1.1\" 200 48860\n",
      "2025-11-10 14:14:53,351 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:53,511 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.05059 HTTP/1.1\" 200 48327\n",
      "2025-11-10 14:14:57,180 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:57,342 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04971 HTTP/1.1\" 200 47370\n",
      "2025-11-10 14:14:58,244 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:58,401 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04981 HTTP/1.1\" 200 45243\n",
      "2025-11-10 14:14:58,507 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:14:58,574 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04984 HTTP/1.1\" 200 47303\n",
      "2025-11-10 14:15:02,887 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:03,076 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04970 HTTP/1.1\" 200 46023\n",
      "2025-11-10 14:15:05,990 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:06,087 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04892 HTTP/1.1\" 200 47369\n",
      "2025-11-10 14:15:07,194 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:07,368 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04949 HTTP/1.1\" 200 47422\n",
      "2025-11-10 14:15:13,043 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:13,201 [DEBUG] https://arxiv.org:443 \"GET /abs/2511.04886 HTTP/1.1\" 200 47222\n",
      "2025-11-10 14:15:13,258 [DEBUG] Scraped 25 detailed entries using threads.\n"
     ]
    }
   ],
   "source": [
    "from utils.arxiv_search import scrape_arxiv_details_from_json_threaded \n",
    "\n",
    "scraped_data = scrape_arxiv_details_from_json_threaded(target_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b1b5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 14:15:13,291 [DEBUG] First item in list: {'url': 'https://arxiv.org/abs/2511.05462', 'title': 'SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning', 'abstract': 'Recent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised and unsupervised learning. However, the application of clustering is often heuristic, and the optimal methodology remains unclear. In this work, we establish connections between these unsupervised clustering methods and classical mixture models from statistics. Through this framework, we demonstrate significant enhancements to these clustering methods, leading to the development of a novel model named SiamMM. Our method attains state-of-the-art performance across various self-supervised learning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground truth labels, uncovering potential instances of mislabeling.', 'authors': 'Xiaodong Wang,Jing Huang,Kevin J Liang', 'date': '[Submitted on 7 Nov 2025]'}\n",
      "2025-11-10 14:15:13,304 [DEBUG] Saved cleaned results to data/scrapped/scrapped_clean.json\n"
     ]
    }
   ],
   "source": [
    "from utils.arxiv_search import save_arxiv_scraped_details\n",
    "\n",
    "output_file = f\"{scrapped_data_dir}/scrapped_clean.json\"\n",
    "save_arxiv_scraped_details(scraped_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d5634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 14:15:13,337 [DEBUG] Threadsafe pdf arxiv\n",
      "2025-11-10 14:15:13,340 [INFO] Saving to data/downloads/pdf\n",
      "2025-11-10 14:15:13,345 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,350 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05462\n",
      "2025-11-10 14:15:13,349 [DEBUG] Started thread for 2511.05462\n",
      "2025-11-10 14:15:13,351 [DEBUG] Downloading https://arxiv.org/pdf/2511.05462 to data/downloads/pdf/2511.05462.pdf\n",
      "2025-11-10 14:15:13,355 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,356 [DEBUG] Started thread for 2511.05483\n",
      "2025-11-10 14:15:13,363 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05483\n",
      "2025-11-10 14:15:13,368 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,369 [DEBUG] Started thread for 2511.05471\n",
      "2025-11-10 14:15:13,371 [DEBUG] Downloading https://arxiv.org/pdf/2511.05483 to data/downloads/pdf/2511.05483.pdf\n",
      "2025-11-10 14:15:13,373 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05471\n",
      "2025-11-10 14:15:13,378 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,379 [DEBUG] Started thread for 2511.05421\n",
      "2025-11-10 14:15:13,385 [DEBUG] Downloading https://arxiv.org/pdf/2511.05471 to data/downloads/pdf/2511.05471.pdf\n",
      "2025-11-10 14:15:13,387 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05421\n",
      "2025-11-10 14:15:13,391 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,393 [DEBUG] Started thread for 2511.05452\n",
      "2025-11-10 14:15:13,395 [DEBUG] Downloading https://arxiv.org/pdf/2511.05421 to data/downloads/pdf/2511.05421.pdf\n",
      "2025-11-10 14:15:13,398 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05452\n",
      "2025-11-10 14:15:13,406 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,406 [DEBUG] Started thread for 2511.05360\n",
      "2025-11-10 14:15:13,413 [DEBUG] Downloading https://arxiv.org/pdf/2511.05452 to data/downloads/pdf/2511.05452.pdf\n",
      "2025-11-10 14:15:13,415 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05360\n",
      "2025-11-10 14:15:13,421 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,423 [DEBUG] Started thread for 2511.05394\n",
      "2025-11-10 14:15:13,429 [DEBUG] Downloading https://arxiv.org/pdf/2511.05360 to data/downloads/pdf/2511.05360.pdf\n",
      "2025-11-10 14:15:13,432 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05394\n",
      "2025-11-10 14:15:13,434 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,435 [DEBUG] Started thread for 2511.05302\n",
      "2025-11-10 14:15:13,439 [DEBUG] Downloading https://arxiv.org/pdf/2511.05394 to data/downloads/pdf/2511.05394.pdf\n",
      "2025-11-10 14:15:13,442 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05302\n",
      "2025-11-10 14:15:13,445 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,446 [DEBUG] Started thread for 2511.05265\n",
      "2025-11-10 14:15:13,449 [DEBUG] Downloading https://arxiv.org/pdf/2511.05302 to data/downloads/pdf/2511.05302.pdf\n",
      "2025-11-10 14:15:13,450 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05265\n",
      "2025-11-10 14:15:13,454 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,454 [DEBUG] Started thread for 2511.05236\n",
      "2025-11-10 14:15:13,457 [DEBUG] Downloading https://arxiv.org/pdf/2511.05265 to data/downloads/pdf/2511.05265.pdf\n",
      "2025-11-10 14:15:13,459 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05236\n",
      "2025-11-10 14:15:13,462 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,462 [DEBUG] Started thread for 2511.05250\n",
      "2025-11-10 14:15:13,466 [DEBUG] Downloading https://arxiv.org/pdf/2511.05236 to data/downloads/pdf/2511.05236.pdf\n",
      "2025-11-10 14:15:13,467 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05250\n",
      "2025-11-10 14:15:13,470 [DEBUG] Downloading https://arxiv.org/pdf/2511.05250 to data/downloads/pdf/2511.05250.pdf\n",
      "2025-11-10 14:15:13,470 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,470 [DEBUG] Started thread for 2511.05169\n",
      "2025-11-10 14:15:13,474 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05169\n",
      "2025-11-10 14:15:13,476 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,477 [DEBUG] Downloading https://arxiv.org/pdf/2511.05169 to data/downloads/pdf/2511.05169.pdf\n",
      "2025-11-10 14:15:13,477 [DEBUG] Started thread for 2511.05106\n",
      "2025-11-10 14:15:13,478 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05106\n",
      "2025-11-10 14:15:13,481 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,481 [DEBUG] Started thread for 2511.05073\n",
      "2025-11-10 14:15:13,482 [DEBUG] Downloading https://arxiv.org/pdf/2511.05106 to data/downloads/pdf/2511.05106.pdf\n",
      "2025-11-10 14:15:13,485 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05073\n",
      "2025-11-10 14:15:13,487 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,489 [DEBUG] Started thread for 2511.05131\n",
      "2025-11-10 14:15:13,490 [DEBUG] Downloading https://arxiv.org/pdf/2511.05073 to data/downloads/pdf/2511.05073.pdf\n",
      "2025-11-10 14:15:13,491 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05131\n",
      "2025-11-10 14:15:13,494 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,495 [DEBUG] Started thread for 2511.04988\n",
      "2025-11-10 14:15:13,496 [DEBUG] Downloading https://arxiv.org/pdf/2511.05131 to data/downloads/pdf/2511.05131.pdf\n",
      "2025-11-10 14:15:13,498 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04988\n",
      "2025-11-10 14:15:13,501 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,501 [DEBUG] Started thread for 2511.04998\n",
      "2025-11-10 14:15:13,503 [DEBUG] Downloading https://arxiv.org/pdf/2511.04988 to data/downloads/pdf/2511.04988.pdf\n",
      "2025-11-10 14:15:13,504 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04998\n",
      "2025-11-10 14:15:13,508 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,509 [DEBUG] Started thread for 2511.05059\n",
      "2025-11-10 14:15:13,510 [DEBUG] Downloading https://arxiv.org/pdf/2511.04998 to data/downloads/pdf/2511.04998.pdf\n",
      "2025-11-10 14:15:13,512 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.05059\n",
      "2025-11-10 14:15:13,514 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,518 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04971\n",
      "2025-11-10 14:15:13,517 [DEBUG] Downloading https://arxiv.org/pdf/2511.05059 to data/downloads/pdf/2511.05059.pdf\n",
      "2025-11-10 14:15:13,515 [DEBUG] Started thread for 2511.04971\n",
      "2025-11-10 14:15:13,519 [DEBUG] Downloading https://arxiv.org/pdf/2511.04971 to data/downloads/pdf/2511.04971.pdf\n",
      "2025-11-10 14:15:13,521 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,521 [DEBUG] Started thread for 2511.04981\n",
      "2025-11-10 14:15:13,525 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04981\n",
      "2025-11-10 14:15:13,526 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,527 [DEBUG] Downloading https://arxiv.org/pdf/2511.04981 to data/downloads/pdf/2511.04981.pdf\n",
      "2025-11-10 14:15:13,528 [DEBUG] Started thread for 2511.04984\n",
      "2025-11-10 14:15:13,529 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04984\n",
      "2025-11-10 14:15:13,534 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,534 [DEBUG] Started thread for 2511.04970\n",
      "2025-11-10 14:15:13,535 [DEBUG] Downloading https://arxiv.org/pdf/2511.04984 to data/downloads/pdf/2511.04984.pdf\n",
      "2025-11-10 14:15:13,536 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04970\n",
      "2025-11-10 14:15:13,538 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,542 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04892\n",
      "2025-11-10 14:15:13,541 [DEBUG] Downloading https://arxiv.org/pdf/2511.04970 to data/downloads/pdf/2511.04970.pdf\n",
      "2025-11-10 14:15:13,540 [DEBUG] Started thread for 2511.04892\n",
      "2025-11-10 14:15:13,543 [DEBUG] Downloading https://arxiv.org/pdf/2511.04892 to data/downloads/pdf/2511.04892.pdf\n",
      "2025-11-10 14:15:13,545 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,546 [DEBUG] Started thread for 2511.04949\n",
      "2025-11-10 14:15:13,548 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04949\n",
      "2025-11-10 14:15:13,550 [DEBUG] Save dir:data/downloads/pdf\n",
      "2025-11-10 14:15:13,550 [DEBUG] Started thread for 2511.04886\n",
      "2025-11-10 14:15:13,551 [DEBUG] Downloading https://arxiv.org/pdf/2511.04949 to data/downloads/pdf/2511.04949.pdf\n",
      "2025-11-10 14:15:13,554 [DEBUG] arXiv url: https://arxiv.org/pdf/2511.04886\n",
      "2025-11-10 14:15:13,558 [DEBUG] Downloading https://arxiv.org/pdf/2511.04886 to data/downloads/pdf/2511.04886.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils.arxiv_search import get_pdf_arxiv\n",
    "from utils.data_collection import DATA_DIR\n",
    "\n",
    "pdf_dir = \"data/downloads/pdf\"\n",
    "json_file = output_file\n",
    "\n",
    "\n",
    "get_pdf_arxiv(cleaned_json=json_file, save_dir=pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b155454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/downloads/pdf/2511.04981.pdf',\n",
       " 'data/downloads/pdf/2511.04971.pdf',\n",
       " 'data/downloads/pdf/2511.04998.pdf',\n",
       " 'data/downloads/pdf/2511.04970.pdf',\n",
       " 'data/downloads/pdf/2511.05421.pdf',\n",
       " 'data/downloads/pdf/2511.04886.pdf',\n",
       " 'data/downloads/pdf/2511.04892.pdf',\n",
       " 'data/downloads/pdf/2511.05059.pdf',\n",
       " 'data/downloads/pdf/2511.05073.pdf',\n",
       " 'data/downloads/pdf/2511.05452.pdf',\n",
       " 'data/downloads/pdf/2511.05169.pdf',\n",
       " 'data/downloads/pdf/2511.05471.pdf',\n",
       " 'data/downloads/pdf/2511.05250.pdf',\n",
       " 'data/downloads/pdf/2511.04984.pdf',\n",
       " 'data/downloads/pdf/2511.05302.pdf',\n",
       " 'data/downloads/pdf/2511.05483.pdf',\n",
       " 'data/downloads/pdf/2511.05360.pdf',\n",
       " 'data/downloads/pdf/2511.04988.pdf',\n",
       " 'data/downloads/pdf/2511.05106.pdf',\n",
       " 'data/downloads/pdf/2511.05131.pdf',\n",
       " 'data/downloads/pdf/2511.04949.pdf',\n",
       " 'data/downloads/pdf/2511.05265.pdf',\n",
       " 'data/downloads/pdf/2511.05236.pdf',\n",
       " 'data/downloads/pdf/2511.05394.pdf',\n",
       " 'data/downloads/pdf/2511.05462.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we now have data to work with.\n",
    "from pathlib import Path\n",
    "\n",
    "pdf_download_dir = Path(pdf_dir)\n",
    "\n",
    "discover_dirs = []\n",
    "for item in pdf_download_dir.iterdir():\n",
    "    if item.is_file():\n",
    "        if item.name.endswith(\".pdf\"):\n",
    "            discover_dirs.append(str(item))\n",
    "\n",
    "discover_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35fcf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 14:15:12,887 [DEBUG] data/downloads/pdf/2511.04981.pdf\n",
      "2025-11-10 14:15:13,193 [DEBUG] data/downloads/pdf/2511.04971.pdf\n",
      "2025-11-10 14:15:13,262 [DEBUG] data/downloads/pdf/2511.04998.pdf\n",
      "2025-11-10 14:15:13,422 [DEBUG] data/downloads/pdf/2511.04970.pdf\n",
      "2025-11-10 14:15:13,509 [DEBUG] data/downloads/pdf/2511.05421.pdf\n",
      "2025-11-10 14:15:13,595 [DEBUG] data/downloads/pdf/2511.04886.pdf\n",
      "2025-11-10 14:15:13,677 [DEBUG] data/downloads/pdf/2511.04892.pdf\n",
      "2025-11-10 14:15:13,922 [DEBUG] data/downloads/pdf/2511.05059.pdf\n",
      "2025-11-10 14:15:14,201 [DEBUG] data/downloads/pdf/2511.05073.pdf\n",
      "2025-11-10 14:15:14,386 [DEBUG] data/downloads/pdf/2511.05452.pdf\n",
      "2025-11-10 14:15:15,183 [DEBUG] data/downloads/pdf/2511.05169.pdf\n",
      "2025-11-10 14:15:15,295 [DEBUG] data/downloads/pdf/2511.05471.pdf\n",
      "2025-11-10 14:15:15,400 [DEBUG] data/downloads/pdf/2511.05250.pdf\n",
      "2025-11-10 14:15:15,485 [DEBUG] data/downloads/pdf/2511.04984.pdf\n",
      "2025-11-10 14:15:15,538 [DEBUG] data/downloads/pdf/2511.05302.pdf\n",
      "2025-11-10 14:15:15,567 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:15,658 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:15,665 [DEBUG] data/downloads/pdf/2511.05483.pdf\n",
      "2025-11-10 14:15:15,783 [DEBUG] data/downloads/pdf/2511.05360.pdf\n",
      "2025-11-10 14:15:15,989 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:16,004 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:16,082 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05169 HTTP/1.1\" 200 6817921\n",
      "2025-11-10 14:15:16,084 [DEBUG] data/downloads/pdf/2511.04988.pdf\n",
      "2025-11-10 14:15:16,111 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04988 HTTP/1.1\" 200 6682506\n",
      "2025-11-10 14:15:16,181 [DEBUG] data/downloads/pdf/2511.05106.pdf\n",
      "2025-11-10 14:15:16,198 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:16,241 [DEBUG] data/downloads/pdf/2511.05131.pdf\n",
      "2025-11-10 14:15:16,251 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04984 HTTP/1.1\" 200 3362172\n",
      "2025-11-10 14:15:16,255 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05452 HTTP/1.1\" 200 3475250\n",
      "2025-11-10 14:15:16,347 [DEBUG] data/downloads/pdf/2511.04949.pdf\n",
      "2025-11-10 14:15:16,460 [DEBUG] data/downloads/pdf/2511.05265.pdf\n",
      "2025-11-10 14:15:16,545 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:16,703 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:16,744 [DEBUG] data/downloads/pdf/2511.05236.pdf\n",
      "2025-11-10 14:15:16,747 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04970 HTTP/1.1\" 200 42529277\n",
      "2025-11-10 14:15:16,948 [DEBUG] data/downloads/pdf/2511.05394.pdf\n",
      "2025-11-10 14:15:17,010 [DEBUG] data/downloads/pdf/2511.05462.pdf\n",
      "2025-11-10 14:15:17,093 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05236 HTTP/1.1\" 200 1041131\n",
      "2025-11-10 14:15:17,095 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05265 HTTP/1.1\" 200 6659732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Deep Progressive Training: scaling up depth\\ncapacity of zero/one-layer models\\nZhiqi Bu\\nMeta FAIR\\nModel depth is a double-edged sword in deep learning: deeper models achieve higher accuracy but\\nrequire higher computational cost. To efficiently train models at scale, an effective strategy is the\\nprogressive training, which scales up model capacity during training, hence significantly reducing\\ncomputation with little to none performance degradation. In this work, we study the depth expansion\\nof large models through the lens of optimization theory and feature learning, offering insights on the\\ninitialization of new layers, hyperparameter transfer, learning rate schedule, and timing of model\\nexpansion. Specifically, we propose zero/one-layer progressive training for the optimal tradeoff between\\ncomputation and loss. For example, zero/one-layer progressive training on GPT2 can save ≈80%\\ncompute, or equivalently accelerate ≈5× while achieving almost the same loss, compared to to a fully\\ntrained 60-layer model with 7B parameters.\\nCorrespondence: zhiqibu@meta.com\\n1\\nIntroduction\\nStrong performance of deep learning models is highly correlated to model sizes, with larger model having\\nhigher accuracy but also incurring higher computation cost to train, e.g. LLAMA-4 training costs over 7M\\nGPU hours and an estimated 2,000 tons of carbon emissions. This phenomenon leads to a tradeoff between\\nmodel utility (measured by loss or accuracy) and computational cost (measured by floating point operation,\\nor FLOP), and has motivated scaling laws to train compute-optimal large language models Hoffmann et al.\\n(2022); Kaplan et al. (2020).\\nTo accelerate the training of large models, one direction is known as progressive training or model growth,\\nwhich initially trains a small model (a.k.a. teacher or source model) and then scales up to large models\\n(a.k.a. student or grown model) during training. In contrast to the fixed-size training, the progressive training\\nformulates the model size as a time-dependent variable, and it is clearly more efficient because the compute is\\n6BTN, proportional to the model size N. For example, consider a progressive training that scales up the\\nmodel size at iteration τ:\\nN(t) =\\n(\\nNsmall\\nif t ≤τ\\nNlarge\\nif t > τ\\n(1.1)\\nThe fixed-size training requires 6BTNlarge FLOPs, whereas the progressive training requires 6B(τNsmall +\\n(T −τ)Nlarge), which is significantly less if (I) τ is close to T and (II) Nsmall ≪Nlarge. As a brief preview, we\\nwill develop techniques to push τ ≈0.8T and to train zero/one-layer small models with Nsmall ≈0.02 · Nlarge,\\nhence accelerating ≈5× in Figure 1.\\nA long list of research has contributed to the development of progressive training, especially on initialization\\nof large models, multi-stage training, training regime, and theory.\\nInitialization from precedented small models.\\nChen et al. (2015); Wang et al. (2023b); Yao et al. study the\\nfunction-preserving initialization, such that the large model has the same loss and function as the small model\\nat the moment of depth expansion. These works scale up the depth of convolution networks and BERT by\\n2× and reduce the computation to ≈70% computation. However, while function-preserving guarantees nice\\nbehavior during depth expansion, it does not guarantee fast convergence after the expansion. Alternatively,\\n1\\narXiv:2511.04981v1  [cs.LG]  7 Nov 2025\\n\\nwithout function-preserving, Chen et al. (2021) linearly combines two layers to initialize a new layer; Gong\\net al. (2019); Yang et al. (2020); Du et al. (2024) stacks the old layers multiple times; Qin et al. (2021); Wang\\net al. (2023a) propose learning-based methods that require extra training. These methods empirically scale\\nup the depth by 2 ∼4× and reduce the “grown v.s. target” computation to ≈55 ∼70% (e.g. Wang et al.\\n(2023a), Figure 1 in Chen et al. (2021), Figure 6 in Pan et al. (2024), and Figure 1 in Du et al. (2024)). In\\ncontrast, we scale up the depth by 60× and reduce the computation to 20%.\\nMulti-stage training.\\nMost works in progressive training expand the small models once like (1.1). However,\\nmany works study multi-stage training and gradual stacking Reddi et al. (2023). For instance, Gong et al.\\n(2019); Shen et al. (2022); Qin et al. (2021); Pan et al. (2024); Yao et al. scale up the sizes of BERT for\\n3 ∼4× during training, optionally freezing some of the layers at some stages Agarwal et al. (2024); Yang et al.\\n(2020); Yano et al. (2025a). We will leverage our discovery of the mixing behaviors to show that multi-stage\\ntraining is less efficient and can be unnecessary.\\nTraining regime.\\nWhile most progressive training methods are tested on classification models like BERT\\nDevlin et al. (2019) and ViT Dosovitskiy et al. (2020), some recent papers have evaluated on generative\\nlanguage models like GPT2 and reported 1.4 ∼2× speedup Du et al. (2024). As shown in Figure 1 and\\nFigure 7, our method enjoys 5× speedup across different model sizes. We also note some papers that scale up\\nMoE (not in terms of depth though) but the speedup seems transient as discussed in Section 7.\\nTheory.\\nTheoretical analysis in progressive training is largely lacking, except Agarwal et al. (2024) on\\nstrongly-convex and smooth loss. In contrast, we give a convergence theory of convex and Lipschitz continuous\\n(non-smooth) loss and empirically validate its insights. Besides a convergence theory, we also study feature\\nlearning and hyperparameter transfer for progressive training.\\n1.1\\nRelated work\\nIn addition to previous works in progrssive training, this work is closely related to convex optimization in\\nSection 4, feature learning theory in Section 3.2, and learning rate schedules (especially warmup-stable-decay;\\nWSD Xing et al. (2018); Hägele et al. (2024)).\\n1.2\\nContributions\\nTo our best knowledge, we are the first to advocate zero/one-layer depth expansion and to explore WSD\\nschedule in progressive training. We summarize our main contributions here and provide extra insights on\\noptimizer choice, optimizer states, model size, batch size, multi-stage expansion, and other topics in Section C.\\n1. We analyze the depth expansion as an initialization problem and ensure feature learning. This approach\\nallows hyperparameter transfer (e.g. learning rate) throughout the progressive training, in contrast to\\nextra hyperparameter tuning Gu et al. (2020); Yano et al. (2025b).\\n2. We reveal the important role of learning rate schedule, especially WSD schedule which theoretically and\\nempirically improves the convergence.\\n3. We discover the mixing behaviors of progressive training, which supports the mixing time transfer and\\nsingle-stage training, in contrast to multi-stage expansion Gong et al. (2019); Yang et al. (2020); Qin\\net al. (2021); Shen et al. (2022); Pan et al. (2024).\\n4. We show that zero/one-layer progressive training has the best tradeoff between computational cost and\\nloss, compared to multi-layer progressive or fixed-size training (see Figure 8). Specifically, zero/one-layer\\ndepth expansion is easy to implement and devoid of ordering issues (see Section 3.3 and Section A.3).\\n5. We theoretically analyze the convergence of progressive training under convex optimization to give\\ninsights on initialization, learning rate schedule, and projected gradient descent.\\n2\\n\\n0\\n100000\\n200000\\n300000\\n400000\\n500000\\n600000\\nIterations\\n2\\n4\\n6\\n8\\n10\\n12\\nValidation loss\\nGPT 0\\n12-layer (random)\\nGPT 0\\n12-layer (early stopped)\\nGPT 1\\n12-layer (copying)\\nGPT 1\\n12-layer (random)\\nGPT 1\\n12-layer (early stopped)\\nGPT 12-layer (fixed-size)\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nFLOPs\\n1e20\\n2.75\\n3.00\\n3.25\\n3.50\\n3.75\\n4.00\\n4.25\\n4.50\\n4.75\\nValidation loss\\nX speedup\\n2.0\\n2.2/\\nGPT 0\\n12-layer (random)\\nGPT 1\\n12-layer (copying)\\nGPT 1\\n12-layer (random)\\nGPT 12-layer (fixed-size)\\n0\\n100000\\n200000\\n300000\\n400000\\n500000\\n600000\\nIterations\\n2\\n4\\n6\\n8\\n10\\n12\\nValidation loss\\nGPT 0\\n60-layer (random)\\nGPT 0\\n60-layer (early stopped)\\nGPT 1\\n60-layer (copying)\\nGPT 1\\n60-layer (random)\\nGPT 1\\n60-layer (early stopped)\\nGPT 60-layer (fixed-size)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\nFLOPs\\n1e21\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\nValidation loss\\nX speedup\\n4.4\\n4.8/\\nGPT 0\\n60-layer (random)\\nGPT 1\\n60-layer (copying)\\nGPT 1\\n60-layer (random)\\nGPT 60-layer (fixed-size)\\nFigure 1\\nZero-layer (red, 39M or 0.15B) and one-layer (blue and green, 46M or 0.27B) progressive training can\\nachieve significant speedup over fixed-sized training (black, 12-layer 124M or 60-layer 7B) on GPT2 pre-trained on\\nOpenWebText under WSD schedule. The difference in final validation loss is < 0.5% for 124M runs and < 0.2% for 7B\\nruns. The depth expansion takes place at 80% of iterations for full runs, and at 2% of iterations immediately after\\nwarmup for early stopped runs.\\n2\\nExperiment settings\\nWe use the GPT2 Radford et al. (2019), Mixture of Experts (MoE, Fedus et al. (2022)), and ResNet He et al.\\n(2016) model families as testbeds1. For GPT2, we train on OpenWebText dataset Gokaslan et al. (2019) with\\n1024 sequence length, following nanoGPT codebase. For ResNet, we train on ImageNet dataset with 224× 224\\nresolutions for 100 epochs.\\nOur main optimizer is Muon-NSGD, with 0.1 weight decay and without gradient clipping. The Muon-NSGD\\nis adapted from the original Muon Jordan et al. (2024) by (1) optimizing all 2D tensors with Muon and other\\ntensors with normalized SGD (NSGD), and (2) using a single learning rate for both Muon and NSGD. We use\\nthe cosine learning rate schedule and WSD schedule, that decay to 0 with 2% warm-up. Additional details\\nare in Section B.\\n3\\nHow to expand depth?\\n3.1\\nDepth expansion approaches\\nWe introduce multiple approaches to expand the depth of a residual neural network.\\n1For ResNet, the models are configured by 4 stages, e.g. ResNet50 with [3,4,6,3] and ResNet101 with [3,4,23,3]. In each stage,\\nthe first layer has one shape, and each of the other layers has the same shape which is different to the first layer. Hence the\\nzero-layer analogy corresponds to ResNet14 with [1,1,1,1] and the one-layer analogy corresponds to ResNet26 with [2,2,2,2].\\n3\\n\\n• [copying]: New layers are copied from the small model Chang et al. (2018); Gong et al. (2019); Li et al.\\n(2020).\\n• [random]: New layers are randomly initialized Wang et al. (2017); Chen et al. (2021).\\n• [zero]: New layers are initialized as zeros. This approach kills the gradient flow and makes the new\\nlayers untrainable, hence invalidating the progressive training.\\n• [copying_zero]: New layers are copied from the small model, except some sub-layers are zero Shen et al.\\n(2022); Wang et al. (2023b); Tan et al. (2024); Wu et al. (2024); Du et al. (2024).\\nTo test these approaches in a minimalist manner, we expand zero/one-layer versions of GPT2 and ResNet to\\nmultiple layers in Figure 2. The experiments of copying_zero approach can be found in Section A.\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nLoss\\nGPT 0-layer (fixed size)\\nGPT 0\\n12-layer (random)\\nGPT 1-layer (fixed size)\\nGPT 1\\n12-layer (zero)\\nGPT 1\\n12-layer (random)\\nGPT 1\\n12-layer (copying)\\nGPT 12-layer (fixed size)\\n50000\\n100000\\n150000\\n200000\\n250000\\nIterations\\n100\\n101\\nLoss\\nResNet14 (fixed size)\\nResNet14\\n50(random)\\nResNet26 (fixed size)\\nResNet26\\n50(zero)\\nResNet26\\n50(random)\\nResNet26\\n50(copying)\\nResNet50 (fixed size)\\nFigure 2\\nConvergence of zero/one-layer progressive training and fixed-size training. Left: ResNet with depth\\nexpansion at 32-th epoch. Right: GPT2 with depth expansion at 50k iterations.\\nTakeaway 1:\\nFor zero/one-layer progressive training, random and copying are empirically the best\\ninitializations of new layers.\\n3.2\\nFeature learning and Hyperparameter transfer\\nTo ensure feature learning and keep the representations non-trivial and stable, each layer’s activation needs to\\nhave consistent element sizes: denoting l-th layer’s activation as Al ∈Rnl, then ∥Al∥2/√nl ∼∥Al+1∥2/√nl+1\\nMei et al. (2019); Yang and Hu (2020); Chizat and Bach (2018); Yang et al. (2022, 2023). For linear layers\\nAl+1 = AlWl, this translates to the spectral scaling condition by muP theory, i.e. the spectral norm\\n∥Wl∥∗∼\\np\\nnl+1/nl for all layers.\\nImportantly, muP allows zero-shot hyperparameter transfer across model sizes, so that the optimal hyper-\\nparameters (e.g. learning rate) are the same for small and large models. In Figure 3, we illustrate this on\\nthe Muon-NSGD optimizer with muP-scaling learning rate. We highlight that hyperparameter transfer is\\nparticularly desirable in progressive training, where model sizes change significantly before and after the\\nmodel expansion.\\nIn fact, without the residual connection, only copying and random satisfy muP condition, but not zero or\\ncopying_zero, since any zero sub-layer will have ∥Wl∥∗= 0 ̸∼\\np\\nnl+1/nl. Consequently, there is a conflict\\nbetween muP condition and function-preserving (FPI) outside the residual path. Here function-preserving\\nmeans the large model has exactly the same loss as the small model Chen et al. (2015); Shen et al. (2022);\\nWang et al. (2023b), hence no loss spikes.\\nTakeaway 2:\\nFor layers outside the residual path, zero initialziation =⇒FPI =⇒not muP, which does\\nnot enjoy fast convergence and feature learning.\\n4\\n\\n10\\n3\\n10\\n2\\n10\\n1\\nlearning rate\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0\\nLoss\\nGPT 0-layer\\nGPT 1-layer\\nGPT 2-layer\\nGPT 6-layer\\nGPT 12-layer\\n10\\n3\\n10\\n2\\n10\\n1\\nlearning rate\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nLoss\\nResNet14\\nResNet26\\nResNet50\\nResNet101\\nFigure 3\\nValidation (solid) and training loss (dashed) at different learning rates of Muon-NSGD.\\nTable 1\\nSummary of initialization approaches in progressive training.\\nfunction-preserving\\ntrainability\\nfeature learning\\ncopying\\nno\\nhigh\\nyes\\nrandom\\nno\\nhigh\\nyes\\nzero\\nyes\\nlow\\nno\\n3.3\\nWhere to expand?\\nFor zero-layer expansion, only random initialization works; for one-layer expansion, random and copying both\\nwork; however, for multi-layer expansion, we must consider the ordering in depth expansion. We consider\\nthree variants of copying initialization: suppose we expand 3 to 6 layers,\\n• [copying_last], copying only the last layer, e.g. [1, 2, 3] →[1, 2, 3, 3, 3, 3].\\n• [copying_stack], copying and stacking all layers, e.g. [1, 2, 3] →[1, 2, 3, 1, 2, 3].\\n• [copying_inter], copying and interpolating all layers, e.g. [1, 2, 3] →[1, 1, 2, 2, 3, 3].\\nWe note that copying_inter (interpolating) is adopted by Chang et al. (2018); Pan et al. (2024); Dong et al.\\n(2020); Qin et al. (2022), as well as Wang et al. (2023b) if some sub-layers are zeros; copying_stack (stacking)\\nis adopted by Gong et al. (2019); Li et al. (2020); Fu et al. (2023), as well as Shen et al. (2022); Du et al.\\n(2024) if some sub-layers are zeros.\\nTo test these variants, we experiment with deeper models such as ResNet50 and GPT 6-layer in Figure 4. We\\nobserve that copying all layers is consistently better than only copying one layer (copying_last), whereas\\ncopying_inter and copying_stack are almost indistinguishable.\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\n3.6\\nLoss\\nGPT 6-layer\\ncopying_last\\ncopying_stack\\ncopying_inter\\nGPT 12-layer\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\n3.6\\nLoss\\nGPT 6-layer\\ncopying_last\\ncopying_stack\\ncopying_inter\\nGPT 12-layer\\n50000\\n100000\\n150000\\n200000\\n250000\\nIterations\\n100\\n6 × 10\\n1\\n2 × 100\\n3 × 100\\n4 × 100\\nLoss\\nResNet50\\ncopying_last\\ncopying_stack\\ncopying_inter\\nResNet101\\n50000\\n100000\\n150000\\n200000\\n250000\\nIterations\\n100\\n6 × 10\\n1\\n2 × 100\\n3 × 100\\n4 × 100\\nLoss\\nResNet50\\ncopying_last\\ncopying_stack\\ncopying_inter\\nResNet101\\nFigure 4\\nConvergence of multi-layer progressive training and fixed-size training. Left: ResNet with depth expansion\\nat 32/64-th epoch. Right: GPT2 with depth expansion at 30/70k iterations.\\nTakeaway 3:\\nCopying_inter and copying_stack are similarly performant for multi-layer depth expansion,\\nbut they are invalid for zero-layer depth expansion and equivalent for one-layer depth expansion (e.g. from\\n[1] →[1, 1, 1, 1, 1, 1]).\\n5\\n\\n4\\nA convergence theory for progressive training\\nWe analyze the convergence of progressive training under convex and G-Lipschitz loss L, so as to offer useful\\ninsights to the training recipe in Section 7. In fact, although deep learning is non-convex, its training dynamics\\nis similar to convex optimization Schaipp et al. (2025); Defazio and Mishchenko (2023); Lee et al. (2019); Bu\\net al. (2021); Jacot et al. (2018); Allen-Zhu et al. (2019); Leclerc and Madry (2020); Bu and Xu (2024).\\nWe denote the small model before depth expansion as wt, the large model after depth expansion as Wt, and\\nthe corresponding minima as w∗and W∗. For any iteration trained with SGD, wt+1 = wt −ηt+1 ∂L\\n∂wt , the\\nclassical analysis gives\\n∥wt+1 −w∗∥2 ≤∥wt −w∗∥2 −2ηt+1(L(wt) −L(w∗)) + η2\\nt+1G2\\n(4.1)\\nand equivalently, for the large model training with the same learning rate schedule,\\n∥Wt+1 −W∗∥2 ≤∥Wt −W∗∥2 −2ηt+1(L(Wt) −L(W∗)) + η2\\nt+1G2\\n(4.2)\\nNow for the progressive training with depth expansion at t = τ, we use telescoping sum (4.1) from t = 0 →τ −1\\nand (4.2) from t = τ →T −1 to obtain\\n∥wτ −w∗∥2 + ∥WT −W∗∥2 ≤∥w0 −w∗∥2 + ∥Wτ −W∗∥2 +\\nT −1\\nX\\nt=0\\nη2\\nt+1G2\\n+ 2\\nτ−1\\nX\\nt=0\\nηt+1(L(w∗) −Lt) + 2\\nT −1\\nX\\nt=τ\\nηt+1(L(W∗) −Lt)\\nwhere Lt = L(wt) for t < τ, and Lt = L(Wt) otherwise.\\nDividing by 2 PT −1\\nt=0 ηt+1 and re-arranging give\\nPT −1\\nt=0 ηt+1Lt\\nPT −1\\nt=0 ηt+1\\n≤\\nPτ−1\\nt=0 ηt+1L(w∗) + PT −1\\nt=τ ηt+1L(W∗)\\nPT −1\\nt=0 ηt+1\\n+ G2 PT −1\\nt=0 η2\\nt+1\\n2 PT −1\\nt=0 ηt+1\\n+ ∥w0 −w∗∥2 −∥WT −W∗∥2 + ∥Wτ −W∗∥2 −∥wτ −w∗∥2\\n2 PT −1\\nt=0 ηt+1\\nOn the left hand side, we apply the Jensen’s inequality to get\\nL( ¯\\nWprogressive\\nT\\n) ≤\\nPT −1\\nt=0 ηt+1Lt\\nPT −1\\nt=0 ηt+1\\n, where ¯\\nWprogressive\\nT\\n=\\nPτ−1\\nt=0 ηt+1[wt, 0] + PT −1\\nt=τ ηt+1Wt\\nPT −1\\nt=0 ηt+1\\nis the running average of iterates, and we have used L([wt, 0]) = L(wt) for residual networks.\\nOn the right hand side, we throw away −∥WT −W∗∥2 because it is small and negative. We obtain\\nL( ¯\\nWprogressive\\nT\\n) ≤\\nPτ−1\\nt=0 ηt+1L(w∗) + PT −1\\nt=τ ηt+1L(W∗)\\nPT −1\\nt=0 ηt+1\\n+ G2 PT −1\\nt=0 η2\\nt+1\\n2 PT −1\\nt=0 ηt+1\\n+ ∥w0 −w∗∥2 + ∥Wτ −W∗∥2 −∥wτ −w∗∥2\\n2 PT −1\\nt=0 ηt+1\\n(4.3)\\nWe can easily recover the fixed-size large model training from scratch by setting τ = 0:\\nL( ¯\\nWfixed-size\\nT\\n) ≤L(W∗) + G2 PT −1\\nt=0 η2\\nt+1\\n2 PT −1\\nt=0 ηt+1\\n+ ∥W0 −W∗∥2\\n2 PT −1\\nt=0 ηt+1\\n(4.4)\\n6\\n\\nSubtracting (4.3) from (4.4), we obtain the difference which is desired to be ⪅0,\\nPτ\\nt=1 ηt(L(w∗) −L(W∗))\\nPT\\nt=1 ηt\\n+ ∥w0 −w∗∥2 −∥W0 −W∗∥2 + ∥Wτ −W∗∥2 −∥wτ −w∗∥2\\n2 PT\\nt=1 ηt\\nWe view the large model as the concatenation of a small model and extra parameters Wt = [wt, xt] for\\nt = 0, τ, and simplify the analysis by assuming W∗= [w∗, x∗]. We now obtain\\nL( ¯\\nWprogressive\\nT\\n) −L( ¯\\nWfixed-size\\nT\\n) ≤\\nPτ\\nt=1 ηt\\nPT\\nt=1 ηt\\n(L(w∗) −L(W∗)) + ∥xτ −x∗∥2 −∥x0 −x∗∥2\\n2 PT\\nt=1 ηt\\n(4.5)\\nFrom the viewpoint of large model, we can mathematically view the progressive training as projected gradient\\ndescent (PGD) that projects the deeper layers to zero, followed by an instant teleportation of xτ from zero to\\nsome good initialization, then continued with SGD. In words, the effectiveness of progressive training comes\\nfrom both optimizers (PGD and SGD) and initialization of new layers.\\nTaking a closer look at (4.5), we can optimize this difference via the following factors.\\n• Initialization strategy of xτ: given that x0 is randomly initialized, (1) if we randomly initialize new\\nlayers, then the second term is zero; (2) if we initialize better than random, then the second term is\\nnegative and the difference is improved. This analysis is visualized in Figure 2.\\n• Learning rate schedule ηt: to minimize\\nPτ\\nt=1 ηt\\nPT\\nt=1 ηt , we prefer smaller ηt for t ≤τ than for t > τ, contrary\\nto learning rate decay but consistent with WSD schedule, where ηt remains constant for most iterations\\n(see Figure 5).\\nTo validate our insights on learning rate schedules, we experiment cosine and WSD schedules each with\\noptimally tuned learning rate in Figure 5. We expand small models to large models at every 10% of total\\ntraining horizon. For ResNet, the small model can still catch up with large model when τ ≈0.8T under WSD\\nschedule, but it fails to catch up around τ ≥0.7T under cosine schedule; for GPT, the small model can catch\\nup until τ ≈0.8T under WSD schedule, but it fails around τ ≥0.5T under cosine schedule.\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 0-layer (cosine schedule)\\nGPT 12-layer (cosine schedule)\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 0-layer (WSD schedule)\\nGPT 12-layer (WSD schedule)\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\nValidation loss\\ncosine schedule\\nWSD schedule\\n50000\\n100000\\n150000\\n200000\\n250000\\nIterations\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\n2.25\\n2.50\\nValidation loss\\nResNet14 (WSD schedule)\\nResNet50 (WSD schedule)\\n50000\\n100000\\n150000\\n200000\\n250000\\nIterations\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\nValidation accuracy\\nResNet14 (WSD schedule)\\nResNet50 (WSD schedule)\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n0.92\\n0.93\\n0.94\\n0.95\\n0.96\\n0.97\\n0.98\\n0.99\\n1.00\\nValidation loss\\n75.25\\n75.50\\n75.75\\n76.00\\n76.25\\n76.50\\n76.75\\n77.00\\n77.25\\nValidation accuracy\\nFigure 5\\nPerformance of zero-layer progressive training and fixed-size training, where WSD schedule significantly\\nenhances the progressive training. See one-layer results in Section C.\\nTakeaway 4:\\nProgressive training is indeed “PGD + initialization of new layers + SGD”, whose effectiveness\\nrelies on good initialization (e.g. random) and learning rate schedule (e.g. WSD).\\n7\\n\\n5\\nWhen to expand depth?\\nTo determine the timing of depth expansion, we need to understand the mixing time, which is the time until\\nthe loss of progressive training is close to the fixed-size large model training. To be specific, we define tmix\\nsuch that L(Wfixed-size\\nτ+tmix ) ≈L(Wprogressive\\nτ+tmix\\n). Clearly, if the mixing time is short, then we can expand the models\\nat later stage and save more compute.\\n5.1\\nPerspectives matter to mixing behaviors\\nWe highlight that the mixing behaviors of progressive training (e.g. Figures 5, 9,13) have not been clearly\\nobserved in the literature, possibly due to the difference in perspectives of comparison.\\nIn figures of Wang et al. (2023a); Chen et al. (2021); Pan et al. (2024); Du et al. (2024), the comparison is\\nbetween the grown model and the target model, while our comparison is based on the entire training (source\\nplus grown models). Such a perspective omits the computational cost of small models and the stated speedup\\nmust be discounted in our context. We re-plot Figure 5 (GPT under WSD schedule) from their perspective\\nand no longer observe the mixing behaviors in Figure 6.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n2.8\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\nValidation loss\\nGPT 12-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0\\n6.5\\nValidation loss\\nGPT 0\\n12-layer ( /T = 0.5)\\nGPT 12-layer\\nPre-growth\\nPost-growth\\nFigure 6\\nDifferent perspectives of Figure 5 to compare\\nprogressive training and fixed-size training. Left: only com-\\nparing the grown model and target model. Right: matching\\nthe pre-growth loss of source model to target model.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\nFLOPs\\n1e20\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\nValidation loss\\n5.0X speedup\\nGPT 0\\n12-layer (random)\\nGPT 1\\n12-layer (copying)\\nGPT 1\\n12-layer (random)\\nGPT 12-layer (fixed-size)\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\nFLOPs\\n1e21\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\nValidation loss\\n5.0X speedup\\nGPT 0\\n60-layer (random)\\nGPT 1\\n60-layer (copying)\\nGPT 1\\n60-layer (random)\\nGPT 60-layer (fixed-size)\\nFigure 7\\nComparing progressive training and fixed-\\nsize training via the grown model and target model of\\nFigure 1. Left: 124M model. Right: 7B model.\\nAnother perspective in the literature is to overlay the loss curve for the grown model over the target model.\\nShen et al. (2022) suggest that the convergence rate of grown model is “the same as the target model trained\\nfrom scratch\", and that the training dynamics is preserved. However, we claim that our method significantly\\nimproves the training dynamics instead of preserving it, as shown in Figure 6 by comparing the dashed orange\\ncurve and the solid black curve.\\nTakeaway 5:\\nThe mixing behaviors of loss and training dynamics are the highlight of our depth expansion,\\nwhich are only observable via the comparison between the entire progressive training and the fixed-size\\ntraining from scratch.\\n5.2\\nSensitivity to τ under different schedules\\nInterestingly, in Figure 5, the mixing time tmix(τ) is highly sensitive to the timing of depth expansion τ for\\ncosine schedule, but robust to τ for WSD schedule. For example, expanding GPT 1-layer at 10% horizon\\n(blue curve) and expanding at 60% horizon (brown curve) both need ≈16B tokens or 30k iterations to mix\\nwith 12-layer training. However, expanding at 80% horizon (grey curve) cannot mix well as the learning rate\\nhas decayed. The same patterns hold for ResNet as well.\\nAs a consequence, we determine the timing of depth expansion as total duration of constant learning rates\\nminus mixing time in Figure 1. To be more precise, our WSD uses 2% warmup, 10% decay, and 528k iterations\\nwith constant learning rate. We subtract ≈40k iterations of mixing time from it (derived from Figure 5 or\\nthe early stopped run), and set the timing of depth expansion at t =480k.\\nTakeaway 6:\\nDuring the stable phase of WSD schedule, the mixing time is almost unaffected by the timing\\nof depth expansion. Hence we can transfer the mixing time at early iterations until the decaying phase (see\\nFigure 1).\\n8\\n\\n6\\nWhich to expand depth?\\nWhile we can expand the depth of any small model, we show the following through 150 runs (3 large model\\nsizes, 5 small model sizes, 10 expansion times) in Figure 8.\\nTakeaway 7:\\nIt is the most computationally efficient to (I) scale up from the zero/one-layer models and\\n(II) scale up only once, i.e. use single-stage progressive training.\\nAs we see in Figure 8, the zero/one-layer progressive training almost captures the loss-compute tradeoff from\\na Pareto-optimal viewpoint, especially in contrast with the progressive training from more than 2 layers.\\nAdditionally, the latest timing of expansion that still allows the progressive training to mix with the fixed-size\\ntraining is not sensitive to small model sizes. In other words, expanding from 1-layer or from 6-layer at\\nτ/T ≈0.6 is similarly effective, but the latter is much more computationally expensive.\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\n4.0\\nLoss\\nGPT 0\\n12-layer\\nGPT 1\\n12-layer\\nGPT 2\\n12-layer\\nGPT 6\\n12-layer\\nGPT 9\\n12-layer\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFLOPs\\n1e20\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\n4.0\\nLoss\\nGPT 0\\n24-layer\\nGPT 1\\n24-layer\\nGPT 2\\n24-layer\\nGPT 6\\n24-layer\\nGPT 12\\n24-layer\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nFLOPs\\n1e20\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\n4.0\\nLoss\\nGPT 0\\n36-layer\\nGPT 1\\n36-layer\\nGPT 2\\n36-layer\\nGPT 6\\n36-layer\\nGPT 18\\n36-layer\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\n4.0\\nLoss\\nGPT 0\\n12-layer\\nGPT 1\\n12-layer\\nfixed-size 1,2,6,9,12-layer\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nFLOPs\\n1e20\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\n4.0\\nLoss\\nGPT 0\\n24-layer\\nGPT 1\\n24-layer\\nfixed-size 1,2,6,12,24-layer\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nFLOPs\\n1e20\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\n4.0\\nLoss\\nGPT 0\\n36-layer\\nGPT 1\\n36-layer\\nfixed-size 1,2,6,18,36-layer\\nFigure 8\\nLoss-compute tradeoff (validation loss v.s. FLOPs) of depth expansion from small models to {12, 24, 36}-\\nlayer GPT2 with {124M, 400M, 1B} parameters.\\nAnother insight from the loss-compute tradeoff is that, it suffices to use single-stage expansion, i.e. we do\\nnot need multi-stage expansion such as 0 →12 →24 (if our target model is 24-layer). The reason lies in\\nthe mixing behaviors such that 0 →12 →24 can be decomposed to two single-stage expansions 0 →12 and\\n12 →24. Therefore, the loss curve of 0 →12 →24 will mix with those of 0 →12 and 12 →24, in the first\\nstage and the second stage, respectively. As a result (see Section C.3), multi-stage training achieves the same\\nloss with worse efficiency than single-stage training, in sharp contrast to Gong et al. (2019); Yao et al. where\\nthe mixing behaviors are not observed.\\n7\\nDeep progressive training recipe\\nWe summarize our progressive training recipe, leveraging the theoretical insights and empirical evidences in\\nprevious sections.\\n1. Train zero/one-layer model and then expand depth by random initialization2.\\n2. Train models with Muon-NSGD (or other muP-scaled optimizers) and employ the same hyperparameters\\nbefore and after depth expansion.\\n3. Train models with WSD learning rate schedule and expand depth during the stable phase.\\n2Alternatively, train one-layer model and expand by copying , e.g. w →[w, w, w].\\n9\\n\\n4. The timing of depth expansion τ (or equivalently the mixing time tmix) can be determined by two\\nsmall-scale runs: one fixed-size training and one progressive training (τ at the end of warmup), both\\nearly stopped when their losses mix.\\nWe further validate our recipe with MoE Wolfe (2024); Xue et al. (2024) on OpenWebText dataset, and we\\nobserve the same patterns as dense models such as the mixing behaviors. We emphasize that our approach is\\ndifferent and orthogonal to existing works that upcycle MoE He et al. (2024), which scale up a small dense\\nmodel to a large MoE without increasing the depth, rather than a shallow MoE to a deep MoE. This upcycling\\napproach has reported some negative results Muennighoff et al.; Komatsuzaki et al.; Nakamura et al.; Liew\\net al.; Wei et al. (2024), because the grown MoE becomes worse than the MoE trained from scratch after a\\nfew hundred billion tokens.\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nnanoMOE 0-layer (random)\\nnanoMOE 12-layer (fixed size)\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nnanoMOE 1-layer (random)\\nnanoMOE 12-layer (fixed size)\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\nValidation loss\\n0-layer (random)\\n1-layer (random)\\nFigure 9\\nConvergence of zero/one-layer progressive training and fixed-size training for MoE, with random initialization\\nof new layers.\\nTakeaway 8:\\nZero/one-layer progressive training is effective on various model architectures including\\nResNet, GPT2 and MoE.\\n8\\nConclusion\\nWe show that zero/one-layer progressive training can significantly accelerate large-scale training, if it is\\nequipped with good initialization method and learning rate schedule. This strategy retains almost the same\\nperformance as fixed-size training thanks to the mixing behaviors. This work demonstrates the power of\\ntheoretical insights into progressive training, drawing tools from feature learning and optimization theory. We\\nexpect future works to continue pushing the efficiency frontier and the applicability of progressive training,\\ne.g. by scaling up both width and depth.\\n10\\n\\nReferences\\nNaman Agarwal, Pranjal Awasthi, Satyen Kale, and Eric Zhao. Stacking as accelerated gradient descent. arXiv\\npreprint arXiv:2403.04978, 2024. 2\\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In\\nInternational conference on machine learning, pages 242–252. PMLR, 2019. 6\\nValentyn Boreiko, Zhiqi Bu, and Sheng Zha. Towards understanding of orthogonalization in muon. In High-dimensional\\nLearning Dynamics 2025, 2025. https://openreview.net/forum?id=ppmyFtr9EW. 15\\nZhiqi Bu and Shiyun Xu. Gradient descent with generalized newton’s method. In The Thirteenth International\\nConference on Learning Representations, 2024. 6\\nZhiqi Bu, Shiyun Xu, and Kan Chen. A dynamical view on optimization algorithms of overparameterized neural\\nnetworks. In International conference on artificial intelligence and statistics, pages 3187–3195. PMLR, 2021. 6\\nBo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert. Multi-level residual networks from dynamical\\nsystems view. In International Conference on Learning Representations, 2018. 4, 5\\nCheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and\\nQun Liu. bert2bert: Towards reusable pretrained language models. arXiv preprint arXiv:2110.07143, 2021. 2, 4, 8\\nTianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv\\npreprint arXiv:1511.05641, 2015. 1, 4\\nLenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using\\noptimal transport. Advances in neural information processing systems, 31, 2018. 4\\nAaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. In International Conference\\non Machine Learning, pages 7449–7479. PMLR, 2023. 6\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of\\nthe association for computational linguistics: human language technologies, volume 1 (long and short papers), pages\\n4171–4186, 2019. 2\\nChengyu Dong, Liyuan Liu, Zichao Li, and Jingbo Shang. Towards adaptive residual network training: A neural-ode\\nperspective. In International conference on machine learning, pages 2616–2626. PMLR, 2020. 5\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2\\nWenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, and Jie Fu. Stacking\\nyour transformers: A closer look at model growth for efficient llm pre-training. Advances in Neural Information\\nProcessing Systems, 37:10491–10540, 2024. 2, 4, 5, 8, 14\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with\\nsimple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022. 3, 16\\nCheng Fu, Hanxian Huang, Zixuan Jiang, Yun Ni, Lifeng Nai, Gang Wu, Liqun Cheng, Yanqi Zhou, Sheng Li, Andrew\\nLi, et al. Triple: revisiting pretrained model reuse and progressive learning for efficient vision transformer scaling\\nand searching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17153–17163,\\n2023. 5, 19\\nAaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github.\\nio/OpenWebTextCorpus, 2019. 3\\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively\\nstacking. In International conference on machine learning, pages 2337–2346. PMLR, 2019. 2, 4, 5, 9, 19\\nXiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. On the transformer growth for progressive\\nbert training. arXiv preprint arXiv:2010.12562, 2020. 2\\nAlex Hägele, Elie Bakouch, Atli Kosson, Leandro Von Werra, Martin Jaggi, et al. Scaling laws and compute-optimal\\ntraining beyond fixed training durations. Advances in Neural Information Processing Systems, 37:76232–76264, 2024.\\n2\\n11\\n\\nEthan He, Abhinav Khattar, Ryan Prenger, Vijay Korthikanti, Zijie Yan, Tong Liu, Shiqing Fan, Ashwath Aithal,\\nMohammad Shoeybi, and Bryan Catanzaro. Upcycling large language models into mixture of experts. arXiv preprint\\narXiv:2410.07524, 2024. 10\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings\\nof the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 3\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language\\nmodels. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pages\\n30016–30030, 2022. 1\\nArthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural\\nnetworks. Advances in neural information processing systems, 31, 2018. 6\\nKeller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon:\\nAn optimizer for hidden layers in neural networks, 2024. https://kellerjordan.github.io/posts/muon/. 3, 15\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361,\\n2020. 1\\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay,\\nMostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. In The\\nEleventh International Conference on Learning Representations. 10\\nGuillaume Leclerc and Aleksander Madry. The two regimes of deep network training. arXiv preprint arXiv:2002.10376,\\n2020. 6\\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey\\nPennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural\\ninformation processing systems, 32, 2019. 6\\nBei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang, and Jingbo Zhu. Shallow-to-deep\\ntraining for neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), pages 995–1005, 2020. 4, 5\\nSeng Pei Liew, Takuya Kato, and Sho Takase. Scaling laws for upcycling mixture-of-experts language models. In\\nForty-second International Conference on Machine Learning. 10\\nSong Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-\\nfree bounds and kernel limit. In Conference on learning theory, pages 2388–2464. PMLR, 2019. 4\\nNiklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete\\nWalsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language models. In The Thirteenth\\nInternational Conference on Learning Representations. 10\\nTaishi Nakamura, Takuya Akiba, Kazuki Fujii, Yusuke Oda, Rio Yokota, and Jun Suzuki. Drop-upcycling: Training\\nsparse mixture of experts with partial re-initialization. In The Thirteenth International Conference on Learning\\nRepresentations. 10\\nYu Pan, Ye Yuan, Yichun Yin, Jiaxin Shi, Zenglin Xu, Ming Zhang, Lifeng Shang, Xin Jiang, and Qun Liu. Preparing\\nlessons for progressive training on language models. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nvolume 38, pages 18860–18868, 2024. 2, 5, 8, 14\\nYujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li, Maosong\\nSun, et al. Knowledge inheritance for pre-trained language models. arXiv preprint arXiv:2105.13880, 2021. 2\\nYujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Elle: Efficient lifelong\\npre-training for emerging data. In Findings of the Association for Computational Linguistics: ACL 2022, pages\\n2789–2810, 2022. 5\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3\\nSashank J Reddi, Sobhan Miryoosefi, Stefani Karp, Shankar Krishnan, Satyen Kale, Seungyeon Kim, and Sanjiv\\nKumar. Efficient training of language models using few-shot learning. In International Conference on Machine\\nLearning, pages 14553–14568. PMLR, 2023. 2\\n12\\n\\nFabian Schaipp, Alexander Hägele, Adrien Taylor, Umut Simsekli, and Francis Bach. The surprising agreement between\\nconvex optimization theory and learning-rate scheduling for large model training. arXiv preprint arXiv:2501.18965,\\n2025. 6\\nSheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer\\nlanguage models. In International Conference on Machine Learning, pages 19893–19908. PMLR, 2022. 2, 4, 5, 8, 14,\\n19\\nZhen Tan, Daize Dong, Xinyu Zhao, Jie Peng, Yu Cheng, and Tianlong Chen. Dlo: Dynamic layer operation for\\nefficient vertical scaling of llms. arXiv preprint arXiv:2407.11030, 2024. 4, 14\\nPeihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris,\\nDavid Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer\\ntraining. arXiv preprint arXiv:2303.00980, 2023a. 2, 8\\nYite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, and Hongxia Yang.\\nLemon: Lossless model expansion. In The Twelfth International Conference on Learning Representations, 2023b. 1,\\n4, 5, 14\\nYu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by increasing model capacity. In\\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 2471–2480, 2017. 4\\nTianwen Wei, Bo Zhu, Liang Zhao, Cheng Cheng, Biye Li, Weiwei Lü, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang,\\nLiang Zeng, et al. Skywork-moe: A deep dive into training techniques for mixture-of-experts language models. arXiv\\npreprint arXiv:2406.06563, 2024. 10\\nCameron Wolfe. nanomoe: Minimal mixture of experts implementation. https://github.com/wolfecameron/nanoMoE,\\n2024. 10\\nChengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, and Ping Luo. Llama pro:\\nProgressive llama with block expansion. arXiv preprint arXiv:2401.02415, 2024. 4, 14\\nChen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint arXiv:1802.08770,\\n2018. 2\\nFuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early\\neffort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739, 2024. 10\\nCheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progressively stacking 2.0: A\\nmulti-stage layerwise training method for bert training speedup. arXiv preprint arXiv:2011.13635, 2020. 2\\nGreg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522,\\n2020. 4\\nGreg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki,\\nWeizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter\\ntransfer. arXiv preprint arXiv:2203.03466, 2022. 4\\nGreg Yang, James B Simon, and Jeremy Bernstein.\\nA spectral condition for feature learning.\\narXiv preprint\\narXiv:2310.17813, 2023. 4\\nKazuki Yano, Takumi Ito, and Jun Suzuki. STEP: Staged parameter-efficient pre-training for large language models.\\nIn Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the\\nAmericas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short\\nPapers), pages 374–384, Albuquerque, New Mexico, April 2025a. Association for Computational Linguistics. ISBN\\n979-8-89176-190-2. doi: 10.18653/v1/2025.naacl-short.32. https://aclanthology.org/2025.naacl-short.32/. 2\\nKazuki Yano, Sho Takase, Sosuke Kobayashi, Shun Kiyono, and Jun Suzuki. Efficient construction of model family\\nthrough progressive training using model expansion. arXiv preprint arXiv:2504.00623, 2025b. 2\\nYiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language model\\npre-training. In The Twelfth International Conference on Learning Representations. 1, 2, 9, 14\\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus.\\nSt-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. 16\\n13\\n\\nA\\nDepth expansion approaches\\nA.1\\nApplicability of random, copying, and zero initialization\\nWe summarize the depth expansion approaches with respect to the depth of source models.\\nTable 2\\nApplicability of depth expansion approaches. Merged cells indicate that multiple approaches are equivalent\\nfor a source model.\\nzero-layer\\none-layer\\nmulti-layer\\nrandom\\nYes\\nYes\\nYes\\ncopying_inter\\nNo\\nYes\\nYes\\ncopying_stack\\nYes\\ncopying_last\\nYes\\nzero\\nYes\\nYes\\nYes\\nWe note that zero-layer or one-layer depth expansion significantly simplifies the copying approaches, as there is\\nno ordering such as stacking or interpolation. In Appendix H of Du et al. (2024), the search space of ordering\\nis enormous but necessary, since a proper ordering indeed improves the progressive training. This aligns with\\nour results that copying all layers is better than copying only the last layer. However, it is debatable which of\\ncopying_inter and copying_stack is more advantageous, e.g. Pan et al. (2024) claims that copying_inter is\\nmore stable but Du et al. (2024) demonstrates that copying_stack converges better.\\nWe highlight that such debate is completely avoided for zero-layer or one-layer progressive training.\\nA.2\\nCopying_zero initialization\\nCompletely zero initialization renders new layers not trainable, despite the depth expansion is function-\\npreserving. It has been shown in the literature that copying with partially zero initialization has better\\ntrainability and is still function-preserving.\\nThere are two known methods that copy all sub-layers except (1) the normalization sub-layers are initialized\\nas zeros Shen et al. (2022), or (2) the last linear sub-layer is initialized as zero (Wang et al. (2023b);\\nTan et al. (2024); Wu et al. (2024) and Du et al. (2024) Gzero), or masked with zeros Yao et al.. These\\napproaches are termed as copying_zeroN and copying_zeroL, which enforce zero output of a new layer and\\nare function-preserving.\\nWe experiment in the same setting as in Figure 2. Empirically, copying_zeroN has weak trainability, but\\ncopying_zeroL converges as fast as copying (without any zero sub-layers) and avoids any loss spike unlike\\ncopying.\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nLoss\\nGPT 1-layer (fixed size)\\nGPT 1\\n12-layer (zero)\\nGPT 1\\n12-layer (copying_zeroN)\\nGPT 1\\n12-layer (copying_zeroL)\\nGPT 1\\n12-layer (copying)\\nGPT 12-layer (fixed size)\\nFigure 10\\nConvergence of one-layer progressive training and fixed-size training, with 2 different approaches of\\ncopying_zero initialization.\\n14\\n\\nA.3\\nOn the performance and ordering of random initialization\\nWe observe that random initialization of new layers works well on GPT2 and MoE, but slightly less so on\\nResNet. We think the reason is the location of insertion: for GPT2 and MoE, the insertion is at the bottom,\\ne.g. [1,2,3] to [1,2,3,R,R,R]; however, for ResNet with 4 stages, the insertion is intermittent since the model\\narchitecture is inhomogeneous, e.g. ResNet26 to ResNet50 is like [[1],[2],[3],[4]] to [[1,R],[2,R], [3,R,R,R],[4,R]].\\nOn a related note, we analyze the ordering of random initialization. We train 6-layer or 12-layer GPT2 and\\nexpand the depth at τ = 0.1T. We insert randomly initialized layers on top or bottom of old layers, i.e.\\n[1, 2, 3, 4, 5, 6] →[R, ..., R, 1, ..., 6] or [1, 2, 3, 4, 5, 6] →[1, ..., 6, R, ..., R].\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 1-layer (fixed size)\\nGPT 1\\n12-layer (random top)\\nGPT 1\\n12-layer (random bottom)\\nGPT 12-layer (fixed size)\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\nValidation loss\\nGPT 1-layer (fixed size)\\nGPT 1\\n12-layer (random top)\\nGPT 1\\n12-layer (random bottom)\\nGPT 12-layer (fixed size)\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 6-layer (fixed size)\\nGPT 6\\n12-layer (random top)\\nGPT 6\\n12-layer (random bottom)\\nGPT 12-layer (fixed size)\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\nValidation loss\\nGPT 6-layer (fixed size)\\nGPT 6\\n12-layer (random top)\\nGPT 6\\n12-layer (random bottom)\\nGPT 12-layer (fixed size)\\nFigure 11\\nConvergence of progressive training and fixed-size training with different insertion of random initialization\\n(right plot is zoom-in of the left). Adding new layers at the bottom of old layers works best, with much smaller loss\\nspikes.\\nWe highlight that appending new layers at the bottom is empirically the best approach, and has much smaller\\nloss spike than inserting at the top. However, such choice is completely avoided for zero-layer progressive\\ntraining.\\nB\\nExperiment settings\\nDefault batch size is 512 and decaying is 20% for WSD schedule, except for the long runs in Figure 1 where\\ndecaying is 10% and batch size=512 for 1B models and 64 for 7B models. For WSD schedule, the learning\\nrate is 0.01 as shown to be optimal in Figure 3 (only here GPT2 are trained for 25k iterations); for cosine\\nschedule, the learning rate is 0.05.\\nRegarding the optimizer, Muon-NSGD uses Muon Jordan et al. (2024) and NSGD as in Boreiko et al. (2025)\\n15\\n\\nin order to orthogonalize all tensors: denoting W as a layer’s parameter, we apply\\nMuon: Wt+1 = (1 −ηλ)Wt −η · NS(mt)\\nNSGD: Wt+1 = (1 −ηλ)Wt −η · mt/∥mt∥2\\nwhere NS is the Newton-Schulz matrix iteration, mt is the momentum, and λ is the weight decay.\\nFor GPT2 models, we always keep n_embd/n_head=64. Different depth uses different n_head: full 12-layer\\nuses 12 heads; full 24-layer uses 16 heads; full 36-layer uses 20 heads; full 60-layer uses 48 heads.\\nFor MoE models, we use the same configurations as GPT2 (12-layer). Additionally, we use 8 experts, auxiliary\\nloss Fedus et al. (2022), and router z loss Zoph et al. (2022).\\nC\\nAdditional experiments\\nC.1\\nComparing progressive training to shorter fixed-size training\\nIn Figure 1, we have observed that progressive training can achieve similar (though sometimes slightly higher)\\nvalidation loss than fixed-size training under the same number of iterations. To make sure that progressive\\ntraining has actual advantage instead of just moving along the loss-compute tradeoff, we launch another\\nfixed-size training with shorter training horizon.\\nTo be concrete, the depth expansion happens at τ = 0.8T, meaning the grown model from progressive training\\nis trained for 120k iterations. Our second fixed-size training runs for the same 120k iterations using the same\\nlearning rate schedule (60k in stable phase, 60k in decaying phase, no warmup, same peak learning rate).\\nIt is clear that the progressive training does inherit from the small model training, despite the loss spike seems\\nto suggest that all progresses before model expansion are lost. This is obvious by comparing the red and grey\\ncurves.\\n0\\n1\\n2\\n3\\n4\\n5\\nFLOPs\\n1e19\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 0\\n12-layer (random), 480~600k iter\\nGPT 1\\n12-layer (copying), 480~600k iter\\nGPT 1\\n12-layer (random), 480~600k iter\\nGPT 12-layer (fixed-size), 0~120k iter\\nGPT 12-layer (fixed-size), last of 600k iter\\n0\\n1\\n2\\n3\\n4\\n5\\nFLOPs\\n1e19\\n2.8\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\nValidation loss\\nGPT 0\\n12-layer (random), 480~600k iter\\nGPT 1\\n12-layer (copying), 480~600k iter\\nGPT 1\\n12-layer (random), 480~600k iter\\nGPT 12-layer (fixed-size), 0~120k iter\\nGPT 12-layer (fixed-size), last of 600k iter\\nFigure 12\\nComparing progressive training and fixed-size training via the grown model and target model (right plot\\nis zoom-in of the left). Under the same compute budget, progressive training converges much faster than fixed-size\\ntraining, despite having high loss after the depth expansion.\\nC.2\\nMixing behaviors across model sizes\\nIn Figure 13, we consistently observe the mixing behaviors on hundreds of runs, from various small models to\\nvarious large models. Specifically, the mixing time is empirically robust to model sizes.\\n16\\n\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 0-layer\\nGPT 12-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 0-layer\\nGPT 24-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 0-layer\\nGPT 36-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 1-layer\\nGPT 12-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 1-layer\\nGPT 24-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 1-layer\\nGPT 36-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 2-layer\\nGPT 12-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 2-layer\\nGPT 24-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 2-layer\\nGPT 36-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 6-layer\\nGPT 12-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 6-layer\\nGPT 24-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 6-layer\\nGPT 36-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 9-layer\\nGPT 12-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 12-layer\\nGPT 24-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 18-layer\\nGPT 36-layer\\nFigure 13\\nScaling up by depth expansion from {0, 1, 2, 6, 18} layers to {12,24,36} layers GPT2 with {124M, 400M, 1B}\\nparameters.\\n17\\n\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nTiming of expansion /T\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\nValidation loss\\nGPT 0\\n12-layer\\nGPT 1\\n12-layer\\nGPT 2\\n12-layer\\nGPT 6\\n12-layer\\nGPT 9\\n12-layer\\nGPT 12-layer (fixed size)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nTiming of expansion /T\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\nValidation loss\\nGPT 0\\n24-layer\\nGPT 1\\n24-layer\\nGPT 2\\n24-layer\\nGPT 6\\n24-layer\\nGPT 12\\n24-layer\\nGPT 24-layer (fixed size)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nTiming of expansion /T\\n2.4\\n2.6\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\nValidation loss\\nGPT 0\\n36-layer\\nGPT 1\\n36-layer\\nGPT 2\\n36-layer\\nGPT 6\\n36-layer\\nGPT 18\\n36-layer\\nGPT 36-layer (fixed size)\\nFigure 14\\nFinal loss of depth expansion at different timing, from {0, 1, 2, 6, 18} layers to {12,24,36} layers GPT2\\nwith {124M, 400M, 1B} parameters.\\nC.3\\nSingle-stage progressive training is sufficient\\nWe experiment with single-stage progressive training (from 0-layer or 2-layer to 12-layer) and multi-stage\\ntraining (from 0-layer to 2-layer then to 12-layer). As we expected, the mixing behaviors lead all runs to\\nsimilar final losses, and multi-stage progressive training does not show improved efficiency: the multi-stage\\nrun has almost the same FLOPs as the 2-layer progressive training, worse than the 0-layer one. Additionally,\\nmulti-stage progressive training is hard to set up because of multiple timing of expansion.\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 0\\n2\\n12-layer\\nGPT 0\\n12-layer ( = 0.5T)\\nGPT 2\\n12-layer ( = 0.5T)\\nGPT 0-layer (fixed size)\\nGPT 2-layer (fixed size)\\nGPT 12-layer (fixed size)\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n2.8\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\nValidation loss\\nGPT 0\\n2\\n12-layer\\nGPT 0\\n12-layer ( = 0.5T)\\nGPT 2\\n12-layer ( = 0.5T)\\nGPT 12-layer (fixed size)\\nFigure 15\\nMulti-stage progressive training does not show better efficiency or loss, due to the mixing behaviors.\\n18\\n\\nC.4\\nOptimizer states\\nWe conduct an ablation study to explore how to deal with the optimizer states (e.g. momentum and variance\\nin AdamW or Muon-NSGD) during the expansion. Previous works have mixed results: Shen et al. (2022);\\nFu et al. (2023) show that copying the old layers’ optimizer states to new layers can be helpful; Gong et al.\\n(2019) resets the optimizer states of all layers.\\nWe consider the following methods for optimizer states (OS): denoting embedding as E, hidden layers as H,\\nand last layer as L,\\n• (inheriting OS) inheriting existing OS: [E, H, L] →[E, 0 × 12, L]\\n• (copying OS) inheriting existing OS and copying hidden layers’ OS: [E, H, L] →[E, H × 12, L]\\n• (no OS) not inheriting any OS: [E, L] or [E, H, L] →[0, 0 × 12, 0]\\nWe observe that all methods seem to work in terms of final losses and mixing behaviors, although copying OS\\nis less stable.\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\nValidation loss\\nGPT 0\\n12-layer (no OS)\\nGPT 1\\n12-layer (no OS)\\nGPT 1\\n12-layer (copying OS)\\nGPT 1\\n12-layer (inheriting OS)\\nGPT 12-layer (fixed size)\\nFigure 16\\nValidation loss of depth expansion at τ = 0.1T with different ways to set optimizer states.\\nC.5\\nChoosing optimizer and learning rate schedule\\nIn Figure 17, we train 100k iterations with two optimizers and two learning rate schedules. The same schedule\\nis used before and after expansion, without changing the learning rate. We observe that Muon-NSGD with\\nWSD schedule achieves best loss at all FLOPs (also at any timing of expansion τ/T). This is consistent with\\nour theory.\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\nValidation loss\\nMuon-NSGD, WSD schedule\\nMuon-NSGD, cosine schedule\\nAdamW, WSD schedule\\nAdamW, cosine schedule\\nFigure 17\\nLoss-compute tradeoff (validation loss v.s. FLOPs) of zero-layer depth expansion under different optimizers\\nand learning rate schedules. The target model is 12-layer GPT2. For WSD schedule, AdamW uses 0.0005 learning rate\\nand Muon-NSGD uses 0.01 learning rate. For cosine schedule, the learning rates are doubled.\\n19\\n\\nC.6\\nMixing needs data, not iterations\\nImportantly, we observe that the mixing time is measured by data size, i.e. images or tokens processed, not\\nby iterations. In Figure 18, we compare a progressive training with constant batch size to another one with\\n4× batch size after the depth expansion (τ = 0.1T). The final loss is similar although large-batch training\\ntakes much fewer iterations.\\n0\\n5000\\n10000\\n15000\\n20000\\n25000\\nIterations\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 6\\n12-layer(B=0.5M)\\nGPT 6\\n12-layer(B=0.5\\n2M)\\nGPT 12-layer (B=0.5M)\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\nTokens(B)\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\n3.6\\nValidation loss\\nGPT 6\\n12-layer(B=0.5M)\\nGPT 6\\n12-layer(B=0.5\\n2M)\\nGPT 12-layer (B=0.5M)\\nFigure 18\\nProgressive training needs sufficient data to mix with fixed-size large model training, largely unaffected\\nby batch size or iterations.\\nC.7\\nOne-layer model expansion figures\\nWe present the one-layer model expansion results in correspondence to Figure 5 and Figure 6 here, due to\\nspace limit.\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 1-layer (cosine schedule)\\nGPT 12-layer (cosine schedule)\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n101\\n3 × 100\\n4 × 100\\n6 × 100\\nValidation loss\\nGPT 1-layer (WSD schedule)\\nGPT 12-layer (WSD schedule)\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n3.0\\n3.2\\n3.4\\n3.6\\n3.8\\nValidation loss\\ncosine schedule\\nWSD schedule\\nFigure 19\\nPerformance of one-layer progressive training and fixed-size training, where WSD schedule significantly\\nenhances the progressive training.\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nFLOPs\\n1e19\\n2.8\\n2.9\\n3.0\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\nValidation loss\\nGPT 12-layer\\n20000\\n40000\\n60000\\n80000\\n100000\\nIterations\\n3.00\\n3.25\\n3.50\\n3.75\\n4.00\\n4.25\\n4.50\\n4.75\\n5.00\\nValidation loss\\nGPT 1\\n12-layer ( /T = 0.5)\\nGPT 12-layer\\nPre-growth\\nPost-growth\\nFigure 20\\nDifferent perspectives to compare (one-layer) progressive training and fixed-size training. Left: only\\ncomparing the grown model and target model. Right: matching the pre-growth loss of source model to target model.\\n20\\n',\n",
       " \"Risk Prediction of Cardiovascular Disease for Diabetic Patients \\nwith Machine Learning and Deep Learning Techniques \\n \\nEsha Chowdhury \\nDept. of CSE \\nDhaka University of Engineering & Technology, \\nGazipur, Bangladesh \\neshachowdhury.cse@gmail.com \\nAbstract— Accurate prediction of cardiovascular disease \\n(CVD) risk is a crucial task for healthcare institutions. This \\nwork is important since diabetes is increasing and is strongly \\nlinked to heart disease. This study proposes an efficient CVD \\nrisk prediction model for diabetic patients using a combination \\nof machine learning (ML) and hybrid deep learning (DL) \\napproaches. The BRFSS dataset was preprocessed by \\nremoving duplicates, handling missing values, identifying \\ncategorical and numerical features, and applying Principal \\nComponent Analysis (PCA) for feature extraction. Several ML \\nmodels, including DT, RF, KNN, SVM, AdaBoost, and \\nXGBoost, were implemented, with XGBoost achieving the \\nhighest accuracy of 0.9050. Various DL models (ANN, DNN, \\nRNN, CNN, LSTM, BiLSTM, and GRU) and hybrid models \\ncombining CNN with LSTM, BiLSTM, and GRU variants \\nwere also explored, with some achieving perfect recall (1.00), \\nwhile the LSTM model achieved the highest accuracy of \\n0.9050. Our research highlights the effectiveness of ML and \\nDL models in predicting cardiovascular disease risk among \\ndiabetic patients, thereby automating and enhancing clinical \\ndecision-making. High accuracy and F1 scores demonstrate \\nthese models’ capability to improve personalized risk \\nmanagement and preventive strategies. \\nIndex Terms— Cardiovascular Disease, Diabetic, ML, DL, \\nHigh Risk, Clinical Decisions. \\nI. \\nIntroduction \\n \\nIncreased prevalence of diabetes is leading to growth in \\ncardiovascular disease in the whole world. This has added to the \\nproactive risk management being a prerequisite health plan of all \\nthe people [1]. Due to the requirement to process a lot of complex \\npatient data and profiles, healthcare practitioners cannot calculate \\nor predict diabetes patients' heart disease risk in a timely manner \\n[2]. Doctors in the past used to use traditional clinical scoring \\nsystems to ascertain the likelihood of a diabetic patient developing \\na heart attack or stroke. Such systems are not always absolutely \\ncorrect and foresee the future events. More advanced procedures \\nare thus desperately needed to ensure more accuracy and \\nautomation of cardiovascular risk prediction of this group of \\nvulnerable population. \\n \\nTechniques of Ensemble Machine Learning have changed \\nhealthcare [3]. We use a machine learning-based analytic platform \\nto make predictions. We use machine learning (ML) and deep \\nlearning (DL) to acquire useful information. We analyze data \\nrelated to patient health variables and biomarkers. We achieve this \\nby using algorithms such as random forest, decision tree, and \\nAdaBoost. This technology enables users to make better decisions \\nand anticipates more robust decision-making systems. We enhance \\nmethods of precise cardiovascular risk assessment through model \\ntraining and validation. We employ cutting-edge deep learning \\narchitectural guidelines to comply with clinical guidelines. We \\nprocess measures of clinical data and standardize physiological \\ndata. We codify categorical health data and measure its accuracy. \\nWe produce a reliable dataset for cardiovascular risk modeling. \\n \\nThere is potential in deep learning in cardiology to enhance the \\nprocess of cardiovascular risk assessment. These complex deep \\nlearning patterns can be studied using models applied to large, \\nmultidimensional datasets, which is typically challenging for \\ntraditional machine learning models [4]. This study will address \\nthe drawback of this, eliminating shortcomings of traditional \\nmethods with an advancement of a more intelligent and \\nreconfigurable system that is capable of managing the dynamic \\nnature of health data [5] and disease progression of diabetic \\npatients. The study on DL is also aimed at increasing the clarity \\nand cognition of cardiovascular risk forecasts and building \\nconfidence in the use of such sophistications in clinical diabetic \\npatient care decision-making. \\n \\nII. Related Work \\n \\nSeveral papers have examined the use of ML and DL models in \\npredicting cardiovascular diseases, diabetic complications, and \\nearly intervention solutions. When the Linear Regression (LR) \\nmodel was used to predict 10-year CVD risk, the highest results \\nof 0.023 and 0.001 were achieved on the test set when utilizing \\nthe entire set of features [2]. Deep learning algorithms are built on \\na solid paradigm of prediction of the risk of coronary artery disease \\n(CAD) and cardiovascular disease (CVD) risk classification [4]. \\nFurthermore, meta-learning models, especially support vector \\nmachines (SVMs), have been developed to improve the prediction \\nof CVD risk by identifying the complicated trends in the large \\ndatasets [6]. The effectiveness of such algorithms usually lies in \\nthe proper methods of selection of features, which play an \\nimportant role in the accuracy and efficiency of predictions [7]. A \\nStudy indicates that the Deep Belief Network (DBN)-based model \\nwas found to have a better prediction accuracy of about 81.20% \\nthan traditional algorithms, including SVM and Artificial Neural \\nNetworks (ANN) models [8]. Basic risk factors had a greater \\npredictive performance in nondiabetic women (AUC = 0.79) but \\nhad the same predictive performance in nondiabetic men (AUC = \\n0.69) [9]. Zarkogianni et al. [10] found that related risk prediction \\nmodels based on hybrid wavelet neural networks and self-\\norganizing maps had an AUC of 71.48. \\n \\nIII. METHODS \\n \\nThis study evaluates the performance of various ML and DL \\nModels for CVD risk prediction in diabetic patients. The \\nmethodology includes several key stages (see figure 1). \\n\\nEpoch \\nAccuracy \\nLearning Rate \\nOptimizer \\n8 \\n95.00 \\n0.01 \\nAdam \\n8 \\n95.00 \\n0.001 \\nAdam \\n8 \\n95.00 \\n0.0001 \\nAdam \\nA. Dataset Description \\n \\nThe research is based on a large dataset from the Behavioral Risk Factor \\nSurveillance System (BRFSS) to determine the risk of cardiovascular \\ndisease among diabetic patients. The data in the 2023 BRFSS dataset of \\nthe US Division of Population Health National Center of Chronic Disease \\nPrevention and Health Promotion were used in the study. The table of \\nattributes of the dataset is shown in TABLE II. The data was obtained on \\nthe Diabetes Health Indicators Dataset page in Kaggle. The dataset will \\nalso have lifestyle variables such as smoking and physical activity in \\naddition to health variables, such as blood pressure and cholesterol levels. \\nThe most significant characteristics of the dataset are shown in TABLE \\nIII. \\n \\nB. Data Preprocessing \\nThe data was critically handled and normalized to prepare it to be \\nused in learning ML and DL models. This entailed some of the key \\nsteps, \\nsuch as eliminating duplication, \\nmissing values, \\ncategorizing columns as numerical and categorical, and dividing \\nthe data into training and test data, as illustrated in Fig. 1. The \\ndataset that was preprocessed had 17 chosen features and 433,324 \\nentries. It was further divided into 80:20 to have 346,242 samples \\nof the training set and 86,662 of the testing set. \\n \\nC. Feature Extraction \\nIn an attempt to simplify the data, this research paper used \\nPrincipal Component Analysis (PCA), which transforms variables \\ninto unrelated elements and preserves the most important patterns \\nat the same time. The method is particularly beneficial in the \\nanalysis of medical data because the information about patients is \\nconsiderably more comprehensible. \\n \\nD. Hyper Parameter Tuning \\nIn this study, we optimized our approach to accurately predict the \\nlikelihood of cardiovascular disease among diabetic patients by \\nutilizing hyperparameter values and cross-validation techniques. \\nWe have tuned parameters in the machine learning models, \\nincluding the regularization strength (C) and the type of kernel, to \\nachieve optimal accuracy and generalization. The objective was to \\nenhance the accuracy of prediction and not to overfit to the training \\ndata. Each of the models was tested with the help of the Grid \\nSearch CV and k-fold cross-validation with various combinations \\nof parameters. The result was a hyperparameter optimization that \\nwas performed on all our models. The best models, after tuning in \\nColab on the BRFSS dataset, were the XGBoost and LSTM \\nmodels. The higher accuracy scores reflect the significant increase \\nin performance that our hyperparameter adjustments produced. \\n \\nTABLE I \\nHYPERPARAMETER SETTING \\n \\n \\n \\nE. Machine Learning Approach \\n \\nIn this research, we investigated several machine learning \\nmodels to forecast cardiovascular disease in diabetic patients. \\nDecision Tree (DT) model is simple to comprehend, and it is \\napplicable to both numerical and categorical data [11]. However, \\nthe Random Forest (RF) model is more capable of dealing with \\ncomplex data and minimizing overfitting [11]. The K-Nearest \\nNeighbors (KNN) algorithm ranks the data according to the \\nproximity of data points, thereby acting well in identifying the \\ncomplex patterns. AdaBoost is performed in stages; each time a \\nnew model is being trained, it corrects the errors made by the \\nprevious \\nmodel. \\nFinally, \\neXtreme \\nGradient \\nBoosting \\n(XGBoost), which integrates multiple classifiers, is especially \\ngood at boosting the performance of models and making them \\nstronger. \\n \\nF. Deep Learning Approach \\nThe paper will analyze how deep learning models can be used to \\nidentify patients who are at risk of developing cardiovascular \\ndiseases when they have diabetes. CNN is particularly useful \\nwith grid-like data and is adept at finding patterns and features \\n[12]. Artificial Neural Networks (ANN) are able to make \\nforecasts, identify errors, and perform repetitive changes to their \\ninternal connections to enhance their performance. The \\nRecurrent Neural Networks (RNN) are created with sequential \\ndependencies and are especially applicable in predicting risk of \\ndisease. Long Short-Term Memory (LSTM) networks are better \\nat dealing with long-term dependencies, and thus they are \\ncapable of storing important information that can be used to \\nmake accurate predictions. Deep Neural Networks (DNN) allow \\nmodel learning of complex patterns of data that are on a multi- \\nlevel, and they increase the accuracy of the model. Bidirectional \\nLSTM (BiLSTM) is more reliable in recognizing the patterns as \\nit processes the data in both directions. Finally, Gated Recurrent \\nUnits (GRU) are used to overcome the vanishing gradient issue \\nand make models learn on long sequences. \\n \\nG. Hybrid Model \\nTo make better predictions, researchers have also looked into \\nhybrid models that use more than one deep learning design. The \\nstudy introduces numerous new hybrid deep learning algorithms \\nfor heart disease risk prediction. Each combination of CNN with \\nLSTM, BiLSTM, and GRU, as well as LSTM with GRU and \\nBiLSTM with GRU. It's beneficial that CNN can pick out \\nfeatures in space, and the recurrent networks make it more \\naccurate, address the problem of disappearing gradients, and \\nunderstand how things relate to each other over time. When \\nlooking for malware, for instance, a mix of LSTM and CNN has \\nbeen used before [12]. Each hybrid model demonstrates its \\nability to enhance and streamline clinical decision-making. This \\nmakes it possible to create more personalized risk management \\nand avoidance plans\\n\\nIV. RESULT AND ANALYSIS \\n \\nA. Confusion Matrix \\n \\nA confusion matrix is a table that visualizes the performance of an \\nalgorithm by showing actual versus predicted classifications [6]. \\nThe confusion matrix and ROC curve results of the models are \\ngiven below (see figures 2, 3, 4, 5, 6, 7, and 8): \\n \\n \\nFig. 1. Proposed Method \\n \\nTABLE II \\nDETAILS OF THE 2023 BRFSS DATASET \\nTABLE III \\nKEY FEATURES OF THE DATASET \\n \\nAttribute \\nDetails \\nTitle \\n2023 BRFSS Survey Data \\nAuthor \\nU.S. \\nCenters \\nFor \\nDisease \\nControl and Prevention \\nFeatures \\n303 variables covering different \\naspects \\nData Items \\nResponses from approximately \\n433,324 participants across the \\nUnited States \\nCollection Method \\nComputer-Assisted \\nTelephone \\nInterview systems \\nGeographical \\nCoverage \\nAll 50 states of the United \\nStates, the District of Columbia, \\nGuam, Puerto Rico, and US \\nVirgin Islands \\nCategories of Features \\nDemographics, Chronic Health \\nConditions, Behavioral Features, \\nPreventive \\nHealth \\nServices, \\nPhysical and \\nMental Health \\nStatus, Disability Status and \\nothers \\nFeature \\nDescription \\nDIABTYPE \\nAccording to your doctor or other \\nhealth professional, which type of \\ndiabetes do you have? \\nDIABETES \\nDiabetes is represented by three \\ndistinct values: 0 indicates no \\ndiabetes, 1 indicates pre-diabetes, \\nand 2 indicates diabetes. \\nHighBP \\nBinary variable indicating high \\nblood pressure. \\nHighChol \\nBinary variable indicating high \\ncholesterol. \\nCHOLCHK \\nIndicates whether cholesterol \\nlevels have been checked within \\nthe past five years. \\nBMI \\nBody Mass Index; weight in \\nkilograms divided by height in \\nmeters squared. \\nSMOKE \\n0 indicates never smoked; 1 \\nindicates having smoked at least \\n100 cigarettes in a lifetime. \\nCVDSTRK \\nIndicates whether the patient has \\never had a stroke. \\nEXERANY \\nDuring the past month, other than \\nyour regular job, did you \\nparticipate in any physical \\nactivities or exercises? \\nMEDCOST \\nDuring the past 12 months, was \\nthere a time when you needed to \\nsee a doctor but could not because \\nof cost? \\nGENHLTH \\nGeneral health status. Categories: \\nExcellent, Very good, Good, Fair, \\nand Poor. \\nMENTHLTH \\nThe number of days in the past \\nmonth when mental health was \\npoor. \\nPHYSHLTH \\nThe number of days in the past \\nmonth when physical health was \\npoor. \\nDIFFWALK \\nDifficulty in walking or climbing \\nstairs; 1 indicates difficulty and 0 \\nindicates no difficulty. \\nSEX \\n0 for Female, 1 for Male. \\nAGE \\nCategorized into 13 levels based \\non 5-year intervals. \\nEDUCAG \\nEducation level on a scale from 1 \\n(lowest) to 6 (highest). \\n\\nB. Machine Learning based prediction Results \\n \\nThe classification results for various machine learning models \\nin predicting cardiovascular disease risk in diabetic patients show \\nthat XGBoost achieved the highest accuracy at 90.50%, with \\nprecision, recall, and F1-score all at 0.95, indicating excellent \\nperformance (see table IV). RF and AdaBoost also performed \\nwell, with accuracies of 90.47% and 90.35%, respectively. Other \\nmodels, such as DT and KNN, also showed high accuracy, though \\nslightly lower. Overall, XGBoost is the top-performing model in \\nthis analysis, demonstrating the best balance of accuracy and \\npredictive metrics. \\n \\n \\nFig. 2. Confusion matrix for XGBoost \\nTABLE IV \\nCLASSIFICATION RESULT FOR ML MODEL \\n \\nModel \\nAccuracy \\nPrecision \\nRecall \\nF1-Score \\nDT \\n0.9014 \\n0.91 \\n0.99 \\n0.95 \\nRF \\n0.9047 \\n0.91 \\n1.00 \\n0.95 \\n AdaBoost \\n0.9035 \\n0.91 \\n0.99 \\n0.95 \\nKNN \\n0.9009 \\n0.91 \\n0.99 \\n0.95 \\nXGBoost \\n0.9050 \\n0.91 \\n0.99 \\n0.95 \\n \\n \\nC. Deep Learning based prediction Results \\n \\nThe \\nclassification results for DL \\nmodels in predicting \\ncardiovascular disease risk in diabetic patients reveal high \\nperformance across all models. The results indicate strong \\nclassification capabilities for each model. The results show that each \\nmodel is very effective at classifying things. LSTM was the most \\naccurate, with 90.50%. BiLSTM was next with 90.49%, while GRU \\nwas last with 90.47%. CNN also did well, getting 90.44% of the \\nanswers right and being the most accurate (0.9109). All three \\nmodels, ANN, DNN, and RNN, had the same accuracy of 90.41% \\nand perfect recall (1.0000). LSTM had the highest overall accuracy, \\nand all of the models gave findings that were highly similar and \\nreliable (see Table V). \\n \\nTABLE V \\nCLASSIFICATION RESULT FOR DL MODEL \\n \\nModel \\nAccuracy \\nPrecision \\nRecall \\nF1- \\nScore \\nANN \\n0.9041 \\n0.9041 \\n1.0000 \\n0.9496 \\nDNN \\n0.9041 \\n0.9041 \\n1.0000 \\n0.9496 \\nRNN \\n0.9041 \\n0.9041 \\n1.0000 \\n0.9496 \\nLSTM \\n0.9050 \\n0.9091 \\n0.9944 \\n0.9498 \\nGRU \\n0.9047 \\n0.9067 \\n0.9973 \\n0.9498 \\nBiLSTM \\n0.9049 \\n0.9084 \\n0.9952 \\n0.9498 \\nCNN \\n0.9044 \\n0.9109 \\n0.9912 \\n0.9494 \\n \\nFig. 3. Confusion matrix for LSTM \\n \\n \\nD. Hybrid Model based prediction Results \\nThe classification outcomes for hybrid models in predicting \\ncardiovascular disease risk in diabetic patients demonstrate \\nsignificant predictive validity. With an accuracy of 90.46%, a \\nprecision of 0.9095, and a recall of 0.9934, the CNN+LSTM \\nmodel was the best. This conclusion means that it could identify \\npositive cases and be reliable at the same time. The LSTM+GRU \\nmodel had similar results, with an accuracy of 90.46%, but \\nslightly less precision and a higher recall of 0.9981. \\nCNN+BiLSTM and BiLSTM+GRU both had 90.44% and \\n90.41% accuracy, respectively. BiLSTM+GRU had a perfect \\nrecall of 1.00. In most cases, CNN+LSTM was the most accurate \\noverall. The hybrid models, in their turn, all were high. This \\nindicates that they achieved good precision, recall, and F1 scores \\nin the process of generating predictions. (see Table VI). \\nTABLE VI \\nCLASSIFICATION RESULT FOR HYBRID MODEL \\nModel \\nAccuracy \\nPrecision \\nRecall \\nF1- \\nScore \\nCNN+LSTM \\n0.9046 \\n0.9095 \\n0.9934 \\n0.9496 \\nCNN+LSTM \\n0.9044 \\n0.9070 \\n0.9965 \\n0.9496 \\nLSTM+GRU \\n0.9046 \\n0.9060 \\n0.9981 \\n0.9498 \\nBiLSTM+GRU \\n0.9041 \\n0.9041 \\n1.0000 \\n0.9496 \\n\\n \\n \\n \\nFig. 4. Confusion matrix for CNN+LSTM \\n \\n \\nFig. 5. Confusion matrix for LSTM +GRU \\n \\nFig.6: ROC curve of the XGBoost model \\n \\n \\nFig.7: ROC curve of the LSTM+GRU \\n \\n \\nFig.8: ROC curve of the LSTM model \\nV. CONCLUSION AND FUTURE WORK \\nThis paper applies machine learning (ML) and deep learning \\n(DL) methods to offer a holistic predictive model to evaluate \\nthe risk of cardiovascular disease among diabetics. We \\nexamined BRFSS data and used PCA to present additional \\ninformation. We split it into 2 parts, 80% training and 20% \\ntesting. This strategy guaranteed the impartiality of the test and \\nthe effectiveness of the model. To maximize the accuracy and \\nreliability of the predictions, this paper employed a number of \\ndifferent machine learning models, such as Decision Tree (DT), \\nK-Nearest Neighbor (KNN), and XGBoost, along with artificial \\nneural network (ANN), deep neural network (DNN), \\nconvolutional neural network (CNN), recurrent neural network \\n(RNN), long short-term memory (LSTM), bidirectional long \\nshort-term memory (BiLSTM), gated recurrent unit (GRU), and \\nhybrid architecture. The suggested procedure of evaluating the \\nprobability of heart problems in diabetics was always very \\nprecise, recalling F1 scores. The dataset may be refined in the \\nfuture by adding more real-time patient data and better methods \\nof ensemble and automated hyperparameter optimization. In \\naddition, it can build a clinical support system that is \\nunderstandable and user-friendly and hence increase its impact \\non healthcare. \\n \\nREFERENCES \\n \\n[1] H. Sang, H. Lee, M. Lee, J. Park, S. Kim, and H. G. Woo, et al., \\n“Prediction model for cardiovascular disease in patients with diabetes \\nusing machine learning derived and validated in two independent Korean \\ncohorts,” Scientific Reports, vol. 14, pp. 14966–14966, 2024. \\n[2] H. Calero-Diaz, D. Chushig-Muzo, H. Fabelo, I. Mora-Jiménez, C. \\nGranja, and C. Soguero-Ruiz, “Data-driven cardiovascular risk prediction \\nand prognosis factor identification in diabetic patients,” Proc. IEEE- \\nEMBS Int. Conf. Biomed. Health Inform. (BHI), vol. 2022, pp. 1–4,2022. \\n[3] M. Dorraki, Z. Liao, D. Abbott, P. J. Psaltis, E. Baker, N. Bidargaddi, A. van den \\nHengel, J. Narula, and J. W. Verjans, “Cardiovascular disease risk prediction via \\nmachine learning using mental health data,” European Heart Journal– Digital Health, \\nvol. 3, pp. 2784–2784, 2022. \\n[4] A. M. Johri, K. V. Singh, L. E. Mantella, L. Saba, A. Sharma, and J. R. Laird, et al., \\n“Deep learning artificial intelligence framework for multiclass coronary artery disease \\nprediction using combination of conventional risk factors, carotid ultrasound, and \\nintraplaque neovascularization,” Computers in Biology and Medicine, vol. 150, pp. \\n106018–106018, 2022. \\n[5] W. Yang, Y. Guo, and Y. Liu, “Improvement of auxiliary diagnosis of diabetic \\ncardiovascular disease based on data oversampling and deep learning,” Applied \\nSciences, vol. 13,pp. 5449–5449, 2023. \\n[6] N. S. Punn and D. K. Dewangan, “Ensemble meta-learning using SVM for improving \\ncardiovascular disease risk prediction,” medRxiv, pp.1–10, 2024. \\n[7] L. C. Jaffrin and J. Visumathi, “Impact of feature selection techniques on machine learning \\nand deep learning techniques for cardiovascular disease prediction-an analysis,” Edelweiss \\nApplied Science and Technology, Learning Gate, vol. 8, no. 5, pp. 1454–1471, 2024. \\n[8] K. Vidhya and R. Shanmugalakshmi, “Deep learning based big medical data analytic \\nmodel for diabetes complication prediction,” J. Ambient Intell. Human Comput., vol. 11, \\nno. 4, pp. 5691–5702, 2020. \\n[9] A. R. Folsom, L. E. Chambless, B. B. Duncan, A. C. Gilbert, and J. S. Pankow, “Prediction \\nof coronary heart disease in middle-aged adults with diabetes,” Diabetes Care, vol. 26, no. \\n10, pp. 2777–2784, Oct. 2003. \\n[10]  K. Zarkogianni, M. Athanasiou, A. C. Thanopoulou, and K. S. Nikita, “Comparison of \\nmachine learning approaches toward assessing the risk of developing cardiovascular \\ndisease as a long-term diabetes complication,” IEEEJ. Biomed. Health Inform., vol. 22, no. \\n5, pp. 1637–1647, 2018. \\n[11]  M. H. Alshayeji, S. Abed, and S. C. B. Sindhu, “Two-stage framework for diabetic \\nretinopathy diagnosis and disease stage screening with ensemble learning,” Expert Syst. \\nAppl., vol. 225, p. 120206, 2023. \\n[12]  P. Thakur, V. Kansal, and V. Rishiwal, “Hybrid deep learning approach based on LSTM    \\nand CNN for malware detection,” Wireless Personal Communications, vol. 136, no. 3, pp. \\n1879–1901, Jun. 2024. \\n\\n[13]  K. Papatheodorou, M. Banach, M. Edmonds, N. Papanas, and D. \\nPapazoglou, “Complications of diabetes,” J. Diabetes Res., vol. 2015, pp. 1– \\n5, 2015. \\n[14]  A. S. Saldanha de Mattos Matheus, L. R. M. Tannus, R. A. Cobas, C. C. S. Palma, C. A. \\nNegrato, and M. de B. Gomes, “Impact of diabetes on cardiovascular disease: an update,” \\nInt. J. Hypertens., vol. 2013, pp. 1–15, 2013. \\n[15] L. Langouche and G. Van den Berghe, “Glucose metabolism and insulin therapy,” \\nCrit. Care Clin., vol. 22, no. 1, pp. 119–129, 2006. \\n[16]  J. A. Bluestone, K. Herold, and G. Eisenbarth, “Genetics, pathogenesis and \\nclinical interventions in type 1 diabetes,” Nature, vol. 464, no. 7293, pp. 1293– \\n1300, 2010. \\n[17]  S. E. Kahn, M. E. Cooper, and S. Del Prato, “Pathophysiology and treatment \\nof type 2 diabetes: perspectives on the past, present, and future,” The Lancet, \\nvol. 383, no. 9922, pp. 1068–1083, 2014 \\n[18]  P. Z. Zimmet, D. J. Magliano, W. H. Herman, and J. E. Shaw,“Diabetes: \\na21stcentury challenge,” Lancet Diabetes Endocrinol., vol. 2, no. 1, pp. 56–64, \\n2014. \\n[19]  V. Lyssenko, A. Jonsson, P. Almgren, N. Pulizzi, B. Isomaa, and T. Tuomi, \\net al. ,“Clinical risk factors, DNA variants, and the development of type 2 \\ndiabetes,” New England Journal of Medicine, vol. 359, no. 21, pp. 2220–2232, \\n2008. \\n[20] D. M. Lloyd-Jones, P. W. F. Wilson, M. G. Larson, A. Beiser, E. P. Leip, R. \\nB. D’Agostino, and D. Levy, “Framingham risk score and prediction of lifetime \\nrisk for coronary heart disease,” Am. J. Cardiol., vol. 94, no. 1, pp. 20–24, Jul. \\n2004. \\n\",\n",
       " \"Graphical Abstract\\nBiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of\\xa0\\nAlcohol and Substance Use Disorder\\xa0with Electronic Health Records in Depression and PTSD Cohorts\\nMulti-Vocabulary EHR Codes Enhance Output Interpretability.\\nTruncated ICD-10 Reduces\\xa0Redundancy and\\xa0Model Complexity.\\nSPE + RoPE→ +9.2% AUROC, +20.4% AUPRC\\xa0compared to SPE Only\\nF35\\n264648\\nF48\\nDB1104\\nD40\\nF48\\n1\\nN34\\nDB1205\\nD40\\nD41\\n102301\\nDB1110\\nConvert to\\xa0Three-Sequence\\nBiPETE\\xa0Inputs\\nEHR Codes\\nDays-ago\\nVisit\\xa0\\nEHR Data Processing: BiPETE Input Generation\\nAbsolute\\xa0&\\xa0Relative\\xa0Visit-Level Positional Encoding\\nIntegrated Gradient:\\xa0Token-Level Attribution\\nSPE\\n(Absolute)\\nToken Embeddings\\nMulti-Head\\xa0\\nAttention\\xa0\\nSkip-Connection\\nxN\\xa0\\nEncoder\\nBlocks\\nLayer-Norm\\nK,Q\\nV\\nRoPE\\n(Relative)\\nFully Connected\\nLayer\\nSkip-Connection\\nLayer-Norm\\nRNN-based\\nBinary Classifier\\nPrediction\\n4\\n4\\n3\\n3\\n0\\n0\\n5\\n5\\n3\\n3\\n2\\n2\\n5\\n5\\n20\\n20\\n1\\n2\\n1\\n1\\n35 20\\n35\\n35\\n1\\n35\\nAssociated\\nEHR Codes\\nPatient\\nHospital Visits\\xa0\\nTrain BiPETE\\non\\xa0Binary\\nClassification\\xa0Task\\nEvaluate Using\\xa0\\n5-Fold\\xa0\\nCross Validation\\nCohort\\nData Source\\nNon-ASUD\\nASUD\\nBiPETE Architecture\\nMedication\\n(DB ID)\\nLab Test\\n(LOINC)\\nER Visit\\nDiagnosis\\n(ICD10)\\nEHR\\nVocab\\nPositive IGi(x): Attribution Towards ASUD\\nNegative\\xa0IGi(x): Attribution Away From ASUD\\n|IGi(x)|: Reflects Influence Strength\\nAggregate by Token Across Test Datasets to Identify Tokens\\nContributing to BiPETE's\\xa0Correct Predictions.\\n0 days ago\\n(Latest Visit)\\n5\\xa0days\\xa0ago\\n20\\xa0days\\xa0ago\\n4th\\xa0Visit\\n3rd\\xa0Visit\\n2nd\\xa0Visit\\n1st\\xa0Visit\\n35\\xa0days\\xa0ago\\nGradients of\\xa0\\nPrediction\\xa0Output w.r.t\\nTest Input Tokens\\nAverage\\xa0\\nith\\xa0Token\\nAttribution\\nIGi(x)\\nLinear Path\\nActual Input's Gradients\\nBaseline Input's Gradients\\nMedications Associated with Decreased ASUD Risk:\\xa0\\nHydroxyzine,\\xa0Escitalopram, Duloxetine, and Dextroamphetamine\\nDiagnosis Associated with Increased\\xa0ASUD\\xa0Risk:\\xa0\\nChronic Pain, Musculoskeletal Injuries, Gastrointestinal Conditions\\nAUROC & AUPRC: 96% & 93%\\xa0(Depression Cohort)\\xa0\\nAUROC & AUPRC:\\xa096% & 94%\\xa0(PTSD\\xa0Cohort)\\xa0\\narXiv:2511.04998v1  [cs.LG]  7 Nov 2025\\n\\nBiPETE: A Bi-Positional Embedding Transformer Encoder for\\nRisk Assessment of Alcohol and Substance Use Disorder with\\nElectronic Health Records\\nDaniel S. Lee1, Mayra S. Haedo-Cruz2, Chen Jiang1, Oshin Miranda1,\\nand LiRong Wang1\\n1Department of Pharmaceutical Sciences, University of Pittsburgh School of Pharmacy,\\nPittsburgh, PA, 15213, USA\\n2Division of Clinical and Translational Cancer Research, University of Puerto Rico\\nComprehensive Cancer Center, San Juan, PR, 00921, USA\\nAbstract\\nTransformer-based deep learning models have shown promise for disease risk\\nprediction using electronic health records (EHRs), but modeling temporal de-\\npendencies remains a key challenge due to irregular visit intervals and lack of\\nuniform structure. We propose a Bi-Positional Embedding Transformer Encoder\\nor BiPETE for single-disease prediction, which integrates rotary positional em-\\nbeddings to encode relative visit timing and sinusoidal embeddings to preserve\\nvisit order. Without relying on large-scale pretraining, BiPETE is trained on\\nEHR data from two mental health cohorts–depressive disorder and post-traumatic\\nstress disorder (PTSD)–to predict the risk of alcohol and substance use disorders\\n(ASUD). BiPETE outperforms baseline models, improving the area under the\\nprecision-recall curve (AUPRC) by 34% and 50% in the depression and PTSD\\ncohorts, respectively. An ablation study further confirms the effectiveness of the\\ndual positional encoding strategy. We apply the Integrated Gradients method\\nto interpret model predictions, identifying key clinical features associated with\\nASUD risk and protection, such as abnormal inflammatory, hematologic, and\\nmetabolic markers, as well as specific medications and comorbidities. Overall,\\nthese key clinical features identified by the attribution methods contribute to a\\ndeeper understanding of the risk assessment process and offer valuable clues for\\nmitigating potential risks. In summary, our study presents a practical and inter-\\npretable framework for disease risk prediction using EHR data, which can achieve\\nstrong performance.\\n1\\n\\nIntroduction\\nIn recent decades, the growing availability of electronic health records (EHRs), together\\nwith advances in machine learning and deep learning, has opened new avenues for en-\\nhancing patient-outcome prediction and have achieved notable successes [1, 2, 3]. Hier-\\narchical, longitudinal EHR data consists of large-scale, chronologically ordered records\\nof patients’ diagnoses, treatments, laboratory tests, and health trajectories [4], and this\\nstructure presents significant opportunities for leveraging transformer-based deep learn-\\ning models to predict outcomes and understand disease progression [5]. Despite these\\nadvances, a major challenge in predictive modeling for healthcare is capturing tempo-\\nral dependencies and inter-visit correlations, as the disease progression, comorbidities,\\nand treatment effects unfold over time [6, 7]. EHR data are primarily organized by\\nvisits, and their irregular and sparse intervals further complicate fine-grained temporal\\nmodeling in EHR data [8, 9]. To enable transformers to capture visit-level interactions\\namong EHR tokens that reflect real-world clinical patterns, input EHR code sequences\\nmust be carefully modeled.\\nTransformer models have demonstrated efficacy in learning complex interdependen-\\ncies within sequence data, a strength attributed to their attention mechanism [5, 10, 11].\\nConsequently, several specialized transformer-based architectures have been introduced\\nto model EHR data, each addressing temporal dependencies in distinct ways [12, 13].\\nBEHRT [10] and Med-BERT [6] both adopt the Bidirectional Encoder Representations\\nfrom Transformers (BERT) [7] framework, pretraining on large-scale EHR data to learn\\ncontextual representation of the EHR tokens, with the goal of fine-tuning on a wide\\nrange of downstream tasks. To model the timing of EHR code occurrences, Med-BERT\\nadds a learned positional embedding (LPE) applied to groups of codes within each pa-\\ntient visit, with an optional ordinal LPE assigned to individual codes [6]. BEHRT adds\\ntwo LPEs: age, and visit-segment embeddings. TransformEHR employs a transformer\\nencoder-decoder structure to pretrain on the task of predicting ICD codes for future\\npatient visits [14]. Similar to Med-BERT, it uses learned visit-level positional encoding,\\nsupplemented with sinusoidal positional embeddings (SPE) derived from visit dates to\\ncapture additional time information.\\nThese systems demonstrate the effectiveness of transformers for modeling EHR data\\nand a reliance on large-scale pretraining. However, both LPE and SPE are forms of\\nabsolute positional encoding (APE) and do not explicitly encode relative position infor-\\nmation of the tokens. While APE specifies the position of each token by adding a unique\\npositional vector to the token embeddings, the burden of learning relative dependencies\\nis left to the subsequent attention and linear layers in the transformer block. Another\\nlimitation of adding multiple APEs is the introduction of noise. Redundant positional\\ninformation could lead to signal superposition that distorts rather than enriches token\\nrepresentations.\\nAs in transformer-based natural language models, transformer-based models that in-\\nput EHR sequence can benefit from encoding relative visit-level temporal information\\nusing RoPE. RoPE encodes positional information through position-dependent rota-\\n2\\n\\ntions of token embeddings. This rotation mechanism enables effective modeling of local\\nrelationships while reducing long-ranged dependency effects [15]. Leveraging these in-\\nsights, we propose BiPETE, a Bi-Positional Embedding Transformer Encoder classifier\\nthat integrates both absolute and relative positional encodings–SPE and RoPE–which\\ncomplement each other to capture temporal dynamics of longitudinal structured EHR\\ndata for disease prediction. BiPETE’s RoPE provides a positional encoding in which\\nEHR token interactions are governed by their relative temporal separation in days be-\\ntween visits, enabling the model to leverage irregular, yet clinically meaningful time\\nintervals. In contrast, BiPETE’s SPE encodes absolute visit order, reinforcing sequenc-\\ning and grouping of the tokens.\\nTo enhance the representation of heterogeneous EHR data, we integrate multi-\\nple standard clinical vocabularies. Unlike BEHRT, MedBERT, and TransformerEHR,\\nBiPETE is not pretrained on large-scale EHR data, but trained on a moderate-sized\\ndataset for a single-disease prediction task (Table1).\\nWhile our approach reduces\\nthe data and computation requirements for deployment, the high-cardinality of multi-\\nvocabulary medical codes can complicate token representation and pattern learning\\n[9, 16, 17]. To address this, we grouped diagnosis codes–which constitutes the largest\\nproportion of our vocabulary–into broader categories that reflect common traits, related\\ndiseases, or conditions, thus preserving category-level information while reducing the\\nvocabulary size.\\nWe evaluate BiPETE on predicting the risk of subsequent alcohol and substance\\nuse disorders (ASUD) in two mental health disorder (MHD) cohorts derived from the\\nNational Institute of Health (NIH) All of Us (AoU) EHR data [18]: depressive dis-\\norders and post-traumatic stress disorder (PTSD) cohorts. Balancing accuracy with\\nexplainability is another key challenge in applying deep learning to health-outcome\\nprediction, as producing outputs that are interpretable and actionable for clinicians is\\nessential [19]. To enhance clinical interpretability, we apply integrated gradients (IG),\\na gradient-based feature attribution method [20, 21], to aggregate token-level attri-\\nbutions, yielding global estimates of EHR tokens that drive BiPETE’s prediction for\\nASUD and non-ASUD classes.\\nMethods\\n1\\nData Source and Cohort Definition\\nWe sourced our cohort data from the AoU Program, a NIH-funded cloud-based biomed-\\nical data repository and analysis platform. AoU data repository comprises clinical, ge-\\nnomic, demographic and other health-related data from de-identified participants aged\\n18 or older, collected across more than 50 healthcare organizations in the United States\\n[18]. We extracted standardized EHR data curated under Observational Medical Out-\\ncomes Partnership (OMOP) Common Data Model (CDM) from controlled-access tier.\\nThe analyses were conducted using data repository version 8, which includes EHR data\\n3\\n\\nCohort Stats\\nDepressive Disorder\\nPTSD\\nCOHORT SIZE\\n65,643\\n9,310\\nCASES : CTRL\\n20.6% : 79.4%\\n24.7% : 75.3%\\nVOCAB SIZE\\nICD10\\nDB ID\\nLOINC\\nER\\n33.1%\\n34.6%\\n32.1%\\n0.2%\\n% of N=4,904\\nICD10\\nDB ID\\nLOINC\\nER\\n41.6%\\n34.7%\\n23.4%\\n0.3%\\n% of N=3,535\\nDATASET-WIDE\\nFREQUENCY\\nDISTRIBUTION\\nICD10\\nDB ID\\nLOINC\\nER\\n65.8%\\n22.1%\\n11.2%\\n0.9%\\n% of N=4,712,501\\nICD10\\nDB ID\\nLOINC\\nER\\n65.5%\\n25.2%\\n8.3%\\n1.0%\\n% of N=749,261\\nTable 1. Characteristics of Depression and PTSD Cohorts. Cohort and data\\ndescriptions are generated after preprocessing EHR data into BiPETE input format.\\nVocabulary size denotes the size of EHR codes set for each standard vocabularies,\\nwhereas dataset-wide EHR code distribution refers to the frequency distribution of\\ncodes across the dataset. ICD10 statistics are derived after truncating codes to the first\\nthree characters.\\nfrom 394,596 participants. Data is available at AoU Research Hub upon registration.\\nTwo cohorts of patients with MHD, depressive disorders and PTSD, were con-\\nstructed using AoU Cohort Builder. AoU patient data are organized using standardized\\nmedical information referred to as “concepts”. The custom medical concept sets used to\\nbuild our cohorts are reported in Supplementary Table 3 and 4. Prior to preprocessing,\\nthe depression and PTSD cohorts comprised 84,163 and 15,334 patients, respectively\\n(Supplementary Table 1). Following data preprocessing, cohort sizes were reduced to\\n65,643 for depression and 9,310 for PTSD (Table 1).\\nFor cohort EHR data preprocessing, we defined the following clinical scenario: a\\npatient with MHD diagnosis—either depressive disorders or PTSD—but without a prior\\ndiagnosis of ASUD presents at a clinic. The current risk of new-onset ASUD is assessed\\nbased on the patient’s medical history from the preceding 15 months. The patient is\\nlabeled as a case if diagnosed with ASUD at a subsequent visit, and as a control if not\\ndiagnosed with ASUD (Figure 1). For control patients, the medical history from their\\nmost recent visits was used while ensuring no ASUD diagnosis was recorded during\\ntheir entire observation period.\\n4\\n\\nProcessing Participant Electronic Health Record Data\\nBiPETE\\n...\\nMHD\\nDx\\nVisit\\n1\\nVisit\\n2\\nVisit\\n3\\n...\\n...\\nMHD\\nDx\\nVisit\\n1\\nVisit\\n2\\nVisit\\n3\\n...\\nVisit\\nN\\nSinusoidal Positional\\nEmbedings\\nRotary Positional\\nEmbeddings\\nMulti-Head\\xa0\\nAttention\\xa0\\nSkip-Connection\\nFully Connected\\nxM\\nEncoder\\nBlocks\\nEHR token\\nEmbeddings\\nRNN-based\\nBinary Classifier\\nRaw Logit\\nPrediction\\nLayer-Norm\\nLayer-Norm\\nSkip-Connection\\nK,Q\\nV\\nEHR Codes\\nDays-ago Indices\\nVisit Indices\\nixx\\nmxx\\nmxx\\ndxx\\ndbxxxxx\\n1234567\\n...\\ndbxxxxx\\n1\\n1\\n3\\n2\\n3\\n3\\n...\\nN\\n50\\n50\\n10\\n24\\n10\\n10\\n...\\n0\\nConvert to BiPETE Input Format\\nExtract EHR Codes\\xa0from 1\\xa0Year & 3 Months Observation Period\\n* Diagnosis\\n(ICD10CM First 3 Letters)\\n* Administered Drugs\\n(Drugbank ID)\\n* Abnormal Measurements\\n(LOINC)\\n* Number of ER Visits\\n1 Year & 3 Months\\nObservation Period\\nClass 1\\n(MHD & ASUD)\\nClass 0\\xa0\\n(MHD)\\nLast\\xa0\\nrecorded\\xa0visit\\nVisit w/\\nASUD Event\\nVisit\\nN\\nFigure 1. Pipeline Flowchart Illustrating Data Preprocessing, Model Input\\nConstruction, and BiPETE Architecture. MHD Dx refers to diagnosis of Mental\\nHealth Disorder. For ASUD and non-ASUD classes, only the EHR codes from visits\\noccurring after MHD diagnosis are extracted. Repeated values within the Visit and\\nDays-ago index sequences indicate that the corresponding EHR codes were recorded\\nduring the same clinical visit. In the BiPETE architecture, the K, Q, V are the key,\\nquery and value embeddings, respectively, and M refers to the number of encoder blocks.\\n2\\nEHR Preprocessing and Input Representation\\n2.1\\nPreprocessing Multi-Vocabulary EHR\\nStructured EHRs offer a wealth of patient information encompassing diagnosis, med-\\nication, laboratory tests and other clinical data.\\nTo capture this rich clinical con-\\ntext, we constructed input instances using standard vocabularies, including the Inter-\\nnational Classification of Diseases, Tenth Revision (ICD10) for diagnosis, DrugBank\\nIdentifiers (DBID) for medications, and Logical Observation Identifiers Names and\\nCodes (LOINC) for laboratory measurements. Laboratory tests were restricted to those\\nflagged abnormal–either high or low–emphasizing clinically significant recordings. Ad-\\nditionally, the number of emergency room (ER) visits associated with each visit was\\nincorporated as an indicator of acute events.\\n5\\n\\nOMOP CDM organizes clinical information using relational tables, requiring a series\\nof preprocess steps to construct EHR input sequences. After defining our cohorts, we\\nextracted medical concepts for diagnosis, medications and measurements for each pa-\\ntient using visit occurrence identifiers and their corresponding visit date ranges, defined\\nby a start and end date. To reduce time-series fragmentation, patient visits occurring\\nwithin a 7-day window were merged into a single visit, grouping temporally related\\nencounters and analyzing them as a single episode of care. Patients with fewer than\\nthree visits within the observation window were excluded to ensure sufficient clinical\\ncontext per input instance.\\nThe extracted concepts were mapped to standardized vocabularies and consolidated\\nto represent the clinical events occurring within visits to generate sequential EHR code\\ndata. There are visits with no recorded or extracted EHR codes, and a unique token\\nwas assigned to such visits to denote the absence of clinical information.\\nTo reduce model complexity and sparsity in the input representation, ICD10 diag-\\nnosis codes were truncated to their three-character root categories, aggregating related\\ndiagnoses into broader groups. This grouping increased token frequency, mitigated re-\\ndundancy and enhanced model efficiency by reducing vocabulary size. Furthermore,\\nthe patient’s EHR history was restricted to a 15-month observation window to focus\\non information most predictive of our classification task.\\n2.2\\nTemporal Modeling with Auxiliary Sequences\\nBiPETE processes a sequence of EHR codes alongside two auxiliary sequences to encode\\ntemporality: visit indices and days-ago indices (Figure 1). In line with prior studies\\n[10, 12], visit indices follow the chronological order of a patient’s visits, with the earliest\\nvisit indexed as 0. Days-ago indices are computed as the time difference in days between\\neach visit and the most recent recorded visit in the patient’s medical record, with codes\\nfrom the most recent visit indexed as 0. The temporal information represented by visit\\nindices and encoded using SPE is hereafter referred to as visit embeddings, while that\\nrepresented by days-ago indices and encoded using RoPE is referred to as days-ago\\nembeddings.\\nDays-ago embeddings capture the relational dynamics of EHR code occurrences be-\\ntween visits. RoPE applies rotation transformations to query and key embeddings of\\nEHR tokens, such that the dot product between queries and keys depend on their rela-\\ntive positions, decaying interdependence with increased relative distance between them\\n[12]. In simpler terms, RoPE explicitly expresses short- and long-ranged token relation-\\nships, assisting the transformer’s attention mechanism to learn the interaction patterns.\\nAll codes within a single visit undergo the same rotational transformation, effectively\\ngrouping them together into a visit while capturing relative temporal relationships of\\nvisits.\\nPatient visits are intermittent and visit gaps vary widely across patients. Applying\\nRoPE directly to such varying days-ago indices can complicate model learning, as each\\nindex produces a different rotation based on the gap between visits. To strengthen visit\\n6\\n\\norder and token grouping, we encode visit order using SPE, applying the same absolute\\npositional encoding to the codes recorded within a visit.\\n3\\nBiPETE Architecture\\nBiPETE is a transformer-based classifier adapted from BERT [11] with architectural\\nmodifications to the encoder and an addition of a bidirectional gated recurrent unit\\n(BiGRU) classifier head (Figure 1). Unlike the original BERT, BiPETE employs the\\nbidirectional self-attention encoder without leveraging pre-training. The model param-\\neters are randomly initialized, and the token relationships are learned solely in the\\ncontext of the single-disease classification task. Two modifications are applied to the\\nencoder block: RoPE embeddings are applied to the query and key representations\\nof tokens, and Pre-LayerNorm configuration is used. After the input EHR token se-\\nquence is processed by stacked encoder blocks, the GRU head generates a single output\\nlogit, which is passed through a sigmoid and optimized with Binary Cross-Entropy loss\\nagainst the ASUD or non-ASUD label.\\n4\\nModel Training and Evaluation\\n4.1\\nModel Training and Cross-validation\\nWe evaluated BiPETE with the ASUD risk prediction task on depressive disorders and\\nPTSD cohorts and employed five-fold cross-validation to assess model generalizability\\n[22]. For each iteration, the data was partitioned into training, validation and test\\nsubsets in a 7:1:2 ratio. Across the iterations, each fold served once as a test set.\\nA 6-encoder-layer architecture was used to train and evaluate the model on the\\ndepression cohort, resulting in a 48-million-parameter model. For the PTSD cohort, a\\nreduced 3-encoder-layer version was employed due to its smaller dataset size, resulting\\nin a 35-million-parameter model. The size of large language models must scale with\\ndataset size to improve performance. A study on compute-optimal training suggested\\nthat, for a given compute budget, model parameters and training data should be scaled\\nat equal proportion, with approximately 20 training tokens per parameter to achieve\\noptimal balance between performance and compute efficiency [23]. In our case, although\\nthe models were larger relative to the dataset size, ablation studies indicated that our\\nconfiguration, combined with early stopping based on average validation loss across\\nfolds, yielded the best performance.\\nTo assess the impact of the positional encodings on model performance, we con-\\nducted ablation experiments in the depression cohort by excluding visit or days-ago\\nembedding, comparing training and test metrics for models configured with visit em-\\nbedding alone, days-ago embedding alone, and both combined. For comparability, all\\nmodels were trained using the same random seeds, deterministic computation settings\\nand five-fold cross-validation protocol.\\n7\\n\\n4.2\\nEvaluation Metrics\\nWe report area under receiver operator characteristics curve (AUROC) and area under\\nprecision-recall curve (AUPRC) as primary performance evaluation metrics. Both AU-\\nROC and AUPRC deliver threshold-independent assessments by summarizing model\\nperformance across a range of decision thresholds.\\nAUROC measures the trade-off\\nbetween true-positive rate and false-positive rate and is relatively robust to class im-\\nbalance, as it can emphasize the model’s ability to identify positives amidst a large\\nproportion of negatives. AUPRC captures trade-off between precision and recall across\\nall decision thresholds. It reflects the effect of missed cases via recall and is well-suited\\nfor evaluating disease risk prediction characterized by rare positive instances and high\\ncosts associated with missed detections. In addition to AUROC and AUPRC, we report\\npositive predictive value (PPV), sensitivity, and specificity on three different thresholds\\nto communicate interpretable evaluation of model performance. All metrics are reported\\nas the mean and standard deviation across the five held-out test folds. Coefficient of\\nvariation (CV) across the thresholds is calculated across the models to compare the\\ndispersion of the metrics.\\n4.3\\nBaseline Models Used for Benchmarking\\nWe compared BiPETE against three baseline models: BiGRU, Logistic Regression (LR)\\nand Bernoulli Naive Bayes (BNB). For LR and BNB, the EHR data was represented\\nas one-hot encoded vectors, whereas for BiGRU, EHR codes were transformed to token\\nembeddings without visit and days-ago embeddings. LR is a robust classifier and was\\nincluded to assess whether the data exhibits linear separability.\\nBNB was used to\\nevaluate whether the presence or absence of EHR codes could predict ASUD risk,\\nassuming conditional independence among the codes as in the Naive Bayes framework.\\nBiGRU served as a deep learning baseline capable of modeling sequential dependencies\\nin EHR data. All baseline models were trained and evaluated under the same five-fold\\ncross-validation setup as BiPETE to ensure consistency.\\n5\\nAttribution Analysis\\n5.1\\nIntegrated Gradients\\nTo interpret the contribution of the EHR codes to ASUD prediction within our cohorts,\\nwe employed Integrated Gradient (IG) to compute token-level attributions. IG provides\\nfeature attributions that quantify the model’s sensitivity to changes in each input,\\ncapturing variations in each feature that affect the prediction [20]. Grounded in integral\\ncalculus, IG follows axiomatic principles that enable instance-level attributions to be\\nnaturally aggregated into global, dataset-level insights.\\nIG requires a predefined, uninformative baseline input to serve as a reference for\\nmeasuring attribution. The completeness axiom ensures that the sum of all feature\\nattributions equals the difference between the model output and the reference output,\\n8\\n\\nwhile the sensitivity axiom guarantees that only features influencing the prediction\\nreceive nonzero attributions [20]. Tokens that consistently drive predictions away from\\nthe baseline yield positive or negative contributions, whereas uninformative features\\nconverge toward zero.\\nIn practice, IG computes the attribution for each token by\\nintegrating the gradient of the model output with respect to the input along a linear\\npath from the baseline input to the actual input. Feature attribution IGi(x) of token\\ni in input x is formally written as:\\nIGi(x) =\\n\\x00xi −x′\\ni\\n\\x01\\n·\\nZ 1\\nα=0\\n∂F(x′ + α (x −x′))\\n∂xi\\ndα\\n(1)\\nwhere x′ denotes the baseline input vector, α ∈[0,1] is a scalar parameter tracing the\\nstraight-line path from x′ to x, and F represents the prediction function. For deep\\nlearning models, this path integral is numerically approximated by averaging gradients\\nover interpolated inputs. The approximation of IGi(x) of token i in input x is formally\\nwritten as:\\nIGi(x) ≈(xi −x′\\ni) · 1\\nm\\nm\\nX\\nk=1\\n∂F\\n\\x00x′ + k\\nm (x −x′)\\n\\x01\\n∂xi\\n(2)\\nwhere x′ denotes the baseline input vector, x is the input being explained, F represents\\nthe prediction function, m is the number of interpolated steps and k is the index of the\\nsteps. In our study, the baseline input is defined as a sequence of padding tokens. To\\nensure consistency in the attribution of tokens across instances with varying sequence\\nlength and positions, we retain the visit and days-ago embeddings of the original input\\nwhen constructing the baseline.\\nIG token attributions quantify the extent to which each feature influences the pre-\\ndicted label, with positive values increasing and negative values decreasing the predicted\\nlikelihood. In our ASUD classification task, the attributions of EHR codes reflect the\\nmodel’s internal reasoning rather than absolute risk.\\n5.2\\nAggregating Token Contribution: Relative Contribution\\nWe compute the Relative Contribution (RC) to quantify the directional attribution at\\nthe token level, defined as the ratio of a token’s average attribution in the true positive\\n(TP) group to its average attribution in the true negative (TN) group. Only instances\\ncorrectly predicted by the model are used in the RC calculation to extract meaningful\\nattribution interpretation [10]. Tokens showing opposite attribution signs across TP and\\nTN groups were excluded, as they indicate inconsistent directional effects. Denoting\\nATP(t) as the average attribution of token t in the true positive (TP) group, and ATN(t)\\nas the average attribution in the true negative (TN) group, the RC of token t is then\\ncalculated as:\\nRC(t) = ATP(t)\\nATN(t)\\n(3)\\nRC values greater than 1 indicate contribution toward the ASUD class, whereas values\\nless than 1 indicate contribution toward the non-ASUD class. To ensure statistical\\n9\\n\\nrobustness and reduce the influence of outliers, we restrict our analysis on tokens that\\nappear in at least 1% of instances within each TP and TN group. For tokens occurring\\nmultiple times within a single instance, attribution values are averaged. RC values are\\ncalculated using the attribution values from the five held-out test sets.\\nResults\\n1\\nCohort Characteristics After Preprocessing\\nCohort EHR vocabulary sizes and distributions are shown in Table 1.\\nWithin the\\ndepression cohort, ICD10, DBID, and LOINC codes contribute approximately equally\\nto the total EHR vocabulary, whereas in the PTSD cohort, the distribution is more\\nuneven. Nevertheless, the dataset-wide frequency distributions are similar across both\\ncohorts. Truncating ICD10 codes reduced model complexity and redundancy in clinical\\ninformation. While this introduces loss in diagnostic specificity, it reduced the ICD10\\nvocabulary size by 95%, from 32,662 unique codes to 1,625 in the depression cohort\\nand by 93%, from 21,137 unique codes to 1,469 in the PTSD cohort. Most data loss\\noccurred during the extraction and mapping of visit-level EHR codes to standardized\\nvocabularies–ICD10, DBID, and LOINC–which reduced the number of usable visits in\\nmany patient records. As a result, 22% of samples in the depression cohort and 39% in\\nthe PTSD cohort were removed (Supplementary Table 1).\\n2\\nBiPETE Performance on ASUD Risk Prediction\\nAUROC and AUPRC scores of BiPETE’s ASUD risk prediction in depression disor-\\nder and PTSD cohorts are shown in Table 2. On the AUROC, BiPETE with both\\ndays-ago and visit embeddings obtain the best results in the depression cohort with\\na score of 96.46%, outperforming the baseline models–BiGRU, LR, and BNB–which\\nscored 85.21%, 83.54% and 77.17%, respectively. The AUPRC score of 93.18% shows a\\nperformance gain of over 30% compared to the three baseline models. A similar pattern\\nfor both metrics is observed in the PTSD cohort.\\nSupplementary Table 3 compares PPV, sensitivity and specificity of BiPETE, Bi-\\nGRU and LR in the decision thresholds of 0.2, 0.5 and 0.8. As expected, increasing\\nthe decision threshold leads to higher PPV and specificity, but lower sensitivity for all\\nmodels. Though this trend is consistent, the CV values for BiPETE metrics remain\\nconsistently lower than those of BiGRU and LR, indicating BiPETE achieves more\\nstable and robust class separation across thresholds.\\n3\\nImpact of Visit and Days-ago Embeddings on Performance\\nIn the depression cohort, we report AUROC and AUPRC scores of BiPETE and its\\nvariants configured with only days-ago embeddings or only visit embeddings to com-\\n10\\n\\nModels\\nAUROC (%)\\nAUPRC (%)\\nDepressive\\nDisorder\\nBiPETE\\n96.46 (±0.21)\\n93.18 (±0.27)\\nRoPE Only\\n94.46 (±0.88)\\n90.73 (±1.17)\\nSPE Only\\n88.33 (±0.35)\\n77.34 (±0.48)\\nBiGRU\\n85.21 (±0.24)\\n69.42 (±0.35)\\nLinear Regression\\n83.54 (±0.47)\\n67.79 (±0.30)\\nBernoulli Naive Bayes\\n77.17 (±0.51)\\n46.73 (±0.77)\\nPTSD\\nBiPETE\\n96.50 (±0.40)\\n94.04 (±0.63)\\nBiGRU\\n80.13 (±0.53)\\n62.29 (±0.24)\\nLinear Regression\\n83.97 (±0.68)\\n70.86 (±1.53)\\nBernoulli Naive Bayes\\n93.59 (±5.44)\\n87.42 (±9.33)\\nTable 2.\\nAUROC and AUPRC Scores for ASUD Risk Prediction using\\nBiPETE and Baseline Models in Depression and PTSD Cohorts. The depres-\\nsion cohort includes two additional rows for positional encoding variants: one using\\nonly days-ago embeddings (RoPE) and the other only visit embeddings (SPE). The\\nbaseline BiGRU shares the same configuration as the BiGRU used as the classifier head\\nin BiPETE. ±indicates standard deviation across five-fold cross-validation.\\npare the impact of different positional embedding strategies. BiPETE outperforms the\\nsingle positional embedding models in both metrics (Table 2). Compared to BiGRU,\\nBiPETE, days-ago-embedding-only model, and visit-embedding-only model improved\\nin AUPRC by 34.23%, 30.70% and 11.41%, respectively.\\nIn addition to achieving\\nhigher AUROC and AUPRC scores, BiPETE exhibits lower standard deviations than\\nthe single-positional-embedding models, indicating a more consistent performance than\\nthe models with a single positional encoding. The average ROC and PR curves of the\\ncross-validation folds for the BiPETE and its positional encoding variant models are\\npresented in Supplementary Figure 1.\\nFigure 2 illustrates the learning behavior of the positional encoding variant models\\nduring training. We report training and validation metrics–including loss, accuracy and\\nROAUC–recorded over the course of 30 epochs. Among the configurations, BiPETE\\ndemonstrates the fastest convergence and exhibits the most stable training performance,\\nindicated by the narrow error bars across all training metrics. BiPETE’s train accu-\\nracy reaches close to 100%, showing near-perfect fit to the training data and signs of\\noverfitting, as evidenced by the increase in validation loss after epoch 15. The days-\\nago-embedding-only model achieves validation metrics comparable to those of BiPETE,\\nbut the validation loss and accuracy exhibit high instability during training, reflected in\\nthe large standard deviations. The visit-embedding-only model shows training stability\\nsimilar to the model with both embeddings, but underperforms across both training\\n11\\n\\n0.18\\n0.14\\n0.10\\n0.06\\n0.02\\nTrain\\nLoss\\n70\\n75\\n80\\n85\\n90\\n95\\n100\\nAccuracy\\n0.7\\n0.8\\n0.9\\n1.0\\nAUROC\\n0.9\\n0.7\\n0.5\\n0.3\\n0.1\\nValidation\\n70\\n75\\n80\\n85\\n90\\n95\\n100\\n0.7\\n0.8\\n0.9\\n1.0\\nBoth Emb\\nOnly RoPE\\nOnly SPE\\nFigure 2. Training and Validation Performance of Classifiers with Different\\nPositional Encoding Configurations. Training and validation loss, accuracy and\\nAUROC are reported across 30 training epochs to compare model learning and general-\\nization. The error bars indicate the standard deviation across five-fold cross-validation\\nfolds.\\nand validation metrics.\\nSupplementary Figure 2 presents the attention matrices of the nine heads in the\\nfinal BiPETE layer for a representative instance, illustrating the effect of visit and\\ndays-ago embeddings.\\nEach square matrix shows token-to-token attention scores of\\nthe input sequence. Both axes are annotated with the EHR vocabulary type and the\\nvisit indices, with the y axis representing tokens attending to those on the x axis.\\nIdeally, different heads capture distinct aspects of the input EHR sequence, enabling\\nthe model to extract meaningful patterns for prediction, and the visualization shows\\nthat individual heads attend to different visit pairs. Tokens tend to exhibit high or low\\nattention in groups according to their visit indices. Attention scores vary within the\\nvisits as well. Together, inter- and intra-visit dynamics can be seen in the attention\\nmaps.\\n4\\nIG-Derived Indicators of Increased and Reduced ASUD Risk\\nWe performed token-level feature attribution using IG and calculated RC by averaging\\nthe token attributions of correctly predicted test samples. RC quantifies the influence\\nof each token on BiPETE’s prediction in the true positive group relative to the true\\nnegative group. We identified potential markers that drove BiPETE’s prediction to-\\nward ASUD or Non-ASUD in both the Depression and PTSD cohorts. The identified\\nindicators–including, abnormal lab test results, medication use, and comorbidities–are\\nreported for each cohort.\\n12\\n\\n4.1\\nIndicators Associated with Higher ASUD Risk in Depression Cohort–Table 3\\nAlterations in lymphocyte-related markers, including the neutrophil-to-lymphocyte ra-\\ntio (NLR) and platelet-to-lymphocyte ratio reflect chronic inflammation that can influ-\\nence neurotransmission and brain function, with evidence suggesting both neurotoxic\\nand neuroprotective effects depending on context [48, 49]. Depression and SUD fre-\\nquently co-occur and share overlapping immune dysfunction, as substances like alcohol\\nand opioids directly impair immune function, while elevated pro-inflammatory cytokines\\ncorrelate with depressive symptoms. Similarly, coagulation markers, such as prolonged\\nprothrombin time (PT) and altered international normalized ratio (INR), are affected\\nby substance use particularly chronic alcohol use through liver dysfunction, and by\\ndepression via medication adherence challenges, highlighting the bidirectional interac-\\ntion between immune and hemostatic pathways and psychiatric and substance-related\\noutcomes [49].\\nVancomycin, while not psychoactive, may intersect with ASUD risk in patients with\\nrecent substance use or complex medical conditions, as altered pharmacokinetics and\\nhospitalization-related stressors can create vulnerabilities for maladaptive substance\\nbehaviors [50]. Metronidazole may increase susceptibility to ASUD via disulfiram-like\\nreactions when alcohol is consumed during or shortly after therapy, producing aversive\\nphysiological effects such as nausea, tachycardia, and abdominal discomfort that can\\npromote maladaptive drinking or relapse [51]. Acyclovir carries potential for neurotoxic\\nside effects, particularly in older adults, patients with renal impairment, or those with\\nmultiple comorbidities, with symptoms such as confusion, ataxia, and altered mental\\nstatus linked to elevated drug or metabolite concentrations and blood–brain barrier\\ndisruption [52]. These neurocognitive effects may impair mood regulation and inhibitory\\ncontrol, creating conditions for self-medication with alcohol or other substances.\\nConditions such as peripheral vascular disease, thyroid disorders, flatulence and re-\\nlated gastrointestinal conditions, hereditary neuropathies, and primary hypertension\\nIndicators of High Risk\\nRisk Marker Type\\nRC\\nN (Case/Ctrl)\\nLymphocytes / Total Leukocytes in Blood\\nAbnormal Lab test result\\n50.7491\\n912/2703\\nInternational Norm Ratio (Platelet Poor Plasma)\\nAbnormal Lab test result\\n22.8152\\n408/938\\nProthrombin Time (PT)\\nAbnormal Lab test result\\n11.1948\\n560/1262\\nVancomycin\\nMedication use\\n5.2091\\n262/982\\nMetronidazole\\nMedication use\\n2.4416\\n522/1515\\nAcyclovir\\nMedication use\\n2.2308\\n156/520\\nOther Peripheral Vascular Diseases\\nComorbidities\\n40.5746\\n358/1575\\nOther Disorders of Thyroid\\nComorbidities\\n21.1796\\n175/717\\nFlatulence and Related Conditions\\nComorbidities\\n0.0644\\n300/1317\\nHereditary and Idiopathic Neuropathy\\nComorbidities\\n15.3311\\n360/811\\nEssential (primary) Hypertension\\nComorbidities\\n9.7897\\n4974/20857\\nTable 3. Key Indicators Associated with Increased Risk of ASUD in Patients\\nwith Depressive Disorder.\\n13\\n\\noften require ongoing management and can involve chronic pain, fatigue, or functional\\nimpairment. These stressors may dysregulate the hypothalamic-pituitary-adrenal axis\\nand alter dopaminergic and serotonergic signaling in reward pathways, increasing sus-\\nceptibility to substance misuse as a coping strategy. Additionally, chronic illness and\\ncomorbidities can contribute to systemic inflammation, oxidative stress, and neuro-\\nplasticity alterations, which may amplify depressive symptoms and further predispose\\npatients to SUD. The cumulative physiological burden, along with potential medica-\\ntion side effects and impaired autonomic or gastrointestinal function, can create a cycle\\nof heightened stress and vulnerability, highlighting the importance of comprehensive\\nmedical and psychiatric management to mitigate substance misuse risk in these popu-\\nlations.\\n4.2\\nIndicators Associated with Lower ASUD Risk in Depression Cohort–Table 4\\nPlatelets and red blood cell indices, such as mean corpuscular hemoglobin concentration\\n(MCHC), may play interconnected roles in depression and ASUD through mechanisms\\ninvolving serotonin signaling, inflammation, and oxygen delivery [53, 54]. Platelets,\\nwhich store the majority of the body’s serotonin, reflect central serotonergic activity\\nand may be hyperactivated in depression [24]. Chronic use of substances such as alcohol,\\ncocaine, heroin, and methamphetamine can induce thrombocytopenia via bone marrow\\nsuppression, immune-mediated destruction, or increased consumption, further impair-\\ning neurocognitive function and contributing to mood dysregulation. Similarly, low\\nMCHC, often resulting from iron deficiency anemia or chronic substance use, reduces\\noxygen delivery to neurons, disrupting neurotransmitter synthesis and exacerbating de-\\npressive symptoms [53, 54]. Platelet abnormalities and low MCHC typically reflect the\\nphysiological consequences of substance use and depression, highlighting the potential\\nbenefit of interventions targeting these parameters to support mood stabilization and\\nreduce indirect risk factors for SUD.\\nNaloxone, a rapid opioid antagonist, remains a cornerstone of harm reduction,\\nwith community-based distribution programs achieving survival rates above 92–98%\\n[55, 56, 57, 58]. β-lactam antibiotics such as cefazolin [59, 60, 61], amoxicillin [62, 63],\\nand clavulanic acid [64, 65, 66, 67] modulate glutamatergic signaling by upregulat-\\ning GLT-1 and the cystine-glutamate exchanger in addiction-relevant brain regions,\\nattenuating ethanol or cocaine reward, reducing cue-induced reinstatement, and facil-\\nitating extinction of drug-seeking behaviors. Other agents, including hydroxyzine (es-\\npecially in combination with 5-HT3 antagonists), famotidine [68], and dexamethasone\\n[69, 70], show promise in mitigating withdrawal symptoms, stress, or neuroinflamma-\\ntion, which are key risk factors for relapse. Psychostimulants like dextroamphetamine\\n[69, 71, 72, 73], when administered under clinical supervision, reduce substance use risk\\nby addressing underlying neurochemical and behavioral deficits, supported by both\\npopulation-level and preclinical data.\\nConditions such as other specified health status, intracranial injury, extrapyramidal\\nand movement disorders, and male erectile dysfunction are generally chronic but man-\\n14\\n\\nIndicators of Low Risk\\nType of Risk Marker\\nRC\\nN (Case/Ctrl)\\nMCHC [Mass/volume]\\nAbnormal Lab test result\\n0.4432\\n1152/4102\\nPlatelets [#/volume] in Blood\\nAbnormal Lab test result\\n0.9242\\n290/606\\nNaloxone\\nMedication use\\n0.0115\\n253/928\\nCefazolin\\nMedication use\\n0.0236\\n418/2721\\nAmoxicillin\\nMedication use\\n0.0457\\n867/4141\\nFamotidine\\nMedication use\\n0.0483\\n498/3312\\nDexamethasone\\nMedication use\\n0.3031\\n521/2962\\nDextroamphetamine\\nMedication use\\n0.3056\\n169/903\\nClavulanic acid\\nMedication use\\n0.3480\\n440/2260\\nDuloxetine\\nMedication use\\n0.3664\\n648/2621\\nOther Specified Health Status\\nComorbidities\\n0.0009\\n403/2811\\nIntracranial Injury\\nComorbidities\\n0.0031\\n216/680\\nOther Extrapyramidal and Movement Disorders\\nComorbidities\\n0.0067\\n536/2429\\nMale Erectile Dysfunction\\nComorbidities\\n0.0095\\n368/1168\\nTable 4. Key Indicators Associated with Decreased Risk of ASUD in Pa-\\ntients with Depressive Disorder.\\nageable, often requiring structured medical oversight or ongoing outpatient care [74].\\nThese conditions may promote regular engagement with healthcare providers, facili-\\ntate adherence to treatment routines, and provide early opportunities for monitoring\\nmental health, all of which can reduce reliance on maladaptive coping strategies such\\nas substance use. While these disorders may impact daily functioning, their gener-\\nally predictable course allows patients to maintain relative emotional and physiological\\nstability, limiting the severity of depressive symptoms that often drive self-medication.\\n4.3\\nIndicators Associated with Higher ASUD Risk in PTSD Cohort–Table 5\\nWe found the presence of abnormalities in hematological and metabolic markers which\\nmay highlight stress-related dysregulation in PTSD and comorbid substance use dis-\\norder (SUD). Serum albumin and albuminuria reflect hepatic and renal reserve, with\\nhypoalbuminemia and elevated albuminuria signaling systemic inflammation and vas-\\ncular risk [25]. Mean platelet volume reflects systemic stress and shows mixed patterns\\nin PTSD, with elevated levels indicating platelet activation and inflammation linked to\\nSUD [24]. Mean corpuscular hemoglobin abnormalities, often worsened by alcohol use,\\ncontribute to fatigue and cognitive impairment. Together, these markers highlight the\\ncompounded biological burden of PTSD and SUD.\\nSeveral commonly prescribed medications can influence the risk of SUD in patients\\nwith PTSD, with effects varying based on neurobiological vulnerability and prior sub-\\nstance use history. Hydrocodone, widely used for pain management, is strongly asso-\\nciated with increased risk of opioid misuse and dependence in trauma-exposed adults,\\nparticularly among women, younger adults (18–34 years), and those with prior SUD,\\nlikely due to dysregulated dopaminergic and endogenous opioid pathways that amplify\\nits reinforcing euphoric effects [26, 27, 28]. Oxybutynin has also been reported in case\\n15\\n\\nIndicators of High Risk\\nRisk Marker Type\\nRC\\nN (Case/Ctrl)\\nAlbumin [Mass/Volume] in Serum/Plasma\\nAbnormal Lab test result\\n80.5480\\n133/231\\nPlatelet mean volume [Entitic Volume] in Blood\\nAbnormal Lab test result\\n38.7960\\n40/154\\nMean corpuscular hemoglobin [Entitic Mass]\\nAbnormal Lab test result\\n3.6885\\n171/358\\nHydrocodone\\nMedication use\\n138.1751\\n192/364\\nOxybutynin\\nMedication use\\n30.3633\\n35/97\\nTrimethoprim/Sulfamethoxazole\\nMedication use\\n16.4505\\n69/182\\nAripiprazole\\nMedication use\\n10.1848\\n85/215\\nMononeuropathies of Lower Limb\\nComorbidities\\n270.3382\\n50/146\\nDislocation/Sprain of Ankle, Foot, or Toe\\nComorbidities\\n26.4504\\n60/109\\nDental Caries\\nComorbidities\\n20.9523\\n169/345\\nFibroblastic Disorders\\nComorbidities\\n19.0740\\n73/214\\nTable 5. Key Indicators Associated with Increased Risk of ASUD in Patients\\nwith PTSD.\\nseries to carry abuse potential because of hallucinogenic and euphoric CNS effects,\\nsuggesting heightened vulnerability in patients with prior SUD [29, 30].\\nTrimetho-\\nprim–sulfamethoxazole (TMP-SMX) has been implicated in neuropsychiatric adverse\\neffects such as hallucinations, particularly in older adults, which may indirectly elevate\\nSUD risk [31, 32]. Aripiprazole exhibits mixed effects: long-acting injectable formula-\\ntions have been associated with improvements in psychiatric symptoms and reductions\\nin alcohol and cocaine use among patients with PTSD or schizophrenia and co-occurring\\nSUD, while other reports caution that it may exacerbate compulsive substance use urges\\nin some individuals, consistent with FDA warnings on impulse control disorders.\\nNeurological conditions such as mononeuropathies of the lower limb and muscu-\\nloskeletal injuries (dislocations and sprains of the ankle, foot, and toe) contribute to\\nchronic pain and functional impairment, often leading to self-medication. Dental caries\\nand fibroblastic disorders reflect ongoing disease burden that may exacerbate stress and\\npromote maladaptive coping. Alongside psychiatric comorbidities and high healthcare\\nutilization, these conditions collectively amplify the biological and psychological burden\\nseen in these patients [33, 34, 35].\\n4.4\\nIndicators Associated with Lower ASUD Risk in PTSD Cohort–Table 6\\nIron, particularly ferritin, and Vitamin B12, which are critical micronutrients, were\\nfound to influence the risk of ASUD in patients with PTSD. Vitamin B12 deficiency\\nhas been linked to both PTSD and SUD, likely due to its essential role in monoamine\\nneurotransmitter synthesis and mood regulation. Lower B12 levels in individuals with\\nalcohol or methamphetamine use have been associated with increased relapse risk, and\\nsupplementation may help restore energy, cognitive function, and emotional regula-\\ntion, indirectly mitigating ASUD risk by reducing self-medication behaviors. Similarly,\\niron homeostasis, especially ferritin levels, is vital for dopaminergic neurotransmission,\\noxidative stress regulation, and myelination. Disruption of iron balance can lead to\\n16\\n\\nIndicators of Low Risk\\nRisk Marker Type\\nRC\\nN (Case/Ctrl)\\nFerritin [Mass/volume] in Serum/Plasma\\nAbnormal Lab test result\\n0.4231\\n48/108\\nVitamin B12 [Mass/volume] in Serum/Plasma\\nAbnormal Lab test result\\n0.5072\\n70/86\\nLidocaine\\nMedication use\\n0.0121\\n210/1248\\nHydroxyzine\\nMedication use\\n0.0838\\n152/529\\nLamotrigine\\nMedication use\\n0.1219\\n73/240\\nEscitalopram\\nMedication use\\n0.2128\\n78/407\\nDuloxetine\\nMedication use\\n0.5460\\n111/489\\nErgocalciferol\\nMedication use\\n0.6546\\n40/222\\nCeftriaxone\\nMedication use\\n0.7809\\n51/168\\nDisorder of Continuity of Bone\\nComorbidities\\n0.0010\\n35/ 87\\nOther Disorders of Ear\\nComorbidities\\n0.0064\\n104/393\\nCOVID-19\\nComorbidities\\n0.0086\\n120/598\\nOther Speech Disturbances\\nComorbidities\\n0.0287\\n32/93\\nTable 6. Key Indicators Associated with Decreased Risk of ASUD in Pa-\\ntients with PTSD.\\nneurocognitive impairments and behavioral vulnerabilities relevant to PTSD and SUD,\\nwith animal studies showing that iron deficiency alters ferritin, dopamine metabolism,\\nand neuroproteins such as prion protein (PrPC) and α-synuclein. Maintaining adequate\\nlevels of both B12 and iron may therefore support neurotransmitter function and brain\\nhealth, providing a nutritional avenue to indirectly reduce ASUD risk.\\nLidocaine, a sodium channel blocker, selectively attenuates cue-induced cocaine-\\nseeking behavior in preclinical models by modulating amygdala circuits, though clinical\\nstudies have yet to show significant reductions in craving [36, 37]. Hydroxyzine, par-\\nticularly when combined with the 5-HT3 antagonist palonosetron, has been shown to\\nalleviate opioid withdrawal severity, potentially lowering relapse risk [38]. Lamotrig-\\nine, a glutamate-modulating antiepileptic, reduces cue-induced alcohol seeking and co-\\ncaine use in both preclinical and clinical settings through modulation of glutamatergic,\\ndopaminergic, and serotonergic neurotransmission [38, 40]. Escitalopram and duloxe-\\ntine, selective serotonin and serotonin/norepinephrine reuptake inhibitors, respectively,\\nhave demonstrated reductions in alcohol consumption and craving, with duloxetine also\\nmitigating anxiety-like behaviors that may trigger relapse [41, 42]. Vitamin D (ergo-\\ncalciferol) supplementation helps in modulating neurotransmitter systems, improving\\npsychological symptoms, and reducing drug-seeking behaviors in at-risk populations\\n[43, 44]. Finally, ceftriaxone restores glutamate homeostasis via upregulation of GLT-1,\\nattenuating cocaine- and alcohol-related reinstatement and partially reversing alcohol-\\ninduced gut dysbiosis, thereby further reducing vulnerability to ASUD [45, 46, 47].\\nCertain comorbid conditions may confer a protective effect, through mechanisms\\nrelated to increased healthcare engagement and structured medical supervision. Our\\nanalysis identified associations with reduced ASUD risk for disorders of bone continu-\\nity, other unspecified ear disorders, COVID-19, and unspecified speech disturbances.\\n17\\n\\nThese conditions typically necessitate regular medical monitoring, specialist care, or\\nongoing management, providing opportunities to reinforce adaptive coping strategies,\\nidentify early signs of substance misuse, and limit exposure to high-risk behaviors. Con-\\nsequently, such comorbidities may indirectly mitigate ASUD risk in PTSD patients by\\npromoting treatment adherence and engagement in structured care environments.\\nDiscussion\\nIn our study, we introduced BiPETE, a transformer-encoder classifier designed to pre-\\ndict single disease using structured, longitudinal EHR data. The central innovation of\\nBiPETE lies in its bi-positional encoding strategy, which combines RoPE for captur-\\ning relative time gaps and SPE for encoding absolute visit order. This dual encoding\\ncaptures fine-grained temporal dependencies in patient EHR sequence, addressing one\\nof the key challenges in EHR modeling. The AUPRC scores of the variant models con-\\nfigured with only SPE or only RoPE indicate that RoPE plays a key role in modeling\\ninterdependencies among EHR tokens within and between visits, thereby enhancing the\\nattention mechanism’s ability to focus on tokens relevant for distinguishing cases from\\ncontrols (Table 1). However, the training and validation metrics of the RoPE only model\\n(Figure 2) indicate instability, likely due to the variability of the days-ago indices across\\ninstances, which hinders learning of consistent temporal patterns. In contrast, the SPE-\\nonly model shows stable training but lower performance. By integrating SPE, which\\nencodes absolute visit order, with RoPE, which models relative token relationships,\\nBiPETE outperforms the single positional embedding variant models whilst retaining\\nstability in model performance. Moreover, incorporating the most recent and clinically\\nrelevant patient visit data provided sufficient contextual information for BiPETE to\\nidentify label-specific patterns. These results demonstrate that optimizing positional\\nencoding and incorporating task-relevant data substantially enhance the classification\\nperformance of transformer-based models.\\nBiPETE, which does not rely on pretraining, exhibited strong performance in distin-\\nguishing cases and controls–achieving over 90% on both AUROC and AUPRC–across\\nthe depression and PTSD cohorts with sizes of 65,643 and 9,310, respectively. This sug-\\ngests that the model identified distinct token patterns for the classes without learning\\nthe semantic meaning of the tokens. It is unlikely that the model had learned meaning-\\nful token embedding during training for the classification task alone. Model learning\\nwas facilitated by reducing diagnosis vocabulary size by over 90% in both depression\\nand PTSD cohorts. Without this step, ICD10 codes would dominate the vocabulary,\\nleading to issues such as attention skew toward diagnosis, reduced representation of\\nother vocabularies, and overfitting to ICD10 codes, all of which could impair model\\nlearning and generalizability.\\nThis process removed redundancy in diagnosis codes,\\nenabling the model to learn meaning token patterns while integrating multiple vocab-\\nularies. We believe that BiPETE’s disease prediction performance could be further\\nimproved through pretraining on large-scale EHR corpora using tasks such as masked\\n18\\n\\nlanguage modeling or disease prediction tasks to learn token context and semantics, as\\ndemonstrated in previous studies [6, 10, 12]; we leave this for the future work.\\nOur IG analysis provided key information about the biological and clinical markers\\nassociated with increased or reduced risk of ASUD among patients with depression and\\nPTSD. In PTSD, elevated inflammatory and metabolic markers such as mean platelet\\nvolume and hypoalbuminemia suggested systemic stress and inflammation as key path-\\nways linking trauma-related dysregulation to substance misuse. Concurrently, medica-\\ntions such as hydrocodone and oxybutynin highlight the complex interaction between\\npain management, neuropsychiatric effects, and addiction risk. Conversely, protective\\nfeatures including adequate vitamin B12 and ferritin levels, and use of agents like lam-\\notrigine and ceftriaxone, point toward neurobiological stabilization and glutamatergic\\nmodulation as potential resilience mechanisms. Similar patterns emerged in the depres-\\nsion cohort, where immune and coagulation markers (e.g., altered NLR, PT/INR) re-\\nflected shared inflammatory and hepatic pathways underlying comorbid depression and\\nSUD. Notably, β-lactam antibiotics and psychostimulant therapies appeared protective,\\nconsistent with emerging evidence for neuroimmune and dopaminergic modulation in\\naddiction recovery. Collectively, these findings demonstrate how BiPETE can bridge\\ndata-driven prediction with mechanistic understanding, revealing clinically meaningful\\ntargets for early identification and personalized intervention in comorbid psychiatric\\nand substance use disorders.\\nThere are several limitations in our current work that should be addressed in future\\nevaluations of BiPETE. First, we tested BiPETE only on the ASUD risk prediction\\ntask, so its performance on other disease prediction tasks remains uncertain. Second,\\nBiPETE has not yet been evaluated on datasets beyond the AoU data, and its gen-\\neralizability remains to be fully validated. Third, without pretraining to learn token\\ncontextual information, BiPETE is currently limited to single-disease prediction and\\nrequires retraining for each new disease prediction task. Fourth, we did not include any\\ndemographic information in our input. Incorporating such information could improve\\nperformance, especially given the diverse racial composition of our cohort (Supplemen-\\ntary Table 1). In addition, there are methodological caveats in interpreting attributions\\nfrom transformer models. The context-dependent nature of the attention mechanism\\ncan lead to varying attributions for the same code across patients or visits. Further-\\nmore, confounding by indication may cause treatment codes associated with more severe\\ndisease to appear linked to ASUD outcomes, even when they reflect disease progression\\nrather than risk.\\nIn conclusion, we proposed a single-disease classifier, BiPETE, and evaluated it on\\nASUD risk prediction. BiPETE’s positional encoding configuration could be applied to\\nother clinical relevant tasks by improving temporal modeling of EHR data. This con-\\nfiguration can be readily adapted to existing transformer-based models to potentially\\nboost their performance. BiPETE is a high-performing classifier, making it particu-\\nlarly valuable in settings where pretrained models or large-scale pretraining data are\\nunavailable.\\n19\\n\\nAcknowledgments\\nWe gratefully acknowledge All of Us participants for their contributions, without whom\\nthis research would not have been possible. We also thank the National Institutes of\\nHealth’s All of Us Research Program for making available the participant data and\\ncohorts examined in this study.\\n20\\n\\nReferences\\n[1] Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J., AI in health and medicine,\\nNature medicine, 28, 31-38, (2022).\\n[2] Hama, T. et al., Enhancing patient outcome prediction through deep learning with\\nsequential diagnosis codes from structured electronic health record data: Systematic\\nreview, Journal of Medical Internet Research 27, e57358, (2025).\\n[3] Rasmy, L. et al., Recurrent neural network models (CovRNN) for predicting out-\\ncomes of patients with COVID-19 on admission to hospital: model development\\nand validation using electronic health record data, The Lancet Digital Health 4,\\ne415–e425, (2022).\\n[4] Atasoy, H., Greenwood, B. N. & McCullough, J. S., The digitization of patient\\ncare: a review of the effects of electronic health records on health care quality and\\nutilization, Annual Review of Public Health 40, 487–500, (2019).\\n[5] Yang, X. et al., A large language model for electronic health records, NPJ Digital\\nMedicine 5, 194, (2022).\\n[6] Rasmy, L., Xiang, Y., Xie, Z., Tao, C. & Zhi, D., Med-BERT: pretrained contex-\\ntualized embeddings on large-scale structured electronic health records for disease\\nprediction, NPJ Digital Medicine 4, 86, (2021).\\n[7] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K., BERT: Pre-training of deep\\nbidirectional transformers for language understanding, in Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational\\nLinguistics (NAACL-HLT), 4171–4186, (2019).\\n[8] Xie, F. et al., Deep learning for temporal data representation in electronic health\\nrecords: A systematic review of challenges and methodologies, Journal of Biomed-\\nical Informatics 126, 103980, (2022).\\n[9] Holmes, J. H. et al., Why is the electronic health record so challenging for research\\nand clinical care?, Methods of Information in Medicine 60, 032–048, (2021).\\n[10] Li, Y. et al., BEHRT: transformer for electronic health records, Scientific Reports\\n10, 1–12, (2020).\\n[11] Si, Y. et al., Deep representation learning of patient data from Electronic Health\\nRecords (EHR): A systematic review, Journal of Biomedical Informatics 115,\\n103671, (2021).\\n[12] Lindhagen, L., Van Hemelrijck, M., Robinson, D., Stattin, P. & Garmo, H., How to\\nmodel temporal changes in comorbidity for cancer patients using prospective cohort\\ndata, BMC Medical Informatics and Decision Making 15, 96, (2015).\\n21\\n\\n[13] Valderas, J. M., Starfield, B., Sibbald, B., Salisbury, C. & Roland, M., Defining\\ncomorbidity: implications for understanding health and health services, The Annals\\nof Family Medicine 7, 357–363, (2009).\\n[14] Yang, Z., Mitra, A., Liu, W., Berlowitz, D. & Yu, H., TransformEHR: transformer-\\nbased encoder-decoder generative model to enhance prediction of disease outcomes\\nusing electronic health records, Nature Communications 14, 7857, (2023).\\n[15] Su, J. et al., Roformer: Enhanced transformer with rotary position embedding,\\nNeurocomputing 568, 127063, (2024).\\n[16] Rabbani, N., Kim, G. Y., Suarez, C. J. & Chen, J. H., Applications of machine\\nlearning in routine laboratory medicine: Current state and future directions, Clin-\\nical Biochemistry 103, 1–7, (2022).\\n[17] Bailly, A. et al., Effects of dataset size and interactions on the prediction per-\\nformance of logistic regression and deep learning models, Computer Methods and\\nPrograms in Biomedicine 213, 106504, (2022).\\n[18] Ramirez, A. H. et al., The All of Us Research Program: data quality, utility, and\\ndiversity, Patterns 3, (2022).\\n[19] Sadeghi, Z. et al., A review of Explainable Artificial Intelligence in healthcare,\\nComputers and Electrical Engineering 118, 109370, (2024).\\n[20] Sundararajan, M., Taly, A. & Yan, Q., Axiomatic attribution for deep networks\\n(Integrated Gradients) in International Conference on Machine Learning (ICML),\\n3319–3328 (PMLR), (2017).\\n[21] Abgrall, G., Holder, A. L., Chelly Dagdia, Z., Zeitouni, K. & Monnet, X., Should\\nAI models be explainable to clinicians?, Critical Care 28, 301, (2024).\\n[22] Rodriguez JD, Perez A, & Lozano JA, Sensitivity analysis of k-fold cross validation\\nin prediction error estimation, IEEE transactions on pattern analysis and machine\\nintelligence 32, 569-575, (2009).\\n[23] Hoffmann, J.et al., Training Compute-Optimal Large Language Models in NeurIPS\\n22: Proceedings of the 36th International Conference on Neural Information Pro-\\ncessing Systems, 30016–30030, (2022). https://doi.org/10.48550/arXiv.2203.15556\\n[24] Mart´ın-Gonz´alez, C. et al., Mean platelet volume and mortality in patients with\\nalcohol use disorder, Digestive and Liver Disease 55, 1236–1241, (2023).\\n[25] Annoni, G., Weiner, F. R., Colombo, M., Czaja, M. J. & Zern, M. A., Albumin\\nand collagen gene regulation in alcohol- and virus-induced human liver disease,\\nGastroenterology 98, 197–202, (1990).\\n22\\n\\n[26] Darwish, M. et al., Abuse potential with oral route of administration of a hy-\\ndrocodone extended-release tablet formulated with abuse-deterrence technology in\\nnondependent, recreational opioid users, Pain Medicine 18, 61–77, (2017).\\n[27] Miller, N. S. & Greenfeld, A., Patient characteristics and risks factors for devel-\\nopment of dependence on hydrocodone and oxycodone, American Journal of Ther-\\napeutics 11, 26–32, (2004).\\n[28] Schwartz, A. C. et al., Pain medication use among patients with posttraumatic\\nstress disorder, Psychosomatics 47, 136–142, (2006).\\n[29] Gulsun, M., Pinar, M. & Sabanci, U., Psychotic disorder induced by oxybutynin:\\nPresentation of two cases, Clinical Drug Investigation 26, 603–606, (2006).\\n[30] Welk, B., Etaby, K., McArthur, E. & Chou, Q., The risk of delirium and falls or\\nfractures with the use of overactive bladder anticholinergic medications, Neurourol-\\nogy and Urodynamics 41, 348–356, (2022).\\n[31] Iqbal, K. M., Luke, P. K. & Ingram, M. T., Psychosis resulting from trimethoprim-\\nsulfamethoxazole treatment for preseptal cellulitis, Taiwan Journal of Ophthalmol-\\nogy 12, 223–226, (2022).\\n[32] Stuhec, M., Trimethoprim-sulfamethoxazole-related hallucinations, General Hospi-\\ntal Psychiatry 36, 230.e237–230.e238, (2014).\\n[33] Scholz, S. M., Thalmann, N. F., M¨uller, D., Trippolini, M. A. & Wertli, M. M.,\\nFactors influencing pain medication and opioid use in patients with musculoskeletal\\ninjuries: a retrospective insurance claims database study, Scientific Reports 14,\\n1978, (2024).\\n[34] Pastore, G. P., Goulart, D. R., Pastore, P. R., Prati, A. J. & de Moraes, M.,\\nSelf-medication among myofascial pain patients: a preliminary study, The Open\\nDentistry Journal 12, 347, (2018).\\n[35] Berlt, M., de Souza, K. B., Zhang, L., Bock, P. M. & Hort, M. A., Prevalence of\\nself-medication for dental issues in the general population: a systematic review and\\nmeta-analysis, Discover Public Health 22, 1–32, (2025).\\n[36] Becker, J. E. et al., The efficacy of lidocaine in disrupting cocaine cue-induced\\nmemory reconsolidation, Drug and Alcohol Dependence 212, 108062, (2020).\\n[37] Kantak, K. M., Black, Y., Valencia, E., Green-Jordan, K. & Eichenbaum, H. B.,\\nDissociable effects of lidocaine inactivation of the rostral and caudal basolateral\\namygdala on the maintenance and reinstatement of cocaine-seeking behavior in\\nrats, Journal of Neuroscience 22, 1126–1136, (2002).\\n23\\n\\n[38] Erlendson, M. J. et al., Palonosetron and hydroxyzine pre-treatment reduces\\nthe objective signs of experimentally-induced acute opioid withdrawal in humans:\\na double-blinded, randomized, placebo-controlled crossover study, The American\\nJournal of Drug and Alcohol Abuse 43, 78–86, (2017).\\n[39] Vengeliene, V., Heidbreder, C. A. & Spanagel, R., The effects of lamotrigine on\\nalcohol seeking and relapse, Neuropharmacology 53, 951–957, (2007).\\n[40] Brown, E. S., Sunderajan, P., Hu, L. T., Sowell, S. M. & Carmody, T. J., A\\nrandomized, double-blind, placebo-controlled, trial of lamotrigine therapy in bipolar\\ndisorder, depressed or mixed phase and cocaine dependence, Neuropsychopharma-\\ncology 37, 2347–2354, (2012).\\n[41] Skelly, M. J. & Weiner, J. L., Chronic treatment with prazosin or duloxetine lessens\\nconcurrent anxiety-like behavior and alcohol intake: evidence of disrupted nora-\\ndrenergic signaling in anxiety-related alcohol use, Brain and Behavior 4, 468–483,\\n(2014).\\n[42] Mohammadi, N. et al., Preventive effects of duloxetine against methamphetamine\\ninduced neurodegeneration and motor activity disorder in rat: Possible role of\\nCREB/BDNF signaling pathway, International Journal of Preventive Medicine 10,\\n195, (2019).\\n[43] Ghaderi, A. et al., Exploring the effects of vitamin D supplementation on cogni-\\ntive functions and mental health status in subjects under methadone maintenance\\ntreatment, Journal of Addiction Medicine 14, 18–25, (2020).\\n[44] Jalilian-Khave, L. et al., Potential roles for vitamin D in preventing and treating\\nimpulse control disorders, behavioral addictions, and substance use disorders: A\\nscoping review, Addiction Neuroscience, 100190, (2024).\\n[45] Duclot, F., Wu, L., Wilkinson, C. S., Kabbaj, M. & Knackstedt, L. A., Ceftriax-\\none alters the gut microbiome composition and reduces alcohol intake in male and\\nfemale Sprague–Dawley rats, Alcohol 120, 169–178, (2024).\\n[46] Stennett, B. A., Frankowski, J. C., Peris, J. & Knackstedt, L. A., Ceftriaxone\\nreduces alcohol intake in outbred rats while upregulating xCT in the nucleus ac-\\ncumbens core, Pharmacology Biochemistry and Behavior 159, 18–23, (2017).\\n[47] Rao, P. & Sari, Y., Effectiveness of ceftriaxone treatment in preventing relapse-\\nlike drinking behavior following long-term ethanol dependence in P rats, Journal of\\nAddiction Research & Therapy 5, 1000183, (2014).\\n[48] Karatoprak, S., Uzun, N., Akıncı, M. A. & D¨onmez, Y. E., Neutrophil-lymphocyte\\nand platelet-lymphocyte ratios among adolescents with substance use disorder: A\\npreliminary study, Clinical Psychopharmacology and Neuroscience 19, 669, (2021).\\n24\\n\\n[49] Gasparyan, A. Y., Ayvazyan, L., Mukanova, U., Yessirkepov, M. & Kitas, G. D.,\\nThe platelet-to-lymphocyte ratio as an inflammatory marker in rheumatic diseases,\\nAnnals of Laboratory Medicine 39, 345, (2019).\\n[50] Sharma, T., Kumar, M., Rizkallah, A., Cappelluti, E. & Padmanabhan, P.,\\nCocaine-induced thrombosis: review of predisposing factors, potential mechanisms,\\nand clinical consequences with a striking case report, Cureus 11, (2019).\\n[51] Alonzo, M. M., Lewis, T. V. & Miller, J. L., Disulfiram-like reaction with metron-\\nidazole: an unsuspected culprit, The Journal of Pediatric Pharmacology and Ther-\\napeutics 24, 445–449, (2019).\\n[52] Martinez-Diaz, G. J. & Hsia, R., Altered mental status from acyclovir, The Journal\\nof Emergency Medicine 41, 55–58, (2011).\\n[53] Ng, M.-H. et al., Macrocytosis among patients with heroin use disorder, Neuropsy-\\nchiatric Disease and Treatment, 2293–2298, (2019).\\n[54] Li, D. et al., The relationship between mean corpuscular hemoglobin concentration\\nand mortality in hypertensive individuals: A population-based cohort study, PLOS\\nONE 19, e0301903, (2024).\\n[55] Bohler, R. M. et al., The policy landscape for naloxone distribution in four states\\nhighly impacted by fatal opioid overdoses, Drug and Alcohol Dependence Reports\\n6, 100126, (2023).\\n[56] Bazazi, A. R., Zaller, N. D., Fu, J. J. & Rich, J. D., Preventing opiate overdose\\ndeaths: examining objections to take-home naloxone, Journal of Health Care for\\nthe Poor and Underserved 21, 1108–1113, (2010).\\n[57] Fischer, L. S. et al., Effectiveness of naloxone distribution in community settings\\nto reduce opioid overdose deaths among people who use drugs: a systematic review\\nand meta-analysis, BMC Public Health 25, 1135, (2025).\\n[58] Petrovitch, D. et al., State program enables the identification of factors associated\\nwith naloxone awareness, self-efficacy, and use for overdose reversal: A cross-\\nsectional, observational study in an urban emergency department population, Jour-\\nnal of Substance Use and Addiction Treatment 167, 209506, (2024).\\n[59] Alasmari, F., Rao, P. & Sari, Y., Effects of cefazolin and cefoperazone on glutamate\\ntransporter 1 isoforms and cystine/glutamate exchanger as well as alcohol drinking\\nbehavior in male alcohol-preferring rats, Brain Research 1634, 150–157, (2016).\\n[60] Rao, P. et al., Effects of ampicillin, cefazolin and cefoperazone treatments on\\nGLT-1 expressions in the mesocorticolimbic system and ethanol intake in alcohol-\\npreferring rats, Neuroscience 295, 164–174, (2015).\\n25\\n\\n[61] Weiland, A., Garcia, S. & Knackstedt, L. A., Ceftriaxone and cefazolin attenuate\\nthe cue-primed reinstatement of alcohol-seeking, Frontiers in Pharmacology 6, 44,\\n(2015).\\n[62] Mergenhagen, K. A., Wattengel, B. A., Skelly, M. K., Clark, C. M. & Russo, T.\\nA., Fact versus fiction: a review of the evidence behind alcohol and antibiotic in-\\nteractions, Antimicrobial Agents and Chemotherapy 64, 10.1128/aac.02167-02119,\\n(2020).\\n[63] Hakami, A. Y., Hammad, A. M. & Sari, Y., Effects of amoxicillin and augmentin\\non cystine-glutamate exchanger and glutamate transporter 1 isoforms as well as\\nethanol intake in alcohol-preferring rats, Frontiers in Neuroscience 10, 171, (2016).\\n[64] Callans, L. S. et al., Clavulanic acid decreases cocaine cue reactivity in addiction-\\nrelated brain areas, a randomized fMRI pilot study, Psychopharmacology Bulletin\\n54, 8, (2024).\\n[65] Philogene-Khalid, H. L. et al., The GLT-1 enhancer clavulanic acid suppresses\\ncocaine place preference behavior and reduces GCPII activity and protein levels in\\nthe rat nucleus accumbens, Drug and Alcohol Dependence 232, 109306, (2022).\\n[66] Maser, J. et al., Clavulanic Acid-Mediated Increases in Anterior Cingulate Glu-\\ntamate Levels are Associated With Decreased Cocaine Craving and Brain Net-\\nwork Functional Connectivity Changes, Current Therapeutic Research 101, 100751,\\n(2024).\\n[67] Schroeder, J. A. et al., Clavulanic acid reduces rewarding, hyperthermic and\\nlocomotor-sensitizing effects of morphine in rats: a new indication for an old drug?,\\nDrug and Alcohol Dependence 142, 41–45, (2014).\\n[68] Mather, J. F., Seip, R. L. & McKay, R. G., Impact of famotidine use on clinical\\noutcomes of hospitalized patients with COVID-19, Official journal of the American\\nCollege of Gastroenterology — ACG 115, 1617–1623, (2020).\\n[69] Aouizerate, B. et al., Glucocorticoid negative feedback in methadone-maintained\\nformer heroin addicts with ongoing cocaine dependence: dose–response to dexam-\\nethasone suppression, Addiction Biology 11, (2006).\\n[70] Capasso, A., Pinto, A., Sorrentino, L. & Cirino, G., Dexamethasone inhibition\\nof acute opioid physical dependence in vitro is reverted by anti-lipocortin-1 and\\nmimicked by anti-type II extracellular PLA2 antibodies, Life Sciences 61, PL127–\\nPL134, (1997).\\n[71] Smith, M. A. et al., Treatment with dextroamphetamine decreases the reacquisi-\\ntion of cocaine self-administration: Consistency across social contexts, Drug and\\nAlcohol Dependence 260, 111328, (2024).\\n26\\n\\n[72] Chang, Z. et al., Stimulant ADHD medication and risk for substance abuse, Journal\\nof Child Psychology and Psychiatry 55, 878–885, (2014).\\n[73] Mariani, J. J. & Levin, F. R., Treatment strategies for co-occurring ADHD and\\nsubstance use disorders, The American Journal on Addictions 16, 45–56, (2007).\\n[74] Musco, S. et al., Characteristics of patients experiencing extrapyramidal symptoms\\nor other movement disorders related to dopamine receptor blocking agent therapy,\\nJournal of Clinical Psychopharmacology 39, 336–343, (2019).\\n27\\n\\nSupplementary Information\\nCohort Stats\\nDepressive Disorder\\nPTSD\\nCOHORT SIZE\\n84,163\\n15,334\\nCASES%:CTRL%\\n21.3% : 78.7%\\n43.9% : 56.1%\\nGENDER RATIO\\nM%:F%\\n26.5% : 73.5%\\n35.4% : 64.6%\\nAGE\\nDISTRIBUTION\\n18-44\\n45-66\\n≥66\\n39.3%\\n43.6%\\n18.1%\\n18-44\\n45-66\\n≥66\\n32.2%\\n39.9%\\n27.9%\\nWHITE\\n54%\\n62%\\nAFAM\\n2%\\n14%\\nAI/AN\\n2%\\n1%\\nASIAN\\n1%\\n2%\\nMENA\\n<1%\\n<1%\\nNHPI\\n<1%\\n<1%\\nOTHER\\n25%\\n21%\\nSupplementary Table 1. Cohort Characteristics Preceding Data Preprocess-\\ning. Gender reflects sex assigned at birth. Age distribution is based on participants’ age\\nat the time of consent to data collection for the AoU program. The ”Other” category\\nin the race row includes individuals who did not answer, selected multiple popula-\\ntions, or indicated a race not listed. Abbreviations of Race: AFAM–African American;\\nAI/AN–American Indian or Alaska Native; MENA–Middle Eastern or North African;\\nNHPI–Native Hawaiian or Other Pacific Islander.\\n28\\n\\n0.0\\n0.5\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.6\\n0.8\\n1.0\\nROC\\nCurve\\nBoth Emb\\nAUROC=0.961\\n0.0\\n0.5\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.6\\n0.8\\n1.0\\nOnly RoPE\\nAUROC=0.942\\n0.0\\n0.5\\n1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.6\\n0.8\\n1.0\\nOnly SPE\\nAUROC=0.882\\n0.0\\n0.5\\n1.0\\n0.2\\n0.4\\n0.6\\n0.6\\n0.8\\n1.0\\nPrecision-Recall\\nCurve\\nAUPRC=0.932\\n0.0\\n0.5\\n1.0\\n0.2\\n0.4\\n0.6\\n0.6\\n0.8\\n1.0\\nAUPRC=0.907\\n0.0\\n0.5\\n1.0\\n0.2\\n0.4\\n0.6\\n0.6\\n0.8\\n1.0\\nAUPRC=0.773\\nSupplementary Figure 1. Test ROC and PR Curve Comparison of Classi-\\nfiers with Different Positional Encoding Configurations. For receiver operating\\ncharacteristic (ROC) curves, the x-axis shows the false positive rate, and the y-axis\\nshows the true positive rate. For precision-recall curves, the x-axis shows recall, and\\nthe y-axis shows precision. Curves and metrics are computed using the mean values of\\nthe corresponding rates across the cross-validation folds.\\n29\\n\\nModel\\nT\\nPPV\\nCV\\nSensitivity\\nCV\\nSpecificity\\nCV\\nDep\\nBiPETE\\n0.2\\n91.42 (±2.06)\\n1.41\\n82.59 (±1.41)\\n1.19\\n97.97 (±0.56)\\n0.35\\n0.5\\n93.28 (±2.04)\\n81.36 (±1.71)\\n98.46 (±0.53)\\n0.8\\n94.61 (±1.93)\\n80.22 (±1.85)\\n98.80 (±0.48)\\nBiGRU\\n0.2\\n67.19 (±5.81)\\n4.34\\n55.23 (±5.56)\\n6.25\\n92.69 (±2.60)\\n1.35\\n0.5\\n71.05 (±5.81)\\n51.23 (±5.42)\\n94.32 (±2.14)\\n0.8\\n74.74 (±5.59)\\n47.38 (±5.30)\\n95.64 (±1.73)\\nLR\\n0.2\\n45.42 (±0.22)\\n25.70\\n74.86 (±0.92)\\n35.28\\n76.71 (±0.24)\\n10.76\\n0.5\\n72.61 (±0.31)\\n49.80 (±0.56)\\n95.14 (±0.09)\\n0.8\\n88.17 (±1.21)\\n30.33 (±0.68)\\n98.94 (±0.14)\\nPTSD\\nBiPETE\\n0.2\\n88.48 (±3.47)\\n1.08\\n85.50 (±2.31)\\n0.52\\n96.28 (±1.34)\\n0.37\\n0.5\\n89.40 (±3.36)\\n85.02 (±2.28)\\n96.62 (±1.34)\\n0.8\\n90.83 (±3.24)\\n84.41 (±2.41)\\n97.15 (±1.14)\\nBiGRU\\n0.2\\n60.60 (±4.68)\\n5.16\\n52.09 (±5.75)\\n9.80\\n88.52 (±3.22)\\n2.28\\n0.5\\n63.89 (±4.26)\\n45.99 (±6.09)\\n91.19 (±2.58)\\n0.8\\n68.69 (±4.73)\\n40.98 (±6.37)\\n93.61 (±2.22)\\nLR\\n0.2\\n49.45 (±1.35)\\n20.37\\n78.40 (±1.23)\\n25.12\\n73.72 (±1.57)\\n11.34\\n0.5\\n67.65 (±1.46)\\n58.45 (±1.14)\\n90.83 (±0.72)\\n0.8\\n82.61 (±1.77)\\n41.81 (±2.83)\\n97.12 (±0.34)\\nSupplementary Table 2. Performance Metrics for ASUD Risk Prediction us-\\ning BiPETE and Baseline Models Across Decision Thresholds in Depression\\nand PTSD Cohorts. PPV (Precision), Sensitivity (Recall) and Specificity (True Neg-\\native Rate) are reported for comparisons with baseline models. Metrics are calculated\\nat three decision thresholds: 0.2, 0.5 and 0.8. ± indicates standard deviation across\\nfive-fold cross-validation. Coefficient of Variation (CV), expressed as a percentage, is\\nreported for each metric to show its variability across thresholds.\\n30\\n\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 1\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.040\\n0.050\\n0.060\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 2\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.040\\n0.045\\n0.050\\n0.055\\n0.060\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 3\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.040\\n0.050\\n0.060\\n0.070\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 4\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.045\\n0.050\\n0.055\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 5\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.035\\n0.040\\n0.045\\n0.050\\n0.055\\n0.060\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 6\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.045\\n0.050\\n0.055\\n0.060\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 7\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.040\\n0.045\\n0.050\\n0.055\\n0.060\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 8\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.040\\n0.050\\n0.060\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nHead 9\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nICD10 (1)\\nER (1)\\nICD10 (2)\\nICD10 (2)\\nICD10 (2)\\nER (2)\\nICD10 (3)\\nICD10 (3)\\nICD10 (3)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\nICD10 (4)\\n0.040\\n0.045\\n0.050\\n0.055\\n0.060\\nSupplementary Figure 2. Attention Heads of Final Layer in BiPETE. The\\nattention is calculated using a representative EHR code sequence of length 20. EHR\\ncodes in axes labels are replaced with the vocabulary type to preserve patient anonymity.\\nAttention maps show tokens tend to receive higher or lower scores in groups of tokens\\nfrom the same visit. Different heads focus on distinct visit-level token groups, seeking\\ndifferent patterns in the sequence.\\nDepressive Disorder Concept Set\\nConcept Id\\nPTSD Concept Set\\nConcept Id\\nDepressive disorder\\n440383\\nAcute post-trauma stress state\\n4100536\\nChronic post-traumatic stress disorder\\n443414\\nComplex posttraumatic stress disorder\\n40484109\\nPosttraumatic stress disorder\\n436676\\nSupplementary Table 3. All of Us Concept Sets Used to Define MHD Cohorts.\\n31\\n\\nASUD Concept Set\\nConcept Id\\nASUD Concept Set (Continued)\\nConcept Id\\nAbuse of antidepressant drug\\n440992\\nDrug dependence in remission\\n43530680\\nAccidental poisoning by hallucinogens\\n438946\\nDrug withdrawal\\n441260\\nAccidental poisoning by heroin\\n440307\\nDrug-induced amnestic syndrome\\n373172\\nAcute alcoholic intoxication in alcoholism\\n433735\\nDrug-induced delirium\\n373449\\nAcute alcoholic intoxication in remission,\\nin alcoholism\\n432609\\nDrug-induced delusional disorder\\n443559\\nAcute alcoholic liver disease\\n201343\\nDrug-induced dementia\\n376095\\nAlcohol abuse\\n433753\\nDrug-induced paranoia or hallucinatory\\nstates\\n4101142\\nAlcohol dependence\\n435243\\nDrug-induced psychosis\\n434900\\nAlcohol withdrawal syndrome\\n375519\\nDrug-induced sleep disorder\\n435792\\nAlcohol-induced anxiety disorder\\n4146660\\nEpisodic acute alcoholic intoxication in\\nalcoholism\\n441261\\nAlcohol-induced mood disorder\\n4205002\\nGastric hemorrhage due to alcoholic\\ngastritis\\n45757783\\nAlcohol-induced psychotic disorder with\\ndelusions\\n442582\\nHallucinogen abuse\\n437245\\nAlcohol-induced sleep disorder\\n375794\\nHallucinogen dependence\\n433180\\nAlcoholic fatty liver\\n193256\\nHallucinogen dependence in remission\\n434921\\nAlcoholic gastritis\\n195300\\nHallucinosis caused by drug\\n440987\\nAmphetamine abuse\\n432878\\nHypnotic or anxiolytic abuse\\n439554\\nAmphetamine and amphetamine derivative\\ndrug dependence\\n3654785\\nInhalant abuse\\n4290538\\nAmphetamine dependence\\n437533\\nInhalant dependence\\n4176120\\nAmphetamine or psychostimulant\\ndependence in remission\\n432884\\nInhalant-induced mood disorder\\n4232492\\nAmphetamine or psychostimulant\\ndependence, continuous\\n434916\\nInhalant-induced organic mental disorder\\n4264889\\nAmphetamine or psychostimulant\\ndependence, episodic\\n441262\\nMood disorder caused by drug\\n436079\\nCannabis abuse\\n434327\\nNicotine dependence\\n4209423\\nCannabis dependence\\n440387\\nNicotine dependence in remission\\n3654548\\nCannabis dependence in remission\\n440996\\nNondependent antidepressant type drug\\nabuse in remission\\n439313\\nCannabis intoxication delirium\\n4220197\\nNondependent cocaine abuse in remission\\n436098\\nCannabis-induced anxiety disorder\\n4221077\\nNondependent cocaine abuse, continuous\\n439796\\nCannabis-induced psychotic disorder with\\nhallucinations\\n4097389\\nNondependent hallucinogen abuse\\n4150794\\nCocaine abuse\\n432303\\nNondependent hallucinogen abuse in\\nremission\\n441272\\nCocaine dependence\\n436389\\nNondependent mixed drug abuse\\n439312\\nCocaine dependence in remission\\n432302\\nNondependent mixed drug abuse in\\nremission\\n4103426\\nCocaine-induced anxiety disorder\\n4198826\\nNondependent opioid abuse\\n4099935\\nCocaine-induced mood disorder\\n4012869\\nNondependent opioid abuse in remission\\n436088\\nCocaine-induced psychotic disorder with\\nhallucinations\\n4272033\\nOpioid abuse\\n438130\\nCombined drug dependence, excluding\\nopioid, in remission\\n433458\\nOpioid dependence\\n438120\\nCombined drug dependence, excluding\\nopioids\\n436370\\nOpioid dependence in remission\\n432301\\nCombined opioid with other drug\\ndependence\\n4099809\\nPoisoning by heroin\\n433919\\nCombined opioid with other drug\\ndependence in remission\\n4100520\\nPoisoning by methadone\\n440919\\nCombined opioid with other drug\\ndependence, continuous\\n4102817\\nPoisoning by opium alkaloid\\n439223\\nCombined opioid with other drug\\ndependence, episodic\\n4103413\\nPsychoactive substance abuse\\n4239381\\nContinuous acute alcoholic intoxication in\\nalcoholism\\n437257\\nPsychoactive substance dependence\\n4080762\\nDelirium due to sedative withdrawal\\n4262566\\nPsychoactive substance use disorder\\n4004672\\nDementia associated with alcoholism\\n378726\\nPsychoactive substance-induced organic\\nhallucinosis\\n4155336\\nDilated cardiomyopathy secondary to\\nalcohol\\n318773\\nPsychotic disorder caused by cocaine\\n37110437\\nDisorder caused by alcohol\\n36714559\\nStimulant abuse\\n40479573\\nDrug dependence\\n440069\\nTherapeutic drug dependence\\n4319165\\nDrug dependence during pregnancy - baby\\ndelivered\\n442915\\nDrug dependence in mother complicating\\npregnancy, childbirth AND/OR\\npuerperium\\n440787\\nSupplementary Table 4. All of Us Concept Set Used to Define ASUD.\\n32\\n\",\n",
       " 'Learning Fourier shapes to probe the geometric\\nworld of deep neural networks\\nJian Wang1, Yixing Yong1, Haixia Bi1, Lijun He1, Fan Li1*\\n1Shaanxi Key Laboratory of Deep Space Exploration Intelligent\\nInformation Technology, School of Information and Communications\\nEngineering, Xi’an Jiaotong University, Xi’an, 710049, China.\\n*Corresponding author(s). E-mail(s): lifan@mail.xjtu.edu.cn;\\nContributing authors: wj851329121@stu.xjtu.edu.cn;\\nyongyx@stu.xjtu.edu.cn; haixia.bi@mail.xjtu.edu.cn;\\nlijunhe@mail.xjtu.edu.cn;\\nAbstract\\nWhile both shape and texture are fundamental to visual recognition, research on\\ndeep neural networks (DNNs) has predominantly focused on the latter, leaving\\ntheir geometric understanding poorly probed. Here, we show: first, that opti-\\nmized shapes can act as potent semantic carriers, generating high-confidence\\nclassifications from inputs defined purely by their geometry; second, that they are\\nhigh-fidelity interpretability tools that precisely isolate a model’s salient regions;\\nand third, that they constitute a new, generalizable adversarial paradigm capa-\\nble of deceiving downstream visual tasks. This is achieved through an end-to-end\\ndifferentiable framework that unifies a powerful Fourier series to parameterize\\narbitrary shapes, a winding number-based mapping to translate them into the\\npixel grid required by DNNs, and signal energy constraints that enhance opti-\\nmization efficiency while ensuring physically plausible shapes. Our work provides\\na versatile framework for probing the geometric world of DNNs and opens new\\nfrontiers for challenging and understanding machine perception.\\nKeywords: Visual understanding, Adversarial attack, Learnable Fourier shapes\\n1\\narXiv:2511.04970v1  [cs.CV]  7 Nov 2025\\n\\nFig. 1 Conceptual overview of adversarial shape learning. a, Human and machine visual sys-\\ntems rely on consistent shape and appearance attributes for robust object recognition. When these\\nattributes are mismatched, such as an apple’s shape with a banana’s texture, perceptual conflict arises,\\nillustrating that shape is an independently salient attribute. b, Prior work on adversarial attacks\\nprimarily targets the appearance domain. This involves either adding subtle, global pixel perturba-\\ntions to misclassify an image (e.g., a panda recognized as a gibbon) or deploying localized adversarial\\npatches to cause detection failures. These methods operate on pixel values without explicitly manipu-\\nlating underlying geometry. c, Our framework enables end-to-end differentiable optimization of object\\nshapes for adversarial machine learning. It addresses three key challenges: (1) Shape parameteriza-\\ntion: Arbitrary closed contours are represented by a compact set of Fourier series coefficients. (2)\\nDifferentiable mapping: A module based on the winding number theorem translates these coefficients\\ninto a 2D grid image, creating a differentiable bridge to DNNs. (3) Effective optimization: Regular-\\nization, inspired by signal energy theory, guides the learning process to ensure physically plausible\\nshapes by constraining high-frequency components. This integrated pipeline allows for the discovery\\nand optimization of effective adversarial shapes. Images in b are from ref. [20] and [23]\\n1 Introduction\\nThe remarkable ability of the human visual system [1, 11] to recognize objects relies on\\na sophisticated synthesis of two fundamental attributes: geometric shape and surface\\ntexture. Shape provides the structural scaffold of an object, defining its boundaries\\nand identity, while texture and colour furnish the finer details of its appearance. A\\nsignificant distortion in either of these attributes can disrupt perception (Fig. 1a), sug-\\ngesting that both are fundamental to robust recognition. A well-trained DNN should\\nideally mirror this biological duality [2], leveraging both shape and texture to make\\nrobust inferences. However, the vast body of research exploring model vulnerabili-\\nties through adversarial attacks [20, 21] has overwhelmingly focused on manipulating\\nthe texture domain [12, 15, 17]. By searching for subtle perturbations in the high-\\ndimensional pixel space, these methods have revealed profound weaknesses in modern\\nDNNs, yet they have largely overlooked the equally fundamental axis of shape.\\nThis intense focus on pixel-level manipulations (Fig. 1b), while fruitful for reveal-\\ning model weaknesses, carries inherent limitations. Adversarial perturbations [8–10],\\ntypically composed of high-frequency signals, are largely confined to the digital\\ndomain and lack direct physical-world applicability. While physically realizable meth-\\nods like adversarial patches [13, 14, 16, 22] have been developed, they still operate by\\nmanipulating texture within a predefined boundary rather than the object’s intrin-\\nsic geometry. Furthermore, the explanatory power [3, 4] of such pixel-based methods\\nis often limited, as the resulting patterns lack clear semantic meaning for human\\nobservers [5–7]. This raises a critical question: can we move beyond the pixel grid to\\n2\\n\\nengage directly with a model’s understanding of geometry? Exploring the domain of\\nshape offers a path to creating more physically robust and interpretable methods for\\nanalyzing and challenging machine perception.\\nDirectly optimizing an object’s shape, however, presents a formidable technical\\nchallenge. A shape is a continuous, geometric entity typically described by abstract\\nparameters, unlike the discrete grid of pixels a DNN processes. Bridging this gap\\nfor gradient-based optimization requires two key components: a shape representation\\nthat is expressive enough to describe a diverse family of forms, and a differentiable\\nmapping to translate those parameters into a pixel array. An effective representation\\nmust therefore combine expressive power with optimization efficiency, providing a rich\\nsearch space in which to discover effective adversarial geometries. Existing approaches\\n[28–34] have struggled to satisfy these requirements simultaneously. Methods that\\nmodel shapes on a discrete grid [29–32], for instance, are differentiable but require\\ncomplex, hand-crafted aggregation constraints to maintain coherence, which restricts\\nthe search space and scales poorly with resolution. Conversely, approaches using\\ncontinuous spline-based representations [33, 34] often lack a differentiable mapping,\\nforcing a reliance on inefficient black-box optimization that yields poor scalability and\\nperformance.\\nHere, we propose a complete framework for learning adversarial shapes through\\na parametric Fourier series representation. Inspired by how complex signals can be\\ndecomposed into a sum of simple sinusoids, we model any arbitrary 2D closed contour\\nusing a compact set of Fourier coefficients. This representation allows us to generate a\\nvast and intricate space of shapes by controlling the amplitude and phase of different\\nfrequency components. To bridge the gap between these abstract parameters and\\nthe pixel domain, we employ a differentiable mapping based on the winding number\\ntheorem from complex analysis, which analytically draws the shape defined by the\\nFourier coefficients onto a 2D grid, generating an image where each pixel’s value is\\na function of its location relative to the contour. The entire pipeline, from Fourier\\ncoefficients to a rasterized shape image, is fully end-to-end differentiable. Furthermore,\\nby introducing regularization constraints based on signal energy principles, we guide\\nthe optimization towards generating shapes that are both physically plausible and\\nadversarially potent.\\nThis framework allows us to explore the role of shape in machine perception with\\nunprecedented control. Our experiments reveal three key findings. First, we demon-\\nstrate that shape alone is a powerful carrier of semantic information, capable\\nof generating high-confidence classifications from a DNN even in the complete absence\\nof texture; moreover, the strength of this semantic information gracefully scales with\\nthe shape’s complexity via the number of Fourier terms. Second, we repurpose our\\nmethod as a high-fidelity interpretability tool. For a given input image, by opti-\\nmizing a shape mask to be as small as possible while preserving correct classification,\\nwe can isolate a model’s region of interest with sharper, more interpretable bound-\\naries than existing methods like Grad-CAM [18]. Conversely, we show that occluding\\na small but critical region, while retaining the vast majority of the original image, is\\nsufficient to guarantee misclassification. Finally, we establish adversarial shapes\\nas a generalizable attack paradigm, analogous to colour-based adversarial patches\\n3\\n\\nFig. 2 Overview of the three experimental frameworks enabled by the differentiable\\nshape learning pipeline. a, Experiment 1: Class-specific shape generation. A set of Fourier coef-\\nficients, c = {ck}K\\nk=−K, is converted via the differentiable mapping into a gray-scale shape image.\\nThis image is fed directly into a classifier. The coefficients are optimized using gradient descent to\\nmaximize the classification confidence for a chosen target class, demonstrating the semantic repre-\\nsentation capability of shape alone. b, Experiment 2: Shape as an interpretability tool. The Fourier\\ncoefficients are mapped to a gray-scale image, which is used as a mask on a given natural image. The\\nmasked input is fed into a classifier. The coefficients are optimized using two symmetric objectives:\\n(1) to maximize the confidence for the true class while simultaneously minimizing the shape’s area,\\nthereby isolating the model’s minimal salient region; or (2) to minimize the true class confidence while\\nmaximizing the shape’s area, identifying the minimal critical region whose occlusion causes misclas-\\nsification. c, Experiment 3: Shape as a generalizable adversarial paradigm. The Fourier coefficients\\nare mapped to a gray-scale image, which is then rendered as an occlusion patch onto a target (e.g.,\\na person) in a natural image. The rendered input is fed into an object detector. The coefficients are\\noptimized to minimize the detection confidence scores for the occluded target, causing the model to\\nfail the detection task.\\nwith fixed shapes. We show that by optimizing the shape of a patch while keeping\\nits colour fixed, we can effectively cause a person covered by the shape to evade the\\nstate-of-the-art object detectors, demonstrating the method’s applicability to diverse\\ndownstream vision tasks.\\n2 Results\\nOur experiments demonstrate that the proposed adversarial shape learning frame-\\nwork is a powerful and versatile tool. We systematically show that our method can:\\n(1) generate shapes from scratch that carry sufficient semantic information to be clas-\\nsified as any target category by state-of-the-art models [35–40]; (2) serve as a novel,\\nhigh-fidelity visualization tool for interpreting a network’s decision-making process\\nby identifying salient object regions; and (3) function as a generalizable adversarial\\nparadigm, analogous to adversarial patches, that can be deployed in diverse down-\\nstream tasks such as object detection. The workflow for these experiments is illustrated\\nin Fig. 2.\\n4\\n\\nFig. 3 Adversarial shapes generated from scratch can embody class-specific semantics.\\na, Qualitative examples of generated shapes by the ResNet-50 model. Left, a shape generated to be\\nclassified as tench using a complexity of K = 10. Right, a more detailed shape generated for the\\ngolden retriever class using K = 25. The top-5 classification predictions and their confidence scores\\nare listed for each shape, demonstrating high confidence for the target class and semantically logical\\nsubsequent predictions. b, The effect of shape complexity on classification confidence for the ice bear\\nclass. As K increases from 5 to 25, the shape incorporates more detail, and the target confidence\\nmonotonically increases from 1.14% to 98.86%. c, Generalization of the learnable Fourier shape across\\ndiverse model architectures and all ImageNet classes. The plot displays the top-1 classification success\\nrate as a function of shape complexity. Each curve represents a different model architecture. The\\nsuccess rate for each point is the average across all 1,000 ImageNet classes. For all models tested, the\\nsuccess rate consistently exceeds 90% as K increases beyond 20.\\n2.1 Fourier shapes can embody class-specific semantic\\ninformation\\nTo investigate whether shape, in complete isolation from colour and texture, can func-\\ntion as an effective semantic carrier for DNNs, we designed an experiment to generate\\nclass-specific shapes from scratch. We employed a targeted optimization process where\\nthe Fourier coefficients defining a shape were iteratively updated to maximize the\\nclassification confidence score for a designated ImageNet [19] class on a pre-trained\\nResNet-50 model [35]. The input to the network was the grayscale image generated\\nby our differentiable mapping pipeline, containing only the optimized shape against a\\nblack background. This setup allows us to directly probe the geometric priors learned\\nby the network.\\nOur findings reveal that this process can successfully generate highly specific and\\nrecognizable shapes that effectively trigger the desired classification (Fig. 3). For\\ninstance, when targeting the tench class, our method generates a shape that is not\\nonly classified with the highest confidence as a tench but is also intuitively recog-\\nnizable to a human observer as the silhouette of a fish (Fig. 3a, left). This result\\n5\\n\\nprovides strong initial evidence that the network’s learned features for this class are\\nintrinsically linked to a distinct geometric form. Notably, the network’s subsequent\\npredictions (top-5) correspond to other visually similar aquatic creatures, such as coho\\nand hammerhead, suggesting that its confusion is semantically logical and rooted in\\nshared shape characteristics, rather than being an arbitrary failure mode.\\nThe capability of our method extends to more challenging, fine-grained categories\\nwhere shape cues are subtler. When tasked with generating a golden retriever, a cat-\\negory distinguished from other dog breeds by features that are often textural, the\\noptimization required a higher shape complexity (K = 25). The resulting shape, while\\nmore abstract to the human eye, was classified as a golden retriever with an excep-\\ntionally high confidence of 81.52% (Fig. 3a, right). Close inspection reveals that the\\nshape evolved to capture characteristic local details, such as the contours of the ears\\nand paws. Again, the model’s top-5 predictions were all other visually similar retriever\\nand spaniel breeds, reinforcing the notion that our method uncovers a hierarchy of\\ngeometric features learned by the model.\\nA key advantage of our Fourier representation is its parametric efficiency. A stan-\\ndard 224×224 pixel-space attack requires optimizing over 50,000 parameters, whereas\\nour shape, even with a high complexity of K = 25, is defined by only 2K + 1 = 51\\nlearnable parameters. This compactness does not sacrifice effectiveness. We found a\\ndirect and graceful correlation between the shape’s complexity K and the adversarial\\nsuccess. As K increases, the shape can incorporate finer details, leading to a mono-\\ntonic increase in the target class confidence score (Fig. 3b). This demonstrates that\\nthe model’s confidence is tied to the fidelity of the geometric details present in the\\nshape.\\nFinally, to confirm that this phenomenon is not specific to one model or a\\nfew object classes, we conducted a large-scale quantitative analysis. We system-\\natically generated shapes for all 1,000 ImageNet classes across a diverse suite of\\nseven leading architectures, including convolutional networks (ResNet-50, ResNet-101,\\nDenseNet121 [37], VGG16 [36], MobileNetv2 [38]) and vision transformers (ViT-B-\\n16 [39], SwinTransformer-B [40]). The results show a universally consistent trend:\\nfor all tested models, the success rate of generating a shape correctly classified as\\nthe target class increases monotonically with K. As the number of Fourier terms\\nincreases beyond 20, most architectures achieve high success rates (above 96%), while\\nthe Swin-Transformer also achieves a success rate of approximately 90% (Fig. 3c).\\nThis comprehensively demonstrates that our shape-learning framework is a general\\nand robust method for instantiating nearly any object category conceivable by modern\\ndeep learning models, using geometry as the sole information carrier.\\n2.2 Learnable shapes serve as a high-fidelity interpretability\\ntool\\nBuilding on the discovery that shapes can intrinsically carry class-specific semantics,\\nwe investigated whether our learnable shapes could be repurposed as a high-fidelity\\ntool to interpret the inner workings of DNNs. A central challenge in AI interpretabil-\\nity is to precisely identify the minimal visual evidence a model uses to make a specific\\n6\\n\\nclassification. Existing methods, such as gradient-based attribution maps, often pro-\\nduce coarse, diffuse heatmaps that highlight general areas of importance but lack\\nprecise boundaries. We therefore sought to determine if our framework could isolate\\nthese critical regions with geometric precision.\\n7\\n\\nFig. 4 Learnable Fourier shapes as a high-fidelity tool for model interpretability. a, The\\noptimization process for identifying the salient region in an image of a racket. As the number of\\niterations increases from 50 to 1,200, the shape mask progressively contracts to focus on the racket\\nhead, while the retained area decreases from 54.03% to 3.95%. Throughout this process, the masked\\ninput image is consistently classified correctly as racket with high confidence. b, Comparison of our\\nlearnable shape method with Grad-CAM for visualizing the salient regions for four different images.\\nFor each example (b1-b4), our method isolates a small, precise region with sharp boundaries (for\\nexample, 4.28% for tiger shark and 1.68% for candle) that is sufficient for correct top-1 classification.\\nIn contrast, the Grad-CAM heatmaps, generated from the final convolutional layer of a ResNet-50,\\nhighlight a more diffuse area. c, The effect of shape complexity (number of Fourier terms, K) on\\nlocalizing the salient region for an image of a tench. Increasing K from 1 to 8 allows the shape to\\nidentify a progressively smaller critical area, reducing the retained area from 18.78% to 4.21%. At\\nhigher complexities (K = 6 and K = 8), high-frequency tails can appear in non-salient regions, as\\nindicated by the red dashed circles. d, Results from the symmetric experiment, where the objective\\nis to induce misclassification by occluding the smallest possible critical region. For the great white\\nshark (d1), masking only the teeth and dorsal fin (occluding 26.17% of the image) results in a high-\\nconfidence misclassification to hammerhead. For the boxer (d2), masking the face (occluding 11.49%\\nof the image) leads to a misclassification as basenji.\\nTo achieve this, we designed a dual-objective optimization experiment. For a given\\ninput image from the ImageNet dataset, we optimize a Fourier shape mask that is\\nelement-wise multiplied with the image. The resulting masked image, which preserves\\nonly the visual information within the shape’s contour, is then fed into a ResNet-\\n50 model. The optimization is guided by two competing objectives: maximizing the\\nclassification confidence for the image’s true label while simultaneously minimizing\\nthe area of the shape mask. This process forces the shape to iteratively contract and\\nconverge upon the most compact and informative region essential for the model’s\\ndecision. Furthermore, we designed a symmetric experiment to validate these findings\\nby inverting the objectives. Specifically, we minimized the classification confidence\\nwhile maximizing the mask area to identify the smallest critical region that, when\\noccluded, guarantees misclassification.\\nOur findings demonstrate that the learnable shape dynamically and efficiently\\nconverges to the semantically salient regions of an object. The optimization process\\nreveals a clear, iterative focusing effect, where an initially large and simple shape\\ngradually refines its contour to tightly envelop the key features of the target object,\\nsuch as the racket head in a tennis image (Fig. 4a). The final optimized shape often\\nconstitutes a remarkably small fraction of the original image area; for instance, the\\nmodel can correctly classify the racket image with high confidence even when only\\n3.95% of the original pixels are retained.\\nWhen compared with established visualization tools like Grad-CAM [18], our\\nmethod offers substantially improved precision and interpretability (Fig. 4b). Grad-\\nCAM, which relies on the gradients of the final convolutional feature maps, inherently\\nproduces low-resolution, diffuse heatmaps that are spatially coarse. In contrast, our\\nshape-based approach generates masks with sharp, unambiguous boundaries derived\\ndirectly from the winding number calculation. This allows for a much clearer delin-\\neation of the model’s focus. For a candle image, our method identified that a mere\\n1.68% of the image, precisely covering the flame and wick, was sufficient for cor-\\nrect classification, providing a far more concentrated and interpretable result than\\n8\\n\\nthe corresponding heatmap (Fig. 4b, bottom right). This heightened precision stems\\nfrom our explicit optimization objective to minimize area, a constraint not present in\\nattribution-based methods.\\nWe also analyzed how the shape’s complexity K influences this localization task\\n(Fig. 4c). We observed that shapes with higher complexity (larger K) are capable of\\nidentifying smaller and more intricate salient regions. This is because the increased\\nflexibility allows the contour to carve out non-essential areas with greater precision.\\nHowever, this flexibility comes with a trade-off. At higher K values (e.g., K = 8),\\nthe optimized shape can develop self-intersections or high-frequency tails that extend\\ninto non-salient background areas, albeit covering a negligible area. These artifacts\\nmay arise partly from the optimization dynamics, where all coefficients are updated\\njointly. More fundamentally, these high-frequency tails are a manifestation of the\\nshape’s own strong semantic information, as demonstrated in our first experiment.\\nThis suggests that the shape’s own semantics could potentially interfere with the\\ngoal of purely isolating the image’s salient features. We therefore recommend using a\\nmoderate complexity (e.g., K = 6), as it strikes an effective balance, enabling fine-\\ngrained localization while mitigating the introduction of confounding shape-based\\npriors.\\nFinally, the symmetric experiment, where we aimed to retain as much of the\\noriginal image as possible while inducing misclassification, offers compelling insights\\ninto the model’s failure modes (Fig. 4d). For an image of a great white shark, the\\nalgorithm learned to precisely mask out the teeth and dorsal fin. Despite preserving\\n73.83% of the image, the model’s classification switched to hammerhead with high\\nconfidence. Similarly, occluding the face of a boxer dog was sufficient to cause a mis-\\nclassification as a basenji. These results starkly reveal the model’s heavy reliance on\\na few local, discriminative features. Unlike human perception, which often relies on\\na holistic understanding of the object, the model’s decision can be completely over-\\nturned by the absence of these key features, revealing a potential vulnerability in their\\ndecision-making process.\\n2.3 Adversarial shapes as a generalizable attack paradigm for\\ndownstream tasks\\nWe next investigated whether the Fourier shape could be generalized to function as a\\nnew adversarial paradigm for complex, downstream vision tasks [41–44]. This positions\\nour method as a conceptual analogue to colour-based adversarial patches [23–26],\\nwhich have proven effective in the physical world [13, 14, 27]. While those methods\\noptimize the texture within a fixed, simple shape (such as a square), we invert this\\nconcept: we optimize the shape itself while keeping its internal colour fixed (e.g., solid\\nwhite), thereby isolating the adversarial power of pure geometry.\\nTo test this paradigm, we targeted the object detection, a basic task of real-\\nworld computer vision systems. We designed an experiment to make a target object,\\nspecifically a person, invisible to a pre-trained YOLOv3 detector [42]. For a given\\nimage containing a person, our Fourier shape was rendered as a solid white patch onto\\nthe target. The patch was scaled relative to the person’s bounding box (e.g., 0.6× the\\nheight and width) and centred on the object. The resulting image was then fed to the\\n9\\n\\nFig. 5 Adversarial shapes as a generalizable attack paradigm for object detection. a,\\nQualitative results of the shape attack against the YOLOv3 detector. In each pair, the left image\\nshows the benign detection (person detected, green box) and the right image shows the attacked\\nversion. The optimized white Fourier shape (K = 10) causes the detector to fail, and the person is no\\nlonger detected (detection confidence ≤0.5). b, Comparison of the optimized Fourier shape against\\nsimple geometric occlusions of similar area. While simple shapes (rectangle, ellipse, triangle, star)\\nhave a negligible effect on detection confidence (e.g., 93.2% - 94.9%), the optimized shape reduces\\nthe confidence to 15.9%, successfully evading detection. c, Quantitative ablation on the effect of\\nshape complexity across a set of 140 COCO images. The Attack Success Rate (ASR) vs. Confidence\\nplot (left) shows that ASR (higher is better) increases with higher K. The Precision-Recall (PR)\\ncurves (right) show that the Average Precision (AP, lower is better) for the person class decreases\\nas K increases. d, Generalization of the shape attack (K = 10) across diverse detector architectures\\n(YOLOv3, RetinaNet, and FCOS). The ASR-Confidence plot (left) shows the attack is effective\\nagainst all models. The PR curves (right) show a significant performance degradation for all attacked\\nmodels (solid lines) compared to their benign baselines (dashed lines).\\n10\\n\\ndetector. Our optimization objective was to minimize the objectness confidence scores\\nfor all detection proposals associated with the target, thereby causing the detector to\\nmiss the person entirely (a false negative).\\nThe qualitative results are striking (Fig. 5a). In benign images, the detector\\nrobustly identifies the person class with high confidence. After the optimized Fourier\\nshape is applied, the person becomes invisible to the detector; the associated bound-\\ning box and confidence score disappear, even though the person remains partially\\nvisible to a human observer. This suggests the shape’s optimized geometry introduces\\nadversarial information that effectively overrides the detector’s learned features for\\nthe person category.\\nA critical question is whether this effect is due to the specific optimized geometry\\nor simply to the act of occlusion. To answer this, we conducted a control experiment\\ncomparing our optimized shape to simple, non-optimized geometric shapes (e.g., a\\nrectangle, ellipse, or star) of similar area (Fig. 5b). The simple shapes had a negligible\\nimpact on the detector’s confidence, which remained high (e.g., 93.2% to 94.9%). In\\ncontrast, our optimized Fourier shape decimated the confidence score to 15.9%, well\\nbelow the typical detection threshold. This finding is crucial, as it demonstrates that\\nthe attack’s potency stems not from mere occlusion, but from the specific, learned\\ngeometric contours of the shape itself.\\nWe further quantified this effect by evaluating the attack on a set of 140 images\\nfrom the COCO dataset [41], analyzing performance as a function of shape complexity.\\nWe used two metrics: the Attack Success Rate (ASR) at different confidence thresh-\\nolds, and the degradation in the model’s Precision-Recall (PR) curve. A higher ASR\\ncurve indicates a more potent attack, as does a lower, more suppressed PR curve. The\\nresults clearly show that attack efficacy scales with shape complexity (Fig. 5c). As K\\nincreases, the shape becomes more intricate, the ASR curves shift upwards, and the\\nPR curves are pressed further downwards, indicating a greater drop in the model’s\\nAverage Precision (AP).\\nFinally, to demonstrate the generalizability of this paradigm, we deployed the\\nattack against three representative detectors: YOLOv3, RetinaNet [43], and FCOS\\n[44]. The shape attack proved universally effective, significantly degrading the perfor-\\nmance of all three models (Fig. 5d). The consistent drop in the PR curves (solid lines)\\ncompared to their benign baselines (dashed lines) confirms that adversarial shapes\\nare a robust attack vector, capable of exploiting vulnerabilities in both anchor-based\\n(YOLOv3, RetinaNet) and anchor-free (FCOS) detectors.\\nThese results position adversarial shapes as a viable new attack modality with\\nsignificant implications for real-world robustness. Unlike texture-based patches, which\\nare highly sensitive to colour distortion from lighting and camera sensors, a shape-\\nbased attack encodes its adversarial information in its geometry, an attribute that is\\nmore resilient to such physical-world variations. While we used a simple white patch\\nfor these experiments, this framework opens the door to hybrid attacks that could\\noptimize both shape and colour simultaneously.\\n11\\n\\n3 Discussion\\nIn this work, we have established a paradigm for understanding and interacting\\nwith DNNs through the direct, holistic optimization of an object’s geometry. We\\ndemonstrated the profound potential of this learnable shape framework through three\\ndistinct lines of inquiry. First, we showed that shape, in complete isolation from colour\\nand texture, can act as a potent carrier of semantic information, capable of eliciting\\nhigh-confidence, class-specific responses from well-trained models. Second, we repur-\\nposed this framework as a high-fidelity interpretability tool, capable of isolating a\\nmodel’s critical regions of interest with a precision and clarity that surpasses exist-\\ning attribution methods. Finally, our findings establish shape as a new, generalizable\\nmodality for adversarial attacks, conceptually analogous to adversarial patches but\\noperating in a fundamentally different domain, with broad applicability to diverse\\nvisual tasks. The success of our approach hinges on a fully end-to-end differentiable\\npipeline. The use of a Fourier series provides a compact, yet powerful, parameteri-\\nzation for arbitrary closed contours. This abstract representation is then analytically\\nbridged to the pixel space required by DNNs via a differentiable mapping derived\\nfrom the winding number theorem. The framework is further guided by regulariza-\\ntion constraints, inspired by signal processing principles, that ensure the generation\\nof plausible, naturalistic shapes. Together, these components create a novel and effi-\\ncient methodology for exploring the vast space of geometric forms in the context of\\nmachine perception.\\nThe implications of this work extend beyond adversarial analysis, opening up\\nseveral promising avenues for future research. First, our framework enables a more tar-\\ngeted data augmentation strategy. By precisely masking a model’s most relied-upon\\nfeatures, we can compel networks to learn from a more holistic global context rather\\nthan exploiting local shortcuts, potentially leading to substantial improvements in gen-\\neralization. Second, this work invites exploration into more sophisticated optimization\\nstrategies. Instead of a joint optimization of all Fourier coefficients, a staged approach\\ncould be employed. This might involve first locating a region of interest with low-\\nfrequency terms and then refining the details with high-frequency ones, perhaps using\\nfrequency-specific learning rates, which could mitigate artifacts and enhance efficiency.\\nFinally, the principles established here can be extended to three-dimensional surfaces.\\nBy parameterizing a 3D mesh with a spherical Fourier series, one could directly opti-\\nmize a 3D object to attack models operating on point clouds [45] or volumetric data\\n[46]. This top-down generative process would bypass many of the complex smooth-\\nness constraints required by traditional mesh manipulation methods [47], offering a\\npowerful new direction for investigating and challenging the frontiers of 3D machine\\nperception.\\n4 Methods\\n4.1 Overview\\nOur method introduces a novel framework for generating adversarial shapes by directly\\noptimizing the geometric form of an object, fundamentally shifting the paradigm\\n12\\n\\nof adversarial attacks from the colour domain to the shape domain. Conventional\\nadversarial attacks manipulate the input by adding subtle, often imperceptible, pixel-\\nlevel perturbations to an existing image. Similarly, adversarial patches introduce a\\nlocalized but fixed-shape pattern onto an object. While effective, these methods do not\\nalter the object’s intrinsic geometry. Our approach, in contrast, generates a holistic\\nand physically realizable shape from scratch, defined by a continuous boundary, that\\nis optimized to deceive a DNN.\\nThe core innovation lies in creating a fully differentiable pipeline that connects a\\nparametric representation of a closed shape to the output of a target DNN. This is\\nachieved through three key stages. First, we model an arbitrary 2D closed shape using\\na Fourier series representation, which provides a compact and powerful parameteriza-\\ntion capable of describing a vast family of complex geometries. Second, we introduce\\na differentiable mapping module based on the winding number theorem from com-\\nplex analysis. This module analytically transforms the Fourier coefficients into a 2D\\nrasterized image of the shape, where each pixel’s value is determined by its position\\nrelative to the shape’s boundary. This step is crucial as it builds a differentiable bridge\\nbetween the abstract shape parameters and the pixel space that DNNs operate on.\\nFinally, with this end-to-end differentiable pipeline, we can feed the generated shape\\nimage into the target DNN and compute the adversarial loss. The gradient of this\\nloss is then backpropagated all the way to the Fourier coefficients, allowing us to iter-\\natively update and grow a shape that maximally fools the network. To ensure the\\ngenerated shapes are both physically plausible and effective for attack, we introduce\\na set of regularization constraints based on signal processing principles, which govern\\nthe energy distribution across different frequency components of the shape.\\n4.2 Shape Modeling\\nTo mathematically represent any arbitrary 2D closed contour in a continuous and\\ndifferentiable manner, we employed a Fourier series representation. This powerful\\ntechnique can approximate any periodic function as an infinite sum of sine and cosine\\nfunctions. By treating the x and y coordinates of a shape’s boundary as functions of a\\nparameter t that traverses the contour, we can define the shape in the complex plane.\\nA shape F(t) is thus represented as:\\nF(t) = f(t) + i · g(t) =\\nK\\nX\\nk=−K\\nckeikt\\nfor\\nt ∈[0, 2π]\\n(1)\\nwhere f(t) and g(t) are the Cartesian coordinates of the boundary, i is the imaginary\\nunit, and t is the parameter that sweeps along the contour. The shape is defined by\\na set of complex Fourier coefficients, ck = ak + ibk, which are the parameters we aim\\nto optimize. The integer K determines the complexity, or degrees of freedom, of the\\nshape.\\nIntuitively, each coefficient ck controls a specific geometric characteristic of the\\nshape:\\n13\\n\\n• DC Offset (c0): This zero-frequency term is a complex number representing the\\nshape’s centre of mass.\\n• Fundamental Frequencies (c1, c−1): These terms, corresponding to k = 1 and k =\\n−1, define the fundamental elliptical or circular form of the shape. They dictate\\nits overall scale, elongation, and orientation. A simple circle, for instance, can be\\ndefined by setting c1 to a real number and all other coefficients to zero.\\n• Higher Harmonics (ck, |k| ≥2): These coefficients add progressively finer details and\\ncomplexity to the base ellipse. For instance, c2 and c−2 might introduce a twofold\\nsymmetry (like a peanut shape), while c3 and c−3 could add a threefold symmetry\\n(like a cloverleaf). By combining these harmonics, we can construct an immense\\nvariety of intricate shapes.\\nThe primary advantage of this representation is its compactness and differentia-\\nbility. Instead of optimizing tens of thousands of pixels, we only need to optimize a\\nsmall set of 2 ∗(2K + 1) real-valued parameters (ak and bk), making the optimization\\nprocess highly efficient. Crucially, the shape’s coordinates f(t) and g(t) are analytic\\nfunctions of these coefficients, which is a prerequisite for gradient-based optimization.\\n4.3 Differentiable Mapping\\nA key challenge is to bridge the parametric shape representation with the grid-like\\ninput required by a DNN. We need a differentiable process that can \"draw\" the\\nshape onto a 2D canvas. We achieve this using a robust method derived from the\\nwinding number theorem. The winding number, W, quantifies how many times a\\nclosed curve travels counter-clockwise around a given point (x0, y0). For a simple,\\nnon-self-intersecting closed curve, the winding number is 1 for any point inside the\\ncurve and 0 for any point outside. This binary property provides a perfect criterion\\nfor defining the interior of our shape.\\nThe winding number can be calculated via the following line integral along the\\ncurve C:\\nW(x0, y0) = 1\\n2π\\nI\\nC\\n(x −x0)dy −(y −y0)dx\\n(x −x0)2 + (y −y0)2\\n(2)\\nBy substituting our parametric expressions x = f(t), y = g(t), dx = f ′(t)dt, and\\ndy = g′(t)dt, we can express the winding number as an integral over the parameter t:\\nW(x0, y0) = 1\\n2π\\nZ 2π\\n0\\n(f(t) −x0)g′(t) −(g(t) −y0)f ′(t)\\n(f(t) −x0)2 + (g(t) −y0)2\\ndt\\n(3)\\nThis integral gives the winding number for a single point (x0, y0). To generate a full\\nimage, we evaluate this integral for every pixel coordinate (xp, yp) in our target image\\ngrid I. The value of each pixel I(p) is thus a function of the winding number at its\\nlocation. In practice, we implement this integral in a discrete and differentiable form.\\nBy sampling N points tj = j · (2π/N) along the curve, the integral is approximated\\n14\\n\\nby the following differentiable sum:\\nW(x0, y0) ≈1\\nN\\nN−1\\nX\\nj=0\\n(f(tj) −x0)g′(tj) −(g(tj) −y0)f ′(tj)\\n(f(tj) −x0)2 + (g(tj) −y0)2\\n(4)\\nThis entire process, from the Fourier coefficients ck to the final raw image I, is fully\\ndifferentiable. The resulting image I contains pixel values that are floating-point\\napproximations of the true winding number at each coordinate. For a simple, non-\\nself-intersecting curve, these values will be close to 1 for the interior and close to 0 for\\nthe exterior. During optimization, the shape may self-intersect, resulting in regions\\nwhere the calculated values approximate other integers (e.g., 2, -1). The key insight is\\nthat any region with a calculated value significantly deviating from zero corresponds\\nto the shape’s interior. Therefore, to create a robust mask for the DNN, we process\\nthe raw image I by first taking its absolute value. This step ensures that regions\\napproximating both 1 and -1 are treated as positive. We then clip the values to the\\nrange [0, 1]. This normalization effectively thresholds the continuous-valued winding\\nnumber field, mapping all significant interior regions (where the approximate wind-\\ning number’s absolute value is high) towards a value of 1 and the exterior towards 0,\\ncreating a well-formed input for the network. The automatic differentiation engines\\nin modern deep learning frameworks like PyTorch can therefore compute the exact\\ngradients ∂I(p)/∂ck for every pixel. This allows the adversarial loss, computed from\\nthe DNN’s output, to flow back and directly inform the update of the shape’s defining\\nparameters.\\n4.4 Regularization Constraints\\nUnconstrained optimization of the Fourier coefficients can lead to shapes that are\\nphysically unrealistic or contain excessive high-frequency noise. Such shapes may\\nbe effective in simulation but are not meaningful as real-world adversarial objects.\\nTo guide the optimization towards plausible and robust shapes, we introduce two\\nregularization terms into our loss function, based on signal energy principles.\\nFirst, we enforce fundamental frequency dominance. The overall structure of\\na natural object is typically defined by its low-frequency components. We therefore\\nconstrain the energy of the fundamental frequencies (c1, c−1) to be dominant over the\\nhigher harmonics. We define the sum of fundamental and harmonic amplitudes as\\nSfund = |c1|+|c−1| and Sharm = PK\\n|k|=2 |ck|, respectively. The constraint is formulated\\nas a penalty term added to the loss if the following condition is violated:\\nSfund ≥λ · Sharm\\n(5)\\nwhere λ > 1 is a hyperparameter that enforces the desired dominance (e.g., λ = 2).\\nThis encourages the optimization to first establish a stable, low-frequency base shape\\nbefore adding details.\\nSecond, we impose an individual higher harmonic amplitude limit. While\\nhigher harmonics are essential for crafting the specific features that deceive the net-\\nwork, allowing any single harmonic to become excessively strong can create unrealistic,\\n15\\n\\nspiky artifacts. We therefore limit the amplitude of each individual higher harmonic\\ncoefficient to be no more than a fraction, γ, of the total fundamental amplitude:\\n|ck| ≤γ · Sfund\\nfor all\\n|k| ≥2\\n(6)\\nwhere γ is a hyperparameter (e.g., γ = 0.25). This constraint ensures that the\\nhigh-frequency details serve to refine the shape rather than dominate its structure.\\nTogether, the overall regularization constraint can be formulated as:\\nLreg = ReLU(λSharm −Sfund) +\\nK\\nX\\n|k|=2\\nReLU(|ck| −γSfund)\\n(7)\\nThe Lreg acts as a prior for plausible geometries, accelerating convergence and\\nresulting in smoother, more robust adversarial shapes.\\n4.5 Optimization Objectives\\nTo formally describe the optimization process for our three main experiments, we\\ndefine the following. Let c = {ck}K\\nk=−K be the set of optimizable Fourier coefficients.\\nLet I(c) be the normalized grayscale image generated from these coefficients. Let C(·)\\nbe a classification network that outputs a probability distribution over classes, and\\nlet D(·) be a detection network.\\nExperiment 1: Class-Specific Shape Generation. To generate a shape that\\nembodies a target class ytarget, we optimize the coefficients c by minimizing the\\nnegative log-probability of the target class, combined with the regularization loss:\\nL1(c) = −log(C(I(c))ytarget) + λreg · Lreg\\n(8)\\nwhere λreg is a weighting hyperparameter for the regularization term.\\nExperiment 2: Shape as an Interpretability Tool. For a given natural image\\nxnat with true label ytrue, we optimize a shape mask I(c) that is element-wise mul-\\ntiplied with the image. To identify the minimal salient region, we maximize the\\nconfidence for the true class while minimizing the mask area:\\nL2a(c) = −log(C(xnat ⊙I(c))ytrue) + λarea · mean(I(c)) + λreg · Lreg\\n(9)\\nTo identify the minimal region to occlude for misclassification, we minimize the\\nconfidence for the true class while maximizing the mask area:\\nL2b(c) = log(C(xnat ⊙I(c))ytrue) −λarea · mean(I(c)) + λreg · Lreg\\n(10)\\nwhere λarea is a weighting hyperparameter for the area term.\\nExperiment 3: Shape as an Adversarial Patch. For a given image xnat con-\\ntaining foreground objects specified by bounding boxes B, we render the shape as an\\nocclusion patch. Let R(xnat, I(c), B) be the function that renders the shape onto the\\n16\\n\\nimage at the specified locations. The goal is to minimize the objectness scores of all\\ndetections associated with the foreground objects occluded by the shape. Let {oj} be\\nthe set of these corresponding object confidence scores output by the detector D. The\\nloss is:\\nL3(c) =\\nX\\nj\\n−log(1 −oj) + λreg · Lreg\\nwhere\\n{oj} from D(R(xnat, I(c), B)) (11)\\nReferences\\n[1] Mahner, F. P. et al. Dimensions underlying the representational alignment of deep\\nneural networks with humans. Nature Machine Intelligence, 7(6), 848-859, 2025.\\n[2] Geirhos, R. et al. ImageNet-trained CNNs are biased towards texture; increasing\\nshape bias improves accuracy and robustness. In Proc. International Conference\\non Learning Representations, 2018.\\n[3] Woods, W., Chen, J., and Teuscher, C. Adversarial explanations for understanding\\nimage classification decisions and improved neural network robustness. Nature\\nMachine Intelligence, 1(11), 508-516, 2019.\\n[4] Ignatiev, A., Narodytska, N., and Marques-Silva, J. On relating explanations\\nand adversarial examples. In Proc. Advances in Neural Information Processing\\nSystems, 32, 2019.\\n[5] Ilyas, A. et al. Adversarial examples are not bugs, they are features. In Proc.\\nAdvances in Neural Information Processing Systems, 32, 2019.\\n[6] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional net-\\nworks: Visualising image classification models and saliency maps. arXiv preprint\\narXiv:1312.6034, 2013.\\n[7] Zeiler, M. D., and Fergus, R. Visualizing and understanding convolutional\\nnetworks. In Proc. European Conference on Computer Vision, 818-833, 2014.\\n[8] Ghaffari Laleh, N. et al. Adversarial attacks and adversarial robustness in\\ncomputational pathology. Nature Communications, 13(1), 5711, 2022.\\n[9] Veerabadran, V. et al. Subtle adversarial image manipulations influence both\\nhuman and machine perception. Nature Communications, 14(1), 4933, 2023.\\n[10] Paniagua, T., Savadikar, C., and Wu, T. Adversarial perturbations are formed\\nby iteratively learning linear combinations of the right singular vectors of the\\nadversarial jacobian. In Proc. International Conference on Machine Learning,\\n2025.\\n17\\n\\n[11] Doerig, A. et al. High-level visual representations in the human brain are aligned\\nwith large language models. Nature Machine Intelligence, 1-15, 2025.\\n[12] Dutta,\\nS.\\nK.,\\nand\\nZhang,\\nX.\\nIAP:\\nInvisible\\nadversarial\\npatch\\nattack\\nthrough perceptibility-aware localization and perturbation optimization. In Proc.\\nIEEE/CVF International Conference on Computer Vision, 2025.\\n[13] Wang, J. et al. Physically realizable adversarial creating attack against vision-\\nbased BEV space 3D object detection. IEEE Transactions on Image Processing,\\n34, 538-551, 2025\\n[14] Wang, J. et al. A unified framework for adversarial patch attacks against visual\\n3D object detection in autonomous driving. IEEE Transactions on Circuits and\\nSystems for Video Technology, 35(5), 4949-4962, 2025.\\n[15] Lee, C., Song, Y., and Son, J. Data-free universal adversarial perturbation with\\npseudo-semantic prior. In Proc. IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 13907-13916, 2025.\\n[16] Wang, T. et al. Exploring the adversarial vulnerabilities of vision-language-action\\nmodels in robotics. In Proc. IEEE/CVF International Conference on Computer\\nVision, 2025.\\n[17] Fang, H. et al. One perturbation is enough: On generating universal adversarial\\nperturbations against vision-language pre-training models. In Proc. IEEE/CVF\\nInternational Conference on Computer Vision, 2025.\\n[18] Selvaraju, R. R. et al. Grad-cam: Visual explanations from deep networks via\\ngradient-based localization. In Proc. IEEE International Conference on Computer\\nVision, 618-626, 2017.\\n[19] Deng, J. et al. Imagenet: A large-scale hierarchical image database. In Proc. IEEE\\nConference on Computer Vision and Pattern Recognition, 248-255, 2009.\\n[20] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adver-\\nsarial examples. arXiv preprint arXiv:1412.6572, 2014.\\n[21] Szegedy, C. et al. Intriguing properties of neural networks. arXiv preprint\\narXiv:1312.6199, 2013.\\n[22] Brown, T. B. et al. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.\\n[23] Thys, S., Van Ranst, W., and Goedemé, T. Fooling automated surveillance\\ncameras: adversarial patches to attack person detection. In Proc. IEEE/CVF\\nConference on Computer Vision and Pattern Recognition Workshops, 2019.\\n[24] Guesmi, A. et al. Dap: A dynamic adversarial patch for evading person detectors.\\nIn Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n18\\n\\npp. 24595-24604, 2024.\\n[25] Hu, Y. C. T. et al. Naturalistic physical adversarial patch for object detectors. In\\nProc. IEEE/CVF International Conference on Computer Vision, 7848-7857, 2021.\\n[26] Lian, J. et al. Benchmarking adversarial patch against aerial detection. IEEE\\nTransactions on Geoscience and Remote Sensing, 60, 1-16, 2022.\\n[27] Cheng, Z. et al. Physical attack on monocular depth estimation with optimal\\nadversarial patches. In Proc. European Conference on Computer Vision, 514-532,\\n2022.\\n[28] Chen, Z. et al. Shape matters: deformable patch attack. In Proc. European\\nConference on Computer Vision, 529-548, 2022.\\n[29] Wei, X., Yu, J., and Huang, Y. Infrared adversarial patches with learnable shapes\\nand locations in the physical world. International Journal of Computer Vision,\\n132(6), 1928-1944, 2024.\\n[30] Wei, X., Yu, J., and Huang, Y. Physically adversarial infrared patches with learn-\\nable shapes and locations. In Proc. IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, 12334-12342, 2023.\\n[31] Zhu, X. et al. Infrared invisible clothing: Hiding from infrared detectors at mul-\\ntiple angles in real world. In Proc. IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition, 13317-13326, 2022.\\n[32] Zhu, X. et al. Fooling thermal infrared pedestrian detectors in real world using\\nsmall bulbs. In Proc. AAAI Conference on Artificial Intelligence, 35(4), 3616-3624,\\n2021.\\n[33] Wei, X. et al. Unified adversarial patch for visible-infrared cross-modal attacks\\nin the physical world. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 46(4), 2348-2363, 2023.\\n[34] Wei, X. et al. Unified adversarial patch for cross-modal attacks in the physical\\nworld. In Proc. IEEE/CVF International Conference on Computer Vision, 4445-\\n4454, 2023.\\n[35] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recog-\\nnition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition,\\n770-778, 2016.\\n[36] Simonyan, K., and Zisserman, A. Very deep convolutional networks for large-scale\\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\\n[37] Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely con-\\nnected convolutional networks. In Proc. IEEE Conference on Computer Vision\\n19\\n\\nand Pattern Recognition, 4700-4708, 2017.\\n[38] Sandler, M. et al. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proc.\\nIEEE Conference on Computer Vision and Pattern Recognition, 4510-4520, 2018.\\n[39] Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image\\nrecognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n[40] Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted\\nwindows. In Proc. IEEE/CVF International Conference on Computer Vision,\\n10012-10022, 2021.\\n[41] Lin, T. et al. Microsoft coco: Common objects in context. In Proc. European\\nConference on Computer Vision, 740-755, 2014.\\n[42] Redmon, J., and Farhadi, A. Yolov3: An incremental improvement. arXiv preprint\\narXiv:1804.02767, 2018.\\n[43] Lin, T. Y. et al. Focal loss for dense object detection. In Proc. IEEE International\\nConference on Computer Vision, 2980-2988, 2017.\\n[44] Tian, Z. et al. Fcos: Fully convolutional one-stage object detection. In Proc.\\nIEEE/CVF International Conference on Computer Vision, 9627-9636, 2019.\\n[45] Wen, Y. et al. Geometry-aware generation of adversarial point clouds. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, 44(6), 2984-2999,\\n2020.\\n[46] Tu, J. et al. Physically realizable adversarial examples for lidar object detection.\\nIn Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition,\\n13716-13725, 2020.\\n[47] Lou, T. et al. Hide in thicket: Generating imperceptible and rational adversarial\\nperturbations on 3d point clouds. In Proc. IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, 24326-24335, 2024.\\n20\\n',\n",
       " 'Sharing the Learned Knowledge-base to Estimate\\nConvolutional Filter Parameters for Continual Image\\nRestoration\\nAupendu Kar*\\nDolby Laboratories, Inc\\nIndia\\nKrishnendu Ghosh\\nIndian Institute of Technology\\nKharagpur\\nIndia\\nPrabir Kumar Biswas\\nIndian Institute of Technology\\nKharagpur\\nIndia\\nABSTRACT\\nContinual learning is an emerging topic in the field of deep learning,\\nwhere a model is expected to learn continuously for new upcoming\\ntasks without forgetting previous experiences. This field has wit-\\nnessed numerous advancements, but few works have been attempted\\nin the direction of image restoration. Handling large image sizes and\\nthe divergent nature of various degradation poses a unique challenge\\nin the restoration domain. However, existing works require heavily\\nengineered architectural modifications for new task adaptation, re-\\nsulting in significant computational overhead. Regularization-based\\nmethods are unsuitable for restoration, as different restoration chal-\\nlenges require different kinds of feature processing. In this direction,\\nwe propose a simple modification of the convolution layer to adapt\\nthe knowledge from previous restoration tasks without touching the\\nmain backbone architecture. Therefore, it can be seamlessly applied\\nto any deep architecture without any structural modifications. Un-\\nlike other approaches, we demonstrate that our model can increase\\nthe number of trainable parameters without significantly increasing\\ncomputational overhead or inference time. Experimental validation\\ndemonstrates that new restoration tasks can be introduced without\\ncompromising the performance of existing tasks. We also show that\\nperformance on new restoration tasks improves by adapting the\\nknowledge from the knowledge base created by previous restoration\\ntasks. The code is available at https://github.com/aupendu/continual-\\nrestore\\nCCS CONCEPTS\\n• Computing methodologies →Supervised learning.\\nKEYWORDS\\nContinual learning, deep learning, image restoration\\nACM Reference Format:\\nAupendu Kar, Krishnendu Ghosh, and Prabir Kumar Biswas. 2025. Sharing\\nthe Learned Knowledge-base to Estimate Convolutional Filter Parameters\\nfor Continual Image Restoration. In Proceedings of 16th Indian Conference\\non Computer Vision, Graphics and Image Processing (ICVGIP’25). ACM,\\nNew York, NY, USA, 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n*Work done while Aupendu Kar was at the Indian Institute of Technology Kharagpur\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nICVGIP’25, December 2025, Mandi, India\\n© 2025 Copyright held by the owner/author(s).\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM.\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1\\nINTRODUCTION\\nDue to various weather conditions and adverse natural phenomena,\\ncaptured images often suffer from various types of degradation\\n- the presence of dust and aerosols causes hazing, rain or water\\ndroplets cause poor image visibility, camera and object motion cause\\nblurring, to name a few. These degradations diminish image quality\\nand clarity, which in turn affect various downstream tasks, such as\\nmedical imaging [13] and surveillance applications [36].\\nFigure 1: The proposed convolutional filter estimation block.\\n𝑀𝑤is continual memory, 𝑇𝑤is restoration task-specific weight\\nvector, and 𝐾𝑤is the estimated convolutional kernel.\\nSince the advent of Deep Learning, several image restoration-\\nspecific deep learning techniques have been proposed by researchers\\nto mitigate the effects of image degradation factors, such as image\\ndehazing [10, 30], deblurring [37], and deraining [8, 11]. Instead of\\ndesigning a specific degradation model, several architectures have\\nbeen proposed to handle different types of restoration [7, 42]. Most\\nof these proposed restoration methods depend entirely on the current\\ntraining samples and drastically forget the learned parameters if a\\nnew restoration task is introduced, a phenomenon commonly known\\nas catastrophic forgetting [28]. This severe drawback renders a deep\\nneural network ineffective for a previously trained task, limiting its\\napplication to the current restoration task.\\nSeveral approaches [15, 38] have been introduced to address the\\nproblem of catastrophic forgetting. Kirkpatrick et al. [20] first pro-\\nposed a parameter regularization-based algorithm where the move-\\nment of weights important to previous tasks is restricted using a\\nquadratic constraint. Memory-aware synapses (MAS) [3] also re-\\nstrict the change of weights critical to previously learned tasks. It\\ndetermines the relative importance of weights by computing the sen-\\nsitivity of the output function with respect to each weight parameter\\nin the network. Learning without forgetting (LwF) [23] proposes\\nto impose parameter regularization by using a distillation loss. A\\ngradual pruning-based method [27] is employed to compress the\\nparameter space for a specific task and reuse the previously fixed pa-\\nrameters for an upcoming task. These regularization-based networks\\nhave a fixed capacity, and their performance gradually reduces as\\narXiv:2511.05421v1  [cs.CV]  7 Nov 2025\\n\\nICVGIP’25, December 2025, Mandi, India\\ndissimilar tasks are added to the network [17]. These make apply-\\ning them directly for image restoration tasks difficult, as the image\\ndegradation factors vary significantly in real life.\\nNetworks with dynamic architectures, such as Progressive Neural\\nNets [34], aim to alleviate the problem of fixed capacity by adding\\nnew sub-networks for each new task. However, this comes with\\nexcessive computational overhead, limiting its application to edge\\ndevices with constrained computational resources. [17] extended\\nthe idea of PackNet [27] to allow the expansion of the network by\\nincreasing the dimension of the CNN filters to accommodate new\\ntasks. However, the simple expansion of the filter dimension causes\\nan increase in computation in an 𝑂(𝑛) manner per pixel. In image\\nrestoration tasks, the high-resolution input images are commonly\\nused and generally forward-propagated throughout the network ar-\\nchitecture without any downsampling operation. Therefore, the com-\\nputational burden increases significantly, and with deeper networks,\\nthe problem compounds to an even higher degree. Methods like\\nLIRA [24] handle multiple degradations in a single image, where\\nthey utilize different task-specific expert networks for each degra-\\ndation task and a common base network shared among all tasks.\\nDue to the introduction of a new sub-network for each new task,\\nsignificant computation overhead is added, and the network size\\nincreases substantially with the addition of new tasks. [44] proposes\\nlifelong learning for image restoration, focusing on a single task of\\nderaining by allowing the network to continually learn from different\\nrain datasets.\\nDistinct from these previous works, we aim to address the cat-\\nastrophic forgetting problem for various image restoration tasks\\nwithout incurring any significant computational burden. For this\\npurpose, we propose a simple modification of the convolution layer,\\nwhere the convolution layer in the network is factored into two parts:\\na task-dependent, learnable vector and a task-independent, learnable\\nweight matrix. The task-independent weight matrix constructs a\\nknowledge base for all restoration tasks and facilitates knowledge\\nsharing from previous tasks by storing and reusing the earlier learned\\nparameters for upcoming restoration tasks. In our method, the net-\\nwork is trained sequentially for each new restoration problem. For the\\nfirst restoration task, a preassigned portion of the task-independent\\nweight matrix is trained along with a task-dependent weight vector,\\nthe product of which generates a simple convolution filter. After\\ncompletion of the training, the trained parameters from the task-\\nindependent matrix are frozen and saved. For the next task, another\\nseparate task-dependent vector is introduced, and a separate portion\\nof the free parameters in the task-independent matrix is trained. Pre-\\nviously learned knowledge is reused to enhance the performance of\\nthe task at hand. This way, a very low computational overhead is\\nincurred through a task-dependent vector for each new restoration\\ntask. The task-dependent vector also introduces a degree of freedom\\nto the kernel generation, providing the network with the flexibility\\nto choose or reject a previously learned filter for a new restoration\\ntask. This simple modification of the convolution layer can be easily\\nadapted to any complex network architecture and can serve as a\\nknowledge base for implementing continual learning-based image\\nrestoration. If the knowledge bank’s parameters are exhausted, new\\nfilter kernels can be prompted by simply appending the dimension\\nof the task-specific vector and the corresponding new dimension of\\nthe task-independent matrix. Even then, the kernel size remains the\\nsame, so no extra computational load gets added to the network.\\nThe main contributions of this paper are as follows.\\n• We introduce a new approach to estimate the kernels of a con-\\nvolutional layer, which eventually facilitates lifelong learning\\nin image restoration tasks. To the best of our knowledge, this\\nis the first work to deal with completely different restoration\\ntasks in continual learning.\\n• We also demonstrate that the proposed module can be easily\\nadapted to any other state-of-the-art network without requir-\\ning any architectural modifications.\\n• We experimentally demonstrate that the knowledge base of\\nthe proposed module can be easily expanded without incur-\\nring any significant computational burden.\\n• We experimentally validate the performance improvement in\\nthe present restoration task by using the knowledge from the\\nprevious restoration task. We also show the superiority of our\\nproposed module as compared to similar lifelong learning\\napproaches.\\n2\\nRELATED WORK\\n2.1\\nAdvancement in Continual Learning\\nThe problem of catastrophic forgetting has been addressed using\\nvarious methods, namely the parameter regularization method, data\\nreplay-based methods, and the dynamic network-based approach.\\nIn regularization-based methods, Kirkpatrick et al. [20] proposed\\na parameter regularization technique in which the weights that are\\nrelatively critical to old tasks were imposed stricter restrictions while\\nupdating for new tasks. [44] followed a similar approach to update\\nparameters based on their importance. [4] estimates the importance\\nby calculating the sensitivity of the output function to the change of\\nparameters in the network. [9, 23] employ regularization in terms of\\ndistillation loss. [27] uses iterative pruning in a trained network and\\nfixes the previously learned critical weights using a binary mask.\\nOther approaches, such as [5, 15, 29, 35], rely on data replay to\\nemulate information from previous tasks. Among these, the rehearsal-\\nbased methods [26, 31, 38] address the problem of catastrophic for-\\ngetting by remembering representative samples from the previous\\ntasks in memory and replaying them while learning a new task. A\\nmajor drawback of these methods is that previous data may not\\nalways remain available for future use. Other methods, such as cite\\nshin2017continual,wu2018memory,wu2018incremental, alleviate\\nthe problem by employing pseudo-rehearsal-based training, primar-\\nily by using generative models to generate mock samples during\\ntraining for new tasks.\\nDynamic network-based approaches address the forgetting prob-\\nlem by dedicating a portion of the network to a particular task and\\nexpanding the network as needed for new tasks. Rusu et al. [34] pio-\\nneered this approach by proposing a Progressive Neural Network that\\nprevents catastrophic forgetting by adding a sub-network for each\\nnew task and transferring previously learned features through lateral\\nconnections from the base network. [17] allows model expansion\\nbut maintains compactness by choosing selected learned weights\\nby means of a learnable mask. [32] proposes a linear combination\\nof existing filters to learn filters corresponding to a new task. [22]\\n\\nSharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration\\nICVGIP’25, December 2025, Mandi, India\\nuse Neural architecture search where as [40] adopt reinforcement\\nlearning based approach for network expansion.\\n2.2\\nImage Restoration Perspective\\nDeep learning architectures have been used extensively in various\\nimage restoration tasks like rain-streak removal, haze removal, im-\\nage denoising, motion blur removal etc [6, 11, 12, 21]. Some recent\\nworks have also focused on designing a single network to perform\\nmultiple restoration tasks, rather than separate dedicated networks\\nfor domain-specific tasks [7, 18, 25, 42]. Recently, the inherent ad-\\nvantage of not forgetting and reusing previously acquired knowledge\\nin continual learning (CL) has garnered interest in the restoration\\ndomain. [24] propose a fork-join model where a new expert network\\nthat is specific to a restoration task is joined to a base pre-trained net-\\nwork, and a generative adversarial network is leveraged to emulate\\nthe memory replay process by generating pseudo-random samples\\nof the previous tasks. Zhou et al. [44] employ the CL mechanism for\\nan image de-raining task by using parameter regularization based on\\nparameters’ individual importance.\\n3\\nMETHODOLOGY\\nWe propose a new formulation of the convolution layer that can effec-\\ntively handle multiple restoration tasks by sharing learned knowledge\\nfrom previous tasks to train a new task. In this section, we discuss\\nthe proposed module, its training methodology, and the procedure\\nfor adapting previous task knowledge to new upcoming restoration\\ntasks.\\nFigure 2: CMC layer during the first restoration task.\\n3.1\\nProposed Formulation of Convolution Layer\\nConventional convolution layers contain learnable weights that are\\nconvolved with the input features. It can be mathematically ex-\\npressed as 𝐹𝑜𝑢𝑡= 𝐹𝑖𝑛⊛𝐾𝑤, where ⊛is the convolution operator, 𝐹𝑖𝑛\\nis the input feature, 𝐹𝑜𝑢𝑡is the corresponding output feature, and 𝐾𝑤\\nis the kernel weights. 𝐾𝑤contains trainable parameters, which are\\nupdated through a gradient back-propagation algorithm.\\nUnlike the conventional method of directly determining the ker-\\nnels, we estimate them indirectly by triggering a task-independent\\nlearnable weight matrix with a task-specific learnable weight vector,\\nas shown in Figure 1. 𝑀𝑡×𝑚\\n𝑤\\nis the task-independent weight matrix\\nthat contains the trainable weights of all the tasks. It can also be re-\\nferred to as the main memory of the convolution layer, as it stores the\\noptimized weights for various tasks. It is also expandable if the train-\\nable parameters are exhausted. Therefore, we term it as Continual\\nMemory in Convolution (CMC). 𝑇1×𝑡\\n𝑤\\nis the task-dependent weight\\nvector. It is fixed for each task, and a new weight vector is introduced\\nduring the adaptation of a new upcoming task in 𝑀𝑡×𝑚\\n𝑤\\n. Here, 𝑡is\\nthe length of the task-dependent weight vector. This 𝑡decides the\\ncapacity of CMC. As 𝑡increases, we need to add more rows in 𝑀𝑡×𝑚\\n𝑤\\nto increase the capacity of CMC seamlessly. 𝑚is the total number\\nof parameters in a convolution kernel. The value of 𝑚is mathemati-\\ncally expressed as 𝑚= 𝑘𝑖𝑛.𝑘𝑜𝑢𝑡.𝑛.𝑛, where 𝑘𝑖𝑛is the number of input\\nfeatures, 𝑘𝑜𝑢𝑡is the number of output features, and 𝑛is the kernel\\ndimension. 𝑚only depends on the network architecture properties.\\nIf the architecture properties are fixed, 𝑚will be the same during\\nlifelong learning. We do not need to change 𝑚for any restoration\\ntask. Therefore, the main computational overhead due to convolution\\non input features remains unchanged for continual learning-based\\nimage restoration tasks. However, the computation may increase as\\nwe extend the dimension 𝑡to expand the CMC capacity, but it is\\nnegligible compared to kernel expansion. During each task, 𝑇1×𝑡\\n𝑤\\nis matrix multiplicated with 𝑀𝑡×𝑚\\n𝑤\\nto estimate the kernels 𝐾1×𝑚\\n𝑤\\n.\\nBoth 𝑇1×𝑡\\n𝑤\\nand 𝑀𝑡×𝑚\\n𝑤\\ncontain trainable free parameters that can be\\ntrained through the gradient back-propagation algorithm. A fraction\\nof the CMC, 𝑀𝑡×𝑚\\n𝑤\\n, is utilized in each task based on performance\\nrequirements.\\nFigure 3: Operations in CMC layer for 𝑛𝑡ℎrestoration task.\\n3.2\\nMulti-task handling\\nThe Continual Memory in Convolution (CMC) 𝑀𝑡×𝑚\\n𝑤\\nis the main\\nmodule whose parameters are trained in each task. In this section,\\nthe mechanism of lifelong training for restoration tasks is explained\\nin two parts, one for the first restoration task and the other for the\\nforthcoming restoration tasks. For the first restoration task, there is\\nno previous knowledge to adapt. However, the forthcoming restora-\\ntion tasks build upon the knowledge base established in the previous\\ntasks. Figure 2 shows the operations involved during adaptation of\\nthe first restoration task, and Figure 3 shows a pictorial representa-\\ntion of adopting the 𝑛𝑡ℎtask in the CMC module.\\n3.2.1\\nFirst restoration task. At the beginning of the first task, all\\nthe weights of the CMC module remain as free parameters. We select\\na random fraction of these free weights by applying a task-specific\\n\\nICVGIP’25, December 2025, Mandi, India\\nbinary mask H𝑡×𝑚\\n𝑤1\\nto the CMC module to train the network based\\non the restoration task requirements. These selected weights 𝑀𝑤1\\nare then represented as 𝑀𝑡×𝑚\\n𝑤\\n⊙H𝑡×𝑚\\n𝑤1 , where ⊙is point-wise multi-\\nplication operator. Only these weights are expected to be updated\\nduring training for the first restoration task. In each convolution\\nlayer, restoration-specific vector𝑇1 and the selected fraction of CMC\\n𝑀𝑤1 are updated to estimate the respective convolution kernels 𝐾𝑤1,\\nas shown in eq.1. Other weights are considered zero during this\\noperation. After training, we get a trained 𝑇1 and 𝑀𝑤1.\\n𝐾1×𝑚\\n1\\n= 𝑇1×𝑡\\n1\\n.(𝑀𝑡×𝑚\\n𝑤\\n⊙H𝑡×𝑚\\n𝑤1 )\\n(1)\\n3.2.2\\nForthcoming restoration task. After the model is trained\\non the first restoration task, the forthcoming restoration tasks are\\ntrained sequentially and utilize all the trained parameters of the\\nprevious tasks, as shown in Figure 3. From the 2𝑛𝑑task onward, the\\ntask-specific binary mask is chosen such that there is no overlap\\nbetween the current mask and previously chosen masks.\\nH𝑡×𝑚\\n𝑤𝑛\\n⊙H𝑡×𝑚\\n𝑤𝑖\\n= 𝑂, 𝑓𝑜𝑟𝑎𝑙𝑙𝑖= 1, ...,𝑛−1\\n(2)\\nHere 𝑂𝑡×𝑚is a zero matrix. If H𝑤is the mask representing all the\\nweights in the network, then the available free parameters in the\\nCMC module for the 𝑛𝑡ℎtask can be mathematically expressed as,\\n𝑀𝑛= 𝑀𝑤⊙(H𝑤−H𝑡×𝑚\\n𝑤1\\n∪H𝑡×𝑚\\n𝑤2\\n∪... ∪H𝑡×𝑚\\n𝑤𝑛−1)\\n(3)\\nThe 𝑛𝑡ℎtask utilizes all the filters learned from the previous tasks\\nand learns a fraction of 𝑀𝑛for its kernel estimation as shown in eq.4.\\n(4)\\n𝐾1×𝑚\\n𝑛\\n= (𝑇1×𝑡\\n1\\n.(𝑀𝑡×𝑚\\n𝑤\\n⊙H𝑡×𝑚\\n𝑤1 ) + ... +𝑇1×𝑡\\n𝑛−1.(𝑀𝑡×𝑚\\n𝑤\\n⊙H𝑡×𝑚\\n𝑤𝑛−1))\\n+𝑇1×𝑡\\n𝑛\\n.(𝑀𝑡×𝑚\\n𝑤\\n⊙H𝑡×𝑚\\n𝑤𝑛)\\nThis way, the filters estimated for the 𝑛𝑡ℎtask become a linear com-\\nbination of previously learned filters and the newly trained kernels\\nfor the current task. All previous weights 𝑀𝑤1, 𝑀𝑤2, ...𝑀𝑤𝑛−1 and\\ntask specific vectors 𝑇1,𝑇2, ...𝑇𝑛−1 remains fixed. After training, the\\n𝑛𝑡ℎtask occupy the fraction 𝑀𝑤𝑛of total weight box 𝑀𝑤.\\nAlgorithm 1 shows the algorithmic representation of training the\\nproposed CMC module for the 𝑛𝑡ℎtask. At the first stage, a fraction\\nof free parameters is allocated at random for the 𝑛𝑡ℎtask by using\\na mask H𝑤𝑛. 𝑀𝑤⊙H𝑤𝑛represents the fraction of CMC module\\n𝑀𝑤, which will be tuned to learn knowledge from the 𝑛𝑡ℎtask.\\nDuring training, all previously learned weights are used to share the\\nknowledge of past experiences as kernel parameters 𝐾𝑜𝑙𝑑with the\\ncurrent task. This knowledge-sharing mechanism does not contribute\\nto any gradient update operations. Therefore, the knowledge gained\\nfrom previous experience remains unchanged. After that, 𝐾𝑜𝑙𝑑is\\nfused with task-specific kernels 𝐾𝑤𝑛to estimate the final kernel\\n𝐾𝑓𝑖𝑛𝑎𝑙to extract features F𝑜𝑢𝑡from the input features F𝑖𝑛.\\n3.2.3\\nExtension of Parameters in a Layer. The dimension 𝑚\\nis fixed as it is the total number of parameters that are required for\\na convolution. Therefore, we can increase the dimension 𝑡if we\\nexhaust all the free trainable parameters. We can also take a bigger 𝑡\\nif any layer demands more trainable parameters. For example, the\\ninput layer in a restoration task extracts key image features, and the\\noutput layer reconstructs the image from this feature domain. It may\\nhamper performance drastically if we allocate the same percentage\\nof weights as other layers. Unlike classification, we can not afford to\\nlose key image features in image restoration. Our proposed modified\\nAlgorithm 1: Training algorithm of CMC module for 𝑛𝑡ℎ\\ntask\\nInput: 𝑇1×𝑡\\n𝑛\\n= task-specific vector, 𝑀𝑡×𝑚\\n𝑤\\n= fractionally\\ntrained CMC module\\nOutput: Fully trained 𝑇1×𝑡\\n𝑛\\nand 𝑀𝑡×𝑚\\n𝑤\\nwith trained\\nparameters of 𝑛𝑡ℎtask\\nAllocation of Random % of free parameters:\\nH𝑡×𝑚\\n𝑤1 , H𝑡×𝑚\\n𝑤2 , ...H𝑡×𝑚\\n𝑤𝑛−1 represents the mask of 𝑛−1 different\\ntasks;\\nselect H𝑤𝑛, where H𝑤𝑛∩H𝑤𝑖= ∅, 𝑖= 1, ...𝑛−1;\\nParameters to train 𝑀𝑤𝑛= 𝑀𝑤⊙H𝑤𝑛;\\nTraining on 𝑛𝑡ℎtask\\nforall CMC layers do\\nGradient Operation Paused:\\nfor 𝑖≤𝑛−1, 𝑖+ + do\\n𝐾𝑤𝑖= 𝑇𝑖.(𝑀𝑤⊙H𝑤𝑖);\\n𝐾𝑜𝑙𝑑+ = 𝐾𝑖;\\nend\\nGradient Operation Resumed:\\n𝐾𝑤𝑛= 𝑇𝑛.(𝑀𝑤⊙H𝑤𝑛);\\n𝐾𝑓𝑖𝑛𝑎𝑙= 𝐾𝑤𝑛+ 𝐾𝑜𝑙𝑑;\\nF𝑜𝑢𝑡= F𝑖𝑛⊛𝐾𝑓𝑖𝑛𝑎𝑙\\nend\\nconvolution provides the flexibility to increase the parameters in\\nthose key layers without increasing computational complexity.\\n4\\nEXPERIMENTAL ANALYSIS\\n4.1\\nExperimental Setting\\nIn this section, we discuss all the experimental settings and the\\ndetails of the implementation. We utilize standard datasets for var-\\nious restoration tasks and employ a simple deep neural network\\narchitecture to validate our proposed idea.\\n4.1.1\\nRestoration Task Selection. We use four different restora-\\ntion tasks for our experimental analysis. Those selected tasks are\\nderaining, denoising, deblocking, and deblurring. These restoration\\ntasks are chosen based on the nature of their degradation factors. De-\\nraining is needed to alleviate the degradation caused by rain streaks.\\nDenoising effectively reduces noise in captured images due to poor\\ncamera sensors. On the other hand, deblurring addresses the degra-\\ndation caused by motion blur or poor resolution during the capture\\nprocess, and deblocking mitigates blocking in an image that occurs\\nwhen storing it on a disk. We explain handling these four restoration\\ntasks throughout our paper. However, any other restoration task can\\nbe included continuously without any significant modifications.\\n4.1.2\\nDataset. We use four different datasets for the four differ-\\nent restoration tasks in our experiment. For deraining, we utilize\\nthe standard Rain100L dataset [41], which comprises 200 training\\nimages and 100 testing images. In the case of image denoising, we\\nrandomly add Gaussian noise with a standard deviation (std) of 50 to\\nthe DIV2K [2] dataset, a high-resolution image dataset, for training\\nand testing the trained model on the BSD68 [33] dataset. In both de-\\nblocking and deblurring, we use the DIV2K dataset and degrade the\\nimage with random JPEG artifacts and blurring, respectively, during\\n\\nSharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration\\nICVGIP’25, December 2025, Mandi, India\\ntraining. We consider the quality range [10, 70] for introducing the\\nJPEG artifact. For blurring, we take account of the Gaussian blur\\nand take 15 × 15 blur kernel with a random standard deviation in\\nthe range [0.2, 3]. However, during testing, we only considered the\\nGaussian blur kernel with a standard deviation of 2.5 for deblurring\\nand the insertion of JPEG artifacts using a quality factor of 20 for de-\\nblocking. In both deblocking and deblurring, we utilize the DIV2K\\nvalidation dataset for testing purposes.\\n4.1.3\\nModel Architecture. We use a simple consecutive residual\\nblock-based network architecture [14] for our experimental purposes.\\nThere are 6 residual blocks in our network, excluding the input and\\noutput convolution blocks. Each residual block consists of 64 input\\nand output channels. The convolution blocks inside each residual\\nblock use 3 × 3 convolution with stride 1. In our experiment, we\\nreplace the conventional convolution blocks with our proposed mod-\\nified block. However, all the kernel parameters remain the same.\\nTherefore, it can be seamlessly integrated into any deep architecture\\nwithout requiring any architectural modifications.\\n4.1.4\\nImplementation details. We use the same experimental\\nsetup for all the experiments. The model is trained for 125 epochs,\\nand each epoch consists of 1, 000 batch updates. There are 16 image\\npatches of size 128×128 in each batch. All images are normalized to\\nthe range [0, 1] during both training and testing. The mean-squared\\nerror (MSE) is used as a loss function for gradient back-propagation.\\nAdam optimizer [19] with learning rate 10−4 is used for updating\\nthe weights, and the learning rate is halved after every 25 epochs.\\nWe use the Peak signal-to-noise ratio (PSNR) metric throughout the\\npaper for performance analysis.\\n%\\nParams\\nKnowledge\\nSharing\\nDerain\\nDenoise\\nDeblocking\\nDeblur\\n20\\n✗\\n33.50\\n27.65\\n30.99\\n29.64\\n✓\\n33.50\\n27.68\\n31.11\\n29.74\\n10\\n✗\\n32.14\\n27.43\\n30.81\\n29.32\\n✓\\n32.14\\n27.53\\n30.96\\n29.64\\n5\\n✗\\n30.36\\n27.08\\n30.55\\n28.97\\n✓\\n30.36\\n27.23\\n30.78\\n29.33\\n2.5\\n✗\\n29.71\\n26.58\\n30.29\\n28.35\\n✓\\n29.71\\n27.01\\n30.64\\n29.02\\n1.25\\n✗\\n29.19\\n26.10\\n30.21\\n27.92\\n✓\\n29.19\\n26.49\\n30.42\\n28.52\\nTable 1: Performance of continual task adaptation on PSNR\\nmetric. ✓means knowledge of previous tasks is adapted during\\ntraining. Derain is the first task. Denoise, deblocking, and deblur\\nare the next tasks on which the model is trained, following that\\nsequence.\\n4.2\\nExperiments on Continual Task Adaptation\\nTable 1 shows the quantitative analysis of lifelong restoration task\\nlearning with knowledge sharing. In the first experiment, we allocate\\nparameters for different tasks and train on restoration datasets with-\\nout sharing knowledge from other tasks. In the second experiment,\\nthe knowledge of past restoration tasks is shared with the current\\ntasks. This way, lifelong learning persists. There is no performance\\ndifference in single-image deraining as it is the first task. Denois-\\ning, deblocking, and deblurring are the next consecutive tasks. We\\nobserve from the table that performance on these three tasks consis-\\ntently yields better results when knowledge is shared. We can also\\nobserve that knowledge sharing performs significantly better as the\\npercentage of parameter allocation decreases. This happens because\\ndecreasing the allocated parameters hinders the learning process, and\\nthe model can not acquire sufficient knowledge for that particular\\ntask. Therefore, similar knowledge of previous restorations becomes\\nmore helpful in learning the current restoration task.\\nFigure 4 shows how the performance metric PSNR changes in\\neach training epoch. We chose the final restoration task, deblurring,\\nfor this analysis purpose. Figure 4(a) shows performance analysis\\nwith 5% of model parameters, and Figure 4(b) shows performance\\nanalysis with 1.25% of model parameters. We can clearly see the\\nimprovement by applying the knowledge gained from deraining,\\ndenoising, and deblocking, as the PSNR in the first epoch already\\nyields an initial difference of approximately 3.5 dB for 5% parameters\\nand approximately 6 dB for 1.25% parameters. Therefore, we can say\\nthat previous task knowledge gives better performance and results in\\nfaster convergence.\\n(a) 5% model parameters used\\n(b) 1.25% model parameters used\\nFigure 4: PSNR in dB vs Each epoch of training\\nMethods\\nDerain\\nDenoise\\nDeblocking\\nDeblur\\nTogether\\n25.44\\n15.66\\n29.26\\n29.80\\nDeform\\n30.90\\n27.26\\n30.67\\n29.29\\nPruning [27]\\n29.56\\n27.45\\n31.00\\n29.50\\nMAS [3]\\n23.80\\n18.99\\n28.39\\n25.98\\nCMC-5\\n32.14\\n27.53\\n30.96\\n29.64\\nTable 2: Quantitative analysis of our proposed CMC module\\nwith other continual learning mechanisms.\\n4.3\\nComparative Analysis\\nMost of the continual learning based frameworks are specifically\\ndesigned for classification tasks. Therefore, it is not feasible to apply\\nthose to restoration models. However, for comparative analysis, we\\nevaluate different baseline models and popular pruning-based con-\\ntinual learning methods. Table 2 shows the quantitative evaluation of\\nour proposed Continual Memory in Convolution (CMC) module with\\ndifferent baseline models. The pruning-based methods directly prune\\nthe filters of the convolution layers and use those pruned weights for\\nupcoming tasks [27]. The ‘Deform’ baseline utilizes a deformable\\nconvolution-based architecture to reduce the model’s parameters.\\nThis baseline aims to compare the advantage of a model with shared\\nknowledge with that of smaller, distinct models for various tasks.\\n\\nICVGIP’25, December 2025, Mandi, India\\nIn the deform baseline, separate models are used for different tasks.\\nThe ‘Together’ baseline model incorporates all tasks into a single\\nmodel. This baseline uses the whole ResNet architecture to train\\nthe model for all four tasks. The ‘Deform’ baseline utilizes around\\n13% more model parameters compared to a plain convolution-based\\narchitecture. The ‘Pruning’ baseline uses 12.5% of model parameters,\\nand our proposed CMC-5 takes 10% of the overall model parameters.\\nIf we consider the total number of parameters, our model has more\\nparameters as compared to pruning-based methods. However, the\\nkernel parameters and throughput speed remain the same. MAS [3]\\nis a regularization-based continual learning method that can be easily\\napplied to restoration tasks. For a fair comparison, we use the same\\nresidual block-based architecture to perform the experiments using\\nMAS. MAS failed to maintain its performance in past restoration\\ntasks. After training on all four restoration tasks sequentially, the\\nPSNR drops significantly from 34.36 dB to 23.80 dB for deraining,\\nfrom 25.42 dB to 18.99 dB for denoising, and from 30.11 dB to 28.39\\ndB for deblocking.\\n4.4\\nAdditional Analysis\\n4.4.1\\nSubjective Comparison. Figure 5 shows the subjective\\ncomparison of restoration performance. We use the model which\\nallocates 1.25% of trainable parameters for each restoration operation.\\nWe present the qualitative analysis of a method that does not utilize\\nknowledge from previous tasks, alongside a method that leverages\\nknowledge from past restoration tasks. The results of the method\\nthat does not share the knowledge are termed ’Image-1’ and the\\nresults of the knowledge sharing are termed ’Image-2’, as shown\\nin Figure 5. In the case of knowledge sharing, we use the trained\\nmodel of a task that has been trained at the end. Therefore, we can\\nobserve significant visual differences. In the case of blurring and\\ndeblocking, the red boxes highlight the performance improvement in\\nboth Figure 5p and Figure 5l as compared to Figure 5o and Figure 5k,\\nrespectively. In the denoising task, Figure 5h shows better results\\nwith fewer artifacts as compared to Figure 5g. We also observe a\\nlesser rain streak effect in Figure 5d as compared to Figure 5c.\\n4.4.2\\nShuffling the Training order. Previously, we demonstrated\\nonly one sequence of different restoration tasks, which are trained\\ncontinually, and found that the performance of a restoration task im-\\nproves if we share the knowledge from past tasks. Here, we shuffle\\nthe sequence of the four restoration tasks in such a manner that each\\ntask occupies every available position in the sequence and perform\\nthe experiments. Table 3 shows the quantitative evaluation of four\\ndifferent experiments where each task is in a different training se-\\nquence position in each experiment. For example, deraining is the\\nfirst task in Experiment 1, while it is the last task in Experiment\\n2, and it is the second and third tasks in Experiment 3 and Experi-\\nment 4, respectively. Figure 6 depicts the graphical representation\\nof Table 3. We observe that the performance of the task improves\\nas the training sequence number increases. The training sequence\\nnumber of a restoration task is the position at which it appears in\\ntraining order while training the tasks continually. If a particular\\ntask’s training sequence position is 4, it means the model has al-\\nready been trained on three different restoration tasks and utilizes\\nthe shared knowledge from those three previous tasks for the current\\ntask. We can conclude two things from these experiments. Firstly, the\\nknowledge of previously learned tasks plays a crucial role in lifelong\\nlearning, and our method successfully helps future tasks to adopt the\\nknowledge of the past tasks. Secondly, the training sequence plays a\\ncrucial role. The performance of the first task always degrades due\\nto the non-availability of previous knowledge. Therefore, backward\\nContinual Learning, in which past-trained tasks can fine-tune their\\nknowledge from future tasks, will be able to mitigate the effect of\\nthe training sequence. We plan to explore it in future work.\\nExperiment 1\\nExperiment 2\\nExperiment 3\\nExperiment 4\\nTraining\\nSequence\\nPSNR\\nin dB\\nTraining\\nSequence\\nPSNR\\nin dB\\nTraining\\nSequence\\nPSNR\\nin dB\\nTraining\\nSequence\\nPSNR\\nin dB\\nDeraining\\n1\\n29.19\\n4\\n29.95\\n3\\n30.02\\n2\\n29.69\\nDenoising\\n2\\n26.49\\n1\\n26.10\\n4\\n26.66\\n3\\n26.61\\nDeblocking\\n3\\n30.42\\n2\\n30.29\\n1\\n30.21\\n4\\n30.46\\nDebluring\\n4\\n28.52\\n3\\n28.52\\n2\\n28.35\\n1\\n27.92\\nTable 3: Quantitative evaluation of the effect of training se-\\nquence. In each experiment, the training sequences are shuffled.\\nKnowledge\\nSharing\\nNoise\\n10\\nNoise\\n20\\nNoise\\n30\\nNoise\\n40\\nReal\\nNoise\\n✗\\n33.75\\n30.44\\n28.22\\n27.12\\n36.90\\n✓\\n33.75\\n30.75\\n29.01\\n27.82\\n37.63\\nTable 4: Performance of continual task adaptation for 5 separate\\ndenoising tasks.\\n4.4.3\\nSimilar restoration tasks. Till now, we have only consid-\\nered completely different restoration tasks. To analyze the continual\\nrestoration task adaptation in similar kinds of tasks, we performed\\nexperiments sequentially on four Gaussian noise levels: 10, 20, 30,\\nand 40, followed by real noise. For training on real-world noise,\\nwe use the popular SIDD dataset [1], and clean images from the\\nDIV2K dataset [2] are used to generate Gaussian noisy images. Ta-\\nble 4 shows the performance of our CMC module-based continual\\nlearning framework in those tasks. The experimental setup is the\\nsame as the previous one, as performed in Table 1. 20% of model\\nparameters are allocated for each task. We can observe from Table 4\\nthat adopting knowledge from previous easy denoising tasks (i.e.,\\nless noisy images) significantly improves the performance on com-\\nplex denoising tasks (i.e., heavily noisy images) and real noise. The\\nexperimental results further validate the feasibility of our proposed\\nmethod.\\nParameter\\nExpansion\\nDerain\\nDenoise\\nDeblocking\\nDeblur\\n✗\\n29.19\\n26.49\\n30.42\\n28.13\\n✓\\n29.51\\n26.65\\n30.51\\n28.72\\nTable 5: Quantitative performance analysis when the parameter\\nof some key layer of restoration is increased without hampering\\nthe computational performance.\\n\\nSharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration\\nICVGIP’25, December 2025, Mandi, India\\n(a) Grouth-Truth\\n(b) Rainy Image\\n(c) Deraining (Image-1)\\n(d) Deraining (Image-2)\\n(e) Grouth-Truth\\n(f) Noisy Image\\n(g) Denoising (Image-1)\\n(h) Denoising (Image-2)\\n(i) Grouth-Truth\\n(j) Blury Image\\n(k) Deblurring (Image-1)\\n(l) Deblurring (Image-2)\\n(m) Grouth-Truth\\n(n) JPEG Artifact Image\\n(o) Deblocking (Image-1)\\n(p) Deblocking (Image-2)\\nFigure 5: Qualitative evaluation of the effect of the knowledge sharing in our continual learning framework. ’Image 1’ represents\\nthe outputs of the model without any knowledge sharing. ’Image 2’ depicts the results of those models where the respective tasks are\\ntrained at last using the knowledge of all the previous tasks. (Zoom for the best view.)\\n4.4.4\\nParameter expansion. In a deep learning network, some\\nlayers can demand more parameters to learn key essential features.\\nAllocating fewer parameters in those layers may result in the loss of\\ncrucial features, ultimately leading to poor performance. In the case\\nof restoration tasks, the input and output layers play a crucial role,\\nas the first one extracts valuable features from the image, and the\\nsecond one reconstructs the image. The number of kernel parameters\\nis generally less in those layers as compared to other layers. This\\nis because the input layer maps from 3 −𝐷RGB image and the\\noutput layer maps to 3 −𝐷RGB image. Therefore, if we allocate the\\nsame percentage of free parameters in those layers similar to other\\nlayers, it will lead to poor performance. However, allocating a higher\\npercentage in those layers will lead to early parameter exhaustion in\\nthose layers. Using our method, we can easily expand the parameter\\nspace of the input and output layers without any computational\\noverhead. Table 5 shows the performance of parameter expansion\\nin those two layers. All models in that table use only 1.25% of\\nparameters. All the layers use the CMC-5 module, except for the\\nMethod\\nTrainable\\nParameters\\nKernel\\nParameters\\nMemory\\nFlops\\n(GMac)\\nInference\\nTime\\nType-1\\n1×\\n1×\\n2083 MB\\n36.86\\n4.220 ms\\n1.8×\\n1.8×\\n2099 MB\\n65.4\\n9.080 ms\\n4×\\n4×\\n2099 MB\\n146.57\\n20.39 ms\\nType-2\\n1×\\n1×\\n2083 MB\\n36.86\\n4.220 ms\\n2×\\n2×\\n2571 MB\\n73.73\\n8.49 ms\\n4×\\n4×\\n3547 MB\\n147.46\\n16.42 ms\\nCMC-n\\n(Ours)\\n1×\\n1×\\n2093 MB\\n36.864\\n4.193 ms\\n2×\\n1×\\n2111 MB\\n36.87\\n4.237 ms\\n4×\\n1×\\n2111 MB\\n36.88\\n4.255 ms\\nTable 6: Computational complexity analysis of different funda-\\nmental continual learning mechanisms under the premises of\\nparameter expansion\\nfirst and final layers, which use the CMC-10 module. We can observe\\nfrom the table that the performance increases significantly across all\\nrestoration tasks with the flexible and straightforward modifications\\nthat our module offers.\\n\\nICVGIP’25, December 2025, Mandi, India\\nFigure 6: Graphical representation of the effect of training se-\\nquence on each task. As the training sequence number of a task\\nis increased, the performance of the task increases due to better\\nadoption of previous tasks’ knowledge.\\n4.4.5\\nComputational Complexity Analysis. The primary ad-\\nvantage of this proposed method is its reduction of computational\\noverhead for lifelong learning, as there are inherent limitations in\\nhardware and computing power. In Table 6, we compare the com-\\nputational overhead of our proposed CMC module and different\\ncontinual learning ideas. In this experiment, we consider a convolu-\\ntion layer that consists of the same kernel parameters. We consider\\nthat as a base. Now, we assume that all the parameters of the layer\\nhave already been occupied by different tasks. Therefore, we need\\nto increase the number of parameters. We calculate and compare the\\ncomputational overhead when increasing the number of parameters\\nby a factor of 2× and 4×. We take two fundamental ways to increase\\nthe parameters in the literature. The first one is termed Type-1 in\\nTable 6, where the kernel size is increased to accommodate a larger\\nnumber of free parameters [17]. The second one is termed Type-2,\\nwhere the number of kernels is increased or a new layer is introduced\\nfor adding new parameters [24, 34]. In this experimental setup, we\\nuse a convolutional layer with a 3 × 3 kernel size as the base layer.\\nIt has 64 input and 64 output channels. Now, this layer processes\\n64 input features, which have spatial dimensions of 1000 × 1000. In\\nType-1, we increase the kernel size to 4 × 4 and 6 × 6 from 3 × 3,\\nthereby increasing the parameters by 1.8× and 4×, respectively. A\\nnew convolution layer is introduced to increase the parameters in\\nType-2. In our case, we use CMC-5 as a base layer and use CMC-10\\nand CMC-20 to double and quadruple the number of parameters. It\\ncan be clearly seen from Table 6 that the CMC module does not sig-\\nnificantly burden the memory requirements, FLOPs, and inference\\ntime. However, Type-1 increases the inference time and Flops signif-\\nicantly as we increase the parameters. This is because it increases\\nthe kernel size, which exponentially increases the computational\\nburden. Type-2 drastically increases both the memory required dur-\\ning processing and inference time. As the CMC module does not\\nsignificantly increase the inference time and memory requirement, it\\ncan serve the purpose of lifelong training.\\nModel\\nKnowledge\\nSharing\\ndenoise\\nderain\\ndeblock\\ndeblur\\nRDN\\n✗\\n29.43/ 0.902\\n26.36/ 0.731\\n30.20/ 0.861\\n27.37/ 0.782\\n✓\\n29.43/ 0.902\\n26.47/ 0.734\\n30.38/ 0.866\\n27.90/ 0.799\\nDense\\n✗\\n28.84/ 0.889\\n25.98/ 0.709\\n30.16/ 0.860\\n27.90/ 0.799\\n✓\\n28.84/ 0.889\\n26.39/ 0.728\\n30.36/ 0.865\\n28.60/ 0.815\\nTable 7: Performance of continual task adaptation on two differ-\\nent model architectures.\\n4.4.6\\nAdopting CMC in different architectures. To prove the ex-\\ntendibility of our proposed CMC in different deep architectures, we\\nconsider two popular network topologies for experiment purposes,\\nnamely dense block [16, 39] and residual dense network (RDN) [43].\\nTable 7 shows the performance of our proposed continual learning\\nframework on two different network architectures. The experimental\\nsetup is the same as the previous one, as performed in Table 1. We\\nconsider 6 blocks for both dense and RDN block-based architecture.\\nWe only consider 1.25% of model parameters for each task. We can\\nobserve from Table 7 that both architectures follow a similar trend,\\nas we witness in the residual architecture. We provide both PSNR\\nand SSIM values, and our experimental results show that SSIM\\nfollows a similar trend to PSNR.\\n5\\nLIMITATIONS, IMPACT AND FUTURE\\nWORK\\nOur proposed Continual Memory in Convolution (CMC) module\\nserves the purpose of lifelong learning, as it allows us to add knowl-\\nedge without forgetting, and it does not impose an extra computa-\\ntional burden on the compact system. However, the main drawback\\nof this approach is the number of parameters. More parameters are\\nrequired in the CMC module to produce the same performance as\\ncompared to a conventional convolution layer. But nowadays, the\\nsystem memory in a compact system is easily extendable. Therefore,\\nour module can easily work in those systems. Currently, our model\\ncan only reuse the knowledge from past tasks. However, we believe\\nthat with a simple modification, this module has tremendous poten-\\ntial to learn backwards, i.e., to improve past restoration performance\\nby utilizing knowledge from future tasks. Handling multiple known\\ndegradations in an image by leveraging knowledge of individual\\ndegradations can be explored through knowledge sharing by fusing\\nthe knowledge of individual tasks. These ideas can be the future\\nscope of this work.\\n6\\nCONCLUSION\\nIn this paper, we propose a modification of the conventional convolu-\\ntion layer. By making this simple modification, we can continuously\\nadapt the learned experience from previous tasks and share those\\nexperiences with the current task to improve its performance. We\\naddress the shortcomings of lifelong learning for image restoration\\ntasks, and our module serves as a prospective solution. This is the\\nfirst-of-a-kind work where diverse restoration tasks have been han-\\ndled through continual learning. The proposed mechanism shares\\nknowledge across tasks without changing the backbone architecture.\\nThe knowledge base can be continuously expanded with a minimal\\ncomputational burden. We experimentally observe the benefits of\\nknowledge sharing between completely different restoration tasks,\\nas it helps to improve the performance by a significant margin.\\n\\nSharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration\\nICVGIP’25, December 2025, Mandi, India\\nREFERENCES\\n[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S. Brown. 2018.\\nA\\nHigh-Quality Denoising Dataset for Smartphone Cameras. In 2018 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. 1692–1700.\\nhttps:\\n//doi.org/10.1109/CVPR.2018.00182\\n[2] Eirikur Agustsson and Radu Timofte. 2017. Ntire 2017 challenge on single image\\nsuper-resolution: Dataset and study. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition workshops. 126–135.\\n[3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and\\nTinne Tuytelaars. 2018. Memory Aware Synapses: Learning what (not) to forget.\\nIn The European Conference on Computer Vision (ECCV).\\n[4] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and\\nTinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget. In\\nProceedings of the European Conference on Computer Vision (ECCV). 139–154.\\n[5] Pratik Prabhanjan Brahma and Adrienne Othon. 2018.\\nSubset replay based\\ncontinual learning for scalable improvement of autonomous systems. In 2018\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\\n(CVPRW). IEEE, 1179–11798.\\n[6] Meng Chang, Qi Li, Huajun Feng, and Zhihai Xu. 2020. Spatial-adaptive network\\nfor single image denoising. In European Conference on Computer Vision. Springer,\\n171–187.\\n[7] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. 2021.\\nHINet: Half instance normalization network for image restoration. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 182–\\n192.\\n[8] Sen Deng, Mingqiang Wei, Jun Wang, Yidan Feng, Luming Liang, Haoran Xie,\\nFu Lee Wang, and Meng Wang. 2020. Detail-recovery image deraining via context\\naggregation networks. In Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition. 14560–14569.\\n[9] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama\\nChellappa. 2019. Learning without memorizing. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition. 5138–5146.\\n[10] Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang, Fei Wang, and Ming-\\nHsuan Yang. 2020. Multi-scale boosted dehazing network with dense feature\\nfusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 2157–2167.\\n[11] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, and John\\nPaisley. 2017. Removing Rain From Single Images via a Deep Detail Network. In\\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR).\\n[12] Yosef Gandelsman, Assaf Shocher, and Michal Irani. 2019. \"Double-DIP\": Unsu-\\npervised Image Decomposition via Coupled Deep-Image-Priors. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n[13] Lovedeep Gondara. 2016. Medical image denoising using convolutional denois-\\ning autoencoders. In 2016 IEEE 16th international conference on data mining\\nworkshops (ICDMW). IEEE, 241–246.\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep resid-\\nual learning for image recognition. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition. 770–778.\\n[15] Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, Zhengwei Tao, Jinwen Ma,\\nDongyan Zhao, and Rui Yan. 2018. Overcoming catastrophic forgetting for\\ncontinual learning via model adaptation. In International Conference on Learning\\nRepresentations.\\n[16] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.\\n2017. Densely connected convolutional networks. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition. 4700–4708.\\n[17] Steven CY Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming\\nChan, and Chu-Song Chen. 2019. Compacting, picking and growing for unforget-\\nting continual learning. arXiv preprint arXiv:1910.06562 (2019).\\n[18] Aupendu Kar, Sobhan Kanti Dhara, Debashis Sen, and Prabir Kumar Biswas.\\n2021. Zero-Shot Single Image Restoration Through Controlled Perturbation of\\nKoschmieder’s Model. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition. 16205–16215.\\n[19] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980 (2014).\\n[20] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\\nDesjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\\nGrabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural\\nnetworks. Proceedings of the national academy of sciences 114, 13 (2017),\\n3521–3526.\\n[21] Boyun Li, Yuanbiao Gou, Jerry Zitao Liu, Hongyuan Zhu, Joey Tianyi Zhou,\\nand Xi Peng. 2020. Zero-shot image dehazing. IEEE Transactions on Image\\nProcessing 29 (2020), 8457–8466.\\n[22] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. 2019.\\nLearn to grow: A continual structure learning framework for overcoming cata-\\nstrophic forgetting. In International Conference on Machine Learning. PMLR,\\n3925–3934.\\n[23] Zhizhong Li and Derek Hoiem. 2017. Learning without forgetting. IEEE transac-\\ntions on pattern analysis and machine intelligence 40, 12 (2017), 2935–2947.\\n[24] Jianzhao Liu, Jianxin Lin, Xin Li, Wei Zhou, Sen Liu, and Zhibo Chen. 2020.\\nLIRA: Lifelong Image Restoration from Unknown Blended Distortions. In Com-\\nputer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August\\n23–28, 2020, Proceedings, Part XVIII 16. Springer, 616–632.\\n[25] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki Okatani. 2019. Dual\\nresidual networks leveraging the potential of paired operations for image restora-\\ntion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. 7007–7016.\\n[26] David Lopez-Paz and Marc’Aurelio Ranzato. 2017. Gradient episodic memory\\nfor continual learning. Advances in neural information processing systems 30\\n(2017), 6467–6476.\\n[27] Arun Mallya and Svetlana Lazebnik. 2018. Packnet: Adding multiple tasks to a\\nsingle network by iterative pruning. In Proceedings of the IEEE conference on\\nComputer Vision and Pattern Recognition. 7765–7773.\\n[28] Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in connec-\\ntionist networks: The sequential learning problem. In Psychology of learning and\\nmotivation. Vol. 24. Elsevier, 109–165.\\n[29] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin\\nNabi. 2019. Learning to remember: A synaptic plasticity driven framework for\\ncontinual learning. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition. 11321–11329.\\n[30] Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie. 2019. Enhanced pix2pix\\ndehazing network. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition. 8160–8168.\\n[31] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H\\nLampert. 2017. icarl: Incremental classifier and representation learning. In Pro-\\nceedings of the IEEE conference on Computer Vision and Pattern Recognition.\\n2001–2010.\\n[32] Amir Rosenfeld and John K Tsotsos. 2018. Incremental learning through deep\\nadaptation. IEEE transactions on pattern analysis and machine intelligence 42, 3\\n(2018), 651–663.\\n[33] Stefan Roth and Michael J Black. 2009. Fields of experts. International Journal\\nof Computer Vision 82, 2 (2009), 205.\\n[34] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James\\nKirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Pro-\\ngressive neural networks. arXiv preprint arXiv:1606.04671 (2016).\\n[35] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. 2017. Continual\\nlearning with deep generative replay. arXiv preprint arXiv:1705.08690 (2017).\\n[36] Pavel Svoboda, Michal Hradiš, Lukáš Maršík, and Pavel Zemcík. 2016. CNN for\\nlicense plate motion deblurring. In 2016 IEEE International Conference on Image\\nProcessing (ICIP). IEEE, 3832–3836.\\n[37] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. 2018. Scale-\\nrecurrent network for deep image deblurring. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition. 8174–8182.\\n[38] Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan\\nPascanu, and Yee Whye Teh. 2020. Functional Regularisation for Continual\\nLearning with Gaussian Processes. In ICLR.\\n[39] Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao. 2017. Image super-resolution\\nusing dense skip connections. In Proceedings of the IEEE international conference\\non computer vision. 4799–4807.\\n[40] Ju Xu and Zhanxing Zhu. 2018. Reinforced Continual Learning. In NeurIPS.\\n[41] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and\\nShuicheng Yan. 2017.\\nDeep joint rain detection and removal from a single\\nimage. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition. 1357–1366.\\n[42] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz\\nKhan, Ming-Hsuan Yang, and Ling Shao. 2021. Multi-stage progressive image\\nrestoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 14821–14831.\\n[43] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. 2020. Residual\\ndense network for image restoration. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence 43, 7 (2020), 2480–2495.\\n[44] Man Zhou, Jie Xiao, Yifan Chang, Xueyang Fu, Aiping Liu, Jinshan Pan, and\\nZheng-Jun Zha. 2021. Image De-Raining via Continual Learning. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n4907–4916.\\n',\n",
       " 'Accepted to AAAI 2026\\nBETA DISTRIBUTION LEARNING FOR RELIABLE ROADWAY\\nCRASH RISK ASSESSMENT\\nAhmad Elallaf1\\nNathan Jacobs2\\nXinyue Ye3\\nMei Chen4\\nGongbo Liang1\\n1Texas A&M University-San Antonio\\n2Washington University in St. Louis\\n3University of Alabama\\n4University of Kentucky\\n{aelallaf, gliang}@tamusa.edu\\njacobsn@wustl.edu\\nxye10@ua.edu\\nmei.chen@uky.edu\\nhttps://www.gb-liang.com/projects/betarisk\\nABSTRACT\\nRoadway traffic accidents represent a global health crisis, responsible for over a million deaths an-\\nnually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often\\nexamine risk factors in isolation, overlooking the spatial complexity and contextual interactions in-\\nherent in the built environment. Furthermore, conventional Neural Network-based risk estimators\\ntypically generate point estimates without conveying model uncertainty, limiting their utility in crit-\\nical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning\\nframework that leverages satellite imagery as a comprehensive spatial input. This approach enables\\nthe model to capture the nuanced spatial patterns and embedded environmental risk factors that con-\\ntribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates\\na full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware\\npredictions—a critical feature for trustworthy AI in safety-critical applications. Our model outper-\\nforms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential\\ndangers, while delivering superior calibration. By providing reliable and interpretable risk assess-\\nments from satellite imagery alone, our method enables safer autonomous navigation and offers a\\nhighly scalable tool for urban planners and policymakers to enhance roadway safety equitably and\\ncost-effectively.\\n1\\nIntroduction\\nRoadway traffic accidents claim over 1.3 million lives annually [1] and impose economic burdens of 3% of the GDP in\\nmany countries [2]. As a critical infrastructure sector [3], transportation safety has garnered significant research [4–6],\\nyet accurately estimating crash risk remains a challenge due to its inherent uncertainties and the sparse nature of crash\\nevents.\\nConventional safety research often analyzes individual factors separately, such as driver behavior [7], road infrastruc-\\nture [8], traffic patterns [9], and weather [10], overlooking the complex interplay between these elements [11]. Since\\ncrash occurrences frequently result from intricate multi-factor interactions, methods that analyze these factors in isola-\\ntion struggle to predict risk holistically [12]. Furthermore, data limitations have constrained the scope of most studies\\nto highways [13–17], leaving comprehensive crash risk analysis for local roads, where data is often less available,\\nrelatively unexplored.\\nTo overcome these limitations, we introduce a novel deep learning framework that learns a full Beta probability\\ndistribution, moving beyond simple point-estimates of fatal crash risk. Our primary contributions are threefold:\\n• A holistic, vision-based model that captures the complex interplay of risk factors embedded in the visual\\ndata, in contrast to methods that study variables in isolation.\\n• A probabilistic formulation that yields well-calibrated, uncertainty-aware predictions, a critical feature for\\ntrustworthy AI in high-stakes, safety-critical domains.\\narXiv:2511.04886v1  [cs.CV]  7 Nov 2025\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\n• A highly scalable and equitable methodology that uses near-globally available satellite imagery, enabling\\nrisk assessment for both highways and previously under-assessed local roads.\\nThe proposed probabilistic model is evaluated through extensive experiments conducted over four major metropolitan\\nareas, which have a population of ≈20 million. Our model achieves a 17-23% improvement in recall over baselines, a\\ncrucial metric for any safety-critical task, while also delivering superior model calibration and F1 scores. By producing\\nreliable and interpretable risk assessments from satellite imagery alone, this work provides a foundational tool for\\nenhancing traffic safety, from enabling safer route selection for drivers and autonomous vehicles to empowering urban\\nplanners and policymakers to mitigate high-risk areas.\\n2\\nBackground\\n2.1\\nEstimate Roadway Crash Risk\\nA primary challenge in data-driven roadway safety is formulating the risk estimation task. Existing methods often\\nframe it as classifications, such as predicting a crash occurrence within a short time frame [9]. While valuable, these\\napproaches do not estimate the inherent, continuous crash risk of a given road segment. A more nuanced approach\\nis to directly estimate a crash probability, such as using Monte Carlo simulations [18–20]. However, this is funda-\\nmentally challenged by the extreme sparsity of crash data. For instance, the average annual accident rate for a 25m2\\nroad segment in the United States is just 0.1% [21]. This level of sparsity renders traditional estimation techniques\\nunreliable, as they can obscure high-risk areas while falsely flagging safe ones [22], leading to false negatives that are\\ndangerous in any safety-critical application. Furthermore, such simulation methods are often ill-suited for large-scale\\napplications due to high computational costs and the need for carefully tuned parameters.\\nDeep Neural Networks (DNNs) offer a powerful alternative, as they can learn complex, task-specific features di-\\nrectly from data and provide near-instantaneous inference. However, supervised DNNs typically rely on large, man-\\nually labeled datasets, such as manually assigned risk levels (e.g., low, neutral, high) [23]. Creating these datasets\\nis prohibitively expensive, and the manual labels can suffer from human bias, potentially misrepresenting the true\\nrisk [24, 25]. These challenges motivate the need for a new approach that can learn a continuous risk score from\\nobjective crash data while effectively handling the probabilistic nature of the task.\\n2.2\\nDeep Neural Network Miscalibration\\nOver the recent years, DNNs have shown promising performance on various domains, such as medical imaging [26,\\n27], cybersecurity [28], transportation [29], and astrophysics [30]. However, for a predictive model to be trustworthy\\nin high-stakes applications, its predicted confidence must accurately reflect its probability of being correct. However,\\nmodern DNNs are often miscalibrated, tending to produce overconfident predictions [31,32].\\nMathematically, a model is perfectly calibrated if, for any given confidence level p, the long-run accuracy of predic-\\ntions with that confidence is indeed p. For DNNs, the calibration error, the difference between a model’s predicted\\nconfidence and its actual accuracy, is often significantly greater than zero [33]. This miscalibration is a critical failure\\npoint in high-stakes applications where decisions depend on the model’s self-assessed certainty.\\nWhile various techniques can mitigate this issue, they often have limitations. Post-processing methods like temperature\\nscaling [32] adjust model outputs without altering the learned features, while in-training regularization [34,35] requires\\ncareful tuning for the weight scaler. Given that model complexity is a key contributor to miscalibration [36], we argue\\nthat an effective solution must be deeply integrated into the learning process. Our work achieves this by reformulating\\nthe risk estimation task as learning a full probability distribution, a method that inherently encourages better-calibrated\\nand more reliable predictions.\\n3\\nMethod\\n3.1\\nProbabilistic Modeling Framework\\nOur method recasts roadway crash risk estimation from a standard classification task into a probabilistic learning\\nproblem, motivated by the limitations of conventional models that provide a single point-estimation. Consider a fatal\\ncrash, a stochastic occurrence, at a specific point in spacetime, C = (x, y, t, d), where (x, y) is the geolocation and\\n(t, d) is the time and date. While any single crash is a random event, its location provides the strongest available\\nevidence for a local maximum in the underlying, continuous risk field, R(·). Therefore, it is intuitive that the inferred\\nrisk should be higher at or near the crash site and should decay smoothly as one moves away in space or time. For\\n2\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nAlgorithm 1 Target Beta Distribution Generation\\nRequire: Original image x, binary label l ∈{0, 1}, base concentration Kbase, minimum positive risk mean µmin,\\nminimum positive concentration kmin, distance weight wdist, size weight wsize, and ϵ = 1e−5\\nif l = 0 then\\n▷For negative samples, create a low-risk,\\n▷high-certainty distribution\\nαt ←ϵ\\nβb ←Kbase\\nelse\\n▷For positive samples (l=1), generate labels\\n▷based on crop geometry\\nx′ ←random crop of x\\ndnorm ←normalized distance of x′ from center of x\\nsnorm ←size(x′)\\nsize(x)\\ninfluence ←wdist · (1 −dnorm) + wsize · snorm\\nµt ←µmin + (1 −µmin) · influence\\nkt ←kmin + (Kbase −kmin) · influence\\nαt ←µt · kt\\nβt ←ϵ\\nend if\\nreturn (αt, βt)\\n▷Return the target Beta distribution\\nnearby points, such as a spatially displaced point C′ = (x −δ, y, t, d), the risk should be lower, i.e., R(C′) < R(C).\\nStandard point-estimate classifiers fail to capture this continuous field, as they are trained to predict a binary outcome\\nfor each location independently.\\nWhile a complete model would account for both spatial and temporal decay, this work focuses on the challenging and\\nfoundational task of estimating the static, inherent risk of a location based on its geographic and structural features.\\nOur goal is to model the spatial component of this uncertainty by learning a distribution over possible risk values,\\ncapturing the intuition that for a nearby point C′, the risk is attenuated but non-zero: 0 < R(C′) < R(C).\\nWe specifically employ the Beta distribution for this task due to its natural support on the [0, 1] interval and its flexi-\\nbility in representing diverse risk profiles. Instead of a single value, our model h(x) maps an input image x to the two\\npositive scalar parameters, (α, β), which define a Beta distribution, Pp ∼Beta(α, β). This formulation allows the\\nmodel to express its uncertainty through the shape of the distribution: a sharp peak indicates high confidence, while a\\nwide distribution signifies high uncertainty. The final risk score R is the mean of this predicted distribution:\\nR = E[Pp] =\\nα\\nα + β .\\n(1)\\nTo achieve this, our framework integrates three key technical contributions: 1) a novel procedural labeling technique\\nthat generates the targeting Beta distributions from data augmentation, 2) a multi-scale deep neural network architec-\\nture, and 3) a compound loss function for joint optimization.\\n3.2\\nTarget Beta Distributions Generation\\nA key innovation of our framework is the procedural generation of supervisory signals in the form of target Beta\\ndistributions. Instead of using static labels, we dynamically create a target Beta distribution, Pt ∼Beta(αt, βt), for\\neach training sample based on the properties of the random crop augmentation. Specifically, given an input image, we\\nfirst apply a random crop. The target Beta distribution is, then, generated using Algorithm 1. This process acts as a\\nsophisticated form of structured label smoothing, transforming data augmentation from a simple regularizer into a rich\\nsource of continuous supervision for risk and uncertainty.\\nFor negative samples (no crash), the objective is to predict low risk with high confidence. The target distribution is\\ntherefore constant: αt is set to a small positive value ϵ and βt is set to a large value representing high certainty Kbase,\\ncreating a distribution sharply peaked at zero.\\nFor positive samples (crash), the target distribution reflects the quality of the visual evidence in the random crop. This\\nis quantified by an influence score, which modulates the target distribution’s mean and concentration to generate\\na supervision signal that is proportional to the information content of the augmented image. The score is a weighted\\ncombination of two geometric properties of the crop: its centrality relative to the crash location and its size.\\n3\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nFigure 1: Training Architecture with Joint Optimization\\nWe set the weights to 0.7 for centrality (wdist) and 0.3 for relative size (wsize). This weighting scheme is based\\non the strong intuition that the visual features most critical to understanding risk–such as specific road geometry,\\nlane markings, the presence of an intersection, or the surrounding environments–are spatially concentrated around the\\nevent’s location. A crop that is well-centered on the crash point provides the clearest and most relevant evidence, thus\\ndeserving a higher influence score and a more confident target distribution. The relative size of the crop provides\\nuseful, but secondary, broader context about the surrounding environment. This principled approach transforms data\\naugmentation into a rich source of supervision, teaching the model to dynamically associate higher risk and confidence\\nwith visual samples that contain the most informative evidence.\\nThis influence score then modulates the target mean µmin and the target concentration kt, which in turn define the\\nfinal Beta parameters. For positive samples, the βt is set to the small constant ϵ, ensuring the distribution is always\\nskewed towards high risk, with the influence score controlling the precise shape and confidence (see the supplement\\nmaterials for the hyperparameters used in this study).\\n3.3\\nModel Architecture\\nThe architecture of our model, illustrated in Figure 1, is designed to process multi-scale satellite imagery. During\\ntraining, a random crop is sampled from the input, which consists of image slices of the same location at different\\nresolutions.\\nThe cropped images are, then, passed through a shared feature extractor backbone to produce multiple corresponding\\nfeature maps. These maps are concatenated along the channel dimension to form a unified feature representation,\\nserving as the input for two parallel prediction heads:\\n• A Distribution Learning Head, which outputs the two Beta parameters (α, β).\\n• An auxiliary Classification Head, which outputs a single logit for the binary crash/no-crash task.\\n3.4\\nTraining and Optimization\\nThe model is trained end-to-end by jointly optimizing the two parallel heads with a compound loss function. The\\nprimary distribution learning head is supervised by the a mean-variance loss that inspired by the squared Wasserstein-2\\n(W 2\\n2 ) distance [37], which measures the dissimilarity between the predicted (Pp) and the target (Pt) Beta distributions:\\nLW 2\\n2 (Pp, Pt) = (µp −µt)2 + (σp −σt)2,\\n(2)\\nwhere the µ and σ are the mean and standard deviation.\\nWe empirically selected this W 2\\n2 surrogate over true W 2\\n2 distance and other distribution divergence metrics, including\\nKL-Divergence [38] and the Cramér-von Mises criterion [39]. As a true metric, our W 2\\n2 surrogate loss provides a\\n4\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nFigure 2: Streamlined Architecture for Inference\\nmore stable gradient than KL-Divergence, especially when the predicted and target distributions have little overlap.\\nMost importantly, for one-dimensional distributions like the Beta, the W 2\\n2 surrogate loss directly optimize of the risk\\nscore (the mean) and confidence level (the standard deviation) simultaneously. Our experimental analysis also shows\\nthis surrogate is a close approximation of the true W 2\\n2 (errors on the order of 10−3 to 10−2), deviating only in extreme\\ncases (see the supplementary materials).\\nThe auxiliary classification head is supervised by a Binary Cross-Entropy loss, which encourages the shared backbone\\nto learn discriminative features relevant to the safety task:\\nLBCE = −1\\nN\\nN\\nX\\ni=1\\n[yi log(pi) + (1 −yi) log(1 −pi)] ,\\n(3)\\nwhere yi and pi are the label and predicted probability.\\nThe overall objective function is a weighted combination of the two losses, balanced by hyperparameters, λ1 and λ2:\\nL = λ1 · LBCE + λ2 · LW 2\\n2 .\\n(4)\\n3.5\\nInference Process\\nThe inference process, illustrated in Figure 2, is direct and computationally efficient. The random crop augmentation\\nand the auxiliary classification head are removed. The full, uncropped multi-scale image is passed through the feature\\nextractor backbone and the distribution head. The risk score R is calculated as the mean of the distribution, per\\nEquation 1. This feed-forward process allows for rapid and scalable risk assessment of any location.\\n4\\nExperiment Setup\\nThis study utilizes the MSCM dataset [40], a large-scale collection of multi-scale satellite images from Texas, USA,\\nwith 16,451 locations labeled with historical fatal crashes. All models use a ResNet-50 [41] backbone, λ1 = 5, λ2 = 1,\\nand were trained on NVIDIA A100 GPUs. See the supplementary materials for more information about the dataset,\\nimplementation details, and hyperparameter analysis for the selection of λ1 and λ2.\\n4.1\\nEvaluation Methodology\\nQuantitative Metrics\\nWe first evaluate our model’s practical effectiveness by framing the risk estimation as a binary\\nclassification task to identify historical crash locations. The model’s predicted risk score R, derived from Equation 1\\nis thresholded at 0.5 to yield a binary prediction. We then assess the model’s predictive performance using standard\\nmetrics: F1-Score, Precision, Recall, AUC (Area Under the Receiver Operating Characteristic curve), and PRC (area\\nunder the precision-recall curve); and assess model’s calibration using Expected Calibration Error (ECE) and Brier\\nscore. Due to safety-oriented, we consider Recall to be the most critical metric that answers the question: “Of all\\ncrash locations, what fraction did our model successfully identify?\"\\n5\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nTable 1: Main Quantitative Results (bold: best performance; underlined: second best performance)\\nMethods\\nPre-Train Probabilistic Multi-Scale\\nPerformance (↑)\\nUncertainty (↓)\\nF1\\nPrecision\\nRecall\\nAUC\\nPRC\\nECE\\nBrier\\nImageNet\\nImageNet\\n✗\\n✗\\n0.4753\\n0.4968\\n0.4555\\n0.7980\\n0.4862\\n0.1281\\n0.1600\\nMSCM-SS\\nMSCM\\n✗\\n✗\\n0.4966\\n0.4981\\n0.4950\\n0.8165\\n0.5185\\n0.1006\\n0.1458\\nMSCM-MS\\nMSCM\\n✗\\n✓\\n0.5409\\n0.6731\\n0.4521\\n0.8572\\n0.6269\\n0.1067\\n0.1296\\nProb-SS (Ours)\\nMSCM\\n✓\\n✗\\n0.5001\\n0.4252\\n0.6070 0.7749\\n0.4409\\n0.1731\\n0.1922\\nProb-MS (Ours)\\nMSCM\\n✓\\n✓\\n0.5762\\n0.6296\\n0.5311 0.8663 0.6489\\n0.0881 0.1211\\nWe also evaluate our method against a Deep Ensemble (DE) of the strongest baseline, constructed from three inde-\\npendent training runs. The final predicted risk score of a DE model, RDE, is calculated as the mean of the predictions\\nfrom each individual model in the ensemble. This single score for each sample is then used to compute all the afore-\\nmentioned performance and calibration metrics.\\nThe ensemble’s predictive uncertainty is quantified in two ways: the variance of the risk scores and the disagreement\\nrate among the final binary predictions. A higher value in either metric reflects greater disagreement among the models\\nand thus higher uncertainty in the final prediction.\\nQualitative Analysis\\nTo intuitively understand the value of our probabilistic approach, we conduct a qualitative\\nanalysis of the model’s outputs from two perspectives. First, we analyze the aggregate behavior of the model’s outputs\\nby comparing the overall distribution of predicted probabilities from our model against the baselines. By plotting a\\nhistogram of all risk scores, we can visually assess model confidence. A well-calibrated, uncertainty-aware model is\\nexpected to utilize the full [0, 1] probability range, whereas overconfident models will show predictions heavily clus-\\ntered at the extremes (near 0 and 1). Second, we visualize the predicted Beta distributions for four distinct scenarios:\\ntrue positives (TP), true negatives (TN), false negatives (FN), and false positives (FP). The goal of this analysis is to\\nprovide an intuitive understanding of the model’s behavior by interpreting its successes and failures.\\nCase Study: San Antonio River Walk\\nTo demonstrate the model’s utility, we conduct a case study of the San\\nAntonio River Walk, providing practical insight into the model’s performance in a challenging, safety-critical area.\\n4.2\\nBaseline Models\\nWe evaluate our method against three baselines to isolate our framework’s contributions. Our primary benchmark is\\nthe Multi-Scale Cross-Matching (MSCM) model [40], the current state-of-the-art for fatal crash risk estimation using\\nonly satellite imagery.\\nImageNet Baseline:\\nA standard model pre-trained on ImageNet [42] that takes single-scale satellite images as input,\\nproviding us the performance of a generic, non-domain-specific feature extractor on our task.\\nMSCM-SS (Single-Scale):\\nThe same single-scale architecture but using weights generated by the self-supervised pre-\\ntraining through cross-matching, proposed in the MSCM paper, to test the value of domain-specific features.\\nMSCM-MS (Multi-Scale):\\nThe full MSCM model, which uses both its domain-specific pre-training and multi-scale\\nimagery as input, represents the strongest available baseline, allowing us to compare against the current state-of-the-art\\nclassification approach directly.\\nThe best checkpoint for each model was selected based on the model accuracy on the validation set.\\n5\\nResults\\n5.1\\nQuantitative Analysis\\nTable 1 summarizes the quantitative results. The single-scale baselines (ImageNet and MSCM-SS) achieve a < 0.5\\nprecision and recall scores, indicating their predictions for positive cases are close to random and exhibit little ability\\nto identify high-risk areas. While the MSCM-MS model achieves high precision (0.6731), its poor recall (0.4521) means\\nit fails to identify over half of all crash locations, rendering it unreliable for safety-critical applications.\\n6\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nTable 2: Deep Ensemble Results over Three Training Trails (bold: best performance)\\nMethods\\nPerformance (↑)\\nUncertainty (↓)\\nDisagreement (↓)\\nF1\\nPrecision\\nRecall\\nAUC\\nPRC\\nECE\\nBrier\\nVariance Disagr. Rate\\nEnsemble MSCM-MS\\n0.5966\\n0.7062\\n0.5165\\n0.8839 0.6890\\n0.0787\\n0.1112\\n0.0925\\n16.93%\\nEnsemble Prob-MS (Ours)\\n0.5976\\n0.6750\\n0.5361\\n0.8761\\n0.6886\\n0.0605 0.1075\\n0.0822\\n15.14%\\nFigure 3: Uncertainty Quantification and Interpretability\\nIn contrast, our models demonstrate a significant improvement in identifying potential dangers. Our multi-scale model,\\nProb-MS, achieves the best overall balance of performance, attaining the highest F1-score. Its most significant contri-\\nbution is boosting the recall to 0.5311, a 17% relative improvement over MSCM-MS, drastically reducing the number of\\nhazardous sites that would be missed. Our single-scale model, Prob-SS (0.6070 recall score), significantly improves\\nthe metric by 23% over the best baseline (0.4950).\\nCrucially, Prob-MS is also the most trustworthy model, achieving the lowest (best) ECE of 0.881 and Brier of 0.1211.\\nThis confirms that our model’s probabilistic outputs are more statistically sound and reliable for real-world decision-\\nmaking.\\nWe also evaluate our method against a Deep Ensemble of the strongest baseline (Table 2). When comparing our\\nsingle Prob-MS model against the baseline Ensemble MSCM-MS, we find that our single model achieves competitive\\nperformance, including a 3% higher recall, better calibration, and lower uncertainty at only 1/3 the computational cost\\nat both training and inference times. This highlights the efficiency and practical advantage of our approach.\\nIn an apples-to-apples comparison between ensembled methods, our Ensemble Prob-MS demonstrates the clear su-\\nperiority of our probabilistic framework. It outperforms the baseline ensemble on the most critical metrics for this\\ntask, achieving a higher F1-score and recall. Most importantly, it is significantly better calibrated and exhibits lower\\nuncertainty, as evidenced by its superior (lower) ECE, Brier, Variance, and Disagreement Rate scores.\\n5.2\\nQualitative Analysis\\nOur qualitative analysis highlights the superior interpretability and trustworthiness of our probabilistic framework. As\\nshown in Figure 3, our model provides a comprehensive and practical understanding of risk that standard classifiers\\ncannot offer. The “Beta Model Uncertainty\" plot (left) confirms the model’s rational behavior, showing that prediction\\nuncertainty is lowest for highly confident predictions and highest for ambiguous ones around a 0.5 risk score. The\\n“Confidence Intervals\" plot (center) demonstrates that every prediction is accompanied by a 95% confidence interval,\\nwith the interval’s width directly communicating the model’s certainty on a per-prediction basis. Finally, the “Predic-\\ntion Interpretability\" table (right) crystallizes this key advantage, showing how our Beta model resolves the ambiguity\\nof a baseline’s “Risk: 0.50\" output by distinguishing between a low-confidence prediction (e.g., with α = 10, β = 10)\\nand a very uncertain one (e.g., with α = 2, β = 2). This additional context is invaluable for any safety-critical\\napplication.\\nThis nuanced, per-prediction behavior leads to a more rational distribution of predictions in aggregate (Figure 4).\\nWhile baseline models behave like overconfident black boxes with predictions heavily clustered at the extremes of 0\\nand 1, our model utilizes the full probability spectrum to express varying degrees of certainty. This ability to be “less\\nconfident\" is not a weakness but a hallmark of a more honest and trustworthy risk assessment tool.\\n7\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nFigure 4: Analysis of Predicted Probability Distributions\\n(a)\\n(b)\\nFigure 5: Qualitative Results for Unambiguous (“Easy\") Cases (a: True Negatives, b: True Positives)\\n5.3\\nVisualizing Model Uncertainty\\nTo be a trustworthy tool for risk assessment, a model must not only make accurate predictions but also provide a\\nreliable measure of its own uncertainty. We visualize this uncertainty using a Beta distribution for each prediction. As\\nshown in Figure 5, our model demonstrates well-calibrated confidence across a spectrum of cases, a crucial feature for\\nreal-world deployment.\\nFor visually unambiguous locations, the model produces predictions with high confidence. For example, in a simple\\nsuburban neighborhood (Figure 5a, left), it predicts a low risk (0.051) with a correspondingly low uncertainty score\\n(0.006), reflected in a sharp Beta distribution. Likewise, for a coastal road with high traffic density and high potential\\nof distractions (Figure 5b, left), it correctly predicts a high risk (0.924) with high confidence (uncertainty of 0.010).\\nThe model’s utility is further demonstrated in more complex scenarios where it appropriately reduces its confidence.\\nFor a visually complex but safe highway overpass (Figure 5a, right), the model still correctly predicts low risk, but\\nthe wider Beta distribution indicates higher uncertainty. This nuanced confidence is also evident when assessing a\\ncomplex highway interchange (Figure 5b, right); the model correctly predicts a high risk of 0.717, but acknowledges\\nthe significant uncertainty due to the challenging visual features.\\n8\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\n(a)\\n(b)\\nFigure 6: Interpreting Model Behavior on Ambiguous (“Hard\") Cases (a: False Negative, b: False Positive)\\nCrucially, the model’s rational expression of uncertainty extends to its failures, a characteristic vital for establishing\\ntrust. For false negatives (Figure 6a), where the model misses a crash, the low-risk predictions are consistently paired\\nwith wider, higher-uncertainty distributions. This correctly signals that the visual evidence was ambiguous, contain-\\ning conflicting features (e.g., Figure 6a Left: an arterial road with many intersections within an otherwise low-risk\\nresidential area).\\nSimilarly, for false positives (Figure 6b), the model flags locations as high-risk despite no recorded crashes, but again\\nwith reduced confidence. This behavior is highly interpretable, as the model correctly identifies latent risk factors,\\nsuch as sharp (L-shape) turns or high-density highways. The prediction thus reflects a successful identification of\\nhazardous features, while the increased uncertainty correctly marks them as borderline cases. This ability to temper\\ncertainty in response to visual complexity, especially when incorrect, distinguishes our model as a more reliable and\\ninterpretable system for risk assessment.\\n5.4\\nCase Study\\nTo demonstrate the practical utility of our model, we conducted a case study of the San Antonio River Walk, a major\\ntourist destination that presents a challenging environment with a complex mix of vehicular, pedestrian, and cyclist\\ntraffic. We generated risk predictions for over 140 locations in this area using Prob-MS and MSCM-MS.\\nThe results, shown in Figure 7, highlight the superior performance of our approach. The baseline MSCM-MS model\\n(middle panel) fails to identify close to half of the historical fatal crash locations (red diamonds), assigning them erro-\\nneously low risk scores. The baseline’s predictions also lack spatial coherence, exhibiting sharp, unrealistic gradients\\nbetween adjacent points and producing polarized risk scores with few intermediate values.\\nIn contrast, our Prob-MS model (right panel) correctly assigns elevated risk scores (yellow and orange) to a greater\\nnumber of the known crash sites. This is exemplified at the intersection near Navarro St and Villita St, a known fatal\\ncrash location at the bottom-right in the map. While the baseline model misses this site, ours correctly assigns the area\\na high-risk score.\\nAn analysis of the satellite and ground-level imagery reveals a confluence of latent risk factors not apparent from an\\noverhead view alone. The location, a major entry point to the River Walk, is surrounded by numerous parking facilities.\\nGround-level images (Figure S1) show that these structures, combined with dense trees and building columns, create\\nsignificant visual obstructions and blind spots for both drivers and pedestrians. This environment forces complex\\ninteractions: vehicles constantly enter and exit parking garages across wide pedestrian walkways as tourists navigate\\nnarrow sidewalks. Our model likely learned to associate this specific combination of visual clutter, unpredictable\\nvehicle maneuvers, and high pedestrian-vehicle conflict with an elevated risk of a fatal crash.\\nFurthermore, our model generates a more nuanced and spatially coherent risk map where predictions transition\\nsmoothly across locations. This case study demonstrates that our model’s strong quantitative performance translates\\ninto more reliable, interpretable, and actionable safety assessments for complex urban environments.\\n9\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nFigure 7: A case study of crash risk assessment for the San Antonio River Walk. Historical fatal crashes (red diamonds)\\nserve as ground truth. (Left) The baseline MSCM-MS model exhibits low recall and spatially inconsistent predictions,\\nwith abrupt risk changes between adjacent points. (Right) Our Prob-MS model demonstrates superior recall by cor-\\nrectly identifying more crash sites and generates a more nuanced and spatially coherent risk field, providing a more\\nrealistic safety assessment. See the Result section for more.\\n6\\nDiscussion and Conclusion\\nOur evaluation demonstrates that the proposed probabilistic framework yields a risk assessment model that is not\\nonly more effective but also more reliable and interpretable than deterministic baselines. By predicting a full Beta\\nprobability distribution instead of a single point-estimate, our model learns a more nuanced and less overconfident\\nrepresentation of risk. This trustworthiness is reinforced by its interpretable behavior; the model’s “mistakes\" are\\noften rational, such as flagging visually complex but historically safe highway interchanges as high-risk. This capac-\\nity to reason about visual factors and express nuanced confidence is highly valuable for practical applications, from\\nenabling more sophisticated path planning for autonomous vehicles to allowing urban planners to confidently priori-\\ntize infrastructure improvements. Furthermore, by relying solely on publicly available satellite imagery, our method\\ncircumvents the significant privacy concerns associated with other data sources\\n6.1\\nEthical Considerations and Responsible Deployment\\nThe ethical implications of deploying an AI tool for public safety are significant. As historical crash data may contain\\nundiscovered biases, such as under-reporting in certain socioeconomic or geographic areas, a model used without\\ncritical oversight could perpetuate inequities. We therefore emphasize that this model is designed as a decision-support\\ntool to augment, not replace, human expertise.\\nA key feature for responsible, human-in-the-loop deployment is the model’s ability to signal its own uncertainty,\\nwhich can serve as a bias and fairness mitigation tool. High uncertainty in any prediction (whether high-risk or low-\\nrisk) can flag regions with potential data disparities or under-reporting. For instance, a visually complex area with high\\n10\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nuncertainty and a low-risk prediction may indicate a dangerous false negative due to a lack of historical crash data.\\nThese uncertain predictions should act as a flag for human experts to conduct a more detailed investigation, enabling\\na more equitable allocation of safety resources.\\n6.2\\nLimitations\\nThis study has several limitations that open avenues for future research. Our model estimates static risk and does\\nnot account for dynamic variables like real-time traffic or weather; future work should focus on integrating these\\ndata streams. Our study is also geographically constrained to Texas, and validation on diverse international datasets\\nis a critical next step to ensure generalizability. Furthermore, this work can be extended by exploring a learned\\nweighting mechanism for the centrality and size components of our procedural labeling scheme. Finally, while our\\nmodel identifies strong correlations, future work could explore methods for moving toward causal inference.\\nIn conclusion, this work demonstrates that moving from deterministic point-estimates to a full probabilistic framework\\nis a crucial step toward creating more reliable and trustworthy AI for public safety. By learning to predict a Beta\\nprobability distribution from satellite imagery, our model not only outperforms existing baselines in identifying high-\\nrisk locations but also provides the well-calibrated uncertainty estimates that are vital for interpretable, human-in-the-\\nloop decision-making in applications from urban planning to autonomous navigation.\\n7\\nConclusion\\nThis work presents a deep learning framework for reliable roadway risk assessment that quantifies uncertainty. Instead\\nof a single risk score, our model predicts a full Beta probability distribution to provide a more comprehensive hazard\\nassessment. This is achieved using a procedural labeling technique with data augmentation to supervise uncertainty,\\nand a compound loss function that jointly optimizes for classification accuracy and probabilistic calibration.\\nOur model significantly outperforms existing baselines, with a 17-23% relative improvement in recall on high-risk\\nlocations, up to 17% in ECE on calibration, and about 11% more stable. More importantly, it yields interpretable\\npredictions, reliably signaling its uncertainty in ambiguous cases. By delivering a more robust and trustworthy as-\\nsessment of roadway risk, our work represents a crucial step toward the responsible deployment of AI in high-stakes\\napplications, such as public safety, urban planning, and autonomous navigation.\\nAcknowledgement\\nThis material is partially based upon work supported by the National Science Foundation under 2401860 and 2526487.\\nAny opinions, findings, and conclusions or recommendations expressed in this material are those of the author and the\\nfunders have no role in the study design, data collection, analysis, or preparation of this article.\\nPortions of this research were conducted with the advanced computing resources provided by the High Performance\\nComputing Research Center at Texas A&M University-San Antonio.\\nReferences\\n[1] WHO, “Road traffic injuries,” World Health Organization, 2023, accessed: 2025-05-22. [Online]. Available:\\nhttps://www.who.int/news-room/fact-sheets/detail/road-traffic-injuries\\n[2] ——, “Global status report on road safety,” World Health Organization, 2018, accessed: 2025-05-22. [Online].\\nAvailable: https://www.who.int/publications/i/item/9789241565684\\n[3] CISA, “Critical infrastructure sectors,” Cybersecurity and Infrastructure Security Agency, 2024, accessed:\\n2024-03-22. [Online]. Available:\\nhttps://www.cisa.gov/topics/critical-infrastructure-security-and-resilience/\\ncritical-infrastructure-sectors\\n[4] C. Caliendo et al., Accident Analysis & Prevention, vol. 39, no. 4, pp. 657–670, 2007.\\n[5] J. Tamerius, X. Zhou, R. Mantilla, and T. Greenfield-Huitt, “Precipitation effects on motor vehicle crashes vary\\nby space, time, and environmental conditions,” Weather, Climate, and Society, vol. 8, no. 4, pp. 399–407, 2016.\\n[6] C. Zhu, B. Dadashova, C. Lee, X. Ye, and C. T. Brown, “Equity in non-motorist safety: Exploring two pathways\\nin houston,” Transportation research part D: transport and environment, vol. 132, p. 104239, 2024.\\n[7] B. G. Simons-Morton, F. Guo, S. G. Klauer, J. P. Ehsani, and A. K. Pradhan, “Keep your eyes on the road: Young\\ndriver crash risk increases according to duration of distraction,” Journal of Adolescent Health, vol. 54, no. 5, pp.\\nS61–S67, 2014.\\n11\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\n[8] A. Pembuain et al., “The effect of road infrastructure on traffic accidents,” in 11th Asia Pacific Transportation\\nand the Environment Conference (APTE 2018).\\nAtlantis Press, 2019, pp. 176–182.\\n[9] T. Huang et al., “Highway crash detection and risk estimation using deep learning,” Accident Analysis & Preven-\\ntion, vol. 135, p. 105392, 2020.\\n[10] D. Jaroszweski and T. McNamara, “The influence of rainfall on road accidents in urban areas: A weather radar\\napproach,” Travel behaviour and society, vol. 1, no. 1, pp. 15–21, 2014.\\n[11] C. Gu, J. Xu, C. Gao, M. Mu, G. E, and Y. Ma, “Multivariate analysis of roadway multi-fatality crashes using\\nassociation rules mining and rules graph structures: A case study in china,” Plos one, vol. 17, no. 10, p. e0276817,\\n2022.\\n[12] C. Carrodano, “Data-driven risk analysis of nonlinear factor interactions in road safety using bayesian networks,”\\nScientific Reports, vol. 14, no. 1, p. 18948, 2024.\\n[13] I. Ahmed, “Road infrastructure and road safety,” Transport and Communications Bulletin for Asia and the Pacific,\\nvol. 83, pp. 19–25, 2013.\\n[14] W. Song, S. Workman, A. Hadzic, X. Zhang, E. Green, M. Chen, R. Souleyrette, and N. Jacobs, “Farsa: Fully\\nautomated roadway safety assessment,” in 2018 IEEE Winter Conference on Applications of Computer Vision\\n(WACV), 2018, pp. 521–529.\\n[15] G. Cheng, R. Cheng, S. Zhang, and X. Sun, “Risk evaluation method for highway roadside accidents,” Advances\\nin Mechanical Engineering, vol. 11, no. 1, p. 1687814018821743, 2019.\\n[16] Q. Ma, H. Yang, Z. Wang, K. Xie, and D. Yang, “Modeling crash risk of horizontal curves using large-scale\\nauto-extracted roadway geometry data,” Accident Analysis & Prevention, vol. 144, p. 105669, 2020.\\n[17] Y.-J. Joo et al., “A generalized driving risk assessment on high-speed highways using field theory,” Analytic\\nMethods in Accident Research, vol. 40, p. 100303, 2023.\\n[18] V. de Almeida Guimarães et al., “Evaluating the sustainability of urban passenger transportation by monte carlo\\nsimulation,” Renewable and Sustainable Energy Reviews, vol. 93, pp. 732–752, 2018.\\n[19] L. Al-Sharif et al., “The use of monte carlo simulation in evaluating the elevator round trip time under up-peak\\ntraffic conditions and conventional group control,” Building Services Engineering Research and Technology,\\nvol. 33, no. 3, pp. 319–338, 2012.\\n[20] S. Jeon and B. Hong, “Monte carlo simulation-based traffic speed forecasting using historical big data,” Future\\ngeneration computer systems, vol. 65, pp. 182–195, 2016.\\n[21] S. Moosavi, M. H. Samavatian, S. Parthasarathy, R. Teodorescu, and R. Ramnath, “Accident risk prediction\\nbased on heterogeneous sparse data: New dataset and insights,” in Proceedings of the 27th ACM SIGSPATIAL\\nInternational Conference on Advances in Geographic Information Systems, 2019, pp. 33–42.\\n[22] S. He, M. A. Sadeghi, S. Chawla, M. Alizadeh, H. Balakrishnan, and S. Madden, “Inferring high-resolution\\ntraffic accident risk maps based on satellite imagery and gps trajectories,” in Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision, 2021, pp. 11 977–11 985.\\n[23] A. Najjar et al., “Combining satellite imagery and open data to map road safety,” in Proceedings of the AAAI\\nConference on Artificial Intelligence, vol. 31, no. 1, 2017.\\n[24] Y. Li et al., “Label bias: A pervasive and invisibilized problem,” Notices of the American Mathematical Society,\\nvol. 71, no. 8, pp. 1069–1077, 2024.\\n[25] C. Chen and S. S. Sundar, “Is this ai trained on credible data? the effects of labeling quality and performance\\nbias on user trust,” in Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 2023,\\npp. 1–11.\\n[26] X. Xing, G. Liang, C. Wang, N. Jacobs, and A.-L. Lin, “Self-supervised learning application on covid-19 chest\\nx-ray image classification using masked autoencoder,” Bioengineering, vol. 10, no. 8, p. 901, 2023.\\n[27] L. Liu, Y. Wang, J. Chang, P. Zhang, G. Liang, and H. Zhang, “Llrhnet: multiple lesions segmentation using\\nlocal-long range features,” Frontiers in Neuroinformatics, vol. 16, p. 859973, 2022.\\n[28] J. Zulu, B. Han, I. Alsmadi, and G. Liang, “Enhancing machine learning based sql injection detection using\\ncontextualized word embedding,” in Proceedings of the 2024 ACM Southeast Conference, 2024, pp. 211–216.\\n[29] R. Jonnala, G. Liang, J. Yang, and I. Alsmadi, “Exploring the potential of large language models in public\\ntransportation: San antonio case study,” 2025.\\n12\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\n[30] S.-C. Lin, Y. Su, G. Liang, Y. Zhang, N. Jacobs, and Y. Zhang, “Estimating cluster masses from sdss multiband\\nimages with transfer learning,” Monthly Notices of the Royal Astronomical Society, vol. 512, no. 3, pp. 3885–\\n3894, 2022.\\n[31] G. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hinton, “Regularizing neural networks by penalizing\\nconfident output distributions,” in International Conference on Learning Representations, 2017.\\n[32] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in International\\nconference on machine learning.\\nPMLR, 2017, pp. 1321–1330.\\n[33] G. E. Hinton et al., “Distilling the knowledge in a neural network,” ArXiv, vol. abs/1503.02531, 2015.\\n[34] A. Kumar et al., “Trainable calibration measures for neural networks from kernel mean embeddings,” in Inter-\\nnational Conference on Machine Learning, 2018, pp. 2805–2814.\\n[35] G. Liang, Y. Zhang, X. Wang, and N. Jacobs, “Improved trainable calibration method for neural networks on\\nmedical imaging classification,” in British Machine Vision Conference (BMVC), 2020.\\n[36] M. Chidambaram and R. Ge, “On the limitations of temperature scaling for distributions with overlaps,” in\\nInternational Conference on Learning Representations, 2023.\\n[37] L. N. Vaserstein, “Markov processes over denumerable products of spaces, describing large systems of automata,”\\nProblemy Peredachi Informatsii, vol. 5, no. 3, pp. 64–72, 1969.\\n[38] I. Csiszár, “I-divergence geometry of probability distributions and minimization problems,” The annals of prob-\\nability, pp. 146–158, 1975.\\n[39] H. Cramér, “On the composition of elementary errors,” Scandinavian Actuarial Journal, vol. 1928, no. 1, pp.\\n13–74, 1928.\\n[40] G. Liang, J. Zulu, X. Xing, and N. Jacobs, “Unveiling roadway hazards: Enhancing fatal crash risk estimation\\nthrough multiscale satellite imagery and self-supervised cross-matching,” IEEE Journal of Selected Topics in\\nApplied Earth Observations and Remote Sensing, vol. 17, pp. 535–546, 2024.\\n[41] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2016, pp. 770–778.\\n[42] A. Krizhevsky et al., “Imagenet classification with deep convolutional neural networks,” Advances in neural\\ninformation processing systems, vol. 25, 2012.\\n[43] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” arXiv preprint arXiv:1711.05101, 2017.\\n[44] ——, “Sgdr: Stochastic gradient descent with warm restarts,” arXiv preprint arXiv:1608.03983, 2016.\\n13\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nSupplementary Materials\\nSan Antonio River Walk Ground-Level Imagery\\nFigure S1 shows the ground-level image at 146-Navarro-St, San Antonio, TX, USA, one main entry point to the San\\nAntonio River Walk area. A previous fatal crash also occurred at this location (i.e., the bottom-right one in Figure 7).\\n(a) North\\n(b) West\\n(c) South\\n(d) East\\nFigure S1: Ground-level images for the four directions at 146-Navarro-St, San Antonio, TX, USA, one main entry\\npoint to the San Antonio River Walk area.\\nDataset\\nThis study utilizes the comprehensive, multi-scale satellite imagery dataset provided by MSCM [40]. The dataset\\ncovers diverse regions in Texas, USA, including the Gulf Coast, Hill Country, and Prairies and Lakes regions.\\nThe dataset contains a total of 240,828 satellite images. The images for each location are provided at three distinct\\nlevels of detail, each with a resolution of 768 × 768 pixels. The images for each location are provided at three distinct\\nlevels of detail: 1.1943 m/pixel, 0.5972 m/pixel, and 0.2986 m/pixel. Examples of this multi-scale imagery are shown\\nin Figure S2.\\nThe data is sampled from 80,276 distinct locations, which are categorized into positive and negative classes. The\\npositive class consists of 16,451 locations where at least one fatal crash occurred between 2010 and 2020; of these,\\n1,185 locations experienced multiple fatal crashes within a 50-meter radius. The remaining locations serve as the\\nnegative class, having no recorded fatal crashes through the end of 2020. These negative samples were selected using\\nspecific criteria, ensuring they were within 1250 meters of a fatal crash location but at least 250 meters away from any\\nsuch site. To create a challenging learning environment, approximately 70% of the negative samples were designated\\nas hard negatives by sampling them along primary and secondary roads. The other 30% were sampled randomly to\\nrepresent a broader variety of environments, including open spaces.\\n14\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nFigure S2: Multi-Scale Satellite Imagery Inputs with Three Detail Levels. From left to right: 1.1943 m/pixel, 0.5972\\nm/pixel, and 0.2986 m/pixel.\\nFigure S3: Comparison between true and surrogate Wasserstein-2 distance for Beta distributions. Left: Absolute\\nError. Right: Relative Error.\\nWasserstein-2 Surrogate Analysis\\nTo evaluate the accuracy of our surrogate Wasserstein-2 loss, we computed the true squared Wasserstein-2 distance\\nbetween a fixed target distribution Beta(2, 5) and a range of predicted Beta distributions with α, β ∈[0.5, 10]. For\\neach pair of predicted parameters, the true distance was estimated via numerical integration of the quantile functions,\\nwhile the surrogate distance was computed using the closed-form mean–variance expression defined in our loss. The\\nresulting absolute and relative differences are visualized in Figure S3. Both plots demonstrate that the surrogate closely\\nmatches the true distance, with errors typically on the order of 10−3 to 10−2 and only slightly increasing in extreme\\nparameter regions.\\n15\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nImplementation Details\\nAll models are built upon a ResNet-50 backbone and trained for 75 epochs using the AdamW [43] optimizer with a\\nCosineAnnealingWarmRestarts [44] learning rate schedule. The initial learning rate for the backbone and classifier\\nwas set to 1e−4, while the distribution learning head used a rate of 0.02. The batch size was 128 for single-scale\\nmodels and 48 for multi-scale models. Based on our hyperparameter analysis, the final weights for our compound loss\\nfunction were set to λ1 = 5 and λ2 = 1 to prioritize recall for this safety-critical task. The same data augmentation\\npipeline are used for all models. The random seeds are set to 0. All experiments were conducted on two NVIDIA\\nA100 GPUs. The rest of this section, provided the complete list of hyperparameters and data augmentation settings.\\nModel Architecture\\n• Backbone: All models use a ResNet-50 architecture with weights pre-trained on ImageNet.\\n• Modification: A 1x1 convolutional layer was inserted before the final Global Average Pooling .\\nMSCM Pre-training and Pre-trained Checkpoint Selection\\nThe MSCM weights, used to initialize our proposed model and the corresponding baselines, were generated by fol-\\nlowing the pre-training procedure described in the original work.\\n• Setup: The pre-training used a contrastive learning approach with an InfoNCE and classification loss. It was\\nrun for 25 epochs with a batch size of 64, using the AdamW optimizer with a learning rate of 10−3 and a\\nCosineAnnealingWarmRestarts schedule (T_0=10, T_mult=2).\\n• Checkpoint Selection: A model checkpoint was saved after each pre-training epoch. To select the optimal\\ncheckpoint, each of the 25 checkpoints was used to initialize a single-scale classification model, which was\\nthen fine-tuned for 5 epochs on the downstream task (batch size 128, AdamW, learning rate 10−4, CosineAn-\\nnealingWarmRestarts schedule). The checkpoint that yielded the best performance after this short fine-tuning\\nprocess was selected for all main experiments.\\nMain Training for Proposed Probabilistic Models\\n• Optimizer: AdamW.\\n• Learning Rates (LR): We used different learning rates for distinct parts of the model:\\n– Feature Extractor Backbone: 10−4\\n– Beta Distribution Learning Head: 0.02\\n– Auxiliary Classification Head: 10−4\\n• LR Schedule: CosineAnnealingWarmRestarts with scheduler parameters T_0=10 and T_mult=2.\\n• Epochs: All models were trained for 75 epochs. The epoch with the highest accuracy on the test set is chosen\\nas the best model.\\n• Batch Size: 128 for single-scale models; 48 for multi-scale models.\\nLoss Function and Hyperparameters\\n• Compound Loss Weights: The reported results use λ1 = 5 (for BCE loss) and λ2 = 1 (for Wasserstein\\nloss).\\n• BCE Loss: Class imbalance was handled using inverse frequency weights applied to the BCE loss: [1.25948,\\n4.85382].\\n• Procedural Beta Distribution Parameters:\\n– base_K: 22.0\\n– ϵ: 0.08\\n– min_positive_risk_mean: 0.18\\n– min_concentration_positives: 18.0\\n– Influence Score Weights: weight_distance=0.7, weight_crop_size=0.3.\\n16\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\nFigure S4: Hyperparameter Analysis: Precision-Recall Trade-off\\nTable S1: Ablation Study on Loss Weights\\nλ1\\nλ2\\nF1 Score (↑)\\nPrecision (↑)\\nRecall (↑)\\n10\\n1\\n0.5880\\n0.5358\\n0.6514\\n5\\n1\\n0.5981\\n0.5607\\n0.6409\\n1\\n1\\n0.5905\\n0.5406\\n0.6505\\n1\\n5\\n0.5979\\n0.5932\\n0.6026\\n1\\n10\\n0.5855\\n0.5577\\n0.6163\\nData Augmentation\\nThe augmentation pipelines were used for our proposed models versus the baselines.\\n• For Proposed Probabilistic Models:\\n– Random Crop Ratio: (0.5, 1.0)\\n– Random horizontal flip (p=0.5)\\n– Random vertical flip (p=0.5)\\n– Random rotations (from -90 to 90 degrees)\\n– ColorJitter (brightness/contrast/saturation: [0.6, 1.4], hue: [0.0, 0.1])\\n• For Baseline Models:\\n– Random Crop Ratio: (0.3, 1.0)\\n– Random horizontal flip (p=0.5)\\n– Random vertical flip (p=0.5)\\n– Random rotations (from -90 to 90 degrees)\\n– ColorJitter (brightness/contrast/saturation: [0.6, 1.4], hue: [0.0, 0.1])\\nHardware\\n• All experiments were conducted on two NVIDIA A100 GPUs, each with 40GB of memory.\\nHyperparameter Analysis: Effect of Loss Weights\\nTo understand how the components of our compound loss function influence model behavior, we conducted a hyper-\\nparameter analysis on the loss weights, λ1 (for the classification loss, LBCE) and λ2 (for the Beta distribution loss,\\nLW 2\\n2 ). Table S1 demonstrates that these weights serve as a practical lever to tune the model’s predictive trade-offs for\\ndifferent application needs. See the supplementary materials for a detailed analysis.\\nOur analysis confirms two key trends. First, increasing the weight of the classification loss (λ1) makes the model\\nprioritize Recall. As shown in Table S1, increasing λ1 to 10 yields the highest recall score of 0.6514. This indicates\\nthat a stronger emphasis on the classification task pushes the model to more aggressively identify all potential high-risk\\nlocations, which is critical for safety applications.\\nConversely, increasing the weight of the Beta distribution loss (λ2) encourages a more balanced and precise model.\\nThe box plots in Figure S4, which show the performance distribution over 25 epochs, provide additional insight. They\\nvisually confirm that as λ2 increases, the median precision rises while recall moderately decreases, bringing the two\\nmetrics into closer alignment. This is exemplified by the λ1 = 1, λ2 = 5 configuration, which achieves the highest\\nprecision of all tested settings (0.5932) while maintaining a strong recall (0.6026), as detailed in Table S1. This\\n17\\n\\nElallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26)\\ndemonstrates that a stronger emphasis on the distribution-matching loss component encourages a more conservative\\nmodel that makes fewer, but more accurate, high-risk predictions.\\nThis analysis provides clear guidance for hyperparameter selection based on the desired outcome. For a safety-critical\\nsystem where failing to identify a hazard is the worst-case scenario, a higher λ1 is optimal. For applications requiring\\nhigh confidence in positive predictions to efficiently allocate resources, a higher λ2 would be chosen. For the main\\nresults reported in this paper, we selected the λ1 = 5, λ2 = 1 configuration, as it achieved the highest F1-score and\\nmaintained a strong recall, offering an excellent balance for our primary task.\\n18\\n',\n",
       " ' \\nINTERNAL \\nExample Journal, XXXX, XX: 1–42 \\n \\n \\n \\nLG-NuSegHop: A Local-to-Global \\nSelf-Supervised Pipeline For Nuclei \\nInstance Segmentation \\nVasileios Magoulianitis1, Catherine A. Alexander1, Jiaxin Yang1 \\nand C.-C. Jay Kuo1 \\n1Minh Hsieh Department of Electrical and Computer Engineering, \\nUniversity of Southern California (USC), Los Angeles, CA, USA \\n \\n \\nABSTRACT \\nNuclei segmentation is the cornerstone task in histology im- \\nage reading, shedding light on the underlying molecular pat- \\nterns and leading to disease or cancer diagnosis. Yet, it is \\na laborious task that requires expertise from trained physi- \\ncians. The large nuclei variability across different organ \\ntissues and acquisition processes challenges the automation \\nof this task. On the other hand, data annotations are ex- \\npensive to obtain, and thus, Deep Learning (DL) models \\nare challenged to generalize to unseen organs or different \\ndomains. This work proposes Local-to-Global NuSegHop \\n(LG-NuSegHop), a self-supervised pipeline developed on \\nprior knowledge of the problem and molecular biology. There \\nare three distinct modules: (1) a set of local processing op- \\nerations to generate a pseudolabel, (2) NuSegHop a novel \\ndata-driven feature extraction model and (3) a set of global \\noperations to post-process the predictions of NuSegHop. \\nNotably, even though the proposed pipeline uses no manu- \\nally annotated training data or domain adaptation, it main- \\ntains a good generalization performance on other datasets. \\nExperiments in three publicly available datasets show that \\nour method outperforms other self-supervised and weakly \\n \\nISSN XXXX-XXXX; DOI XXXXXXXX \\nXXXX XXXXXXXX \\n\\n2 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nsupervised methods while having a competitive standing \\namong fully supervised methods. Remarkably, every mod- \\nule within LG-NuSegHop is transparent and explainable to \\nphysicians. \\n \\n \\nKeywords: Histopathology images, Nucleus segmentation, \\nSelf-supervision, Data-driven feature extraction \\n \\n1 \\nIntroduction \\n \\nCancer diagnosis from biopsy tissue specimens has been the standard \\nway to tumor detection and grading. Cancerous and healthy cells have \\ndistinct molecular profiles which can provide important visual cues to \\npathologists. Nuclei segmentation is a fundamental task within this \\ndiagnosis pipeline, since the nuclei cell topology, size and shape play a \\ncrucial role to cancer grade reading. Hematoxylin and Eosin (H&E)- \\nstained images give rise to this molecular profile by highlighting the \\nnuclei cells and it has been the cornerstone process for histolopatho- \\nlogical slides preparation [40]. \\nUndoubtedly, histopathological image reading is a painstaking task. \\nIt relies on very subtle visual cues, requiring also highly expertise. On \\ntop of this, digitized slides are captured usually under a high magnifi- \\ncation level, typically ranging from 20x-40x. That results in very high \\nresolution images which pathologists need to examine thoroughly to \\nrecognize potentially cancerous regions. Given that multiple cores usu- \\nally sampled out of each patient, one can realize that analyzing histol- \\nogy slides is a fairly time consuming and laborious task [10]. Computer- \\naided diagnosis (CAD) tools are meant to automate certain physicians’ \\ntasks, offering also a more objective decision making process. Auto- \\nmated nuclei segmentation can expedite the slide reading process by \\nhighlighting the molecular patterns and enhance pathologist’s reading. \\nMoreover, it can be used as the intermediate step toward whole-slide \\nclassification (WSI) for models aiming to learn the pattern of clusters \\nthat nuclei form and map that to a grade group of cancer [34, 4]. \\nNuclei segmentation poses several challenges to models and algo- \\nrithms. At first, the H&E staining process [35] involves many steps \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n3 \\n \\n \\nINTERNAL \\ncarried out manually from humans and thus it is far from stable and \\nnoise-free. Staining artifacts can also increase the intra-class distance, \\nwhile during the image acquisition process, the type of scanner and its \\nparameterization can also affect the nuclei appearance [54]. Another \\nchallenge that modern Deep Learning (DL) models are faced with is the \\nlack of large annotated datasets, since it is a labor intensive task that \\nonly expert pathologists can perform. Therefore, data annotation is \\nexpensive and also subject to high inter-observer variability [6], which \\nregards this problem as learning from noisy labels. \\nThere is a plethora of works in existing literature which approach \\nthe problem from different angles. Prior to the DL-based methods, \\nmost of the works focused on unsupervised methodologies. For in- \\nstance, different variants of thresholding operations [59, 41], active \\ncontours [1] and level sets [7], watershed algorithm [17, 32], Graph \\ncuts [16] and K-means clustering [58]. Those approaches were mostly \\nrelied on biological priors of the problem, particularly about the nuclei \\nappearance, shape and size. \\nIt has been almost a decade since the advent of DL in the medical \\nimaging field. For segmentation tasks, fully convolutional pipelines, \\nsuch as U-Net [47] are popular choices among the researchers for se- \\nmantic segmentation. Fully supervised methods use U-Net as backbone \\narchitecture [8, 14], also coupled with attention mechanisms tailored to \\nfocusing the learning on the error-prone regions (i.e. nuclei bound- \\naries) [70, 29]. Since fully supervised methods are heavily challenged \\nfrom the lack of large annotated datasets, weakly supervised meth- \\nods[67, 27] attempt to learn either using less labels or point-wise an- \\nnotations [26, 44]. Furthermore, unsupervised learning methods use \\nself-supervision and specifically employ domain adaptation [13] and \\npredictive learning [49] to transfer the nuclei appearance from other \\ndomains. Nevertheless, they fail to achieve a competitive performance. \\nDespite their success in other computer vision problems, DL mod- \\nels are challenged in the medical imaging tasks, mainly due to the lack \\nof large datasets. More importantly, DL models are often criticized \\nas “black-boxes\" from physicians [42], since inherently their feature \\nlearning process is intricate. Moreover, to achieve a good performance, \\nbackbone models require pretraining on the ImageNet. As such, it is \\nunclear how the representations can be adapted from a natural imag- \\ning domain to the biological one. Furthermore, those models fail to \\n\\n4 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nexplicitly incorporate human’s prior knowledge which is important for \\na transparent decision making from the tools. \\nAll the mentioned reasons motivate this work to attempt a fully un- \\nsupervised pipeline and also decouple from the DL paradigm. Instead, \\na novel data-driven feature extraction model for histology images is \\nintroduced, namely NuSegHop. It is a linear, feedfoward and multi- \\nscale model to learn the local texture from the histology images. Our \\napproach is based on the Green Learning (GL) [20] paradigm which of- \\nfers a framework for feature learning at a significantly lower complexity, \\nwhere the features can be seamlessly interpreted [21]. The proposed \\npipeline consists of three major modules, starting with a set of local \\nprocessing operations using priors of the task to generate a pseudola- \\nbel. Then, NuSegHop is used in a self-supervised manner to predict a \\nheatmap for nuclei presence. Finally, a set of global processing oper- \\nations takes place as a post-processing to decrease the false negative \\nand positive rates, also in a self-supervised manner. Overall, the full \\npipeline incorporates self-supervised learning and priors insights, rang- \\ning from local areas (i.e. patches), up to global image decisions. There- \\nfore, the overall proposed pipeline is named Local-to-Global NuSegHop \\n(LG-NuSegHop). \\nThe main contributions of this work are: \\n \\n• NuSegHop as a data-driven feature extraction model to learn the \\ntexture in histology images in an unsupervised way. \\n• LG-NuSegHop self-supervised pipeline which combines local and \\nglobal image processing techniques, along with the NuSegHop \\nmodel for nuclei segmentation. \\n• Competitive performance in three diverse datasets among other \\nDL-based supervised and weakly supervised models. Also, ex- \\ntensive quantitative and qualitative comparisons and discussions \\nare offered to help realizing in which cases supervision makes a \\ndifference in this task. \\n• Cross-domain experiments showing high generalization perfor- \\nmance across multi-organ datasets, with different staining meth- \\nods. \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n5 \\n \\n \\nINTERNAL \\n2 \\nRelated Work \\n \\nIn this section we provide an overview of methods about nuclei seg- \\nmentation across different categories, beginning with the traditional \\npipelines that our work has elements from, and further including the \\nDL state-of-the-art methods. For a more comprehensive and detailed \\noverview of nuclei segmentation, a recent survey [35] conducted that \\nprovides detailed explanations and comparisons. \\n \\n2.1 \\nŁraditional Methods \\nEarlier works relied mostly on priors from the nuclei appearance and \\ncertain assumptions to solve the problem. Threshodling was funda- \\nmental in early segmentation works, where different methods propose \\nmechanisms for calculating the appropriate threshold to binarize the \\ninput image. A popular algorithm in many works is Otsu’s threshold- \\ning [41, 2], trying to minimize the intra-class variance or maximizing \\nthe inter-class one and automatically discover the best threshold. Win \\net.al. [59] apply a median filter on each color component and then \\nperform Otsu’s thresholding on the grayscaled image, followed by mor- \\nphological operations to refine the output. A locally adaptive thresh- \\nolding mechanism on linear color projections has been also proposed \\nin [38]. \\nWatershed algorithm [46] is another popular approach that uses \\ntopological information to segment an image into regions called catch- \\nment basins. This algorithm requires initial markers which are the \\nseeds of the catchment basis. It is a popular choice in many works a \\npost-processing step to find the nuclei boundaries [32, 60]. \\n \\n2.2 \\nLearning-based Methods \\n2.2.1 Full Supervision \\nThe initial DL-based works [62] fully relied on pathologists’ annota- \\ntions to learn the nuclei color and texture variations. Region-proposal \\nworks employ Mask-RCNN to detect the nuclei [24, 48]. One of the \\nmost popular architectures for medical image segmentation used as a \\nbackbone in several works is the U-Net [47]. Kumar et al. [19] pro- \\nposed a 3-way CNN model trained to supervise the nuclei boundaries. \\n\\n6 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nInstead of a binary map, it produces a ternary one which helps dis- \\ntinguishing better the nuclei from background. Essentially, this study \\nshows that attention on the nuclei boundary improves the accuracy of \\nthe detection and segmentation. \\nCIA-Net [70] leverages the mutual dependencies between nuclei and \\ntheir boundaries across different scales, and combines two distinct \\ndecoders, one for nuclei and one for their contours. It also proposes \\nthe Truncated Loss for diminishing the influence of outlier regions and \\nmitigate the noisy labels effect. Moreover, to enhance the multi- \\nscale learning capabilities, it introduces lateral connections between the \\nencoder and decoder in each layer. In this way, the texture information \\nlearned in the early layers can be combined with the semantic features \\nfrom the deeper ones. Graham et al. [8] introduce Hover-Net, a multi- \\nbranch network that is trained on segmentation, classification and pixel \\ndistance from the nuclear mass targets. In the loss function, the \\nmean squared error (MSE) is calculated between the ground truth and \\nthe distance map, as well as the MSE of the distance gradients. By \\nincluding the gradients into the loss function, it was found that it helped \\nto delineate the nuclei boundaries more accurately. \\nAs emphasized, full label collection in this task is expensive and not \\nin abundance. To this end, point-wise labels [44, 66] can be used to \\nlearn the appearance of nuclei from partial point annotations. Further- \\nmore, it is possible to combine point annotations and a limited number \\nof full nuclei masks to enhance the learning process and improve the \\nresults [45]. \\n \\n2.2.2 Self-Supervision \\nSeveral methods have used self-supervision, to learn from a different \\ntask domain and transfer the knowledge into nuclei segmentation. Do- \\nmain adaptation is a popular self-supervised choice since it exploits the \\nlarge volumes of labeled data from other domains, and then apply it to \\nthe target domain. Domain Adaptive Region-based CNN (DARCNN) \\nis proposed in [13] which learns definition of objects from a generic \\nnatural object detection dataset [25] and adapts it on the biomedical \\ndatasets. This is possible through a domain separation module that \\nlearns domain specific and domain invariant features. Liu et al. [28] \\npropose the Cycle Consistent Panoptic Domain Adaptive Mask RCNN \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n7 \\n \\n \\nINTERNAL \\n(CyC-PDAM) that learns from fluorescence microscopy images and \\nsynthesizes H&E stained images using the CycleGAN [71]. Contrastive \\nlearning is another way for applying self-supervision. Xie et al. [61] \\npropose an instance aware self-supervised method which involves scale- \\nwise triplet learning and count ranking to implicitly learn nuclei from \\nthe different magnification levels. \\nPredictive learning is another alternative to learn representations \\nimplicitly from the data. Sahasrabudhe et al. [49] have proposed a \\nmethod on the assumption that the image magnification level can be \\ndetermined by the texture and size of nuclei. In turn, this can be \\nused as a self-supervised signal to detect nuclei locations and seed the \\nwatershed algorithm. Zheng et al. [68] have proposed a method that \\ngenerates pseudo labels obtained from an unsupervised module using \\nk-means clustering on the Hue-Saturation-Intensity (HSI) colorspace. \\nThen, an SVM classifier is trained on a feature vector with color and \\ntexture, as well as topological attributes. Our work is conceptually sim- \\nilar to that work, since it creates a pseudolabel from local thresholding \\noperations and then uses self-supervision at a global level. \\n \\n2.3 Green Learning \\nGreen Learning (GL), has been recently introduced in[20], aiming to \\nprovide a more transparent feed-forward feature extraction process, at \\na small complexity and model size [21, 64]. The proposed feature ex- \\ntraction model create a multi-scale feature extraction and creates a rich \\nspatial-spectral representation of the input image [5]. Instead of con- \\nvolutional filters trained with backpropagation, principal component \\nanalysis (PCA) is used to learn the local subspace across different lay- \\ners, where each feature has larger receptive field along deeper stages. \\nFollowing GL’s terminology, each layer is called “Hop\", in which fea- \\ntures are learned in an unsupervised and data-driven way. \\nWithin the medical imaging field, Liu et. al. [31, 30] have proposed \\nthe first works on segmentation and classification tasks. Also, GL has \\nrecently achieved competitive results in prostate cancer detection from \\nMagnetic Resonance Images [37]. From the same framework, a U-Net \\ninspired model, namely GUSL, has been introduced for medical image \\nanalysis segmentation problems, with applications to prostate gland \\nsegmentation. It is a fully supervised method, meant to offer multi- \\n\\n8 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nscale feature extraction and semantic segmentation at different scales, \\nintroducing a novel coarse-to-fine scale regression model. Our core \\nproposed module, NuSegHop, uses the channel-wise Saab transform \\nfrom GL for pixel-wise feature extraction and classification in a self- \\nsupervised manner. To the best of our knowledge, this is the first \\nGL-based work in digital histology. \\n \\n3 \\nMaterials And Methods \\n \\nAlthough the existing DL-based methods usually comprise one model \\nthat is trained in an end-to-end manner using backpropagation, LG- \\nNuSegHop has distinct modules, and each of those has a discrete task \\nwithin the pipeline. This approach decouples from the DL paradigm, \\nand one of its key benefits is the transparency of every step, since ev- \\nery module has a specific task and role within the overall pipeline. The \\nproposed pipeline comprises three distinct modules that operate suc- \\ncessively. In Section 3.1, we describe the image preprocessing steps, \\nmeant to enhance the input image towards the subsequent operations. \\nIn Section3.2, the local pixel-wise operations are described to predict \\na pseudolabel for NuSegHop, where its architecture and process is de- \\ntailed in Section 3.3. The global processing modules are presented in \\n3.4. An overview of the entire proposed LG-NuSegHop pipeline is il- \\nlustrated in Fig. 1. \\n \\n3.1 \\nPreprocessing \\nThe preprocessing modules aim at preparing the input image tile for the \\nsubsequent local processing module which is mainly based on thresh- \\nolding. Thus, the key goals are: (1) highlight the nuclei over the back- \\nground and (2) convert the color image into grayscale. \\nPrior to thresholding, the goal is to make nuclei more distinct over \\nthe background tissue. From the theory of the H&E staining pro- \\ncess, hematoxylin principally colors the nuclei cells to a darker color \\n(e.g. blue or dark-purple), while eosin mainly stains the cytoplasm \\nand other structures in the background area. There are several meth- \\nods in literature for carrying out this color conversion. We choose the \\nwork of Salvi et al. [50]. It is an Singular Value Decomposition (SVD)- \\ngeodesic based method for stain separation, after converting the input \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n9 \\n \\n \\nINTERNAL \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nPseudolabel \\nHeatmap \\nCandidate ROIs \\nFP Instances \\n \\nFigure 1: An overview of LG-NuSegHop pipeline. Pre-processing applies image \\nenhancement to prepare the image for the local processing modules. NuSegHop \\nreceives as input the pseudolabel and predicts a heatmap. In the last step, global \\nprocessing modules increase the nuclei detection rate using information across the \\nentire input image. \\n \\n \\nimage from RGB to the optical density space, where SVD can be more \\neffective. Another benefit from stain separation, it helps mitigating \\nthe large stain variability across different images which is one of the \\nchallenges in this task. \\nAfter separating the stain colors using the orthogonal spaces from \\nSVD, we project all pixels on the Hematoxylin’s subspace -eigenvector \\nfrom SVD corresponding to nuclei- to create image H. To further \\nenhance this separation and make also nuclei boundary more distinct – \\nespecially for images that suffer from blurry artifacts due to the staining \\nor acquisition process– we apply histogram equalization. \\nThe last preprocessing step to prepare the input tile for the thresh- \\nolding operation is convert it to grayscale. Although other colorspaces \\n(e.g. LAB) could be an option, in our earlier work [36] we have shown \\na more optimal way to convert the image into grayscale. Transforma- \\ntion across different colorspaces use certain formula to map pixel values \\nPreprocessing \\n \\n \\nHematoxylin \\nExtraction \\nInput \\nPredicted Mask \\nContrast \\nEnhancement \\n \\n \\n \\n \\nLocal Processing \\nGlobal Processing \\nIntermediate Outputs \\nPatch Splitting \\nAdaptive \\nThresholding \\nInstance \\nRefinement \\nAnomalous \\nInstance \\nRemoval \\nNuSegHop \\nHeatmap \\nFiltering \\nLocal Maxima \\nDetection \\nWatershed + \\nThresholding \\nSelf-Supervised \\nClassification \\n\\n10 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n \\nFigure 2: An illustration of the main preprocessing steps, involving stain separation \\nand the PQR method to convert color into grayscale. \\n \\n \\n \\nfrom one domain to the other. PQR, named after the three channels \\nof principal component analysis, is a data-driven way for color conver- \\nsion, adapted to the input content. It is based on SVD to calculate the \\ncolor subspace direction that maximizes the data variance. One ad- \\nvantage is the better energy compaction in one channel, comparing to \\na fixed colorspace conversion. Moreover, finding the color conversion \\nthat maximizes the variance is particularly important for the subse- \\nquent thresholding operation, since we assume that along the direction \\nof that subspace, the separation between nuclei and background is max- \\nimized. Therefore, after SVD, we linearly project patches Hp from the \\nH image on its first principal component P (p-value) to convert from \\ncolor to grayscale. Since, different areas of the tile may have different \\nstatistics, we perform PQR independently after the image is split in \\nlocal patches which are subject to thresholding. An illustrative exam- \\nple of PQR is shown in Fig. 2. The color conversion formulae are as \\nfollows: \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n11 \\n \\n \\nINTERNAL \\n \\nHp = U · S · V ⊺ \\n(1) \\n \\nP ≜ V1,1:,  Q ≜ V2,1:,  R ≜ V3,1: \\n(2) \\n \\n3.2 Local Processing \\nThe main purpose of this module is to create a pseudolabel for training \\nNuSegHop. This module uses simple, yet effective and intuitive image \\nprocessing techniques at a local level. It employs prior knowledge of the \\nproblem and self-supervision locally, to filter out error predictions from \\nthe unsupervised local processing. To this end, certain assumptions are \\nmade, to overcome the lack of supervision: \\n1. The bi-modal distribution according to which at a local area \\nhistogram there are two peaks, where the lower intensity corre- \\nsponds to nucleus and the brighter to background. Preprocessing \\nis meant to accentuate this assumption \\n2. Local similarity where adjacent nuclei tend to have less color or \\ntexture variations \\n3. Larger low intensity components are less likely to be false posi- \\ntives than the smaller instances \\n \\n \\n3.2.1 Adaptive Thresholding \\nThe thresholding method we propose is adaptive in two ways: (1) scale- \\nwise and (2) intensity-wise. The input image is split into patches of \\nsize P 50 × 50 and the process starts out with estimating the local \\ndistribution. If the bi-modal criteria are not met, the process tries \\nalso patches of 25 × 25 and 100 × 100. This is to adapt on different \\nnuclei sizes or magnification levels. On the other hand, the threshold \\nat each local patch is automatically adjusted based on the local area \\nstatistics. One choice for threshold calculation is to simply pick the \\nintermediate value between the two peaks. Yet, we opt for a more \\nadaptive way to calculate the optimal threshold [38], thus reducing \\nthe under or over segmentation effects and eventually create a less \\n\\n12 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n2 \\n \\n \\nFigure 3: Demonstration of the P-value distribution in a local patch where the \\nbimodal assumption holds. The auxiliary lines to calculate the adapted threshold \\nTˆ are also depicted. \\n \\n \\nnoisy pseudolabel for self-supervision. Given the histogram and the \\ntwo main peaks T1 and T2 under the bi-modal assumption, we define \\nas L12 the line crosses through T1, T2, the intermediate point To where \\nthe threshold correction is about To = T1+T2 . Also, Tc is defined as \\nthe intercept point of the intensity value axis of the histogram and the \\nperpendicular line of L12, passing from To (see Fig. 3). By using this \\nheuristic and simple method to correct the intermediate p-value from \\n(P)QR pre-processing, the local thresholding is more resilient to false \\npositive and negative pixels. More details can be found in [38]. A λ \\nhyperparameter is used to control the amount of correction about To. \\nThe adjusted threshold Tˆ formula is calculated using Eq. 3. \\n \\nTˆ = To + λ(To − Tc) \\n(3) \\n \\n3.2.2 Morphological Instance Refinement \\nAlthough adaptive thresholding works well in areas with relatively low \\nvariation, there are patches where the color variance is higher, thereby \\ncausing over or under segmentation effects. Morphological processing \\nhas been widely used in literature for processing binarized images. To \\nrefine the thresholding operation, we apply a set of morphological oper- \\nations, such as hole filling, small instance removal and nuclei splitting. \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n13 \\n \\n \\nINTERNAL \\nPriors about the nuclei size and shape are incorporated to apply sim- \\nple morphological processing and filter out noisy instances. For nuclei \\nsplitting, the convex hull algorithm is employed to detect highly deep \\ncurvatures that are not indicative of nuclei shape. This step is signifi- \\ncant since subsequent operations operate on a per-instance base. \\n \\n3.2.3 Locally Anomalous Instance Removal (LAIR) \\nTo further filter out larger instances that are possible to be false pos- \\nitives, we carry out a simple local comparison among the detected in- \\nstances. For this operation we need a larger patch, in order to in- \\nclude more nuclei instances and make the comparison effective. Hence. \\nfor this submodule a 200 × 200 patch is used. In each large patch, \\nthe first criterion for query instances q is the size. That is, if an in- \\nstance has a small to medium size, it will be compared against the \\nrest larger instances r that provide reference, according to assumption \\n3 (see Section 3.2). We create a reference representation by forming \\nthe ensemble from the non-query instances. This can be viewed as a \\nfirst step of introducing self-supervision locally in our pipeline, coupled \\nwith priors from the task. Intuitively, abnormally looking instances \\nat a certain feature space can be regarded as anomalies and in turn \\neliminated. We define, Q = {q1, q2, . . . , qN } the instances being tested \\nand R = {r1, r2, . . . , rM } the reference instances. \\nRegarding the feature representation, we use the HSI colorspace \\nfrom the H image, along with the channel-wise contrast value. For \\nsimilarity comparison, a Gaussian kernel is used to measure the dis- \\ntance between each query instance xj and the ensemble reference xR \\n(see Eq. 4), and determine the anomaly instances that are subject to \\nremoval obtaining a similarity score S (see Eq. 5). Instances q that have \\na lower similarity with their local reference class R from a predefined \\nthreshold Ts are removed from the foreground. \\n \\n1 \\nxR = \\n|R| \\nΣ \\nxi, \\n(4) \\ni∈R \\n \\nS(x , x ) = e−γ||xj −xR||2 \\nR \\nj \\n2 , \\n∀j ∈ Q, \\n(5) \\n\\n14 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n \\n \\n \\nFigure 4: Graphical overview of the proposed NuSegHop for feature extraction. It \\nconsists of two layers that operate in two different scales. From both layers two \\ntypes of features are extracted: (1) spatial and (2) spectral. With red we depict the \\nextracted feature maps that have low energy and will be discarded. All the spatial \\nfeature maps (green color) are concatenated to extract the spectral ones (gray color). \\n \\n \\n3.3 NuSegHop \\nAfter the local processing module operations, we have obtained an ini- \\ntial segmentation output using no labels or training data. This output \\ncan be used as a pseudolabel to a classifier. Aiming at obtaining a \\nprobability heatmap for nuclei segmentation, we propose a novel and \\nunsupervised feature extraction method, named NuSegHop, to learn \\nthe local texture of nuclei for pixel-wise classification. Other methods \\nin the past [68] have used hand-crafted features or pure color-based \\nmethods which lack robustness. A data-driven and multi-scale feature \\nextraction is proposed in this paper for pixel-wise nuclei segmentation \\nusing the GL paradigm [21]. This method has as a key advantage the \\nlow complexity and small model size, which is essential for fast infer- \\nence. Moreover, the GL-based feature extraction module is linear and \\nthereby more transparent and interpretable [64]. This is another major \\ndistinction from the DL models that use backprogagation for learning \\nfeature representations. NuSegHop learns features in a feed-forward \\nmanner and does not require large datasets in order to obtain robust \\nfeature representations. Therefore, GL-based methods have certain \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n15 \\n \\n \\nINTERNAL \\nadvantages in specific tasks and this is why we opt for such a feature \\nextraction method. A detailed architecture of NuSegHop is illustrated \\nin Fig. 4. \\nFor NuSegHop, the H image is converted into the HSI colorspace \\nbefore feature extraction. Originally, a window area A of size S(1)×S(1) \\nis considered to characterize the area about a pixel. For this problem, \\nwe choose S(1) = 9, since too small windows may not be able to learn \\nthe local texture, while larger ones may induce more noise into fea- \\nture learning. NuSegHop learns the texture within the window area \\nin two scales S1 and S2, to give multi-scale properties in the feature \\nspace. The core operation in NuSegHop for texture learning is the \\nSaab transform [21], which is based on the Principal Component Anal- \\nysis (PCA), applied in two ways: (1) spatial and (2) spectral. The full \\nfeature extraction diagram of NuSegHop is shown in Figure 4. \\n \\n \\n3.3.1 Feature learning - Spatial Saab \\nTo learn the texture across different local areas within a window, a \\nneighborhood construction with filters of spatial size of f (1) × f (1) is \\napplied with stride 1 and equal padding. Since the input image has \\nthree color channels, each local neighborhood defines a cuboid C(1) with \\nsize K(1) = f (1)×f (1)×3 which contains the local HSI color information. \\nAs a consequence, K(1) is the maximum number of extracted subspaces \\nfrom the Saab transform in layer 1. L(1) = S(1) × S(1) such cuboids \\ncan be extracted from a window at layer-1 using padding on A. By \\nsampling across windows centered on the pixels of the original image, \\none can create a tensor T (1) for training with size N × L(1) × K(1), \\nwhere N the number of sampled pixels from the input image to training \\nNuSegHop. In turn, T (1) can be used for training layer-1 and calculate \\nthe subspaces (Eq. 6) used for feature extraction. After SVD matrix \\ndecomposition, the rows of V are the eigenvectors correspond to the \\northogonal subspaces of the signal (see Eq. 7). \\n \\nT (1) = U · S · V ⊺ \\n(6) \\n \\n \\nW(1) = V1:M,1: \\n(7) \\n\\n16 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nIn training, Saab transform is applied on C(1) to extract K(1) or- \\nthogonal subspaces (i.e. principal components). Moreover, because \\nmany principal components may carry no significant energy –as it is \\ndictated from their corresponding eigenvalues–, they can be discarded \\nto remove unnecessary complexity and noise. That is, the first M prin- \\ncipal components are retained, with M < K(1). \\nWe define the weight matrix W (1) ∈ RM×K(1) which contains all the \\ninformation to decomposing the cuboids into their spectral represen- \\ntations by projecting onto the extracted principal components. After \\ntraining, W incudes the weights for feature extraction. For instance, \\nto perform spatial feature extraction in layer 1, one needs to multiply \\nL(1) cuboids extracted within A window, and project them along the \\nM principal components. To formulate this operation, we construct \\nmatrix C with size L(1) × K(1), where its rows contain the cuboids. \\nBy multiplying them, we calculate matrix F of size L(1) × M which \\nincludes all the spatial features of layer-1 (see Eq. 8). Spectral maps \\nF (1) in layer 1 are obtained from the matrix F , by reshaping it back to \\nsize S(1) × S(1) × M . We view each principal component as a different \\nspectral local representation of A. \\n \\nF = C · W ⊺ \\n(8) \\nWe also concatenate as additional feature (one more column in F ) \\nthe mean of each local cuboid in both layers (Eq. 9), since apart from \\nthe texture, the local color is also important to differentiate nuclei from \\nbackground. If we are to draw a parallel with circuit theory, the DC \\ncomponent is the mean color and AC are the textures derived from the \\nSaab transform. Thus, F is now of size L(1) × (M + 1). \\n \\nF = FDC ⊕ FAC \\n(9) \\nIn layer-2, feature map F (1) is fed as input after a max-pooling layer. \\nThe spatial feature extraction process in layer-2 is similar to layer-1, \\nwith one difference, the Saab transform is applied independently on \\neach of the M +1 feature maps of layer-1 [5]. Therefore, after neighbor- \\nhood construction each cuboid C(2) has a shape of K(2) = f (2)×f (2)×1. \\nAlso, the spatial size of L(2) = S(2) × S(2) = ⌈(S(1)/2)⌉ × ⌈(S(1)/2)⌉ \\nafter max-pooling. The filter sizes and output shapes of each layer are \\nshow in in Table 1. \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n17 \\n \\n \\nINTERNAL \\ns \\ns \\nTable 1: Architecture of the proposed NuSegHop. \\n \\nF Resolution \\nf Size \\nStride \\nLayer 1 \\n(9 × 9) × 3 \\n(3 × 3) × 1 \\n(1 × 1) × 1 \\nMax-pool 1 \\n(5 × 5) × 1 \\n(2 × 2) × 1 \\n(2 × 2) × 1 \\nLayer 2 \\n(5 × 5) × 1 \\n(3 × 3) × 1 \\n(1 × 1) × 1 \\n \\n \\nIndependent Saab transforms as many as M + 1 are applied on \\ntensors with size of N × L(2) × K(2). After channel-wise Saab transform \\nin layer-2, each spectral map originally has a size of S(2) × S(2) × (M + \\n1) × K(2), after concatenating all the feature maps from the channel- \\nwise Saab transforms. Energy-based spectral truncation is also applied \\nin layer-2. Supposing that Q principal components are kept from each \\nchannel-wise Saab transform (Q < C(2)), then the final layer-2 spectral \\nmaps will have a shape of S(2) × S(2) × (M + 1) × (Q + 1), adding also \\nthe DC channels in the same way as in layer 1. The energy threshold \\nfor both layers is set at Te = 1e − 03. \\n \\n \\n3.3.2 Feature learning - Spectral Saab \\nSpatial-wise Saab provides a spectral analysis of A across all its spatial \\nregions at the scales of S(1) and S(2) A. Therefore, each feature has a \\nspatial correspondence. Yet, it is required to extract features that have \\na global reference to A as well. Those features are unassociated from \\nthe spatial domain and meant to capture different patterns within A \\narea, such as boundary transitions from nuclei to background. To this \\nend, on each spectral map F (1) and F (2) from layers 1 and 2, we apply a \\nPCA using the spectral maps’ spatial components as features. By doing \\nso, the transformed signal will have no spatial correspondence anymore. \\nThis is performed independently for every M + 1 and (M + 1) ×(Q + 1) \\nspectral maps for layers 1 and 2, respectively. The spectral features \\nG(l) from each layer is simply the union of all PCA transformed spatial \\nfeatures F (1). (see Eq. 10 and 11.) The same Te is used to filter out \\nthe principal components and reduce the dimensionality. \\n \\nG(1) = ∪M+1{PCA(F (1))T } \\n(10) \\ns \\ns=1 \\ns \\ne \\n\\n18 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n \\nG(2) = ∪M+Q+2{PCA(F (2))T } \\n(11) \\ns \\ns=1 \\ns \\ne \\nThe last step in NuSegHop is to concatenate all the spatial and \\nspectral features from both layers to form the final feature X that \\ncharacterizes A. After concatenation the top 100 discriminant features \\nare selected [65]. This provides a rich spatial-spectral feature represen- \\ntation about the color and texture of the local neighborhood under A. \\nBesides, in the Saab feature space, the spectral dimensions are uncor- \\nrelated because the principal components are orthogonal by definition. \\nNuSegHop enables for fast pixel-wise predictions, requiring no su- \\npervision for its feature extraction part. Having extracted the features \\non each W , one can train a classifier using the pseudolabels from the lo- \\ncal processing module. We train an Xtreme Gradient Boosting (XGB) \\nclassifier and use its probability predictions to generate a heatmap Pˆ . \\nEach pixel contains the probability of belonging to the foreground. \\nIn the LG-NuSegHop pipeline each module has a clear scope for im- \\nproving certain aspects of the nuclei segmentation task. Therefore, each \\nmodule’s purpose and output are transparent. The linearity property \\nin NuSegHop features enables the visualization of the local patterns \\nthat weigh in classifying a pixel. Concretely, during inference, one can \\ntrace back the more “informative\" NuSegHop feature dimensions of the \\nclassifier. In turn, those features can be mapped back to the input layer \\n–using the inverse Saab transform–, to elucidate the texture and color \\npatterns they correspond to. In other words, NuSegHop enables the \\npathologist to review the visual elements that classify a certain region \\nas nuclei or background. This seamless decision interpretation provides \\ngreat advantages in clinical applications since pathologists can under- \\nstand the decision-making process of the tool, thereby making it more \\ntrustworthy for clinical deployment. \\n \\n3.4 Global Processing \\nThis module aims at integrating the locally made decisions, based on \\ncolor and texture, and perform a global post-processing. Most opera- \\ntions from the local processing group are pixel-wise and carried out in \\nlocal patches of the original image to reduce variability, whereas global \\nprocessing has the entire information about the image. \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n19 \\n \\n \\nINTERNAL \\n \\n \\n \\n \\nFigure 5: An overview of the global processing pipeline. The Laplacian of Gaus- \\nsian filter detects local minima from the NuSegHop heatmap to decrease the false \\nnegative rate. Watershed and probability thresholding binarize the image and de- \\nlineate nuclei boundary from the heatmap. Candidate instances are classified in a \\nself-supervised manner to detect any false positives. \\n \\n \\n \\nAs mentioned, locally-based decisions may miss faintly stained nu- \\nclei or misclassify background areas as nuclei. The goal of moving from \\nlocal to global decisions is to decrease the false negative (FN) rate by \\nincluding more instances as candidates, based on the probability ar- \\neas from the local decisions. Since, it is inevitable for this process to \\ngive rise to false positives (FP), self-supervision is also employed at \\nthe end of the global processing modules to help discern potentially \\nFP instances. A diagram of the global processing pipeline is shown in \\nFig. 5. \\n\\n20 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n3.4.1 \\nHeatmap Filtering \\nOut of our observations on the obtained probability heatmap P , most \\nof the nuclei are predicted with high confidence from NuSegHop mod- \\nule. This refers to solid stained instances that can be easily recognized \\nfrom their color and texture. As we want to decrease the complexity \\nof the global processing unit, large instances are retained and we con- \\nsider only the less confident ones for the subsequent module (i.e. local \\nmaxima detection). This helps both the complexity and efficiency. To \\ndo so, the heatmap and the predicted mask from NuSegHop are con- \\nsidered to calculate the per instance confidence, by taking the average \\nfrom all pixels belong to the instance. Highly confident instances with \\naverage probability more than 0.95 are removed from the local max- \\nima detection submodule. After this submodule we obtain the filtered \\nheatmap P\\n′ . \\n \\n \\n3.4.2 Local Maxima Detection \\nThis submodule aims mainly at increasing the recall ratio of nuclei, on \\nthe remaining areas after instance filtering, where the NuSegHop unit \\nis not confident. Texture and color variations or faintly stained nuclei \\nfrom the local processing module result in scattered high probability ar- \\neas that during binarization become isolated small instances. The goal \\nis to detect those areas and create candidate ROIs as foreground. Given \\nthe filtered heatmap P\\n′ this task boils down to a local maxima detec- \\ntion (LMD). We apply a Laplacian of Gaussian filter (LoG) to detect \\n“blob-like\" regions which corresponds to potential nuclei instances. The \\nGaussian filter is meant to smooth and unify the pixel-wise heatmap \\nestimation, thus to mitigating the color and texture variance. \\n \\n \\n3.4.3 Watershed Post-processing \\nFor the highly confident instances, their boundary is typically distinct \\nand can be determined from the local processing modules and in turn \\nfrom the heatmap. However, for the low confidence instances that are \\nhard to be accurately classified based on the heatmap. We can detect \\nthe position of the nuclei but it is hard to estimate accurately their \\nboundaries, since those areas are outliers when training the classifier. \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n21 \\n \\n \\nINTERNAL \\nAs long as the rough locations of candidates are obtained from the pre- \\nvious module, we their centroids to seed the watershed algorithm, and \\nfind the adjacent nuclei boundary lines. This helps areas with multiple \\nless confident nuclei located therein, where their boundaries estimation \\nis more challenging. As a last step, we binarize the filtered heatmap, ap- \\npending back also the confident instances using a probability threshold \\nto create instances for the subsequent classification and false positive \\nreduction. As it is desired to include many candidates, so to increase \\nthe recall rate, we choose a lower probability threshold Tp = 0.35. This \\ntypically increases the false positive rate, but the subsequent module \\nis devised to mitigate that. \\n \\n \\n \\n3.4.4 Self-Supervised Instance Classification \\n \\nThe final step of the proposed pipeline is a self-supervised instance- \\nbased classification in order to detect nuclei that their representation \\nfalls out of the normal nuclei and their appearance is closer to back- \\nground. \\nThis ROI instance based classification is similar to FLAIR module \\nbut with two main differences: (1) it is performed in a global level and \\n(2) there are no size-based criteria to select instances. \\nThe hypothesis here is the following: so long as the majority of \\ninstances is correctly classified, the minority of instances that are false \\npositives do not affect the ensemble learning, as they are statistically \\nless significant. Moreover, if their representation is closer to the back- \\nground, rather than the foreground then they are simply classified as \\nfalse positives and are removed from the final segmentation output. \\nFor feature extraction and classification, we use the H image and \\nconvert it to the HSI colorspace (as in NuSegHop). We apply feature \\nextraction on each channel separately and concatenate them before the \\nclassifier’s input. For features we opt for the first order statistics to \\nlearn the color characteristics, as well as the gray-level zone matrix \\nwhich includes several features that capture the rough texture of the \\nnuclei [33]. Here, we choose an SVM classifier with a radial basis kernel, \\nto predict for each instance the probability of being a true positive. \\n\\n22 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nTable 2: Summary of hyperparameters configuration in LG-NuSegHop, finetuned \\non a small subset of training images from MoNuSeg. \\n \\nHyperparameter \\nValue \\n \\nLocal Processing \\nλ (Adapt. Thresh.) \\n0.2 \\nS (LAIR) \\n0.7 \\nγ (LAIR) \\n0.1 \\nNuSegHop \\nEnergy Te \\n10e − 4 \\n# Spectral Dimensions \\n10 \\nXGB – # trees \\n100 \\nXGB – Tree depth \\n4 \\nXGB – Learning rate \\n0.075 \\nGlobal Processing \\nBlob Threshold (LoG) \\n0.05 \\nTp \\n0.35 \\n \\n4 \\nExperimental Results \\n \\nThis section includes details of the experiments conducted in this work \\nto validate the efficiency of our work, as well as potential areas of im- \\nprovement. The datasets used for experiments and comparisons are \\nbriefly introduced in subsection 4.1, while the metrics for the quanti- \\ntative analysis in 4.2. Additionally, our method is compared against \\nother state-of-the-art methods, from unsupervised to weakly and fully \\nsupervised methods in 4.3. Furthermore, an ablation study is carried \\nout in 4.4 to evaluate how different modules affect the performance, \\naccompanied with visualization examples (4.4.1). In subsection 4.5 we \\ndiscuss on the findings from the comparisons with the state-of-the-art. \\n \\n4.1 \\nDatasets \\nThree publicly available datasets are chosen to compare our proposed \\nmethodology. \\nMoNuSeg [18] The Multi-Organ Nuclei Segmentation (MoNuSeg) \\ndataset includes 30 training image tiles of size 1000x1000 and magnifica- \\ntion level 40x, comprising 21, 623 nuclei. Also, 14 testing imag tiles are \\navailable for benhmarking. The extracted tiles come from histological \\nslides of breast, liver, kidney, prostate, bladder, colon, and stomach col- \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n23 \\n \\n \\nINTERNAL \\nlected from The Cancer Genome Atlas(TCGA) [53]. Also, the samples \\ncollected come from different hospitals and patients. Therefore, it is a \\nfairly diverse dataset across different aspects that challenges the gen- \\neralization ability of the segmentation model. The Aperio ImageScope \\nwas used for digitizing the slides. \\nCryoNuSeg [39] This is the first H&E multi-organ dataset from \\nfrozen samples. Slides digitization from frozen samples involves a dif- \\nferent process and therefore the nuclei appearance is different. This \\ntechnique pertains intra-operative surgical sessions and its major ben- \\nefit is that it can be performed rapidly. Yet, the requirement for a \\nquick slide preparation, staining and digitization comes at the expense \\nof the image quality. The dataset provides 30 digitized images of size \\n512 × 512, acquired at a 40× magnification level. The slides come \\nfrom 10 different organs (larynx gland, adrenal, lymph nodes, pan- \\ncreas, skin, pleura, mediastinum, thyroid gland, thymus, testes) and \\nthere exist 7, 596 annotated nuclei. \\nCoNSeP [9] This dataset includes 41 image tiles from 16 slides of \\npatients with colorectal adenocarcinoma. 27 tiles are used for training \\nand 14 for testing. The extracted tiles are of size 1000 × 1000. The \\nOmnyx VL120 scanner was used for the slides digitization at a 40× \\nmagnification level. Overall, 24, 319 nuclei are annotated. \\n \\n4.2 Evaluation Metrics \\nFor performance evaluation, we use three different metrics, that have \\nbeen commonly used in the literature. It is worth noting that nuclei \\nsegmentation is an instance-level segmentation problem. That is, a \\nnucleus instance needs to be detected and then segmented. F1 score is \\nthe harmonic mean of the precision and recall. The F1 score regards \\nnuclei segmentation as an instance detection problem, without taking \\ninto account the segmentation aspect. To complete our metrics, we \\nalso include the Aggregated Jaccard Index (AJI) metric [19] and the \\nPanoptic Quality (PQ) [15]. These two metrics are more suitable for \\ninstance-level segmentation problems as they take into account both \\naspects. In particular, PQ calculates the detection quality (DQ), as well \\nas the segmentation quality as a similarity measure with the ground \\ntruth. Dice similarity coefficient is also included in our comparisons to \\nmeasure the segmentation performance. \\n\\n24 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n4.3 Results & Comparisons \\n4.3.1 Experimental Setup \\nTo have a thorough understanding on the advantages and weaknesses \\nof our work, we compare it against several state-of-the-art works with \\ndifferent levels of supervision. At first, we compare our method with \\nother self-supervised methods on the MoNuSeg dataset, which use no \\nlabels from the target datasets. Another category is the weakly super- \\nvised methods that either use less training samples from the annotation \\nmasks or point annotations. Moreover, we include in our analysis a few \\npopular fully supervised methods, so as to provide a thorough compar- \\nison of our work. \\nBefore we delve into the comparisons, one aspect we would like \\nto stress is that our method does not use any training data for pa- \\nrameters learning. Yet, since there are several hyperparameters (see \\nTable 2) need adjustment, we use 6 out of the 30 training images ran- \\ndomly from the MoNuSeg dataset to finetune the LG-NuSegHop. This \\ncan be viewed as the validation set of our experiments. Hence, the \\nhyperparameters are empirically finetuned using this validation set. \\nAt first, we adjust the local processing module, using the AJI and F1 \\nscore calculated from the intermediate (i.e. pseudolabel) output. Then, \\nNuSegHop and global processing modules are adjusted together. For all \\nthe modules we tune the hyperparameters to maximize the AJI metric, \\nwhereas in the last module of the global processing set (self-supervised \\ninstance classification) we try to maximize the F1 score, since it is an \\nROI-wise classification task. \\nAfter the model is fixed, we test it out on the three testing datasets. \\nThis aims at testing how well our model generalizes to data with in- \\nherent discrepancies. However, we individually finetune LG-NuSegHop \\nhyperparameters on each datasets using their a subset of their train- \\ning data, in order to also test the performance when LG-NuSegHop is \\nadapted to a certain domain. \\n \\n4.3.2 Performance benchmarking \\nAt first glance, LG-NuSegHop has a competitive standing in compari- \\nson with other works, including also the fully supervised ones (Tables 3, \\n4 and 5). In the MoNuSeg dataset, it outperforms by large margins \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n25 \\n \\n \\nINTERNAL \\nTable 3: Performance benchmarking with self, weakly and fully supervised methods \\nin the MoNuSeg dataset. \\n \\nAJI \\nF1 \\nDice \\nSelf Supervised \\nDARCNN [13] \\n0.446 \\n0.5410 \\n- \\nSelf-Attention [49] \\n0.535 \\n- \\n0.747 \\nCyC-PDAM [28] \\n0.561 \\n0.748 \\n- \\nNucleus-Aware [51] \\n0.593 \\n0.759 \\n- \\nWeakly Supervised \\nPartial Points [43] \\n0.543 \\n0.776 \\n0.732 \\nPoint Annotations [26] \\n0.562 \\n0.776 \\n0.744 \\nBoNuS [27] \\n0.607 \\n0.780 \\n0.767 \\nCyclic Learning [69] \\n0.636 \\n0.774 \\n0.774 \\nFully Supervised \\nU-Net [47] \\n0.543 \\n0.779 \\n- \\nRCSAU-Net [56] \\n0.619 \\n0.82 \\n- \\nHoVer-Net [8] \\n0.618 \\n0.826 \\n- \\nCDNet [11] \\n0.633 \\n0.831 \\n- \\nTopoSeg [12] \\n0.643 \\n- \\n- \\nGUSL [63] \\n0.673 \\n0.886 \\n0.803 \\nNucleiSegNet [22] \\n0.688 \\n0.813 \\n- \\nCIA-Net [70] \\n0.691 \\n0.901 \\n- \\nLG-NuSegHop (Baseline) \\n0.651 \\n0.887 \\n0.778 \\nLG-NuSegHop (Dom. Adapted) \\n0.658 \\n0.892 \\n0.791 \\n \\n \\n \\nthe self and weakly supervised works in terms of all the reported met- \\nrics, with a 0.651 AJI. It also has an impressive performance stand- \\ning among the fully supervised works, including sophisticated models \\nfor nuclei segmentation, such as HoVer-Net [8] and CDNet [11] (see \\nTable 3). However, other models such as NucleiSegNet [22] and CIA- \\nNet [70] achieve higher performance in AJI, yet LG-NuSegHop achieves \\na competitive standing in detecting nuclei based on the F1 score (0.892 \\nvs. 0.901). Comparing our method with GUSL model from the GL \\nmethodology, it achieves an AJI score of 0.673. The full supervision \\nof GUSL helps to learn the more challenging distinctions between nu- \\nclei and background, which are hard to capture by a self-supervised \\nmethod. Yet, in terms of F1 score, the detection performance is sim- \\nilar. Also, when fine-tuned using all the MoNuSeg training images, \\nthe performance increases slightly to the 0.658 AJI score and 0.892 F1 \\nscore. In terms of the Dice score our method also achieves the best per- \\n\\n26 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nTable 4: Performance benchmarking with weakly and fully supervised methods in \\nthe CryoNuSeg dataset. \\n \\nAJI \\nDice \\nPQ \\nWeakly Supervised \\nBoNuS [27] \\n0.431 \\n0.693 \\n0.399 \\nPartial Points [43] \\n0.410 \\n0.682 \\n0.357 \\nDAWN [67] \\n0.508 \\n0.804 \\n0.476 \\nPseudoedgenet [66] \\n0.321 \\n0.620 \\n0.306 \\nDoNuSeg [57] \\n0.441 \\n0.672 \\n0.306 \\nFully Supervised \\nU-Net [47] \\n0.469 \\n0.697 \\n0.403 \\nHoVer-Net [8] \\n0.526 \\n0.804 \\n0.495 \\nSwin-unet [3] \\n0.524 \\n0.849 \\n0.498 \\nCDNet [11] \\n0.539 \\n0.776 \\n0.499 \\nLG-NuSegHop (Baseline) \\n0.545 \\n0.703 \\n0.419 \\nLG-NuSegHop (Dom. Adapted) \\n0.567 \\n0.723 \\n0.479 \\n \\n \\n \\nformance among the weakly and self supervised methods by significant \\ndifference from the second leading performance (0.791 vs. 0.774). \\nIn the CryoNuSeg, our method surpasses all the weakly supervised \\nmethods by large margins, achieving an AJI of 0.545 (Table 4). Com- \\nparing with the Dice coefficient and PQ, only the recently proposed \\nDAWN [67] has a better performance. Besides, from the fully super- \\nvised category LG-NuSegHop achieves a higher AJI score from the \\nstate-of-the-art. Remarkably, even when it is not finetuned on the \\nCryoNuSeg data (i.e. baseline model), LG-NuSegHop achieves a com- \\npetitive performance in this dataset, where the acquisition process is \\nconsiderably different than the standard H&E staining process. When \\nfinetuned on the same domain, the AJI performance increases further \\nto 0.567. Overall, our method achieves a high AJI and PQ perfor- \\nmance comparing to the other supervised methods. Full supervision \\nin this dataset seems to boost more the segmentation performance, as \\none can see from the higher Dice score, comparing to the weakly or \\nunsupervised methods. \\nIn the third dataset under comparison, CoNSeP, all methods achieve \\na lower performance –compared to the other datasets–, since it has a \\nlarge intra-class variance, where nuclei have quite different textures. \\nHence, this dataset is challenging for most methods. LG-NuSeg achieves \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n27 \\n \\n \\nINTERNAL \\nTable 5: Performance benchmarking with weakly and fully supervised methods in \\nthe CoNSeP dataset. \\n \\nAJI \\nDice \\nPQ \\nWeakly Supervised \\nPseudoedgenet [66] \\n0.221 \\n0.331 \\n0.153 \\nBoNuS [27] \\n0.354 \\n0.651 \\n0.380 \\nPartial Points [43] \\n0.366 \\n0.646 \\n0.391 \\nPoint Annotations [52] \\n0.464 \\n0.749 \\n0.398 \\nDAWN [67] \\n0.509 \\n0.805 \\n0.477 \\nFully Supervised \\nU-Net [47] \\n0.499 \\n0.761 \\n0.434 \\nHoVer-Net [8] \\n0.513 \\n0.837 \\n0.492 \\nCDNet [11] \\n0.541 \\n0.835 \\n0.514 \\nMulvernet [55] \\n0.515 \\n0.833 \\n0.482 \\nLG-NuSegHop (Baseline) \\n0.422 \\n0.654 \\n0.407 \\nLG-NuSegHop (Dom. Adapted) \\n0.461 \\n0.691 \\n0.427 \\n \\n \\n \\nan AJI of 0.422 without any finetuning (Table 5). Comparing to the \\nweakly supervised methods it has a competitive performance across all \\nmetrics. Only the point annotations [52] and DAWN [67] methods per- \\nform better. Compared to the fully supervised works, the performance \\ngap is larger. Yet, if we finetune the hyperparameters on the training \\nimages, the AJI score increases significantly to 0.461, surpassing most \\nof the weakly supervised works and narrowing the gap with the fully \\nsupervised ones, such as U-Net. Considering all the metrics, in this \\ndataset fully supervised methods achieve a higher performance in both \\nsegmentation and detection metrics. Therefore, one can infer that when \\nthe nuclei appearance variance is higher, supervision is crucial for both \\ndetecting and delineating the nuclei cells. This can be also observed \\nin Fig. 6, where our method achieves a good segmentation result in \\nMoNuSeg and CryoNuSeg, while in CoNSeP, for certain nuclei types \\nit is more challenging to detect and segment them accurately without \\nsupervision or any domain adaptation. \\n \\n4.4 Ablation Study \\nAs LG-NuSegHop is a pipeline consisting multiple processing steps \\nand modules, we conduct an ablation study with different modules to \\n\\n28 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n \\nInput \\nGT \\n \\nU-Net \\nHoVer-Net \\n \\nLG-NuSegHop \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 6: Visual comparisons of LG-NuSegHop with HoVer-Net and U-Net on patch \\nexamples from MoNuSeg (first row), CryoNuSeg (second row) and ConSeP (third \\nrow) datasets. \\n \\n \\n \\ndemonstrate their efficacy and importance within the overall pipeline. \\nTable 6 shows a few comparisons between the PQR and L,AB color con- \\nversion as a pre-processing step, as well as the contribution of the local \\nprocessing modules. Table 7 at first compares a handcrafted feature \\nextraction approach against NuSegHop. In turn, we progressively add \\nthe global processing modules to test the performance improvement. \\nFirst of all, one can observe that our PQR pre-processing conversion \\nhelps both the detection and segmentation metrics. Also, the adaptive \\nthresholding improves significantly the AJI over the non-adaptive one. \\nMorphological post-processing is important to remove noisy instances \\nand split nuclei and it is reflected from the large improvement of the F1 \\nscore (see Table 6). LAIR module also provides a small improvement \\nin F1 score, by filtering some false positives in images, wherever it \\nis more likely to have a high false positive rate. On the other hand, \\nby replacing a handcrafted feature extraction in lieu of NuSegHop, \\nall metrics drop significantly. Moreover, local maxima detection on \\nNuSegHop’s heatmap improves mainly the detection performance by \\nrecalling areas that indicate nuclei existence. It is also evident that both \\nwatershed and self-supervised instance classification help to improve \\nmainly the detection aspect of the task, by reducing the false positive \\nrate and delineate the nuclei more precisely (see Table 7). \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n29 \\n \\n \\nINTERNAL \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nMorph. Refin. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n0.605 \\n0.763 \\n0.732 \\n0.611 \\n0.779 \\n0.738 \\n0.583 \\n0.745 \\n0.705 \\n0.595 \\n0.749 \\n0.721 \\n0.608 \\n0.774 \\n0.728 \\n \\n \\n \\n \\n \\n \\n \\n0.622 \\n0.813 \\n0.740 \\n0.641 \\n0.836 \\n0.763 \\n✓ \\n✓ \\n0.647 \\n0.875 \\n0.769 \\n✓ \\n✓ \\n✓ \\n0.651 \\n0.883 \\n0.772 \\n✓ \\n✓ \\n✓ \\n✓ \\n0.658 \\n0.892 \\n0.778 \\nTable 6: Ablation study on the MoNuSeg dataset with combinations of preprocessing \\nand local processing modules. \\n \\n \\nTable 7: Ablation study on the MoNuSeg dataset with different global processing \\nmodules. NuSegHop data-driven features are also compared with a set of hand- \\ncrafted features for nuclei segmentation. All the pre-processing and local processing \\noperations are kept to their best configuration. \\n \\n \\n \\n4.4.1 Qualitative Analysis \\nIn the last part of our analysis, two visualization comparisons are pro- \\nvided. In Fig. 7, one can observe how the adaptive thresholding can \\nhelp in segmenting local patches, where the bi-modal assumption is \\nnot very pronounced. The binarization performance is compared over \\na more conservative thresholding by choosing the intermediate point. \\nIt is demonstrated that the adjustment of the threshold about the in- \\ntermediate point between the histogram peaks, helps to reduce the \\nfalse positive areas upfront in LG-NuSegHop and thereby provides a \\nless noisy pseudolabel to NuSegHop. Moreover, in Fig. 8, we illustrate \\na few examples across the three datasets and compare the instance \\nself-supervised classification module operation in removing false pos- \\nitive instances as the last post-processing step of LG-NuSegHop. In \\nMoNuSeg, our method achieves better results and in turn we can see \\nthat the false positive and negative rate is lower. CryoNuSeg yields a \\nhigher rate of false positives, which is mitigated from the global pro- \\ncessing module. The ConSeP image has a higher false negative rate and \\nhence the instance classification module does not have much effect. \\n\\n30 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFigure 7: Illustrative examples of the adaptive filtering from the local processing \\nmodule. To is the intermediate point between the two peaks of the bi-modal distri- \\nbution and the Tˆ the adapted threshold about To. The input patch is shown after \\nthe staining normalization. \\n \\n \\n \\n4.5 Limitations & Discussion \\nFrom the quantitative and qualitative analysis, it is evident the impor- \\ntance and role of each individual module. In LG-NuSegHop pipeline \\nother modules (e.g. instance classification) are meant to increase the \\ndetection accuracy and other the segmentation (i.e. NuSegHop). Over- \\nall, our proposed methodology achieves a very competitive performance \\namong other self-supervised or weakly and fully supervised methods. \\nIt is worth noting that even without any finetuning on the target \\ndatasets, LG-NuSegHop outperforms most of the weakly supervised \\nmethods and can be compared to the state-of-the-art fully supervised \\nones. This seamless generalization ability provides a huge advantage of \\nour method, since there is no need to form a training dataset or have a \\npathologists to annotate data. It can be deployed in a plug-n-play man- \\nner for nuclei segmentation and achieve a competitive performance in \\ncertain datasets (e.g. CryoNuSeg) or to provide pseudolabel to another \\nself-supervised model. \\nAs already mentioned, LG-NuSegHop is a pipeline that relies on hu- \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n31 \\n \\n \\nINTERNAL \\nman’s prior knowledge in solving the problem and requires certain as- \\nsumptions to hold. From experimenting with the three diverse datasets, \\nwe can observe that our method shows a relatively lower performance \\non the CoNSeP dataset. This can be attributed to the fact that the \\nlocal similarity assumption is very weak in this dataset. The nuclei \\ninstances have a large distance in shape and color, thus challenging \\nfor the local processing module to predict a less noisy pseudolabel for \\nNuSegHop. In turn, it is also hard for the self-supervision to improve \\nsignificantly over the pseudolabel prediction, resulting in a higher false \\nnegative rate. \\nFrom the experiments, fully supervised methods on the CoNSep \\ndataset have a higher gap in performance both from the weakly su- \\npervised methods and the LG-NuSegHop one. Our method fits better \\ninto MoNuSeg and CryoNuSeg assumptions where the intra-class dis- \\ntance is relatively smaller and can yield a competitive performance even \\nwithout any finetuning. One higher level conclusion that can be drawn \\nfrom this comparison is that the nuclei segmentation problem on cer- \\ntain histology images can be solved solely by relying on clinical and \\nbiology prior knowledge using no or little supervision (Tables 3 and \\n4). Yet, when certain assumptions are not well met, full supervision \\nis needed to learn the nuclei variability and achieve a higher perfor- \\nmance (see Table 5). Another observation from the results and the \\ncomparison with different metrics is that our method is more effective \\non the detection aspect of the problem over the segmentation one, es- \\npecially on the CryoNuSeg. The main reason is the appearance of the \\nnuclei –faintly stained–, sourcing from the staining process. The lack \\nof explicit full supervision makes harder for our method to accurately \\nlearn the boundaries distinctions for this type of staining. Thus, our \\nmethod achieves a better Dice score, compared to the weakly super- \\nvised methods, but HoVer-Net and CDNet achieve better results due \\nto their fully supervised training, which is crucial for this dataset in \\nthe accurate delineation of nuclei. \\nLG-NuSegHop can offer a high nuclei segmentation performance \\nand generalize well with no specific domain adaptation. For example, \\nalthough our method does not require any training from MoNuSeg, \\nwe use 6 images for hyperparameter finetuning. So, LG-NuSegHop is \\ninherently adapted to this domain to a certain extend. Two recent \\nworks that have carried out domain adaptation from MoNuSeg (train) \\n\\n32 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nto CryoNuSeg (test), have achieved an AJI of 0.452 [23] and 0.484 [67]. \\nFor the same domain shift, LG-NuSegHop achieves an AJI of 0.545. \\nTherefore, one can infer that supervision may not always be in favor of \\nthe cross domain generalization. We contend that for this task, biology \\npriors and human insights play a pivotal role to mitigate the domain \\nadaptation requirement. \\nFrom a complexity standpoint, LG-NuSegHop uses simple image \\nprocessing operations pre and post the NuSegHop module with very \\nlow complexity. NuSegHop has a total number of parameters equal \\nto 40K for feature extraction. Notably, the local processing pipeline \\nand NuSegHop can be implemented in parallel to achieve a very short \\ninference time. On the other hand, DL state-of-the-art solutions require \\nseveral million of parameters and special equipment (e.g. large GPUs) \\nfor model deployment. For instance, as a reference point, HoVer-Net \\nrequires about 11.04 seconds to predict an image of size 1000 × 1000 \\nusing a GPU with 12GB memory [8]. On the same comparison, LG- \\nNuSegHop requires about 9.38 seconds on average to predict a histology \\nimage of the same size, adopting a multi-thread implementation and \\ndeployed on a Intel Xeon CPU E5-2620 v3 at 2.40GHz. \\nAs a final remark, it is also important to emphasize once more that \\nwithin our pipeline every module is intuitive and transparent, including \\nthe feature extraction process in NuSegHop module. We underline this \\nadvantage as it is crucial for medical image solutions to be explainable \\nto physicians, so that they can be effectively utilized in real clinical \\nsettings. \\n \\n \\n5 \\nConclusion \\n \\nThis work proposes the LG-NuSegHop pipeline for unsupervised nuclei \\nsegmentation from histology images. A novel feature extraction model \\nnamed NuSegHop is introduced to learn the local texture. Regarding \\nNuSegHop, several custom-made image processing modules are pro- \\nposed to preprocess the input image, provide a pseudo label, and post- \\nprocess the predicted heatmap to increase the nuclei detection rate. \\nKey advantages of our method are the generalization ability to unseen \\ndomains with inherent discrepancies and the small number of parame- \\nters. Every proposed module is intuitive and transparent, based on spe- \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n33 \\n \\n \\nINTERNAL \\ncific biological priors of the problem. In future work, we will investigate \\nways to focus NuSegHop feature extraction on the nuclei boundaries, \\naiming to improve its segmentation performance. Moreover, another \\nresearch direction is the incorporation of supervision into our current \\npipeline, in the challenging areas where the unsupervised approach has \\ncertain limitations, thus benefiting from the expert’s knowledge. \\n \\n \\nFigure 8: Visualization examples of the nuclei segmentation performance in three \\ndatasets. It is also compared the performance of the instance ROI classification \\nwithin the global processing module. The true positive areas are marked with white, \\nthe false positives with yellow and the false negatives with blue. Red boxes highlight \\nareas where the false positive removal is successful. \\n\\n34 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nReferences \\n \\n[1] Sabeena Beevi, Madhu S Nair, and GR Bindu, “Automatic seg- \\nmentation of cell nuclei using Krill Herd optimization based multi- \\nthresholding and localized active contour model”, Biocybernetics \\nand Biomedical Engineering, 36(4), 584–96. \\n[2] Hongmin Cai, Zhong Yang, Xinhua Cao, Weiming Xia, and Xi- \\naoyin Xu, “A new iterative triclass thresholding technique in im- \\nage segmentation”, IEEE transactions on image processing, 23(3), \\n1038–46. \\n[3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng \\nZhang, Qi Tian, and Manning Wang, “Swin-unet: Unet-like pure \\ntransformer for medical image segmentation”, in European con- \\nference on computer vision, Springer, 2022, 205–18. \\n[4] Tsai Hor Chan, Fernando Julio Cendra, Lan Ma, Guosheng Yin, \\nand Lequan Yu, “Histopathology whole slide image analysis with \\nheterogeneous graph representation learning”, in Proceedings of \\nthe IEEE/CVF Conference on Computer Vision and Pattern Recog- \\nnition, 2023, 15661–70. \\n[5] Yueru Chen, Mozhdeh Rouhsedaghat, Suya You, Raghuveer Rao, \\nand C-C Jay Kuo, “Pixelhop++: A small successive-subspace- \\nlearning-based (ssl-based) model for image classification”, in 2020 \\nIEEE International Conference on Image Processing (ICIP), IEEE, \\n2020, 3294–8. \\n[6] Joann G Elmore, Heidi D Nelson, Margaret S Pepe, Gary M \\nLongton, Anna NA Tosteson, Berta Geller, Tracy Onega, Patricia \\nA Carney, Sara L Jackson, Kimberly H Allison, et al., “Variability \\nin pathologists’ interpretations of individual breast biopsy slides: \\na population perspective”, Annals of internal \\nmedicine, 164(10), 649–55. \\n[7] Pegah Faridi, Habibollah Danyali, Mohammad Sadegh Helfroush, \\nand Mojgan Akbarzadeh Jahromi, “An automatic system for cell \\nnuclei pleomorphism segmentation in histopathological images of \\nbreast cancer”, in 2016 IEEE Signal Processing in Medicine and \\nBiology Symposium (SPMB), IEEE, 2016, 1–5. \\n[8] S. Graham, Q. D. Vu, S. E A. Raza, A. Azam, Y. W. Tsang, J. T. \\nKwak, and N. Rajpoot, “Hover-net: Simultaneous segmentation \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n35 \\n \\n \\nINTERNAL \\nand classification of nuclei in multi-tissue histology images”, Med- \\nical image analysis, 58, 101563. \\n[9] Simon Graham, Mostafa Jahanifar, Ayesha Azam, Mohammed \\nNimir, Yee-Wah Tsang, Katherine Dodd, Emily Hero, Harvir Sa- \\nhota, Atisha Tank, Ksenija Benes, et al., “Lizard: A large-scale \\ndataset for colonic nuclear instance segmentation and classifica- \\ntion”, in Proceedings of the IEEE/CVF international conference \\non computer vision, 2021, 684–93. \\n[10] Metin N Gurcan, Laura E Boucheron, Ali Can, Anant Madab- \\nhushi, Nasir M Rajpoot, and Bulent Yener, “Histopathological \\nimage analysis: A review”, IEEE reviews in biomedical engineer- \\ning, 2, 147–71. \\n[11] \\nHongliang He, Zhongyi Huang, Yao Ding, Guoli Song, Lin Wang, \\nQian Ren, Pengxu Wei, Zhiqiang Gao, and Jie Chen, “Cdnet: \\nCentripetal direction network for nuclear instance segmentation”, \\nin Proceedings of the IEEE/CVF International Conference on \\nComputer Vision, 2021, 4026–35. \\n[12] Hongliang He, Jun Wang, Pengxu Wei, Fan Xu, Xiangyang Ji, \\nChang Liu, and Jie Chen, “Toposeg: Topology-aware nuclear in- \\nstance segmentation”, in Proceedings of the IEEE/CVF Interna- \\ntional Conference on Computer Vision, 2023, 21307–16. \\n[13] Joy Hsu, Wah Chiu, and Serena Yeung, “Darcnn: Domain adap- \\ntive region-based convolutional neural network for unsupervised \\ninstance segmentation in biomedical images”, in Proceedings of \\nthe IEEE/CVF conference on computer vision and pattern recog- \\nnition, 2021, 1003–12. \\n[14] Iqra Kiran, Basit Raza, Areesha Ijaz, and Muazzam A Khan, \\n“DenseRes-Unet: Segmentation of overlapped/clustered nuclei from \\nmulti organ histopathology images”, Computers in Biology and \\nMedicine, 143, 105267. \\n[15] \\nAlexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, \\nand Piotr Dollár, “Panoptic segmentation”, in Proceedings of the \\nIEEE/CVF conference on computer vision and pattern recogni- \\ntion, 2019, 9404–13. \\n[16] Yousef Al-Kofahi, Wiem Lassoued, William Lee, and Badrinath \\nRoysam, “Improved automatic detection and segmentation of cell \\nnuclei in histopathology images”, IEEE Transactions on Biomed- \\nical Engineering, 57(4), 841–52. \\n\\n36 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n[17] \\nCan Fahrettin Koyuncu, Ece Akhan, Tulin Ersahin, Rengul Cetin- \\nAtalay, and Cigdem Gunduz-Demir, “Iterative h-minima-based \\nmarker-controlled watershed for cell nucleus segmentation”, Cy- \\ntometry Part A, 89(4), 338–49. \\n[18] Neeraj Kumar, Ruchika Verma, Deepak Anand, Yanning Zhou, \\nOmer Fahri Onder, Efstratios Tsougenis, Hao Chen, Pheng-Ann \\nHeng, Jiahui Li, Zhiqiang Hu, et al., “A multi-organ nucleus \\nsegmentation challenge”, IEEE transactions on medical imaging, \\n39(5), 1380–91. \\n[19] Neeraj Kumar, Ruchika Verma, Sanuj Sharma, Surabhi Bhar- \\ngava, Abhishek Vahadane, and Amit Sethi, “A dataset and a \\ntechnique for generalized nuclear segmentation for computational \\npathology”, IEEE transactions on medical imaging, 36(7), 1550– \\n60. \\n[20] C-C Jay Kuo and Azad M Madni, “Green learning: Introduction, \\nexamples and outlook”, Journal of Visual Communication and \\nImage Representation, 103685. \\n[21] C-C Jay Kuo, Min Zhang, Siyang Li, Jiali Duan, and Yueru Chen, \\n“Interpretable convolutional neural networks via feedforward de- \\nsign”, Journal of Visual Communication and Image Representa- \\ntion, 60, 346–59. \\n[22] Shyam Lal, Devikalyan Das, Kumar Alabhya, Anirudh Kanfade, \\nAman Kumar, and Jyoti Kini, “NucleiSegNet: Robust deep learn- \\ning architecture for the nuclei segmentation of liver cancer histopathol- \\nogy images”, Computers in Biology and Medicine, 128, 104075. \\n[23] Zhongyu Li, Chaoqun Li, Xiangde Luo, Yitian Zhou, Jihua Zhu, \\nCunbao Xu, Meng Yang, Yenan Wu, and Yifeng Chen, “Toward \\nsource-free cross tissues histopathological cell segmentation via \\ntarget-specific finetuning”, IEEE Transactions on Medical Imag- \\ning, 42(9), 2666–77. \\n[24] H. Liang, Zh. Cheng, H. Zhong, A. Qu, and L. Chen, “A region- \\nbased convolutional network for nuclei detection and segmen- \\ntation in microscopy images”, Biomedical Signal Processing and \\nControl, 71, 103276. \\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro \\nPerona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick, \\n“Microsoft coco: Common objects in context”, in Computer Vision– \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n37 \\n \\n \\nINTERNAL \\nECCV 2014: 13th European Conference, Zurich, Switzerland, Septem- \\nber 6-12, 2014, Proceedings, Part V 13, Springer, 2014, 740–55. \\n[26] Yi Lin, Zhiyong Qu, Hao Chen, Zhongke Gao, Yuexiang Li, Lili \\nXia, Kai Ma, Yefeng Zheng, and Kwang-Ting Cheng, “Nuclei \\nsegmentation with point annotations from pathology images via \\nself-supervised learning and co-training”, Medical Image Analysis, \\n89, 102933. \\n[27] Yi Lin, Zeyu Wang, Dong Zhang, Kwang-Ting Cheng, and Hao \\nChen, “BoNuS: Boundary Mining for Nuclei Segmentation with \\nPartial Point Labels”, IEEE Transactions on Medical Imaging. \\n[28] Dongnan Liu, Donghao Zhang, Yang Song, Fan Zhang, Lauren \\nO’Donnell, Heng Huang, Mei Chen, and Weidong Cai, “Unsuper- \\nvised instance segmentation in microscopy images via panoptic \\ndomain adaptation and task re-weighting”, in Proceedings of the \\nIEEE/CVF conference on computer vision and pattern recogni- \\ntion, 2020, 4243–52. \\n[29] Wenxi Liu, Qing Zhang, Qi Li, and Shu Wang, “Contrastive and \\nuncertainty-aware nuclei segmentation and classification”, Com- \\nputers in Biology and Medicine, 108667. \\n[30] Xiaofeng Liu, Fangxu Xing, Hanna K Gaggin, Weichung Wang, \\nC-C Jay Kuo, Georges El Fakhri, and Jonghye Woo, “Segmenta- \\ntion of cardiac structures via successive subspace learning with \\nsaab transform from cine mri”, in 2021 43rd Annual International \\nConference of the IEEE Engineering in Medicine & Biology So- \\nciety (EMBC), IEEE, 2021, 3535–8. \\n[31] Xiaofeng Liu, Fangxu Xing, Chao Yang, Chung-Chieh Jay Kuo, \\nSuma Babu, Georges El Fakhri, Thomas Jenkins, and Jonghye \\nWoo, “Voxelhop: Successive subspace learning for als disease clas- \\nsification using structural mri”, IEEE journal of biomedical and \\nhealth informatics, 26(3), 1128–39. \\n[32] Xiaoming Liu, Zhengsheng Guo, Jun Cao, and Jinshan Tang, \\n“MDC-net: A new convolutional neural network for nucleus seg- \\nmentation in histopathology images with distance maps and con- \\ntour information”, Computers in Biology and Medicine, 135, 104543. \\n[33] Tommy Löfstedt, Patrik Brynolfsson, Thomas Asklund, Tufve \\nNyholm, and Anders Garpebring, “Gray-level invariant Haralick \\ntexture features”, PloS one, 14(2), e0212110. \\n\\n38 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n[34] Wei Lou, Xiang Wan, Guanbin Li, Xiaoying Lou, Chenghang Li, \\nFeng Gao, and Haofeng Li, “Structure embedded nucleus classifi- \\ncation for histopathology images”, IEEE Transactions on Medical \\nImaging. \\n[35] Vasileios Magoulianitis, Catherine A Alexander, and C-C Jay \\nKuo, “A Comprehensive Overview of Computational Nuclei Seg- \\nmentation Methods in Digital Pathology”, arXiv preprint arXiv:2308.08112. \\n[36] Vasileios Magoulianitis, Peida Han, Yijing Yang, and C-C Jay \\nKuo, “An Unsupervised Parameter-Free Nuclei Segmentation Method \\nfor Histology Images”, in 2022 IEEE International Conference on \\nImage Processing (ICIP), IEEE, 2022, 226–30. \\n[37] Vasileios Magoulianitis, Jiaxin Yang, Yijing Yang, Jintang Xue, \\nMasatomo Kaneko, Giovanni Cacciamani, Andre Abreu, Vinay \\nDuddalwar, C-C Jay Kuo, Inderbir S Gill, et al., “PCa-RadHop: \\nA transparent and lightweight feed-forward method for clinically \\nsignificant prostate cancer segmentation”, Computerized Medical \\nImaging and Graphics, 102408. \\n[38] Vasileios Magoulianitis, Yijing Yang, and C-C Jay Kuo, “HUNIS: \\nHigh-Performance Unsupervised Nuclei Instance Segmentation”, \\nin 2022 IEEE 14th Image, Video, and Multidimensional Signal \\nProcessing Workshop (IVMSP), IEEE, 2022, 1–5. \\n[39] Amirreza Mahbod, Gerald Schaefer, Benjamin Bancher, Chris- \\ntine Löw, Georg Dorffner, Rupert Ecker, and Isabella Ellinger, \\n“CryoNuSeg: A dataset for nuclei instance segmentation of cryosec- \\ntioned H&E-stained histological images”, Computers in biology \\nand medicine, 132, 104349. \\n[40] Esha Sadia Nasir, Arshi Parvaiz, and Muhammad Moazam Fraz, \\n“Nuclei and glands instance segmentation in histology images: a \\nnarrative review”, Artificial Intelligence Review, 56(8), 7909–64. \\n[41] Hady Ahmady Phoulady, Dmitry B Goldgof, Lawrence O Hall, \\nand Peter R Mouton, “Nucleus segmentation in histology im- \\nages with hierarchical multilevel thresholding”, in Medical Imag- \\ning 2016: Digital Pathology, Vol. 9791, SPIE, 2016, 280–5. \\n[42] Markus Plass, Michaela Kargl, Tim-Rasmus Kiehl, Peter Regit- \\nnig, Christian Geißler, Theodore Evans, Norman Zerbe, Rita Car- \\nvalho, Andreas Holzinger, and Heimo Müller, “Explainability and \\ncausability in digital pathology”, The Journal of Pathology: Clin- \\nical Research, 9(4), 251–60. \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n39 \\n \\n \\nINTERNAL \\n[43] H. Qu, P. Wu, Q. Huang, J. Yi, G. M Riedlinger, S. De, and \\nD. N Metaxas, “Weakly supervised deep nuclei segmentation us- \\ning points annotation in histopathology images”, in International \\nConference on Medical Imaging with Deep Learning, PMLR, 2019, \\n390–400. \\n[44] H. Qu, P. Wu, Q. Huang, J. Yi, Z. Yan, K. Li, G. M Riedlinger, S. \\nDe, S. Zhang, and D. N Metaxas, “Weakly supervised deep nuclei \\nsegmentation using partial points annotation in histopathology \\nimages”, IEEE transactions on medical imaging, 39(11), 3655– \\n66. \\n[45] H. Qu, J. Yi, Q. Huang, Pengxiang Wu, and D. Metaxas, “Nuclei \\nsegmentation using mixed points and masks selected from uncer- \\ntainty”, in 2020 IEEE 17th International Symposium on Biomed- \\nical Imaging (ISBI), IEEE, 2020, 973–6. \\n[46] Jos BTM Roerdink and Arnold Meijster, “The watershed trans- \\nform: Definitions, algorithms and parallelization strategies”, Fun- \\ndamenta informaticae, 41(1-2), 187–228. \\n[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, “U-net: \\nConvolutional networks for biomedical image segmentation”, in \\nMedical Image Computing and Computer-Assisted Intervention– \\nMICCAI 2015: 18th International Conference, Munich, Germany, \\nOctober 5-9, 2015, Proceedings, Part III 18, Springer, 2015, 234– \\n41. \\n[48] Kaushiki Roy, Subhadeep Saha, Debapriya Banik, and Debotosh \\nBhattacharjee, “Nuclei-Net: A multi-stage fusion model for nuclei \\nsegmentation in microscopy images”. \\n[49] Mihir Sahasrabudhe, Stergios Christodoulidis, Roberto Salgado, \\nStefan Michiels, Sherene Loi, Fabrice André, Nikos Paragios, and \\nMaria Vakalopoulou, “Self-supervised nuclei segmentation in histopatho- \\nlogical images using attention”, in Medical Image Computing and \\nComputer Assisted Intervention–MICCAI 2020: 23rd Interna- \\ntional Conference, Lima, Peru, October 4–8, 2020, Proceedings, \\nPart V 23, Springer, 2020, 393–402. \\n[50] Massimo Salvi, Nicola Michielli, and Filippo Molinari, “Stain \\nColor Adaptive Normalization (SCAN) algorithm: Separation and \\nstandardization of histological stains in digital pathology”, Com- \\nputer methods and programs in biomedicine, 193, 105506. \\n\\n40 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\n[51] \\nZhiyun Song, Penghui Du, Junpeng Yan, Kailu Li, Jianzhong \\nShou, Maode Lai, Yubo Fan, and Yan Xu, “Nucleus-aware self- \\nsupervised pretraining using unpaired image-to-image translation \\nfor histopathology images”, IEEE Transactions on Medical Imag- \\ning. \\n[52] Kuan Tian, Jun Zhang, Haocheng Shen, Kezhou Yan, Pei Dong, \\nJianhua Yao, Shannon Che, Pifu Luo, and Xiao Han, “Weakly- \\nsupervised nucleus segmentation based on point annotations: A \\ncoarse-to-fine self-stimulated learning strategy”, in Medical Image \\nComputing and Computer Assisted Intervention–MICCAI 2020: \\n23rd International Conference, Lima, Peru, October 4–8, 2020, \\nProceedings, Part V 23, Springer, 2020, 299–308. \\n[53] Katarzyna Tomczak, Patrycja Czerwińska, and Maciej Wiznerow- \\nicz, “Review The Cancer Genome Atlas (TCGA): an immeasur- \\nable source of knowledge”, Contemporary Oncology/Współczesna \\nOnkologia, 2015(1), 68–77. \\n[54] Thaína A Azevedo Tosta, Paulo Rogério de Faria, Leandro Alves \\nNeves, and Marcelo Zanchetta do Nascimento, “Computational \\nnormalization of H&E-stained histological images: Progress, chal- \\nlenges and future potential”, Artificial intelligence in medicine, \\n95, 118–32. \\n[55] Vi Thi-Tuong Vo and Soo-Hyung Kim, “Mulvernet: nucleus seg- \\nmentation and classification of pathology images using the HoVer- \\nNet and multiple filter units”, Electronics, 12(2), 355. \\n[56] Huadeng Wang, Guang Xu, Xipeng Pan, Zhenbing Liu, Rushi \\nLan, and Xiaonan Luo, “Multi-task generative adversarial learn- \\ning for nuclei segmentation with dual attention and recurrent con- \\nvolution”, Biomedical Signal Processing and Control, 75, 103558. \\n[57] Ziyue Wang, Ye Zhang, Yifeng Wang, Linghan Cai, and Yongbing \\nZhang, “Dynamic Pseudo Label Optimization in Point-Supervised \\nNuclei Segmentation”, arXiv preprint arXiv:2406.16427. \\n[58] K. Y. Win, S. Choomchuay, and K. Hamamoto, “K mean clus- \\ntering based automated segmentation of overlapping cell nuclei \\nin pleural effusion cytology images”, in 2017 International Con- \\nference on Advanced Technologies for Communications (ATC), \\nIEEE, 2017, 265–9. \\n[59] Khin Yadanar Win and Somsak Choomchuay, “Automated seg- \\nmentation of cell nuclei in cytology pleural fluid images using \\n\\nLG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance \\nSegmentation \\n41 \\n \\n \\nINTERNAL \\nOTSU thresholding”, in 2017 International Conference on Digi- \\ntal Arts, Media and Technology (ICDAMT), IEEE, 2017, 14–8. \\n[60] Lipeng Xie, Jin Qi, Lili Pan, and Samad Wali, “Integrating deep \\nconvolutional neural networks with marker-controlled watershed \\nfor overlapping nuclei segmentation in histopathology images”, \\nNeurocomputing, 376, 166–79. \\n[61] Xinpeng Xie, Jiawei Chen, Yuexiang Li, Linlin Shen, Kai Ma, \\nand Yefeng Zheng, “Instance-aware self-supervised learning for \\nnuclei segmentation”, in Medical Image Computing and Computer \\nAssisted Intervention–MICCAI 2020: 23rd International Confer- \\nence, Lima, Peru, October 4–8, 2020, Proceedings, Part V 23, \\nSpringer, 2020, 341–50. \\n[62] Fuyong Xing, Yuanpu Xie, and Lin Yang, “An automatic learning- \\nbased framework for robust nucleus segmentation”, IEEE trans- \\nactions on medical imaging, 35(2), 550–66. \\n[63] Jiaxin Yang, Vasileios Magoulianitis, Catherine Aurelia Christie \\nAlexander, Jintang Xue, Masatomo Kaneko, Giovanni Caccia- \\nmani, Andre Abreu, Vinay Duddalwar, C-C Jay Kuo, Inderbir \\nS Gill, et al., “GUSL: A Novel and Efficient Machine Learn- \\ning Model for Prostate Segmentation on MRI”, arXiv preprint \\narXiv:2506.23688. \\n[64] Yijing Yang, Vasileios Magoulianitis, and C-C Jay Kuo, “E-pixelhop: \\nAn enhanced pixelhop method for object classification”, in 2021 \\nAsia-Pacific Signal and Information Processing Association An- \\nnual Summit and Conference (APSIPA ASC), IEEE, 2021, 1475– \\n82. \\n[65] Yijing Yang, Wei Wang, Hongyu Fu, C-C Jay Kuo, et al., “On su- \\npervised feature selection from high dimensional feature spaces”, \\nAPSIPA Transactions on Signal and Information Processing, 11(1). \\n[66] Inwan Yoo, Donggeun Yoo, and Kyunghyun Paeng, “Pseudoed- \\ngenet: Nuclei segmentation only with point annotations”, in Med- \\nical Image Computing and Computer Assisted Intervention– \\nMICCAI 2019: 22nd International Conference, Shenzhen, China, \\nOctober 13–17, 2019, Proceedings, Part I 22, Springer, 2019, 731– \\n9. \\n[67] Ye Zhang, Yifeng Wang, Zijie Fang, Hao Bian, Linghan Cai, \\nZiyue Wang, and Yongbing Zhang, “DAWN: Domain-Adaptive \\n\\n42 \\nVasileios Magoulianitis et al. \\n \\n \\nINTERNAL \\n \\nWeakly Supervised Nuclei Segmentation via Cross-Task Interac- \\ntions”, arXiv preprint arXiv:2404.14956. \\n[68] Xin Zheng, Yong Wang, Guoyou Wang, and Jianguo Liu, “Fast \\nand robust segmentation of white blood cell images by self-supervised \\nlearning”, Micron, 107, 55–71. \\n[69] Yang Zhou, Yongjian Wu, Zihua Wang, Bingzheng Wei, Maode \\nLai, Jianzhong Shou, Yubo Fan, and Yan Xu, “Cyclic learning: \\nBridging image-level labels and nuclei instance segmentation”, \\nIEEE Transactions on Medical Imaging, 42(10), 3104–16. \\n[70] Yanning Zhou, Omer Fahri Onder, Qi Dou, Efstratios Tsouge- \\nnis, Hao Chen, and Pheng-Ann Heng, “Cia-net: Robust nuclei \\ninstance segmentation with contour-aware information aggrega- \\ntion”, in Information Processing in Medical Imaging: 26th Inter- \\nnational Conference, IPMI 2019, Hong Kong, China, June 2–7, \\n2019, Proceedings 26, Springer, 2019, 682–93. \\n[71] \\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros, \\n“Unpaired image-to-image translation using cycle-consistent ad- \\nversarial networks”, in Proceedings of the IEEE international con- \\nference on computer vision, 2017, 2223–32. \\n',\n",
       " 'SurgiATM: A Physics-Guided Plug-and-Play\\nModel for Deep Learning-Based Smoke\\nRemoval in Laparoscopic Surgery\\nMingyu Sheng, Jianan Fan, Dongnan Liu, Guoyan Zheng, Ron Kikinis, Weidong Cai\\nAbstract— During laparoscopic surgery, smoke gener-\\nated by tissue cauterization can significantly degrade the\\nvisual quality of endoscopic frames, increasing the risk\\nof surgical errors and hindering both clinical decision-\\nmaking and computer-assisted visual analysis. Conse-\\nquently, removing surgical smoke is critical to ensuring\\npatient safety and maintaining operative efficiency. In this\\nstudy, we propose the Surgical Atmospheric Model (Sur-\\ngiATM) for surgical smoke removal. SurgiATM statistically\\nbridges a physics-based atmospheric model and data-\\ndriven deep learning models, combining the superior gen-\\neralizability of the former with the high accuracy of the\\nlatter. Furthermore, SurgiATM is designed as a lightweight,\\nplug-and-play module that can be seamlessly integrated\\ninto diverse surgical desmoking architectures to enhance\\ntheir accuracy and stability, better meeting clinical require-\\nments. It introduces only two hyperparameters and no\\nadditional trainable weights, preserving the original net-\\nwork architecture with minimal computational and modi-\\nfication overhead. We conduct extensive experiments on\\nthree public surgical datasets with ten desmoking meth-\\nods, involving multiple network architectures and cover-\\ning diverse procedures, including cholecystectomy, par-\\ntial nephrectomy, and diaphragm dissection. The results\\ndemonstrate that incorporating SurgiATM commonly re-\\nduces the restoration errors of existing models and rela-\\ntively enhances their generalizability, without adding any\\ntrainable layers or weights. This highlights the conve-\\nnience, low cost, effectiveness, and generalizability of the\\nproposed method. The code for SurgiATM is released at\\nhttps://github.com/MingyuShengSMY/SurgiATM.\\nIndex Terms— Endoscopy, laparoscopic surgery, smoke\\nremoval, plug-and-play, parameter-efficient\\nI. INTRODUCTION\\nM\\nINIMALLY invasive surgery (MIS) has noticeably ben-\\nefited patients by reducing risk and enabling faster\\nrecovery [1], [2]. However, surgical smoke, commonly gen-\\nerated by high-energy instruments (e.g., electrosurgical units,\\nlaser surgical tools, and high-frequency ultrasonic scalpels),\\nMingyu Sheng, Jianan Fan, Dongnan Liu, and Weidong Cai are\\nwith the School of Computer Science, the University of Sydney, Aus-\\ntralia (e-mail: {mshe0136, jfan6480}@uni.sydney.edu.au, {dongnan.liu,\\ntom.cai}@sydney.edu.au).\\nGuoyan\\nZheng\\nis\\nwith\\nthe\\nSchool\\nof\\nBiomedical\\nEngi-\\nneering,\\nthe\\nShanghai\\nJiao\\nTong\\nUniversity,\\nChina\\n(e-mail:\\nguoyan.zheng@sjtu.edu.cn).\\nRon Kikinis is with the Department of Radiology, Harvard Medical\\nSchool, USA (e-mail: kikinis@bwh.harvard.edu).\\nWeidong Cai is the corresponding author.\\ncan severely impair endoscopic visibility, posing a notable\\nchallenge. The presence of dense smoke obstructs the sur-\\ngeon’s field of view, increases the risk of surgical errors,\\nand reduces procedural efficiency. Moreover, smoke-induced\\nimage degradation adversely affects vision-based downstream\\nsurgical tasks, such as phase recognition [3]–[8], tool detec-\\ntion, semantic segmentation [9]–[11], depth estimation [12]–\\n[14], 3D surface reconstruction [15], [16], etc., which require\\nboth high frame quality and real-time processing speed [17]–\\n[21] . At present, mechanical smoke evacuation or filtration is\\ncommonly achieved through hardware-based systems, which\\noften require additional effort and impose extra workload\\nduring surgery, disrupting the surgical workflow and causing\\nsurgeon fatigue [22], [23]. Therefore, ensuring a clear visual\\nfield in real time is critical [24]–[26].\\nTo this end, real-time image processing has gained increas-\\ning attention as a means of digitally removing smoke during\\nintraoperative visualization in a more streamlined and auto-\\nmated way by leveraging computer vision and deep learning\\ntechniques. It offers a cost-effective, low-risk, and efficient\\nalternative for surgical smoke removal [27]–[31]. However,\\ntraditional models often struggle with this task, as they were\\noriginally designed for outdoor hazy environments. Unlike nat-\\nural hazy images, in vivo surgical scenes present unique chal-\\nlenges, including complex physics parameter estimation due\\nto non-uniform distribution of smoke, spatially heterogeneous\\nillumination caused by radial light attenuation, inaccurate color\\nrestoration on non-smoke objects (e.g., surgical instruments),\\nsevere information loss resulting from highly dense smoke ob-\\nscuration, and temporal incoherence of smoke occurrence due\\nto the intermittent use of energy devices [32]–[37]. To address\\nthese limitations, deep learning-based approaches have been\\nincreasingly adopted for their powerful non-linear mapping\\ncapabilities and ease of deployment in laparoscopic systems\\n[38], [39]. These methods show considerable promise for\\nsurgical desmoking, offering more consistent color restoration\\nand improved visual clarity. Nevertheless, due to the scarcity\\nof paired desmoking benchmarks, most deep learning-based\\nmodels face the challenge of generalizability, as they can\\nproduce high-quality predictions on a dataset with synthetic\\nsmoke while suffering from performance degradation on an-\\nother dataset with real smoke. This is an important concern\\nfor real-world clinical applications [40].\\nTo overcome these limitations, we propose the Surgical\\narXiv:2511.05059v1  [cs.CV]  7 Nov 2025\\n\\nAtmospheric Model (SurgiATM), a physics-guided model\\ntailored to the laparoscopic desmoking task. SurgiATM can\\nbe seamlessly embedded into existing neural networks with\\nminimal modifications to their original architectures, while\\nenhancing both performance and generalizability. Notably,\\nSurgiATM introduces no additional trainable parameters,\\nindicating that the observed improvements arise from the\\nintegration of physical guidance rather than merely increasing\\nmodel trainable weights and complexity. Specifically, the main\\ncontributions of this study are:\\n1) We propose SurgiATM, a parameter-efficient, plug-and-\\nplay, physics-guided model, that improves performance\\nand generalizability of surgical desmoking approaches\\nto better meet clinical requirements.\\n2) The proposed model introduces zero trainable weights\\nand can be seamlessly integrated into state-of-the-art\\n(SOTA) methods built upon various backbone architec-\\ntures (e.g., U-Net, Swin Transformer).\\n3) We validate SurgiATM on three public surgical datasets,\\ndemonstrating improvements in both performance and\\ngeneralizability across multiple SOTA desmoking meth-\\nods and various surgical scenes, thereby confirming its\\neffectiveness, stability, and generalizability.\\nII. RELATED WORK\\nA. Physics-Based Methods\\nThe Atmospheric Scattering Model (ASM) is a widely used\\nphysical model that describes the formation of hazy natural\\nimages by modeling the absorption, scattering, and reflection\\nof light by objects and particles in the atmospheric medium\\n[41], [42]. Based on the ASM, clear images can be restored\\nfrom degraded observations by estimating the required phys-\\nical parameters, such as medium transmission, atmospheric\\nlight, and scene depth, which has motivated extensive research\\nin the field. In early studies, the Gaussian distribution and\\nMarkov Random Field (MRF) model were classic and well-\\nknown techniques for estimating the parameters [43], [44].\\nThen, [45] proposed the Dark Channel Prior (DCP) model for\\nnatural image dehazing, in which the medium transmission and\\nglobal atmospheric light were statistically estimated according\\nto their physical properties. The DCP assumes that haze in an\\nimage is uniformly distributed and that, in hazeless images,\\nat least one color channel in most non-sky patches or pixels\\nhas very low intensity. In contrast, haze tends to raise pixel\\nintensities across all channels, causing the dark channel to\\nbecome brighter and the image to appear more whitish. Since\\n2012, the field of surgical desmoking has gained increasing\\nattention, with many valuable studies published over the past\\ndecade. [46] developed a digital defogging system that uses\\nthe DCP to enhance image quality in laparoscopic surgery.\\n[47] combined the desmoking and denoising tasks of laparo-\\nscopic images and transformed them into a Bayesian inference\\nproblem by proposing a novel probabilistic graphical model.\\n[48] extends [47] by introducing additional image priors of\\ncolor and texture derived from sparse dictionary models. In\\n[49], histogram equalization is combined with an adaptive\\nDCP to remove surgical smoke. [27] rearranged the original\\natmospheric model by introducing an atmospheric veil to\\nrepresent the transmission map and then solved the parameter\\nestimation problem via the Poisson equation in the frequency\\ndomain. [50] first developed an energy function to optimize\\nand estimate a smoke veil to recover smokeless images based\\non [37]. Combining the hardware camera and the software\\nalgorithm, [51] proposed a method based on multiple-exposure\\nfusion, which enhances local contrast and color saturation,\\nthereby improving image quality. These works offer valuable\\ninsights into the feasibility of physics-based approaches for\\nsurgical desmoking, encouraging further research.\\nB. Deep Learning-Based Methods\\nTo achieve higher restoration quality, the powerful fitting\\nand generative capabilities of Deep Neural Networks (DNNs)\\noffer an alternative solution [52]–[57]. Current deep learning-\\nbased desmoking methods can be broadly categorized into two\\nframeworks: non-adversarial learning and adversarial learning.\\n1) Non-adversarial Learning Methods: This framework indi-\\ncates that the model is tasked with learning a mapping from\\nsmoky or hazy images to their corresponding clean versions.\\n[58] trained a U-Net model [59] on laparoscopic videos with\\nsynthetic smoke using Blender1. [56] attempted to address\\nsurgical smoke removal via AODNet [53], a Convolutional\\nNeural Network (CNN) designed to estimate global light\\nilluminance and the transmission map simultaneously. Similar\\nto [58], [56] trained the model on synthetically generated\\nsmoky in vivo frames by blending Perlin noise with clean\\nframes. Furthermore, instead of training on synthetic smoky\\nframes, [60] attempted to use adjacent frames, along with\\nthe presence or absence of smoke, as training inputs and\\ncorresponding ground truth. Similarly, in the most recent study\\n[40], they implemented a real-world self-supervised video\\ndesmoking approach by capturing the informative features\\nfrom frames recorded prior to the activation of high-energy\\nsurgical instruments. Based on the above training data strategy,\\nconsiderable efforts have been made to improve performance\\nfurther. [61] designed a U-Net-like network and proposed\\nLaplacian image pyramid decomposition to capture multi-scale\\nfeatures. [28] presented a novel generative-collaborative learn-\\ning framework (De-smokeGCN), dividing the desmoking into\\ntwo sub-tasks: smoke detection and smoke removal. A smoke\\ndetector predicted the smoke mask, which was leveraged as\\na prior for the smoke removal network and motivated several\\nsubsequent studies [62], [63]. [64] improved the decoder part\\nof the U-Net by appending a convolutional block attention\\nmodule to generate a guidance mask for feature maps. [65]\\nintroduced a workflow to simultaneously remove smoke and\\nrestore colors, using U-Net as the backbone. [66] leveraged\\ndeformable convolution incorporated with a mutual attention\\nmechanism to model temporal features and combined Local\\nBinary Patterns with input frames as a texture prior. [67] devel-\\noped a method consisting of three U-Nets to produce distinct\\noutputs, each serving as an element in the energy function\\nderived from their previous study [50]. [30] developed a sur-\\ngical desmoking method based on the Diffusion Model [68],\\n1https://www.blender.org/\\n\\n[69], incorporating a multi-level frequency analysis module to\\nintegrate features across different frequency bands. In addition\\nto convolutional network architectures, the Swin Transformer\\n(SwinT) [70] has recently gained increasing attention in this\\nfield because of its hierarchical representation and efficient\\nlocal-to-global attention mechanism. [71] proposed a network\\nthat utilized several convolutional layers to extract low-level\\nfeatures and then used SwinT to analyze deep and global\\ninformation from smoky frames. [31] developed a U-shaped\\ntransformer model based on U-Net and SwinT to enhance\\nfeature representation by combining local detail extraction\\nwith global context modeling.\\n2) Adversarial Learning Methods: Apart from conventional\\nimage-to-image translation, converting laparoscopic images\\nfrom the smoky domain to the smokeless domain offers an\\nalternative paradigm for addressing the surgical desmoking\\nproblem. By regarding desmoking as an image style transfer\\ntask, the Generative Adversarial Network (GAN) [72] and its\\nvariants have emerged as effective solutions. [57] modeled\\na mapping between smoky and smoke-free domains using\\na conventional GAN as the backbone and leveraging the\\nperceptual image quality score as a loss function to improve\\nthe result quality. [29] developed a method based on a Con-\\nditional Generative Adversarial Network (cGAN) [73] and\\nused an embedded dark channel as a prior. Compared to\\nthe above GANs, the Cycle-Consistent Adversarial Network\\n(CycleGAN) [74] is well-suited for image style transfer tasks\\nand is therefore more commonly used for surgical smoke\\nremoval [75]–[77]. [78] enhanced CycleGAN by incorporating\\nmulti-scale feature extraction and introducing an upsampling\\nloss to improve the contrast of the desmoked frames. In their\\nsubsequent studies, they further included structure-consistency\\nloss and designed a refinement module to improve restoration\\nquality [79], [80]. [81] further designed internal-channel and\\ndark-channel loss functions based on the characteristics of\\nsmoky pixels to improve model performance. Similar to [28],\\n[63] added an extra detection network to the CycleGAN archi-\\ntecture to estimate the smoke mask as a prior for subsequent\\ndesmoking. [62] adopted a multilevel strategy to adaptively\\nlearn non-homogeneous smoke features, using the predicted\\nsmoke distribution as a prior.\\nTo date, existing studies have made significant contribu-\\ntions; however, some limitations remain, as the feasibility and\\npracticality of physics-based methods are primarily restricted\\nby accuracy, while those of deep learning-based methods are\\nmainly hindered by robustness and generalizability. In this\\npaper, we conduct a statistical analysis of their respective\\nadvantages and subsequently derive a plug-and-play module\\nto augment the existing end-to-end deep learning desmoking\\nmethods, further enhancing their performance and generaliz-\\nability across different benchmarks.\\nIII. METHODS\\nThe proposed method is motivated by an interesting obser-\\nvation in laparoscopic surgical video smoke removal: predic-\\ntions from most desmoking methods yield a Laplacian-like\\nerror distribution (see Fig. 1). Building upon this finding, we\\nintroduce a new restoration formula that minimizes prediction\\nerrors and enhances model robustness by blending physics-\\nbased and deep learning-based approaches from the perspec-\\ntive of probability distributions.\\nA. Background\\n1) Physics-Based Modeling: The atmospheric scattering\\nmodel [41]–[44] is typically formulated as:\\nI(x, c) = J(x, c) · t(x) + A(c) · (1 −t(x)),\\n(1)\\nwhere c denotes a color channel; x represents the spatial\\nlocation of a pixel; I ∈RH×W ×C is the observed image,\\nwhere C, W, and H are the number of color channels, width\\nand height, respectively; J indicates the corresponding haze-\\nfree image; A ∈R1×1×C denotes the global atmospheric light;\\nand t ∈RH×W ×1 is the medium transmission, which can\\nalternatively be expressed as t = 1−S with smoke density S.\\nIn particular, t and J can be further formulated as:\\nt(x) = e−β(λ)·d(x),\\n(2)\\nJ(x, c) = A(c) · ρ(x, c),\\n(3)\\nwhere β(λ) ∈R is the scattering coefficient for a specific\\nwavelength λ of light; d ∈RH×W ×1 represents the scene\\ndepth; and ρ ∈RH×W ×C is the normalized radiance [42] or\\nreflectance [44] of a scene point.\\n[45] proposed the Dark Channel Prior (DCP), which\\nestimates the transmission map t using the dark channel:\\nt(x) = 1 −D(x)\\n= 1 −min\\nc∈C\\n\\x12\\nmin\\nu∈Ω(x;z)\\n\\x12I(u, c)\\nA(c)\\n\\x13\\x13\\n,\\n(4)\\nwhere D denotes the dark channel and Ω(x) represents a\\nsquare window of size z centered at location x. Accordingly,\\nthe restoration formula is given by:\\nJDCP(x, c) = I(x, c) −A(c)\\n1 −D(x)\\n+ A(c),\\n(5)\\nwhere JDCP indicates that the image J is estimated using\\nDCP; 1 −D(x) is empirically clipped by a lower bound t0 in\\npractical implementations to prevent division by zero, while\\nthis application-based adjustment is omitted for simplicity and\\nclarity of the subsequent mathematical derivation.\\n2) Deep Learning-Based Modeling: Clearly, in pixels where\\nthe smoke density is high and white or gray surgical instru-\\nments are present (i.e., D →1), the conventional physics-\\nbased model (5) may fail. Moreover, radial light attenuation\\ncan violate the assumption that A ∈R1×1×C; under such\\nconditions, A is more appropriately represented as A ∈\\nRH×W ×C. Adhering to the original assumption may intro-\\nduce more errors, whereas adopting the latter formulation\\nsignificantly increases the degrees of freedom of the solution\\nspace. Therefore, Deep Neural Network (DNN), a powerful\\ntechnique for non-linear mapping, is widely employed for\\n\\nSurgical Smoky Video\\nNatural Hazy Image\\n0.00\\n0.01\\n0.01\\n0.02\\n0.02\\n0.03\\n0.03\\n0.04\\n0.04\\n-255 -204 -153 -102\\n-51\\n0\\n51\\n102\\n153\\n204\\n255\\nNatural\\nSurgical\\nLaplacian-Like\\nGaussian-Like\\nError Distribution\\nFig. 1.\\nThe left images are from a natural dehazing dataset, O-HAZE [82]; the middle group is from a real-world surgical desmoking benchmark\\nVASST-desmoke [23]; and the line chart (right) shows the difference in error distributions between surgical desmoking and natural dehazing,\\ncomputed from the entire dataset. The 1st row displays the hazy or smoky images; the 2nd row shows the smoke or haze mask estimated from the\\nground truth; and the 3rd row presents the error magnitude map of the DCP prediction.\\nsmoke removal. Its formulation and training objective are\\ntypically expressed as:\\nJDNN(x, c) =Φθ\\n\\x10\\nI(x, c), ˜Z\\n\\x11\\n,\\n(6)\\nθ∗= arg min\\nθ\\nL(J, JDNN; X, θ),\\n(7)\\nwhere Φ indicates a non-linear mapping parameterized by\\ntrainable weights θ; θ∗represents the optimized weights\\nobtained through training on the dataset X; ˜Z and L denote\\npriors and loss function, respectively, as determined by the\\nspecific method design. Nevertheless, its performance and\\ngeneralizability may be limited when deployed in unseen\\nsurgical scenarios.\\nB. Mixture-of-Experts and Optimization\\nInspired by the Mixture-of-Experts (MoE), an effective\\nmechanism for complementary advantages [83], [84], we begin\\nwith an ideal gating model:\\nJMoE(x, c) = ˜W(x) · JDCP(x, c)\\n+ (1 −˜W(x)) · JDNN(x, c),\\n(8)\\nwhere JMoE is the restored result of the gating model, and\\n˜W ∈{0, 1}H×W ×1 is a pixel-wise indicator specifying which\\nmethod is activated for smoke removal. Specifically, ˜W(x) =\\n1 if JDCP(x, c) is deemed more reliable than JDNN(x, c); oth-\\nerwise, ˜W(x) = 0. Optimizing ˜W facilitates the most accurate\\nprediction. For simplification and tractability, ˜W is relaxed to\\na continuous domain, yielding W ∈[0, 1]H×W ×1, and the\\nrestorations are formulated using additive error models:\\nJDCP(x, c) = J(x, c) + εDCP(x, c),\\n(9)\\nJDNN(x, c) = J(x, c) + εDNN(x, c),\\n(10)\\nwhere the predicted image is represented as the ground\\ntruth augmented by an independent and identically distributed\\n(i.i.d.) error term ε ∈RH×W ×C. The error term is commonly\\nassumed to follow a zero-mean Gaussian distribution with\\nvariance σ2, denoted as ε(x, c)\\ni.i.d.\\n∼\\nN(µ = 0, σ2), which\\nis a classic assumption in statistical learning. Then, we have:\\nJMoE(x, c) = J(x, c) + εMoE(x, c),\\n(11)\\nεMoE(x, c) = W(x) · εDCP(x, c)\\n+ (1 −W(x)) · εDNN(x, c).\\n(12)\\nTABLE I\\nCOMPARISON OF PREDICTION ERROR DISTRIBUTIONS WITH GAUSSIAN\\nAND LAPLACIAN DISTRIBUTIONS VIA JENSEN-SHANNON (JS)\\nDIVERGENCE IN VASST-DESMOKE WITH REAL-WORLD SMOKE AND\\nCHOLEC80 WITH SYNTHETIC SMOKE\\nMethod\\nVASST-desmoke\\nCholec80\\nGaussian\\nLaplacian\\nGaussian\\nLaplacian\\nDCP [45]\\n0.044989\\n0.013664\\n0.002782\\n0.010886\\nAODNet [53]\\n0.077348\\n0.056151\\n0.006405\\n0.012263\\nCGAN-DC [29]\\n0.033480\\n0.033894\\n0.007939\\n0.005278\\nDe-smokeGCN [28]\\n0.027138\\n0.001988\\n0.007484\\n0.005802\\nGCANet [85]\\n0.009448\\n0.003621\\n0.003164\\n0.008474\\nLGUTransformer [31]\\n0.010483\\n0.003129\\n0.016931\\n0.003623\\nMARS-GAN [62]\\n0.024441\\n0.064570\\n0.009878\\n0.005369\\nMSBDN [54]\\n0.020062\\n0.007690\\n0.005995\\n0.005536\\nRSTN [71]\\n0.026959\\n0.006900\\n0.012523\\n0.004193\\nSSIM-PAN [57]\\n0.018315\\n0.011999\\n0.005900\\n0.006517\\nSVPNet [67]\\n0.022240\\n0.000675\\n0.007876\\n0.006492\\nAvg.\\n0.028628\\n0.013795\\n0.008734\\n0.006734\\nClearly, optimizing JMoE is equivalent to minimizing the error\\nterm εMoE:\\nmin\\nW (x) E[ε2\\nMoE],\\ns.t. 0 ≤W(x) ≤1.\\n(13)\\nInterestingly, in the context of laparoscopic surgical video\\nsmoke removal, we observe that, for real-world surgical\\ndesmoking, the error terms do not follow a Gaussian-like\\ndistribution; rather, they follow a Laplacian-like distribu-\\ntion, as shown in Fig. 1 and reported in Table I, where\\nmost methods yield lower Jenson-Shannon (JS) divergence to\\nLaplacian distribution. We attribute this phenomenon to the\\nnon-uniform and temporally incoherent nature of real-world\\nsurgical smoke, which, compared to natural haze, results in\\nmore zero-error regions in smokeless areas and frames, with\\nnotable errors at smoky pixels. Based on this finding, we first\\nmodel the error terms in surgical desmoking as zero-mean\\nLaplacian, that is, ε\\ni.i.d.\\n∼Laplace (µ = 0, b), where µ and b are\\nthe location and scale parameters, respectively. Furthermore,\\nour analysis indicates that µ and b are potentially correlated\\nwith the dark channel D (see Fig. 2). Hence, in the proposed\\napproach, dark channel D is incorporated as a conditioning\\nfactor for the non-zero-mean Laplacian distribution:\\nεM(x, c)\\ni.i.d.\\n∼Laplace (µM(D(x)), bM(D(x))) ,\\n(14)\\n\\nFig. 2.\\nThe Laplacian parameters (i.e., µ and b) estimated from the specific dataset, and indicator W ∗calculated with (19), are presented across\\ndifferent methods with the Confidence Interval (CI). The upper and lower rows correspond to Cholec80 and VASTT-desmoke, respectively.\\nwhere M represents a method, either DCP or DNN. Collecting\\n(12), and (14), we have:\\nVar[εMoE] = 2 ·\\n\\x00W 2(x) · b2\\nDCP(D(x))\\n+ (1 −W(x))2 · b2\\nDNN(D(x))\\n\\x01\\n,\\n(15)\\nE[εMoE] = W(x) · µDCP(D(x))\\n+ (1 −W(x)) · µDNN(D(x)).\\n(16)\\nThen, the original optimization problem (13) is rewritten as:\\nmin\\nW (x) Var(εMoE) + (E(εMoE))2 ,\\n(17)\\ns.t. 0 ≤W(x) ≤1.\\n(18)\\nWe solve (17) and obtain the unconstrained solution:\\nW ∗(x; D) =\\n2b2\\n2,x −µ2,x · (µ1,x −µ2,x)\\n2b2\\n1,x + 2b2\\n2,x + (µ1,x −µ2,x)2 ,\\n(19)\\nwhere W ∗(x; D) is the optimized W(x) conditioned on the\\ndark channel D without constraints. For clarity, bDCP(D(x))\\nand µDNN(D(x)) are written as b1,x and µ2,x, respectively.\\nSubsequently, W ∗(x) is clipped to the interval [0, 1] to satisfy\\nthe constraint (18).\\nC. A New Restoration Formula\\nFollowing (19), Fig. 2 presents the trends of W ∗given\\nD, based on statistics across all methods in two datasets:\\nCholec80 [86] with synthetic smoke, and the benchmark\\nVASST-desmoke [23], [87] containing real-world smoke. Deep\\nlearning models display varied trends, while a consistent neg-\\native correlation between W ∗and D is observed. Therefore,\\nin this work, W is estimated by:\\nˆW(x) = 1 −D(x),\\n(20)\\nwhere\\nˆW represents the approximated W given D. The\\nmotivations are as follows:\\n1) Developing a general plug-and-play mechanism for the\\nexisting DNN methods is one of the objectives of this\\nstudy, and this function captures the overall correlation\\nbetween W and D;\\n2) Equation (20) is intuitive: a high intensity of dark chan-\\nnel (i.e., D(x) →1) statistically indicates dense smoke\\n[23], [45], which challenges DCP due to information\\nloss and division by zero or very small values, thereby\\nmaking DNN the dominant model (i.e., W(x) →0);\\n3) It encourages consistent predictions in smokeless regions\\n(i.e., D(x) →0);\\n4) The denominator 1 −D and the unknown parameter\\nA in (5) are eliminated in the subsequent derivation,\\nstabilizing the computation, simplifying the modeling,\\nand precluding hard value clipping.\\nBy combining (5), (8), and (20), we obtain:\\nJMoE(x, c) = I(x, c) −A(c) · D(x)\\n+ D(x) · JDNN(x, c),\\n(21)\\nwhere A remains an unknown parameter that requires an\\nadditional model for its estimation. To address this issue, we\\nfirst use (3) to further decompose JDNN:\\nJMoE(x, c) = I(x, c) −A(c) · D(x)\\n+ D(x) · A(c) · ρDNN(x, c),\\n(22)\\nwhere ρDNN(x, c) = JDNN(x, c)/A(c) represents the re-\\nflectance calculated from the normalized DNN prediction.\\nEquation (22) can be reformulated as:\\nJMoE(x, c) = I(x, c) −D(x) · A(c) · (1 −ρDNN(x, c))\\n= I(x, c) −D(x) · (1 −ρDNN(x, c)),\\n(23)\\nwhere D denotes the denormalized dark channel D. Recalling\\n(4), D is defined as:\\nD(x) = D(x) · A(c) = min\\nc∈C\\n\\x12\\nmin\\nu∈Ω(x;z) (I(u, c))\\n\\x13\\n,\\n(24)\\nwhere A is eliminated, thereby avoiding the challenging es-\\ntimation of global light illuminance in laparoscopic surgical\\nscenes. In this case, the original DNN modeling in (6) and (7)\\ncan be improved as:\\nρDNN(x, c) =σ\\n\\x10\\nΦθ\\n\\x10\\nI(x, c), ˜Z\\n\\x11\\x11\\n,\\n(25)\\nθ∗= arg min\\nθ\\nL (J, (I −D · (1 −ρDNN)); X, θ) ,\\n(26)\\n\\nwhere σ is a differentiable function (typically a sigmoid) that\\nmaps the DNN output to the interval [0, 1]. It can be ignored\\nif the output is already constrained within this range, depend-\\ning on the specific model design. The proposed formulation\\nintroduces only minimal modifications to the original DNN\\nmodel Φθ, without adding any additional trainable parameters.\\nNotably, SurgiATM leverages the denormalized dark channel\\nas a key element, in line with recent methods [28], [62], [85],\\nsuggesting that the dark channel prior plays a useful role in\\nsurgical desmoking [23].\\nD. Gradient Computation and Refinement\\nIn this section, we compute the gradient with respect to the\\nmodel prediction ϱDNN for two commonly used loss functions:\\nMean Absolute Error (MAE or L1) loss and Mean Squared\\nError (MSE or L2) loss. Their gradients are denoted as ∇ϱL1\\nand ∇ϱL2, respectively:\\n∇ϱL1 =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n−D,\\nJ > 1 −D · (1 −ρ);\\nD,\\nJ < 1 −D · (1 −ρ);\\n0,\\nJ = 1 −D · (1 −ρ),\\n(27)\\n∇ϱL2 = −D · (J −(1 −D · (1 −ρ))),\\n(28)\\nwhere, for simplicity, ρDNN is written as ρ. A potential issue\\narises when D(x) = 0, as this results in a zero gradient,\\npreventing the DNN model from updating its weights at those\\npixels. To address this, we refine D as:\\nˆD = D + η\\n1 + η ,\\nη ∈[0, +∞),\\n(29)\\nwhere η is a smoothing factor; η = 0 corresponds to no\\nrefinement; and η →+∞implies ˆD →1, causing (26)\\nto degenerate into conventional residual prediction. Ablation\\nresults for η and z are reported in Section IV-E.\\nIV. EXPERIMENTS AND RESULTS\\nA. Implementation Details\\nBy default, we set the smoothing factor η = 0.1 and the\\nwindow size z = 15 [45]. All experiments are conducted on a\\n24GB NVIDIA RTX 3090 GPU. To ensure fair comparisons,\\ntraining configurations are largely standardized across all\\nmethods, employing the Adam optimizer with a learning rate\\nof 0.0002 and epochs of 50. Due to GPU memory constraints,\\nthe batch size depends on their respective publications.\\nB. Datasets and Metric\\n1) Datasets:\\nIn this study, we utilize three datasets:\\nCholec80 [86], VASST-desmoke [23], [87], and the Hamlyn\\nCentre Laparoscopic and Endoscopic Dataset Videos (referred\\nto as the Hamlyn Dataset for simplicity) [88], [89]. The\\nHamlyn Dataset is specifically reserved for external evaluation.\\nFor Cholec80, 500 smokeless and 500 smoky frames are\\nrandomly sampled from each video [90], with the smoke-free\\nframes then blended with synthetic smoke [56] for training.\\nVASST-desmoke is a recently released paired benchmark\\nfor laparoscopic surgical smoke removal. It provides both\\nreal smoky frames and corresponding ground-truth smoke-\\nfree annotations, thereby enabling evaluation on real surgi-\\ncal smoke with intuitive full-reference metrics. Potentially,\\nVASST-desmoke also allows models to learn from real smoke.\\nThe Hamlyn Dataset is excluded from all training and sta-\\ntistical analysis procedures in this work, and its frames are\\nsampled from the videos at 25 frames per second (FPS). We\\nperform five-fold cross-evaluation, ensuring no video overlap\\nbetween the training and evaluation sets. The three datasets\\nrecord various surgical procedures with inconsistent camera\\ndevices and lighting environments in different hospitals, signif-\\nicantly challenging the stability and generalizability of surgical\\ndesmoking methods.\\n2) Metrics: For the evaluation of paired datasets, we lever-\\nage full-reference metrics: CIEDE2000, PSNR, RMSE, and\\nSSIM, to evaluate perceptual color difference, reconstruction\\nquality, prediction error, and structure similarity, respectively.\\nFor the unpaired datasets, we evaluate model performance\\nusing non-reference metrics, including BRISQUE and NIQE\\nto measure how close an image is to natural scenes, and FADE\\n[91], which specifically measures fog-related degeneration.\\nC. Comparison of Quantitative Results\\nWe train five folds of ten existing methods and their corre-\\nsponding SurgiATM modified versions, using Cholec80 with\\nsynthetic smoke and VASST-desmoke with real-world smoke,\\nrespectively. We then compare their five-fold average perfor-\\nmance on the real-world surgical smoke datasets: VASST-\\ndesmoke, Cholec80 (real smoke), and the Hamlyn Dataset.\\nIn total, we train 200 models and conduct 500 evaluations.\\nThe quantitative results on VASST-desmoke are presented\\nin Tables II and III. They demonstrate that incorporating\\nSurgiATM effectively improves model performance, regardless\\nof the backbone architecture or loss function. However, for\\nSVPNet in Table III, SurgiATM yields no improvement in\\nSSIM. We argue that, compared to other desmoking methods,\\nSVPNet [67] employs an energy function [50] for restoration.\\nThis is a physics-guided approach similar to ours, and its three\\nphysics-related parameters are predicted by three independent\\nU-Nets. As a result, incorporating an extra physics-guided\\ncomponent (our SurgiATM) into SVPNet may slightly disrupt\\nthe internal physical consistency, leading to the observed\\ndeterioration in SSIM.\\nFor Cholec80, we report the evaluation results in Table IV.\\nOur SurgiATM effectively improves most model performances\\nin terms of the fog-related metric (i.e., FADE score). In\\nterms of the external evaluation on the Hamlyn Dataset, as\\nshown in Tables VI and V, for the defogging metric FADE,\\nemploying our SurgiATM enhances most methods, except\\nfor the traditional dehazing method AODNet in Table VI.\\nWe attribute this to two factors: 1) the simple architecture\\nand insufficient trainable weights of AODNet (i.e., only five\\nconvolutional layers) compared to other methods, and 2) the\\nnon-Laplacian error distribution of AODNet when trained\\non Cholec80 with synthetic smoke, as reported in Table I.\\nIn contrast, although GCANet and SSIM-PAN also exhibit\\nnon-Laplacian error distributions in Cholec80, and CGAN-\\nDC and MARS-GAN in VASST-desmoke, their performances\\n\\nTABLE II\\nTRAINING AND EVALUATION ON VASST-DESMOKE (BASELINE / BASELINE + SURGIATM)\\nMethod\\nYear\\nBackbone\\nLoss Function\\nCIEDE2000 ↓\\nPSNR ↑\\nRMSE ↓\\nSSIM ↑\\nAODNet [53]\\n2017\\nCNN\\nMSE\\n10.349 / 6.327\\n17.378 / 21.994\\n0.147 / 0.090\\n0.693 / 0.789\\nGCANet [85]\\n2019\\nCNN\\nMSE\\n7.893 / 5.369\\n21.091 / 23.907\\n0.095 / 0.073\\n0.729 / 0.806\\nDe-smokeGCN [28]\\n2020\\nU-Net\\nMAE + Others\\n9.577 / 7.348\\n19.779 / 22.127\\n0.115 / 0.094\\n0.550 / 0.717\\nMSBDN [54]\\n2020\\nU-Net\\nMSE\\n6.679 / 5.164\\n21.604 / 23.881\\n0.088 / 0.073\\n0.651 / 0.813\\nSVPNet [67]\\n2024\\nU-Net\\nMSE + Others\\n5.574 / 5.141\\n23.464 / 24.362\\n0.073 / 0.070\\n0.789 / 0.820\\nCGAN-DC [85]\\n2019\\ncGAN\\nAdversarial + Others\\n6.735 / 5.812\\n22.453 / 23.809\\n0.083 / 0.079\\n0.676 / 0.782\\nMARS-GAN [62]\\n2023\\nCycleGAN\\nAdversarial + Others\\n6.389 / 5.899\\n20.121 / 23.488\\n0.113 / 0.076\\n0.602 / 0.819\\nSSIM-PAN [57]\\n2020\\nGAN\\nAdversarial + Others\\n7.046 / 5.811\\n21.465 / 23.477\\n0.090 / 0.075\\n0.655 / 0.750\\nRSTN [71]\\n2023\\nSwinT\\nCharbonnier + Others\\n5.385 / 4.976\\n23.263 / 24.306\\n0.074 / 0.071\\n0.750 / 0.824\\nLGUTransformer [31]\\n2024\\nSwinT + U-Net\\nCharbonnier\\n9.927 / 5.990\\n18.238 / 23.376\\n0.121 / 0.080\\n0.510 / 0.761\\nTABLE III\\nTRAINING ON CHOLEC80 WITH SYNTHETIC SMOKE AND EVALUATION\\nON VASST-DESMOKE\\nMethod\\nCIEDE2000 ↓\\nPSNR ↑\\nRMSE ↓\\nSSIM ↑\\nAODNet [53]\\n9.609 / 7.774\\n18.621 / 21.526\\n0.132 / 0.109\\n0.729 / 0.798\\nGCANet [85]\\n9.803 / 7.042\\n19.338 / 22.220\\n0.119 / 0.097\\n0.747 / 0.801\\nDe-smokeGCN [28]\\n8.370 / 7.585\\n20.654 / 22.088\\n0.109 / 0.102\\n0.716 / 0.788\\nMSBDN [54]\\n8.277 / 7.327\\n20.360 / 22.153\\n0.114 / 0.103\\n0.769 / 0.810\\nSVPNet [67]\\n8.105 / 7.797\\n21.196 / 21.712\\n0.110 / 0.109\\n0.806 / 0.793\\nCGAN-DC [85]\\n8.177 / 7.525\\n20.663 / 21.988\\n0.113 / 0.105\\n0.770 / 0.805\\nMARS-GAN [62]\\n7.808 / 7.444\\n21.444 / 22.110\\n0.107 / 0.104\\n0.792 / 0.811\\nSSIM-PAN [57]\\n8.350 / 7.600\\n20.338 / 21.805\\n0.115 / 0.106\\n0.761 / 0.806\\nRSTN [71]\\n7.963 / 7.533\\n21.034 / 22.065\\n0.111 / 0.107\\n0.763 / 0.813\\nLGUTransformer [31]\\n7.357 / 7.352\\n22.108 / 22.111\\n0.105 / 0.104\\n0.812 / 0.818\\nTABLE IV\\nTRAINING WITH SYNTHETIC SMOKE AND EVALUATION WITH REAL\\nSMOKE ON CHOLEC80\\nMethod\\nBRISQUE ↓\\nFADE ↓\\nNIQE ↓\\nAODNet [53]\\n12.551 / 14.814\\n0.440 / 0.484\\n10.449 / 9.434\\nGCANet [85]\\n7.795 / 11.566\\n0.407 / 0.403\\n9.866 / 9.331\\nDe-smokeGCN [28]\\n4.857 / 5.575\\n0.384 / 0.376\\n10.885 / 9.569\\nMSBDN [54]\\n11.838 / 12.176\\n0.421 / 0.411\\n10.299 / 9.026\\nSVPNet [67]\\n12.812 / 12.134\\n0.416 / 0.402\\n10.579 / 9.387\\nCGAN-DC [85]\\n10.258 / 9.945\\n0.413 / 0.404\\n9.517 / 9.039\\nMARS-GAN [62]\\n13.999 / 11.680\\n0.416 / 0.413\\n13.336 / 13.884\\nSSIM-PAN [57]\\n10.539 / 11.093\\n0.411 / 0.403\\n9.811 / 11.999\\nRSTN [71]\\n13.868 / 12.652\\n0.420 / 0.414\\n12.245 / 16.910\\nLGUTransformer [31]\\n9.126 / 11.236\\n0.417 / 0.416\\n13.734 / 12.471\\nare effectively enhanced owing to both their well-designed\\narchitectures and the strong generalizability of SurgiATM.\\nOn the other hand, SurgiATM demonstrates limited effec-\\ntiveness in terms of BRISQUE and NIQE across the ten\\nexisting methods in Tables IV, V, and VI. The two possible\\nreasons are: 1) these two metrics are designed to measure\\nsimilarity to natural images, which may not generalize well\\nto surgical endoscopic frames due to the substantial domain\\ngap between them [31]; and 2) the Laplacian-guided error\\nminimization of SurgiATM may contribute to dissimilarity\\nfrom natural images (Gaussian-like error).\\nD. Comparison of Qualitative Results\\nIn this section, we present qualitative comparisons across\\nseveral representative baselines: De-smokeGCN, GCANet,\\nMARS-GAN, and SVPNet, each selected for different rea-\\nsons. Specifically, De-smokeGCN [28] is a classic DNN-\\nbased desmoking method introduced in 2020; GCANet [85]\\nis designed for natural dehazing and deraining; MARS-GAN\\nTABLE V\\nTRAINING ON VASST-DESMOKE AND EVALUATION ON HAMLYN\\nDATASET\\nMethod\\nBRISQUE ↓\\nFADE ↓\\nNIQE ↓\\nAODNet [53]\\n36.549 / 30.515\\n0.331 / 0.322\\n12.418 / 11.851\\nGCANet [85]\\n13.354 / 26.555\\n0.324 / 0.294\\n11.512 / 10.983\\nDe-smokeGCN [28]\\n33.236 / 28.007\\n0.329 / 0.325\\n29.448 / 18.802\\nMSBDN [54]\\n26.992 / 25.198\\n0.384 / 0.322\\n14.224 / 10.944\\nSVPNet [67]\\n39.862 / 30.325\\n0.401 / 0.317\\n16.245 / 13.306\\nCGAN-DC [85]\\n14.052 / 16.699\\n0.352 / 0.336\\n16.616 / 13.216\\nMARS-GAN [62]\\n33.722 / 30.601\\n0.377 / 0.369\\n15.011 / 16.381\\nSSIM-PAN [57]\\n11.936 / 16.047\\n0.357 / 0.298\\n16.613 / 20.297\\nRSTN [71]\\n35.629 / 30.232\\n0.412 / 0.330\\n16.442 / 12.645\\nLGUTransformer [31]\\n35.260 / 29.706\\n0.295 / 0.294\\n15.300 / 12.135\\nTABLE VI\\nTRAINING ON CHOLEC80 WITH SYNTHETIC SMOKE AND EVALUATION\\nON HAMLYN DATASET\\nMethod\\nBRISQUE ↓\\nFADE ↓\\nNIQE ↓\\nAODNet [53]\\n30.292 / 31.168\\n0.426 / 0.432\\n11.074 / 11.834\\nGCANet [85]\\n19.366 / 26.102\\n0.385 / 0.293\\n11.628 / 11.083\\nDe-smokeGCN [28]\\n13.920 / 17.647\\n0.330 / 0.315\\n11.628 / 11.467\\nMSBDN [54]\\n26.630 / 27.559\\n0.409 / 0.332\\n12.152 / 11.882\\nSVPNet [67]\\n29.065 / 28.375\\n0.406 / 0.267\\n14.358 / 12.292\\nCGAN-DC [85]\\n25.175 / 24.127\\n0.408 / 0.315\\n10.445 / 10.741\\nMARS-GAN [62]\\n29.811 / 27.287\\n0.409 / 0.311\\n16.443 / 15.136\\nSSIM-PAN [57]\\n24.605 / 26.001\\n0.395 / 0.312\\n10.852 / 19.337\\nRSTN [71]\\n26.305 / 28.347\\n0.414 / 0.328\\n13.875 / 14.217\\nLGUTransformer [31]\\n23.974 / 26.497\\n0.383 / 0.346\\n16.786 / 17.448\\nrepresents an adversarial-learning-based approach; and SVP-\\nNet [67], published in 2024, uses a physics-based loss function\\ntailored for surgical desmoking. We train the models on\\nCholec80 with synthetic smoke and evaluate them on VASST-\\ndesmoke, Cholec80, and the Hamlyn Dataset with real surgical\\nsmoke, challenging their stability and generalizability.\\nAs shown in the error maps in Fig. 3, SurgiATM effectively\\nreduces desmoking errors in a computationally efficient and\\ninterpretable manner, making it well-suited for deployment\\non edge devices and for stable surgical desmoking. For the\\nnatural dehazing method GCANet, it causes color distoration\\nin Fig. 3(a), which is corrected to a certain degree by using\\nSurgiATM. In addition, the proposed approach can mitigate the\\ngrid artifact (see the first two rows in Fig. 3), demonstrating\\npromising stability. As shown in Fig. 4, the desmoking results\\nof the baselines are improved to varying degrees. For instance,\\nDe-smokeGCN and GCANet can produce more stable results\\nwith SurgiATM by reducing the grid artifacts (see Fig. 4(a)-(d)\\n\\nFig. 3.\\nError comparison in VASST-desmoke. “+” indicates the base-\\nline integrated with our SurgiATM. ”GT” represents ground truth. The\\nmethods are trained in Cholec80 with synthetic smoke. Orange boxes\\nmark the errors declined by SurgiATM. Overall, “Baseline + SurgiATM”\\nexhibits more accurate restoration.\\nof De-smokeGCN) and correcting the restoration colors (see\\nFig. 4(d) of GCANet). Moreover, SurgiATM helps baselines\\neliminate more smoke, as shown in Fig. 4(a) for MARS-GAN\\nand Fig. 4(c) for SVPNet, GCANet, and MARS-GAN.\\nE. Ablation Study\\nWe conduct ablation studies to investigate the influence\\nof two hyperparameters (i.e., smoothing factor η and shift-\\ning window size z) on model performance. Experiments are\\ncarried out on the real-world benchmark VASST-desmoke\\nusing three representative baselines (i.e., AODNet, RSTN, and\\nSSIM-PAN), chosen for their minimal reliance on auxiliary\\nmodules and loss functions, thereby enabling a clear assess-\\nment of the impact of the only two hyperparameters, smoother\\nη and window size z, with minimal confounding factors. The\\nRMSE metrics are averaged from five-fold cross-validation\\nand reported in Fig. 5. We observed that for most methods,\\nη = 0.1 usually leads to better performance. No obvious trend\\nis observed for the window size z: a larger window size is\\npreferred for AODNet and SSIM-PAN, while a smaller one is\\nbetter for RSTN. Therefore, for the use of SurgiATM, η = 0.1\\nis a recommended configuration, whereas the optimal window\\nsize should be determined for each specific method.\\nV. CONCLUSION AND FUTURE WORK\\nIn this work, we propose SurgiATM for laparoscopic surgi-\\ncal smoke removal, which can be seamlessly incorporated into\\nexisting DNN-based desmoking methods without additional\\ntrainable parameters. By using SurgiATM, the performance\\nand generalizability of these methods are enhanced, enabling\\nthe generation of cleaner frames for both surgeons and down-\\nstream tasks. We analyze the discrepancy between natural\\ndehazing and surgical desmoking and design SurgiATM by\\nfirst formulating a Mixture-of-Experts model. We then conduct\\na comprehensive statistical analysis of existing desmoking\\nmethods and derive a new restoration formula tailored for\\nFig. 4.\\nComparison of SurgiATM with baselines on real datasets:\\nCholec80 (left two columns) and the Hamlyn Dataset (right two\\ncolumns). Blue boxes highlight the comparatively accurate and stable\\nrestoration achieved by SurgiATM.\\nWindow \\nSize\\nAODNet\\nRSTN\\nSSIM-PAN\\nRMSE\\nSmoother \\nFig. 5.\\nEach heatmap illustrates the ablation study for a specific\\nbaseline model incorporated with SurgiATM, showing the impact of\\ndifferent hyperparameter settings in terms of the RMSE metric (lower\\nis better). Each heatmap uses an independent color scale.\\nsurgical smoke removal. Extensive experiments across three\\ndistinct datasets validate the effectiveness of SurgiATM.\\nNevertheless, several limitations accompany its advantages.\\nFirst, the indicator W is currently estimated as a linear func-\\ntion of the dark channel D, which captures the general trends\\nacross methods, and thus leaves some room for improvement\\nwhen focusing on a particular method; this could poten-\\ntially be addressed by deriving method-specific formulations.\\nSecond, the denormalized dark channel obtained from DCP\\nremains relatively coarse-grained. Developing an appropriate\\nrefinement strategy may further improve its performance and\\nresult quality; for example, using guided image filtering or\\nattention mechanisms could generate a comparatively fine-\\ngrained denormalized dark channel.\\n\\nREFERENCES\\n[1] L. Maier-Hein et al., “Surgical data science for next-generation inter-\\nventions,” Nature Biomedical Engineering, vol. 1, no. 9, pp. 691–696,\\n2017.\\n[2] L. Maier-Hein et al., “Surgical data science – from concepts toward\\nclinical translation,” Medical Image Analysis, vol. 76, p. 102306, 2022.\\n[3] S. Yang, L. Luo, Q. Wang, and H. Chen, “Surgformer: Surgical trans-\\nformer with hierarchical temporal attention for surgical phase recogni-\\ntion,” in Medical Image Computing and Computer Assisted Intervention\\n– MICCAI, 2024, pp. 606–616.\\n[4] X. Zou, D. Yu, and G. Zheng, “Capturing action triplet correlations for\\naccurate surgical activity recognition,” Computerized Medical Imaging\\nand Graphics, vol. 124, p. 102604, 2025.\\n[5] M. Wagner et al., “Comparative validation of machine learning al-\\ngorithms for surgical workflow and skill analysis with the heichole\\nbenchmark,” Medical Image Analysis, vol. 86, p. 102770, 2023.\\n[6] W. Yue, H. Liao, Y. Xia, V. Lam, J. Luo, and Z. Wang, “Cascade\\nmulti-level transformer network for surgical workflow analysis,” IEEE\\nTransactions on Medical Imaging, vol. 42, no. 10, pp. 2817–2831, 2023.\\n[7] J. Guan, X. Zou, R. Tao, and G. Zheng, “Label-guided teacher for\\nsurgical phase recognition via knowledge distillation,” in Medical Image\\nComputing and Computer Assisted Intervention – MICCAI, 2024, pp.\\n349–358.\\n[8] J. Zhang, M. Xu, Y. Wang, and Q. Dou, “Csap-assist: Instrument-agent\\ndialogue empowered vision-language models for collaborative surgical\\naction planning,” in Medical Image Computing and Computer Assisted\\nIntervention – MICCAI, 2025, pp. 139–148.\\n[9] M. Sheng, J. Fan, D. Liu, R. Kikinis, and W. Cai, “Amncutter: Affinity-\\nattention-guided multi-view normalized cutter for unsupervised surgical\\ninstrument segmentation,” in 2025 IEEE/CVF Winter Conference on\\nApplications of Computer Vision (WACV), 2025, pp. 4533–4544.\\n[10] M. Sheng, J. Fan, D. Liu, R. Kikinis, and W. Cai, “Revisiting surgical in-\\nstrument segmentation without human intervention: a graph partitioning\\nview,” in Proceedings of the 1st International Workshop on Multimedia\\nComputing for Health and Medicine, 2024, p. 16–25.\\n[11] W. Yue, J. Zhang, K. Hu, Y. Xia, J. Luo, and Z. Wang, “Surgicalsam:\\nEfficient class promptable surgical instrument segmentation,” Proceed-\\nings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 7,\\npp. 6890–6898, 2024.\\n[12] X. Zou et al., “Ssifnet: Spatial–temporal stereo information fusion\\nnetwork for self-supervised surgical video inpainting,” Computerized\\nMedical Imaging and Graphics, vol. 125, p. 102622, 2025.\\n[13] R. Tao, B. Huang, X. Zou, and G. Zheng, “Svt-sde: Spatiotemporal vi-\\nsion transformers-based self-supervised depth estimation in stereoscopic\\nsurgical videos,” IEEE Transactions on Medical Robotics and Bionics,\\nvol. 5, no. 1, pp. 42–53, 2023.\\n[14] X. Cheng, Y. Zhong, M. Harandi, T. Drummond, Z. Wang, and Z. Ge,\\n“Deep laparoscopic stereo matching with transformers,” in Medical\\nImage Computing and Computer Assisted Intervention – MICCAI, 2022,\\npp. 464–474.\\n[15] S. Gong et al., “Self-supervised cyclic diffeomorphic mapping for soft\\ntissue deformation recovery in robotic surgery scenes,” IEEE Transac-\\ntions on Medical Imaging, vol. 43, no. 12, pp. 4356–4367, 2024.\\n[16] L. Maier-Hein et al., “Comparative validation of single-shot optical tech-\\nniques for laparoscopic 3-d surface reconstruction,” IEEE Transactions\\non Medical Imaging, vol. 33, no. 10, pp. 1913–1930, 2014.\\n[17] Z. Han, J. Zhou, J. Pei, J. Qin, Y. Fan, and Q. Dou, “Towards reliable\\nar-guided surgical navigation: Interactive deformation modeling with\\ndata-driven biomechanics and prompts,” IEEE Transactions on Medical\\nImaging, pp. 1–1, 2025.\\n[18] Y. Long et al., “Surgical embodied intelligence for generalized task\\nautonomy in laparoscopic robot-assisted surgery,” Science Robotics,\\nvol. 10, no. 104, p. eadt3093, 2025.\\n[19] H. Wang et al., “Learning dissection trajectories from expert surgical\\nvideos via imitation learning with equivariant diffusion,” Medical Image\\nAnalysis, vol. 103, p. 103599, 2025.\\n[20] M. Xu, Z. Huang, J. Zhang, X. Zhang, and Q. Dou, “Surgical action\\nplanning with large language models,” in Medical Image Computing and\\nComputer Assisted Intervention – MICCAI, 2026, pp. 563–572.\\n[21] Q. Dou, K. Nyangoh-Timoh, P. Jannin, and Y. Shen, “Artificial intel-\\nligence in gynecology surgery: Current status, challenges, and future\\nopportunities,” Chinese Medical Journal, vol. 138, no. 6, 2025.\\n[22] T. G. Manning et al., “Laparoscopic lens fogging: Solving a common\\nsurgical problem in standard and robotic laparoscopes via a scientific\\nmodel,” Surgical Endoscopy, vol. 32, no. 3, pp. 1600–1606, 2018.\\n[23] W. Xia, T. M. Peters, V. Fan, H. Sthanunathan, O. Qi, and E. C. S.\\nChen, “In vivo laparoscopic image de-smoking dataset, evaluation, and\\nbeyond,” IEEE Transactions on Medical Imaging, pp. 1–1, 2025.\\n[24] B. C. Ulmer, “The hazards of surgical smoke,” AORN Journal, vol. 87,\\nno. 4, pp. 721–738, 2008.\\n[25] H. Carbajo-Rodr´ıguez, J. L. Aguayo-Albasini, V. Soria-Aledo, and\\nC. Garc´ıa-L´opez, “Surgical smoke: Risks and preventive measures,”\\nCirug´ıa Espa˜nola (English Edition), vol. 85, no. 5, pp. 274–279, 2009.\\n[26] K. J. Weld et al., “Analysis of surgical smoke produced by various\\nenergy-based instruments and effect on laparoscopic visibility,” Journal\\nof Endourology, vol. 21, no. 3, pp. 347–351, 2007.\\n[27] X. Luo, A. J. McLeod, S. E. Pautler, C. M. Schlachta, and T. M. Peters,\\n“Vision-based surgical field defogging,” IEEE Transactions on Medical\\nImaging, vol. 36, no. 10, pp. 2021–2030, 2017.\\n[28] L. Chen, W. Tang, N. W. John, T. R. Wan, and J. J. Zhang, “De-\\nsmokegcn: Generative cooperative networks for joint surgical smoke\\ndetection and removal,” IEEE Transactions on Medical Imaging, vol. 39,\\nno. 5, pp. 1615–1625, 2020.\\n[29] S. Salazar-Colores, H. M. Jim´enez, C. J. Ortiz-Echeverri, and G. Flo-\\nres, “Desmoking laparoscopy surgery images using an image-to-image\\ntranslation guided by an embedded dark channel,” IEEE Access, vol. 8,\\npp. 208 898–208 909, 2020.\\n[30] H. Li et al., “Multi-frequency and smoke attention-aware learning\\nbased diffusion model for removing surgical smoke,” in Medical Image\\nComputing and Computer Assisted Intervention – MICCAI, 2024, pp.\\n47–56.\\n[31] W. Wang, F. Liu, J. Hao, X. Yu, B. Zhang, and C. Shi, “Desmoking of the\\nendoscopic surgery images based on a local-global u-shaped transformer\\nmodel,” IEEE Transactions on Medical Robotics and Bionics, vol. 7,\\nno. 1, pp. 254–265, 2024.\\n[32] R. Modrzejewski, T. Collins, A. Hostettler, J. Marescaux, and A. Bartoli,\\n“Light modelling and calibration in laparoscopy,” International Journal\\nof Computer Assisted Radiology and Surgery, vol. 15, no. 5, pp. 859–\\n866, 2020.\\n[33] P. Azagra et al., “Endomapper dataset of complete calibrated endoscopy\\nprocedures,” Scientific Data, vol. 10, no. 1, p. 671, 2023.\\n[34] J. Rodr´ıguez-Puigvert et al., “Lightdepth: Single-view depth self-\\nsupervision from illumination decline,” in 2023 IEEE/CVF International\\nConference on Computer Vision (ICCV), 2023, pp. 21 216–21 226.\\n[35] V. Stelzer et al., “Generation and distribution of surgical smoke during\\nhigh frequency electrocauterization,” in New Results in Numerical and\\nExperimental Fluid Mechanics XIV, 2024, pp. 559–568.\\n[36] S. Kumar, C. Crowley, M. F. Khan, M. D. Bustamante, R. A. Cahill,\\nand K. Nolan, “Understanding surgical smoke in laparoscopy through la-\\ngrangian coherent structures,” PLOS ONE, vol. 18, no. 11, p. e0293287,\\n2023.\\n[37] H. Tian, W. Li, P. Ogunbona, and L. Wang, “Single image smoke\\ndetection,” in Computer Vision – ACCV 2014, 2015, pp. 87–101.\\n[38] W.-T. Chen et al., “Desmokenet: a two-stage smoke removal pipeline\\nbased on self-attentive feature consensus and multi-level contrastive\\nregularization,” IEEE Transactions on Circuits and Systems for Video\\nTechnology, vol. 32, no. 6, pp. 3346–3359, 2022.\\n[39] W. Li et al., “Endoscopy image enhancement method by generalized\\nimaging defect models based adversarial training,” Physics in Medicine\\nand Biology, vol. 67, no. 9, p. 095016, 2022.\\n[40] R. Wu et al., “Self-supervised video desmoking for laparoscopic\\nsurgery,” in Computer Vision – ECCV 2024, 2025, pp. 307–324.\\n[41] S. G. Narasimhan and S. K. Nayar, “Vision and the atmosphere,”\\nInternational Journal of Computer Vision, vol. 48, no. 3, pp. 233–254,\\n2002.\\n[42] S. G. Narasimhan and S. K. Nayar, “Contrast restoration of weather\\ndegraded images,” IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, vol. 25, no. 6, pp. 713–724, 2003.\\n[43] R. Fattal, “Single image dehazing,” ACM transactions on graphics\\n(TOG), vol. 27, no. 3, pp. 1–9, 2008.\\n[44] R. T. Tan, “Visibility in bad weather from a single image,” in 2008 IEEE\\nConference on Computer Vision and Pattern Recognition (CVPR), 2008,\\npp. 1–8.\\n[45] H. Kaiming, S. Jian, and T. Xiaoou, “Single image haze removal using\\ndark channel prior,” IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, vol. 33, no. 12, pp. 2341–2353, 2011.\\n[46] L. Gu, P. Liu, C. Jiang, M. Luo, and Q. Xu, “Virtual digital defogging\\ntechnology improves laparoscopic imaging quality,” Surgical Innovation,\\nvol. 22, no. 2, pp. 171–176, 2015.\\n[47] A. Kotwal, R. Bhalodia, and S. P. Awate, “Joint desmoking and denois-\\ning of laparoscopy images,” in 2016 IEEE 13th International Symposium\\non Biomedical Imaging (ISBI), 2016, pp. 1050–1054.\\n\\n[48] A. Baid, A. Kotwal, R. Bhalodia, S. N. Merchant, and S. P. Awate,\\n“Joint desmoking, specularity removal, and denoising of laparoscopy\\nimages via graphical models and bayesian inference,” in Proceedings -\\nInternational Symposium on Biomedical Imaging, 2017, pp. 732–736.\\n[49] K. Tchaka, V. M. Pawar, and D. Stoyanov, “Chromaticity based smoke\\nremoval in endoscopic images,” in Medical Imaging 2017: Image\\nProcessing, vol. 10133, 2017, pp. 463–470.\\n[50] C. Wang, F. Alaya Cheikh, M. Kaaniche, A. Beghdadi, and O. J. Elle,\\n“Variational based smoke removal in laparoscopic images,” BioMedical\\nEngineering OnLine, vol. 17, no. 1, p. 139, 2018.\\n[51] M. A. Azam, K. B. Khan, E. Rehman, and S. U. Khan, “Smoke removal\\nand image enhancement of laparoscopic images by an artificial multi-\\nexposure image fusion method,” Soft Computing, vol. 26, no. 16, pp.\\n8003–8015, 2022.\\n[52] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “Dehazenet: an end-to-end\\nsystem for single image haze removal,” IEEE Transactions on Image\\nProcessing, vol. 25, no. 11, pp. 5187–5198, 2016.\\n[53] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng, “Aod-net: All-in-one\\ndehazing network,” in Proceedings of the IEEE international conference\\non computer vision (ICCV), 2017, pp. 4780–4788.\\n[54] H. Dong et al., “Multi-scale boosted dehazing network with dense\\nfeature fusion,” in 2020 IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), 2020, pp. 2154–2164.\\n[55] X. Qin, Z. Wang, Y. Bai, X. Xie, and H. Jia, “Ffa-net: Feature fusion\\nattention network for single image dehazing,” Proceedings of the AAAI\\nConference on Artificial Intelligence, vol. 34, no. 07, pp. 11 908–11 915,\\n2020.\\n[56] S. Bolkar, C. Wang, F. A. Cheikh, and S. Yildirim, “Deep smoke\\nremoval from minimally invasive surgery videos,” in 2018 25th IEEE\\nInternational Conference on Image Processing (ICIP), 2018, pp. 3403–\\n3407.\\n[57] O. Sidorov, C. Wang, and F. A. Cheikh, “Generative smoke removal,”\\nin Proceedings of the Machine Learning for Health NeurIPS Workshop,\\nvol. 116, 2020, pp. 81–92.\\n[58] L. Chen, W. Tang, and W. John, “Unsupervised learning of surgical\\nsmoke removal from simulation,” in The 11th Hamlyn Symposium on\\nMedical Robotics, 2018.\\n[59] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\\nfor biomedical image segmentation,” in Medical Image Computing and\\nComputer-Assisted Intervention – MICCAI, 2015, pp. 234–241.\\n[60] L. Ma, H. Song, X. Zhang, and H. Liao, “A smoke removal method\\nbased on combined data and modified u-net for endoscopic images,” in\\n2021 43rd Annual International Conference of the IEEE Engineering in\\nMedicine and Biology Society (EMBC), 2021, pp. 3783–3786.\\n[61] C. Wang, A. K. Mohammed, F. A. Cheikh, A. Beghdadi, and O. J.\\nElle, Multiscale Deep Desmoking for Laparoscopic Surgery, ser. SPIE\\nMedical Imaging, 2019, vol. 10949.\\n[62] T. Hong et al., “Mars-gan: Multilevel-feature-learning attention-aware\\nbased generative adversarial network for removing surgical smoke,”\\nIEEE Transactions on Medical Imaging, vol. 42, no. 8, pp. 2299–2312,\\n2023.\\n[63] Y. Zhou, Z. Hu, Z. Xuan, Y. Wang, and X. Hu, “Synchronizing detection\\nand removal of smoke in endoscopic images with cyclic consistency\\nadversarial nets,” IEEE/ACM Transactions on Computational Biology\\nand Bioinformatics, vol. 21, no. 4, pp. 670–680, 2024.\\n[64] J. Lin et al., “A desmoking algorithm for endoscopic images based on\\nimproved u-net model,” Concurrency and Computation: Practice and\\nExperience, vol. 33, no. 22, p. e6320, 2021.\\n[65] A. Kanakatte, K. Seemakurthy, J. Gubbi, J. Saha, A. Ghose, and\\nB. Purushothaman, “Surgical smoke dehazing and color reconstruction,”\\nin 2021 IEEE 18th International Symposium on Biomedical Imaging\\n(ISBI), 2021, pp. 280–284.\\n[66] C. Ma, C. Wang, and M. Zhao, “Laparoscopic video desmoking with\\nmutually attention-guided deformable convolutional networks and lbp\\nprior,” in 2024 International Joint Conference on Neural Networks\\n(IJCNN), 2024, pp. 1–8.\\n[67] C. Wang et al., “Smoke veil prior regularized surgical field desmoking\\nwithout paired in-vivo data,” Computers in Biology and Medicine, vol.\\n168, p. 107761, 2024.\\n[68] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”\\nin Advances in Neural Information Processing Systems 33 (NeurIPS\\n2020), vol. 33, 2020, pp. 6840–6851.\\n[69] S. Jiaming, M. Chenlin, and E. Stefano, “Denoising diffusion im-\\nplicit models,” in International Conference on Learning Representations\\n(ICLR), 2021.\\n[70] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\\nshifted windows,” in 2021 IEEE/CVF International Conference on\\nComputer Vision (ICCV), 2021, pp. 9992–10 002.\\n[71] F. Wang, X. Sun, and J. Li, “Surgical smoke removal via residual\\nswin transformer network,” International Journal of Computer Assisted\\nRadiology and Surgery, vol. 18, no. 8, pp. 1417–1427, 2023.\\n[72] I. Goodfellow et al., “Generative adversarial networks,” Commun. ACM,\\nvol. 63, no. 11, p. 139–144, 2020.\\n[73] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”\\narXiv preprint arXiv:1411.1784, 2014.\\n[74] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image\\ntranslation using cycle-consistent adversarial networks,” in 2017 IEEE\\nInternational Conference on Computer Vision (ICCV), 2017, pp. 2242–\\n2251.\\n[75] X. Su and Q. Wu, “Multi-stages de-smoking model based on cyclegan\\nfor surgical de-smoking,” International Journal of Machine Learning\\nand Cybernetics, vol. 14, no. 11, pp. 3965–3978, 2023.\\n[76] W. Wang, Q. Yuan, P. Huang, X. Wang, and H. Song, “Desmoke-vcu:\\nImproved unpaired image-to-image translation for removing smoke from\\nlaparoscopic images,” Digital Signal Processing, vol. 162, p. 105177,\\n2025.\\n[77] A. J. Islam, S. Salehin, S. U. Alam, K. Islam, S. Paul, and P. Paul,\\n“Lvqe: Laparoscopic video quality enhancement using gan-based smoke\\nelimination guided by an embedded dark channel,” in 2024 IEEE Inter-\\nnational Conference on Signal Processing, Information, Communication\\nand Systems (SPICSCON), 2024, pp. 01–06.\\n[78] V. Vishal, N. Sharma, and M. Singh, “Guided unsupervised desmoking\\nof laparoscopic images using cycle-desmoke,” in OR 2.0 Context-Aware\\nOperating Theaters and Machine Learning in Clinical Neuroimaging,\\n2019, pp. 21–28.\\n[79] V. Vishal, V. Venkatesh, K. Lochan, N. Sharma, and M. Singh, “Unsuper-\\nvised desmoking of laparoscopy images using multi-scale desmokenet,”\\nin Advanced Concepts for Intelligent Vision Systems, 2020, pp. 421–432.\\n[80] V. Venkatesh, N. Sharma, V. Srivastava, and M. Singh, “Unsupervised\\nsmoke to desmoked laparoscopic surgery images using contrast driven\\ncyclic-desmokegan,” Computers in Biology and Medicine, vol. 123, p.\\n103873, 2020.\\n[81] Y. Pan, S. Bano, F. Vasconcelos, H. Park, T. T. Jeong, and D. Stoy-\\nanov, “Desmoke-lap: Improved unpaired image-to-image translation for\\ndesmoking in laparoscopic surgery,” International Journal of Computer\\nAssisted Radiology and Surgery, vol. 17, no. 5, pp. 885–893, 2022.\\n[82] C. O. Ancuti, C. Ancuti, R. Timofte, and C. D. Vleeschouwer, “O-\\nhaze: a dehazing benchmark with real hazy and haze-free outdoor\\nimages,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition Workshops (CVPRW), 2018, pp. 867–8678.\\n[83] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, “Adaptive\\nmixtures of local experts,” Neural Computation, vol. 3, no. 1, pp. 79–87,\\n1991.\\n[84] R. Ding, H. Lu, and M. Liu, “Denseformer-moe: a dense transformer\\nfoundation model with mixture of experts for multi-task brain image\\nanalysis,” IEEE Transactions on Medical Imaging, pp. 1–1, 2025.\\n[85] D. Chen et al., “Gated context aggregation network for image dehazing\\nand deraining,” in 2019 IEEE Winter Conference on Applications of\\nComputer Vision (WACV), 2019, pp. 1375–1383.\\n[86] A. P. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. d. Mathelin,\\nand N. Padoy, “Endonet: a deep architecture for recognition tasks on\\nlaparoscopic videos,” IEEE Transactions on Medical Imaging, vol. 36,\\nno. 1, pp. 86–97, 2017.\\n[87] W. Xia, V. Fan, T. Peters, and E. C. S. Chen, “A new benchmark in vivo\\npaired dataset for laparoscopic image de-smoking,” in Medical Image\\nComputing and Computer Assisted Intervention – MICCAI, 2024, pp.\\n3–13.\\n[88] S. Giannarou, M. Visentini-Scarzanella, and G. Z. Yang, “Probabilistic\\ntracking of affine-invariant anisotropic regions,” IEEE Transactions on\\nPattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 130–143,\\n2013.\\n[89] M. Ye, E. Johns, A. Handa, L. Zhang, P. Pratt, and G.-Z. Yang, “Self-\\nsupervised siamese learning on stereo image pairs for depth estimation\\nin robotic surgery,” in The Hamlyn Symposium on Medical Robotics,\\n2017, p. 27.\\n[90] A. Leibetseder, M. J. Primus, S. Petscharnig, and K. Schoeffmann,\\n“Real-time image-based smoke detection in endoscopic videos,” in\\nProceedings of the on Thematic Workshops of ACM Multimedia 2017,\\n2017, p. 296–304.\\n[91] L. K. Choi, J. You, and A. C. Bovik, “Referenceless prediction of per-\\nceptual fog density and perceptual image defogging,” IEEE Transactions\\non Image Processing, vol. 24, no. 11, pp. 3888–3901, 2015.\\n',\n",
       " 'DEEP LEARNING MODELS ARE VULNERABLE, BUT ADVERSARIAL\\nEXAMPLES ARE EVEN MORE VULNERABLE\\nA PREPRINT\\nJun Li†1,2,\\nYanwei Xu*,†1,\\nKeran Li1, and\\nXiaoli Zhang3\\n1School of Management Science and Information Engineering, Jilin University of Finance and Economics, Jingyue\\nStreet, Changchun 130117, China\\n2Center for Artificial Intelligence, Jilin University of Finance and Economics, Jingyue Street, Changchun 130117,\\nChina\\n3College of Computer Science and Technology, Jilin University, Qianjin Street, Changchun 130012, China\\nNovember 10, 2025\\nABSTRACT\\nUnderstanding intrinsic differences between adversarial examples and clean samples is key to en-\\nhancing DNN robustness and detection against adversarial attacks. This study first empirically finds\\nthat image-based adversarial examples are notably sensitive to occlusion. Controlled experiments\\non CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples,\\npaired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE)\\nto quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE cal-\\nculations—supported by Mask Entropy Field Maps and statistical distributions—show adversarial\\nexamples have significantly higher confidence volatility under occlusion than originals. Based on\\nthis, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which\\navoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and\\nattacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up\\nto 96.5%.\\nKeywords Adversarial Examples · Adversarial Detection · Deep Learning · Computer Vision\\n1\\nIntroduction\\nIn the era of rapid digital transformation, artificial intelligence (AI), particularly deep learning, has become deeply\\nintegrated into diverse domains, including image recognition, speech processing, and natural language understanding,\\nthereby significantly enhancing daily life and work. Deep learning models, renowned for their exceptional feature\\nextraction and learning capabilities, have demonstrated outstanding performance in complex tasks. Critical applications,\\nsuch as facial recognition in security systems, medical image diagnostics, and road condition monitoring in autonomous\\nvehicles, heavily depend on the accurate decision-making of these models. However, as AI technology evolves, emerging\\nsecurity concerns surrounding deep learning systems have gained attention (Chakraborty et al., 2021). (Szegedy et al.,\\n2013) first introduced the concept of adversarial examples, wherein subtle, nearly imperceptible perturbations are added\\nto input data, leading to drastic mispredictions by deep neural networks. This phenomenon underscores a fundamental\\ndivergence in how deep learning models and human cognition interpret data, while also posing significant threats to\\nsystems relying on AI. For instance, in the context of autonomous driving, adversarial attacks could mislead a vehicle\\ninto misidentifying traffic signs, potentially resulting in catastrophic accidents.\\nIn response, researchers have developed a range of adversarial example generation techniques, including gradient-based\\nattacks, optimization-based methods, and meta-learning-based approaches, etc. Notable methods include the Fast\\n*Corresponding author: xuyanwei@s.jlufe.edu.cn\\n†These authors contributed equally to this work.\\narXiv:2511.05073v1  [cs.CV]  7 Nov 2025\\n\\narXiv Template\\nA PREPRINT\\nGradient Sign Method (FGSM), Projected Gradient Descent (PGD) (Madry et al., 2017), DeepFool (Moosavi-Dezfooli\\net al., 2016), and You Only Attack Once (YOAO) (Li et al., 2025b), among others. These adversarial attacks are not\\nconfined to image classification tasks but have been extended to domains such as object detection, speech recognition,\\nand real-world scenarios. To counteract the threats posed by adversarial examples, researchers have proposed various\\ndefense and detection strategies. Early defense mechanisms focused on adversarial training, where adversarial examples\\nwere incorporated into the model’s training process. However, studies have shown that this approach often faces\\nchallenges, such as reduced detection accuracy and the risk of catastrophic overfitting during training. As research\\nadvanced, more sophisticated defense strategies emerged, including model ensembles that combine predictions from\\nmultiple models to enhance robustness. However, most defense methods still center on improving model robustness,\\nwhich typically requires significant computational resources and increases model complexity.\\nTo tackle the growing challenges of adversarial attacks, increasing attention has been directed toward adversarial\\nexample detection technologies. Unlike traditional defense methods, which focus on enhancing model robustness,\\nadversarial example detection targets the identification and rejection of adversarial inputs before they can compromise\\nthe model, thereby bolstering the security of AI systems. These detection approaches typically offer lower computational\\ncomplexity and reduced resource demands compared to robust training strategies, allowing for better cost control while\\neffectively mitigating the impact of adversarial examples. The capacity to detect and filter adversarial examples prior\\nto their integration holds considerable practical value, ensuring the reliability of deep learning models in real-world\\napplications. Thus, establishing an efficient and accurate detection mechanism is paramount to preserving the integrity\\nof AI-driven systems.\\nDespite the significant advancements in adversarial example detection, numerous challenges remain. As adversarial\\nexample generation techniques continuously evolve, attackers develop increasingly sophisticated perturbation strategies,\\nwhich raise the difficulty of detection. New attack algorithms are adept at bypassing existing detection methods and\\nexhibit greater concealment. Moreover, deep learning models themselves are highly complex and heterogeneous, with\\neach model responding differently to adversarial inputs, complicating the development of universal detection models.\\nTo address these issues, this study delves into the causes of adversarial examples and the inherent vulnerabilities within\\ndeep learning models. While adversarial examples are often attributed to the weaknesses of these models, a more\\npressing issue lies in their inherent instability, whereby slight changes can lead to misclassifications across different\\ncategories. This phenomenon, termed the “vulnerable of adversarial examples\" represents a critical aspect that requires\\nfurther investigation.\\nBuilding upon this finding, the study introduces the concept of Sliding Mask Confidence Entropy (SMCE) to quantify\\nthe vulnerable of adversarial examples and the stability of deep learning models. By applying a sliding window to mask\\nlocal areas of an image and calculating the average of confidence entropy values for each window, the method assesses\\nnot only the stability of the image itself but also the robustness of the classifier when confronted with partial occlusion\\nor perturbation. Through extensive experimentation, the study reveals a striking finding: as model detection accuracy\\nimproves, the use of the SMCE strategy results in enhanced robustness of the deep learning model, significantly boosting\\nsecurity. This approach effectively avoids the catastrophic overfitting associated with traditional adversarial training\\nmethods, thereby providing a novel research avenue for subsequent work in this domain.\\nIncorporating SMCE, the study proposes the Sliding Window Masking-Adversarial Example Detection (SWM-AED)\\nalgorithm, an innovative solution designed to improve model robustness for adversarial example detection. The\\ncore of this algorithm lies in leveraging SMCE values to detect adversarial examples. Specifically, the SWM-AED\\nalgorithm calculates the Sliding Mask Confidence Entropy of an image, identifying samples with abnormal entropy\\nvalues indicative of adversarial perturbations. Notably, the SWM-AED algorithm is non-specific to any particular type\\nof adversarial example, making it resilient to a broad range of adversarial attacks generated by various adversarial\\ngeneration algorithms, demonstrating superior performance. The study further highlights a key observation: as the\\nmodel’s detection accuracy improves, the SWM-AED strategy adeptly mitigates catastrophic overfitting, enhancing\\nboth robustness and security. From a theoretical perspective, this study establishes a positive correlation between the\\naccuracy and robustness of deep learning models after the integration of the SWM-AED algorithm.\\nThis work fills a critical gap in the measurement of adversarial example vulnerable by introducing the concept of SMCE.\\nBased on this metric, the SWM-AED detection algorithm demonstrates outstanding detection performance, achieving\\nover 80% accuracy in detecting adversarial examples generated by multiple attack algorithms. Moreover, by proactively\\nidentifying and filtering adversarial examples, the SWM-AED algorithm significantly mitigates their impact on deep\\nlearning models, thereby substantially improving the overall security of AI systems.\\n2\\n\\narXiv Template\\nA PREPRINT\\n2\\nBackground and Related Work\\nAdversarial examples are generated by introducing subtle perturbations to an original image, resulting in misclas-\\nsifications by deep neural networks (DNNs). Despite these perturbations being imperceptible to human observers,\\nthey cause the model to misidentify the original input—incorrectly assigning it a label B instead of the true label\\nA. The vulnerability is particularly concerning in safety-critical applications, such as autonomous driving, where an\\nadversarially altered “Stop\" sign could be misinterpreted as “Go\", potentially leading to catastrophic outcomes (Xu\\net al., 2023; Li et al., 2025c; West et al., 2023).\\nAdversarial Defense and Detection: To enhance the security and robustness of deep learning models against adversarial\\nexamples, (Goodfellow et al., 2014) first introduced adversarial examples into the training process, pioneering the\\nresearch into model robustness. This approach was later extended by studies showing that training models with\\nadversarial examples generated via PGD attacks could significantly improve robustness (Madry et al., 2017). Further\\nadvancements led to ensemble adversarial training, which incorporated transferable perturbations, further enhancing\\nmodel robustness (Tramèr et al., 2017). The concept of Smooth Adversarial Training introduced a different perspective,\\nachieving notable robustness improvements (Xie et al., 2020). Other methods, including self-supervised adversarial\\ntraining (Naseer et al., 2020) and causal parameter estimation (Lee et al., 2023), further diversified the strategies to\\nimprove robustness. Additionally, researchers efforts extended to 3D point cloud recognition, highlighting the broad\\napplicability of adversarial defense techniques (Ji et al., 2023). These studies have cumulatively enriched the adversarial\\ntraining landscape.\\nAs research progressed, the focus shifted toward optimizing the finer details of adversarial training. The analysis\\nof smoothness enhancement revealed its specific role in domain adversarial training for improved target domain\\ngeneralization (Rangwani et al., 2022). Later, techniques to constrain loss variations between epochs and introduce\\nconvergence strides were developed to enhance adversarial training efficiency (Zhao et al., 2023). Feature denoising\\nmethods emerged as a way to increase robustness by removing noise from input features during inference, although they\\nfaced limitations with resistance to white-box attacks and increased computational costs Xie et al. (2019). In response,\\nselective feature regeneration was proposed as an alternative (Borkar et al., 2020), while image restoration-based\\ndenoising was explored, though it was prone to content distortion and vulnerability to EOT attacks (Mustafa et al.,\\n2020).\\nAs adversarial attack strategies diversified, targeted defenses were developed. Image-semantic dual adversarial training\\n(ISDAT) addressed the limited diversity of adversarial examples (Sui et al., 2025). For high-intensity perturbations,\\nnon-robust loss adjustment methods proved effective in improving model resilience (Le et al., 2025). Advances in\\nbidirectional mapping and self-attention feature alignment further bolstered resistance to attacks (Zhang et al., 2025),\\nwhile universal attention mechanisms successfully countered diverse attention-based adversarial attacks (Zhao et al.,\\n2025). These targeted strategies have significantly improved defenses across various attack scenarios.\\nIn parallel, adversarial sample detection has become an increasingly critical area of research. Early methods focused on\\ngradient masking to reduce the model’s sensitivity to small input perturbations (Papernot et al., 2017). Subsequently,\\ndenoising-based reconstruction methods were developed to detect adversarial examples by identifying reconstruction\\nerrors and prediction discrepancies (Meng and Chen, 2017). Projection-based methods measured the prediction\\nprobability distance for sample detection (Xu et al., 2017), while local intrinsic dimensionality analysis emerged as an\\neffective technique (Ma et al., 2018). Other approaches, including Mahalanobis distance (Lee et al., 2018), natural\\nscene statistics (Kherchouche et al., 2020), and autoencoders (Sotgiu et al., 2020), collectively established a robust\\nadversarial detection framework.\\nNovel perspectives have further propelled advancements in the field. Key feature modulation frameworks solved\\nadversarial patch detection (Wu et al., 2024), while clustering effect analysis explained adversarial robustness and\\nintroduced regularization techniques (Jin et al., 2023). Lightweight ensemble attacks (LEA2) presented new detection\\nstrategies for adversarial examples (Qian et al., 2023), and dual-function defense frameworks provided comprehensive\\nmitigation for adversarial instances (Yang et al., 2025). Models such as Adversarial Surgery and Regeneration (ASR)\\nsignificantly improved generalization and robustness (Fu et al., 2025).\\nMasking Techniques and Entropy-Based Adversarial Attacks: Masking techniques have emerged as essential\\ntools in adversarial defense. Instance-binding augmentation techniques, for example, reconstructed perturbation\\ndistributions via masking branches (Zhang et al., 2024b). Gradient-based masking was shown to enhance attack\\ntransferability by perturbing sensitive regions (Zhang et al., 2024a). Information masking and region intersection\\nstrategies were developed to purify adversarial examples (Liu et al., 2025), while in audio attacks, random masking\\noptimized adversarial training processes (Bui et al., 2025). Variance-based masking (RAPID) was another approach\\nfor detecting candidate regions (Kim et al., 2024). These techniques underscore the growing importance of masking\\nstrategies in adversarial research.\\n3\\n\\narXiv Template\\nA PREPRINT\\nFigure 1: Seeking the vulnerability of adversarial examples\\nFrom an information-theoretic perspective, high entropy in adversarial patches facilitates their localization and removal\\n(Tarchoun et al., 2024). Entropy-based detectors have successfully identified adversarial examples by analyzing entropy\\ndifferences before and after bit-depth reduction (Ryu and Choi, 2024). Advanced entropy analysis further refined patch\\nlocalization methods (Tarchoun et al., 2023), providing novel detection techniques for the field.\\n3\\nMethod\\nThe fundamental principle of adversarial example detection lies in identifying and leveraging discriminative features to\\neffectively distinguish adversarial examples from clean ones. This section first establishes the theoretical foundation\\nand the motivation for this study, followed by a visualization-based analysis of the distinct vulnerability characteristics\\nexhibited by adversarial examples in contrast to clean samples. Building on these observations, we introduce a\\nquantitative metric, termed Sliding Mask Confidence Entropy (SMCE), to systematically assess the degree of adversarial\\nvulnerability. To further elucidate the effectiveness of this metric, we visualize the quantified vulnerability distributions,\\nthereby establishing a robust framework for distinguishing adversarial examples from their clean counterparts.\\n3.1\\nResearch Objective\\nThe central objective of this study is to enhance the security of artificial intelligence systems by enabling them to\\neffectively resist adversarial attacks. Specifically, this work aims to uncover the inherent characteristics of adversarial\\nexamples from a novel perspective, and to leverage these characteristics to accurately classification between adversarial\\nexamples and clean samples, thereby mitigating the impact of adversarial attacks.\\n3.2\\nMotivation and Intuition\\nThe goal of adversarial attacks is to induce misclassification in DNN models with minimal perturbations, which can be\\nformally expressed as the following optimization problem:\\n4\\n\\narXiv Template\\nA PREPRINT\\nf(x) ̸= f(x + r) subject to r ≤ϵ\\n(1)\\nWhere f(x) denotes the predicted class for the original image, f(x+r) represents the predicted class for the adversarial\\nexample, and ϵ is a small, predefined perturbation bound. Based on the preceding formulation, it follows that an\\nadversarial example x + r arises from the superposition of a small perturbation r onto the original image x. Although\\nthe magnitude of this perturbation is minimal, it is sufficient to induce misclassification in deep learning models.\\nMathematically, the adversarial example x + r can be viewed as a linear combination of the original image and the\\nperturbation, underscoring the fundamental mechanism of adversarial attacks—where imperceptible perturbations in\\nthe input space lead to substantial alterations in the model’s output.\\nFurther analysis suggests that the perturbation r can be interpreted as a specialized form of noise. Despite its constrained\\nnumerical range, its influence on the model’s classification outcomes is nontrivial. Related study reveals that the\\nintroduction of noise to an image not only perturbs its pixel value distribution but also compromises its stability\\n(Tian et al., 2020; Buades et al., 2005). We posit that adversarial examples exhibit increased vulnerability relative\\nto their unperturbed counterparts, characterized by more ambiguous class boundaries that are inherently more prone\\nto misclassification. This heightened vulnerability arises from the proximity of adversarial examples to decision\\nboundaries in feature space, rendering them particularly sensitive to small input variations.\\nAdversarial examples exhibit greater susceptibility to perturbations compared to clean samples. Building upon this\\nfoundation, we further investigate the underlying factors contributing to this vulnerability. Notably, the human visual\\nsystem can reliably recognize objects even when images are partially occluded. Motivated by this observation, we\\nexamine how classification outcomes for clean and adversarial examples evolve when their occluded counterparts are\\nfed into a deep neural network.\\n3.3\\nVulnerability Exploration through Mask Box\\nDrawing inspiration from human perception, we systematically apply an m × m dark occlusion block that traverses\\nlocal regions of both clean and adversarial examples. By analyzing the resulting shifts in classification decisions, we\\naim to uncover the fundamental basis of adversarial vulnerability.\\nFigure 1 presents a detailed analysis of the vulnerability of adversarial examples in the context of image classification.\\nIn this experiment, we introduced a occlusion block, referred to as the “mask box\", and used it to occlude various\\nparts of both clean images and adversarial examples. The occlusion was applied systematically from left to right\\nand top to bottom. We then utilized a ResNet-18 classifier to make predictions on the occluded images, with the\\naim of investigating the stability of clean images and adversarial examples under occlusion and their corresponding\\nclassification results. The 16 sub-figures on the left side of Figure 1 illustrate the sliding of a 7 × 7 green mask box\\nacross the original sample. Each sub-figure presents the classification results, including the predicted class label and the\\nconfidence score for that label, as determined by the ResNet-18 classifier. Notably, when occluding the clean sample\\nwith the sliding mask, the model’s confidence and predicted class label remained largely unaffected. Even with partial\\nocclusions, the clean image was consistently classified correctly, demonstrating the robustness of the model to such\\nperturbations.\\nIn contrast, the 16 sub-figures on the right side of Figure 1 reveal a striking difference in behavior when adversarial\\nexamples are occluded. In these cases, the classification results exhibited significant instability, characterized by\\nconsiderable drops in confidence scores. This observation underscores the inherent vulnerable of adversarial examples,\\nwhich are highly susceptible to perturbations. This instability can be exploited as a key characteristic for detecting\\nadversarial examples, offering a potential avenue for enhancing defenses against adversarial attacks.\\nAdversarial examples have been shown to exhibit significant instability, necessitating the development of a robust metric\\nto quantify this instability. This paper introduces a novel formula to measure the variations in confidence scores and\\nclass labels resulting from occlusions in an image. By analyzing the differences in instability between clean samples\\nand adversarial examples, we provide a clear methodology for distinguishing between these two categories. The key\\ninnovation lies in the introduction of a precise metric that enables effective differentiation between adversarial and clean\\nsamples, providing new insights into the behavior of adversarial attacks and advancing adversarial examples detection\\ntechniques.\\n3.4\\nSliding Mask Confidence Entropy\\nThis study addresses a fundamental challenge in adversarial example detection: quantifying how an image’s classification\\nconfidence varies under partial occlusion. To systematically evaluate confidence stability, we propose Sliding Mask\\nConfidence Entropy (SMCE), a novel metric that measures the uncertainty in classification confidence as an image\\n5\\n\\narXiv Template\\nA PREPRINT\\nFigure 2: The process of calculating confidence entropy for a single occluded image.\\nundergoes successive occlusions via a sliding window mask. One of the key innovation of this work is the entropy-based\\nformulation of SMCE, which enables precise quantification of confidence stability during occlusion.\\nThe formula for Sliding Mask Confidence Entropy (SMCE) is as follows:\\nHSMCE(I) = 1\\nn\\nn\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed−\\nm\\nX\\nj=1\\npij log2(pij)\\n\\uf8f6\\n\\uf8f8\\n(2)\\nwhere HSMCE(I) represents the Sliding Mask Confidence Entropy of image I. pij is the confidence that image I is\\nclassified as the j-th class after being occluded by the i-th sliding window mask position. n is the number of sliding\\npositions of the window mask, i.e., the number of different positions the mask moves across the image. m is the total\\nnumber of classes that the classifier can identify, i.e., the number of possible categories for the image.\\nThe formula 2 calculates the average of confidence entropy across all sliding window mask positions to assess the\\nstability of the image’s confidence during the occlusion process. Specifically, for each sliding position i, we first\\ncompute the confidence entropy at that position, i.e., −Pm\\nj=1 pij log2(pij). Then, we sum up the confidence entropies\\nfor all positions and take the average to obtain the final Sliding Mask Confidence Entropy HSMCE(I). A higher value of\\nthis metric indicates greater uncertainty in confidence changes under different occlusion conditions, implying poorer\\nstability of the confidence; conversely, a lower value indicates more stable confidence changes, suggesting that the\\nimage’s classification results are more reliable under various occlusion conditions.\\n3.4.1\\nProperty about SMCE\\nProperty 1. Non-negativity .\\nHSMCE(I) ≥0\\n(3)\\nThe Sliding Mask Confidence Entropy (SMCE) metric is inherently non-negative, as established in formula 3. When the\\nSMCE value approaches zero, it indicates that the predicted labels remain consistent across diverse occlusion conditions,\\nwith model confidence converging towards one. This behaviour reflects high resilience to localized perturbations and\\nsuggests strong intrinsic image stability.\\nProperty 2. Maximum Value of the SMCE .\\nHSMCE(I) ≤log2 m\\n(4)\\nFurthermore, formula A.1 demonstrates that the maximum value of SMCE is log2 m. As the image undergoes successive\\nocclusions through sliding masks, greater variations in confidence scores and class labels lead to higher SMCE values.\\nThese elevated values signify reduced stability, emphasizing the image’s vulnerability to adversarial examples. One of\\nthe key innovation of this work lies in quantitatively linking SMCE to the degree of stability.\\n3.4.2\\nThe Calculation Process of SMCE\\nIn the previous section, we introduced the Sliding Mask Confidence Entropy (SMCE) as a quantitative measure of image\\nstability about adversarial or clean samples. This section provides a detailed exposition of the SMCE computation\\nprocess.\\nWhen an image is classified through a deep learning model, the model outputs a classification vector and a classification\\nlabel. As shown in the probability distribution diagram in Figure 2, the classification vector represents the probability\\n6\\n\\narXiv Template\\nA PREPRINT\\nFigure 3: Mask Entropy Field Map.\\ndistribution output by the classifier, reflecting the confidence levels of the input image belonging to each predefined\\ncategory. The classification vector is a numerical vector, with each element corresponding to a category, having a value\\nrange of [0, 1], and the sum of all elements equals 1. The classification label is the final classification result generated by\\nthe classifier based on the classification vector, indicating the most likely category to which the image belongs. Figure 2\\nillustrates the confidence entropy computation process for a single occluded image. Specifically, the image is first\\npartially occluded using a mask block. The occluded image is then fed into the deep learning model for classification\\nto obtain the classification vector and label. Subsequently, the classification vector is substituted into the confidence\\nentropy calculation formula to obtain the confidence entropy of the image under the occluded state.\\nBy sliding the mask block from left to right and top to bottom in sequence, and repeatedly computing the confidence\\nentropy, the SMCE value of the image can be obtained by averaging all the computed results.\\n3.4.3\\nMask Entropy Field Map\\nThe Sliding Mask Confidence Entropy (SMCE) is an indicator used to evaluate the stability of images under occlusion\\nconditions. To more intuitively display the differences in stability between adversarial and clean samples, a visualization\\nmethod called the Mask Entropy Field Map (MEFM) is proposed. As shown in Figure 3, this method involves sliding a\\nmask over the image to progressively occlude different regions and calculating the output confidence entropy of the\\nclassifier for each occluded image. Specifically, the entropy values at each position are mapped onto the image, where\\nregions with higher entropy values (i.e., lower classifier confidence) are represented by warm colors (such as red and\\nblack), and regions with lower entropy values (i.e., higher classifier confidence) are represented by cool colors (such\\nas yellow and white). The Mask Entropy Field Map is not only clearly demonstrates the changes in stability under\\ndifferent occlusion conditions but also intuitively reveals the significant differences in stability between adversarial and\\nclean samples.\\nTo further elucidate the stability differences between adversarial examples and clean samples, a subset of clean samples\\nfrom the CIFAR-10 dataset was randomly selected, and their Mask Entropy Field Map (MEFM) were generated, as\\nshown in the “Clear\" subfigure of Figure 4. Adversarial examples were then crafted from these clean samples using the\\nFast Gradient Sign Method (FGSM), and the corresponding MEFM were plotted, as illustrated in the “Adversarial\"\\nsubfigure of Figure 4. The color intensity within the maps intuitively reflects image stability: darker regions correspond\\nto higher Sliding Mask Confidence Entropy (SMCE) values, indicating reduced stability. A direct comparison between\\nthe two subfigures reveals that adversarial examples consistently exhibit darker MEFM, reflecting significantly lower\\nstability compared to clean samples. This finding substantiates the inherent vulnerability of adversarial examples.\\nThe presence of elevated entropy values in certain clean samples is likely due to underfitting in deep neural networks.\\nWhile neural network classifiers achieve high performance on training datasets, attaining perfect classification accuracy\\nremains challenging. Consequently, some clean samples are misclassified, thereby affecting the probability distribution\\nin the SMCE computation and leading to higher entropy values. Conversely, the lower entropy values observed in\\nsome adversarial examples can be attributed to the loss of essential original features and the introduction of new\\ncategory-related features induced by adversarial attack algorithms. This transformation enhances the robustness of the\\n7\\n\\narXiv Template\\nA PREPRINT\\nFigure 4: Mask Entropy Field Map for clean samples and adversarial examples.\\nFigure 5: The Sliding Window Masking-Adversarial Example Detection\\nclassifier against adversarial examples in the presence of occlusion, thereby yielding lower SMCE values. Experimental\\nresults reveal that adversarial examples consistently exhibit higher Sliding Mask Confidence Entropy (SMCE) values\\nthan clean samples. This distinct characteristic enables effective detection of adversarial examples through SMCE-based\\nthreshold.\\n3.4.4\\nAdversarial Example Detection Algorithm\\nIn the preceding analysis, the vulnerability of adversarial examples was systematically examined, revealing a marked\\ndecline in classification confidence and shifts in predicted class labels when subjected to occlusion via a sliding window.\\n8\\n\\narXiv Template\\nA PREPRINT\\nTo quantitatively capture this behavior, we propose the Sliding Mask Confidence Entropy (SMCE) metric, which\\nmeasures fluctuations in confidence scores and the frequency of class transitions under localized occlusion, thereby\\nproviding a precise quantitative assessment. Comparative analysis demonstrates that adversarial examples consistently\\nyield significantly higher SMCE values than their clean counterparts, establishing a robust criterion for distinguishing\\nadversarial examples from benign data.\\nBuilding on these findings, we propose an SMCE threshold-based approach for adversarial example detection. Given\\nan input image, its SMCE value is first computed, if the value exceeds a predefined threshold, the image is identified as\\nan adversarial example, prompting appropriate defensive measures, such as flagging or removing it from the dataset to\\nmitigate its impact on model misclassification. Conversely, if the SMCE value falls below the threshold, the image is\\nclassified as a clean sample and can be retained for model training or inference.\\nThe proposed method for detecting adversarial examples, as depicted in Figure 5, Upon inputting an image into the\\ndetection system, it is inherently uncertain whether the image is a pristine sample or one subjected to adversarial\\nperturbations. To resolve this ambiguity, a novel strategy is employed: sliding masks are sequentially applied to occlude\\nportions of the image, moving from the top-left corner to the bottom-right corner. Each occluded version of the image is\\nthen processed through a deep learning model, yielding a key metric—Sliding Mask Confidence Entropy (SMCE). The\\nSMCE quantifies the stability of the image’s classification under occlusion, with a low value indicating high confidence\\nthat the image is a clean sample. Conversely, a high SMCE value signals instability in classification, suggesting the\\npresence of adversarial manipulations. By setting a predefined threshold for SMCE, the method effectively classifies\\nimages as either unaltered or adversarial. Algorithm 1 introduces a crucial innovation in adversarial detection, as it\\nenables precise identification of adversarial examples by leveraging occlusion-induced uncertainty, providing a robust\\napproach to safeguard against adversarial attacks in real-world applications.\\nAlgorithm 1 SWM-AED: The Sliding Window Masking-Adversarial Example Detection\\n1: procedure SWM-AED(x, f, m, threshold)\\n2:\\nInput: Image x, classifier f, mask size m, threshold 0.1 (default).\\n3:\\nOutput: Boolean indicating if x is an adversarial example.\\n4:\\nInitialize: HSMCE ←0, n ←number of windows.\\n5:\\nfor each sliding window mask Mi in image x do\\n6:\\npij = fj(x ⊙Mi)\\n7:\\nH = −Pm\\nj=1 pij log2(pij)\\n8:\\nHSMCE ←HSMCE + H\\n9:\\nend for\\n10:\\nHSMCE ←HSMCE\\nn\\n11:\\nif HSMCE > threshold then\\n12:\\nreturn True\\n▷Adversarial example\\n13:\\nelse\\n14:\\nreturn False\\n▷Not an adversarial example\\n15:\\nend if\\n16: end procedure\\nwhere x ⊙Mi denotes the region-wise occlusion of the image x by the i-th sliding window mask Mi of size m × m,\\nand pij denotes the probability score assigned by the classifier f when predicting that the image x belongs to class j,\\nand x is occluded by the i-th sliding mask of size m × m.\\n4\\nExperimental\\n4.1\\nDatasets and Classifiers\\nAll experiments are conducted on the widely adopted CIFAR-10 (Krizhevsky et al., 2009), which comprises 60,000\\ncolour images (32 × 32 pixels) evenly distributed across ten object categories. Its balanced class distribution and\\nmoderate image resolution make it a standard benchmark in adversarial examples research. In the comparative\\nexperiments that reveal the sensitivity of adversarial examples to occlusion (e.g., Figure 4 and Figure 6), we randomly\\nselected 1,800 images from the 10,000-image CIFAR-10 test set to form the control group. For the experimental\\ngroup, we generated corresponding adversarial examples from these clean images using nine different adversarial\\nattack algorithms, with each algorithm producing 200 adversarial samples. In the evaluation of the detection algorithm,\\n9\\n\\narXiv Template\\nA PREPRINT\\nwe combined the experimental group (adversarial examples) and the control group (clean images), and applied the\\nSWM-AED detection method to assess the accuracy of adversarial example detection.\\nTo comprehensively assess adversarial susceptibility, multiple deep neural network architectures with varying levels\\nof accuracy—including ResNet-18, ResNet-50 and VGG-11—are trained on CIFAR-10. These models serve both as\\nbaselines and as entropy calculators for the proposed Sliding Window Masking-Adversarial Example Detection (SWM-\\nAED) framework, which introduces the Sliding Mask Confidence Entropy (SMCE) as a discriminative feature for\\nadversarial detection. Through extensive comparative experiments, SWM-AED consistently outperforms conventional\\ndefense methods across multiple evaluation metrics. These results highlight its robustness, scalability and potential as a\\ngeneralizable solution to adversarial vulnerability in deep learning models.\\n4.2\\nEvaluation Metrics\\nThe SWM-AED algorithm we proposed can be regarded as a binary classifier, used to distinguish between adversarial\\nexamples and clean samples. In this context, the positive class represents adversarial examples, while the negative class\\nrepresents clean samples that have not been attacked. We use Precision, Recall and F1 score as Three important metrics\\nto evaluate the performance of this adversarial example detection algorithm.\\nConfusion Matrix: A confusion matrix is a table used to evaluate the performance of a classification model. It contains\\nfour key components, TP (True Positive): Correctly predicted as adversarial. FP (False Positive): Incorrectly predicted\\nas adversarial. FN (False Negative): Incorrectly predicted as clean. TN (True Negative): Correctly predicted as clean.\\nFor binary classification problem, the confusion matrix can be visualized as follows:\\nTable 1: Confusion Matrix\\nActual\\nPredicted\\nAdversarial (Positive)\\nClean (Negative)\\nAdversarial\\nTP\\nFN\\nClean\\nFP\\nTN\\nPrecision: Precision is the proportion of samples that are actually adversarial among those predicted as adversarial. The\\nformula for calculating Precision is:\\nPrecision =\\nTP\\nTP + FP\\nRecall: Recall is the proportion of samples that are correctly predicted as adversarial among all actual adversarial\\nexamples. It reflects the extent to which the algorithm can detect adversarial examples. The formula for calculating\\nRecall is:\\nRecall =\\nTP\\nTP + FN\\nAccuracy: Accuracy denoted as Acc, is a widely used and intuitive evaluation metric. It represents the proportion of\\ncorrectly predicted samples out of the total number of samples. This metric provides a comprehensive reflection of the\\noverall prediction accuracy of a model.\\nAcc =\\nTP + TN\\nTP + TN + FP + FN\\nF1 Score: The F1 score is the harmonic mean of Precision and Recall, providing a single metric that balances both. The\\nformula for calculating the F1 score is:\\nF1 = 2 × Precision × Recall\\nPrecision + Recall\\nThis score is particularly useful when the class distribution is unbalanced, as it gives a more balanced view of the\\nmodel’s performance.\\n10\\n\\narXiv Template\\nA PREPRINT\\nFigure 6: The figure presents a comparative analysis of the Sliding Mask Confidence Entropy distributions for\\nadversarial examples generated by nine distinct attack algorithms and their original, unperturbed counterparts. Empirical\\ndistributions are visualized, while fitted Gaussian distributions are overlaid with line graphs. Specifically, yellow bars\\nand lines represent adversarial examples, whereas blue bars and lines denote clean samples. The x-axis corresponds\\nto the range of Sliding Mask Confidence Entropy values, which reflect the uncertainty in classification outcomes for\\nimage samples under occlusion. The y-axis indicates the statistical frequency of Sliding Mask Confidence Entropy,\\nproviding insight into the density distribution across varying entropy levels.\\n4.3\\nExperimental Results\\nIn this experiment, the vulnerability of adversarial examples is comprehensively investigated, with particular emphasis\\non analyzing the distribution of SMCE values for adversarial examples generated using various attack methods. These\\ndistributions are systematically compared to those of clean samples. To further enhance the generalizability of the\\nSWM-AED algorithm, we explore the impact of threshold settings on its performance, ensuring its efficacy across\\na wide range of adversarial attacks. Additionally, the influence of key factors (including mask size, model accuracy,\\nmodel depth, and architecture) on the detection performance of the SWM-AED algorithm is thoroughly examined.\\nFinally, comparative experiments with existing state-of-the-art adversarial defense algorithms demonstrate the superior\\ndetection accuracy of the proposed approach, validating its significant advantages.\\n4.3.1\\nThe Empirical Distribution of the Sliding Mask Confidence Entropy\\nThis study systematically evaluates nine widely used adversarial attack algorithms—JSMA(Papernot et al., 2016),\\nPGD(Madry et al., 2017), DeepFool(Moosavi-Dezfooli et al., 2016), FGSM(Goodfellow et al., 2014), BIM(Kurakin\\net al., 2018), FFGSM(Wong et al., 2020), APGD(Croce and Hein, 2020), Pixel(Pomponi et al., 2022), and\\nPIFGSMPP(Gao et al., 2020)—by analyzing the Gaussian empirical distributions of Sliding Mask Confidence Entropy\\n11\\n\\narXiv Template\\nA PREPRINT\\nFigure 7: The figure illustrates the detection performance of the proposed method against adversarial examples generated\\nby multiple attack algorithms. To ensure statistical significance, each algorithm generates a large-scale adversarial\\nexample dataset. The x-axis represents the range of classification decision thresholds, while the y-axis depicts the\\naccuracy of the SWM-AED.\\nfor adversarial examples and their original, unperturbed counterparts. The results reveal notable differences in the\\nconfidence entropy distributions between adversarial and clean samples, which are visually distinguishable. As can be\\nseen from Figure 6, yellow distributions correspond to adversarial examples, while blue distributions represent clean\\nsamples. Differences in both the position and shape of these distributions serve as indicators of adversarial robustness.\\nThe findings demonstrate a significant correlation between the magnitude of distributional differences and the robustness\\nof adversarial examples. When the entropy distribution of adversarial examples deviates substantially from that of\\nclean samples, the adversarial examples tend to be more fragile, exhibiting higher susceptibility to perturbations\\nand greater instability in classification outcomes. Conversely, smaller distributional discrepancies indicate that the\\ngenerated adversarial examples yield more stable classification results. As illustrated in Figure 6, among the nine tested\\nalgorithms, adversarial examples generated by BIM, PGD, APGD, and PIFGSMPP exhibit entropy distributions that\\nclosely resemble those of clean samples, suggesting enhanced stability. This similarity in entropy profiles indicates that\\nadversarial examples produced by these four algorithms share characteristics more akin to normal, non-adversarial data.\\nConsequently, these adversarial examples are more stable, as they are better able to evade detection by the SWM-AED\\nalgorithm when compared to samples generated by other attack methods in the evaluation.\\nNotably, the study finds that complete overlap between the distributions of adversarial and clean samples is exceedingly\\nrare. This observation aligns with the fundamental nature of adversarial example generation: adversarial perturbations\\nintroduce deliberate noise into the original samples, inevitably disrupting their statistical properties and shifting their\\ndistributions.\\nThe entropy distribution plots provide direct insight into the vulnerability of adversarial examples. By examining these\\ndistributions, researchers can systematically assess the susceptibility of adversarial examples generated by different\\nattack algorithms and further explore the impact of various adversarial attacks on model confidence.\\n12\\n\\narXiv Template\\nA PREPRINT\\n4.3.2\\nThe Relationship between Threshold and Model Performance\\nSince the proposed detection algorithm is inherently a binary classification task, the selection of an appropriate decision\\nthreshold is crucial for effectively distinguishing between adversarial and clean samples. The choice of this threshold\\ndirectly influences detection performance. To systematically assess the optimal detection accuracy across different\\nthreshold settings, the experiment evaluates ten representative adversarial attack algorithms, including FGSM, PGD,\\nand DeepFool, among others. This comprehensive analysis aims to rigorously examine the effectiveness of the detection\\nmethod against a wide range of adversarial attacks.\\nFigure 7 depicts the variation in accuracy across different threshold conditions. The Sliding Mask Confidence Entropy\\n(SMCE) values are computed using the ResNet-18 model to systematically assess the classification-based detection\\nperformance of the proposed SWM-AED algorithm across a range of threshold settings. Experimental findings\\nreveal that SWM-AED consistently delivers strong detection performance against a wide spectrum of adversarial\\nattacks. Notably, when the detection threshold is set to 0.1, the algorithm achieves robust generalisation, attaining\\ndetection accuracies exceeding 75% for the majority of adversarial examples—excluding only the BIM and OnePixel\\nattacks. In particular, the detector demonstrates exceptional resilience against targeted attacks such as JSMA and\\nDeepFool, achieving detection accuracies above 90%. Although performance against BIM-like attacks—characterised\\nby iterative gradient-based perturbation—is comparatively lower, the algorithm still maintains detection rates above\\n60%, underscoring its robustness even in challenging scenarios. A comprehensive analysis of the detection results\\nreveals that SWM-AED achieves an average detection accuracy exceeding 80% across randomly sampled adversarial\\nexamples. Among the nine evaluated attack methods, two yield detection rates above 90% and three exceed 80%,\\nhighlighting the algorithm’s strong generalisation capability and reliable performance across diverse adversarial threat\\nmodels.\\n4.3.3\\nInvestigation of Key Factors Influencing Algorithm Performance\\nTable 2: Based on the ResNet-18 model with a classification accuracy rate of 96%, the mask detection method based on\\na 3 × 3 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack\\nalgorithms.\\nAttack Method\\nPrecision (%)\\nRecall (%)\\nF1 Score\\nAccuracy (%)\\nJSMA\\n96.04\\n97.00\\n0.96\\n96.50\\nPGD\\n71.43\\n70.00\\n0.70\\n71.00\\nDeepFool\\n93.68\\n0.89\\n0.91\\n91.50\\nFGSM\\n76.56\\n98.00\\n0.85\\n84.00\\nBIM\\n61.54\\n56.00\\n0.58\\n60.50\\nFFGSM\\n79.05\\n83.00\\n0.80\\n80.50\\nAPGD\\n64.66\\n75.00\\n0.69\\n67.00\\nOnePixel\\n77.27\\n68.00\\n0.72\\n74.00\\nPixle\\n87.04\\n94.00\\n0.90\\n90.00\\nPIFGSMPP\\n67.97\\n87.00\\n0.76\\n73.00\\nTable 3: Based on the ResNet-18 model with a classification accuracy rate of 96%, the mask detection method based on\\na 7 × 7 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack\\nalgorithms.\\nAttack Method\\nPrecision (%)\\nRecall (%)\\nF1 Score\\nAccuracy (%)\\nJSMA\\n95.10\\n97.00\\n0.96\\n96.00\\nPGD\\n76.53\\n75.00\\n0.75\\n76.00\\nDeepFool\\n86.36\\n95.00\\n0.90\\n90.00\\nFGSM\\n80.87\\n93.00\\n0.86\\n85.50\\nBIM\\n59.12\\n81.00\\n0.68\\n62.50\\nFFGSM\\n79.63\\n86.00\\n0.82\\n82.00\\nAPGD\\n71.93\\n82.00\\n0.76\\n75.00\\nOnePixel\\n71.96\\n77.00\\n0.74\\n73.50\\nPixle\\n84.21\\n96.00\\n0.89\\n89.00\\nPIFGSMPP\\n76.85\\n83.00\\n0.79\\n79.00\\n13\\n\\narXiv Template\\nA PREPRINT\\nTable 4: Based on the ResNet-18 model with a classification accuracy rate of 96%, the mask detection method based on\\na 9 × 9 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack\\nalgorithms.\\nAttack Method\\nPrecision (%)\\nRecall (%)\\nF1 Score\\nAccuracy (%)\\nJSMA\\n88.39\\n99.00\\n0.93\\n93.00\\nPGD\\n71.30\\n82.00\\n0.76\\n74.50\\nDeepFool\\n82.76\\n96.00\\n0.88\\n88.00\\nFGSM\\n79.13\\n91.00\\n0.84\\n83.50\\nBIM\\n60.19\\n62.00\\n0.61\\n60.50\\nFFGSM\\n72.87\\n94.00\\n0.82\\n79.50\\nAPGD\\n67.65\\n92.00\\n0.77\\n74.00\\nOnePixel\\n68.18\\n75.00\\n0.71\\n70.00\\nPixle\\n83.18\\n89.00\\n0.85\\n85.50\\nPIFGSMPP\\n69.85\\n95.00\\n0.80\\n77.00\\nTable 5: Based on the ResNet-18 model with a classification accuracy rate of 80.8%, the mask detection method based\\non a 7 × 7 sliding window is used to evaluate its detection success rate for samples generated by various adversarial\\nattack algorithms.\\nAttack Method\\nPrecision (%)\\nRecall (%)\\nF1 Score\\nAccuracy (%)\\nJSMA\\n73.33\\n88.00\\n0.80\\n78.00\\nPGD\\n58.70\\n81.00\\n0.68\\n62.00\\nDeepFool\\n67.94\\n89.00\\n0.77\\n73.50\\nFGSM\\n60.87\\n84.00\\n0.70\\n65.00\\nBIM\\n59.09\\n13.00\\n0.21\\n52.00\\nFFGSM\\n59.40\\n79.00\\n0.67\\n62.50\\nAPGD\\n57.96\\n91.00\\n0.70\\n62.50\\nOnePixel\\n53.05\\n87.00\\n0.65\\n55.00\\nPixle\\n68.09\\n96.00\\n0.79\\n75.50\\nPIFGSMPP\\n60.31\\n79.00\\n0.68\\n63.50\\nTable 6: Based on the ResNet-50 model with a classification accuracy rate of 79.1%, the mask detection method based\\non a 7 × 7 sliding window is used to evaluate its detection success rate for samples generated by various adversarial\\nattack algorithms.\\nAttack Method\\nPrecision (%)\\nRecall (%)\\nF1 Score\\nAccuracy (%)\\nJSMA\\n78.07\\n89.00\\n0.83\\n82.00\\nPGD\\n67.16\\n90.00\\n0.76\\n73.00\\nDeepFool\\n70.87\\n90.00\\n0.79\\n76.50\\nFGSM\\n72.36\\n89.00\\n0.79\\n77.50\\nBIM\\n50.81\\n94.00\\n0.65\\n51.50\\nFFGSM\\n69.23\\n90.00\\n0.78\\n75.00\\nAPGD\\n67.54\\n77.00\\n0.71\\n70.00\\nOnePixel\\n55.00\\n11.00\\n0.18\\n51.00\\nPixle\\n61.74\\n92.00\\n0.73\\n67.50\\nPIFGSMPP\\n70.25\\n85.00\\n0.76\\n74.50\\nFrom the analysis of formula 2, it is evident that the SWM-AED algorithm requires a well-performing deep learning\\nmodel to classify occluded images and generate the corresponding class probability distributions. These distributions\\nare then used to calculate the SMCE, which is subsequently compared to a predefined threshold to detect and classify\\nadversarial examples. The performance of the algorithm is primarily influenced by two key factors: the mask size and\\nthe intrinsic classification capability of the model. To systematically assess the impact of these factors on detection\\nperformance, we conduct a series of comparative experiments. These experiments examine the effects of mask size,\\nclassification accuracy of deep learning models, model architecture (including depth), and the generalization ability of\\nthe SWM-AED algorithm under various adversarial attacks.\\n14\\n\\narXiv Template\\nA PREPRINT\\nTable 7: Based on the Vgg-11 model with a classification accuracy rate of 81.3%, the mask detection method based on\\na 7 × 7 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack\\nalgorithms.\\nAttack Method\\nPrecision (%)\\nRecall (%)\\nF1 Score\\nAccuracy (%)\\nJSMA\\n73.68\\n98.00\\n0.84\\n81.50\\nPGD\\n73.28\\n85.00\\n0.78\\n77.00\\nDeepFool\\n75.63\\n90.00\\n0.82\\n80.50\\nFGSM\\n73.28\\n85.00\\n0.78\\n77.00\\nBIM\\n54.78\\n63.00\\n0.58\\n55.50\\nFFGSM\\n69.17\\n92.00\\n0.78\\n75.50\\nAPGD\\n71.68\\n81.00\\n0.76\\n74.50\\nOnePixel\\n54.23\\n77.00\\n0.63\\n56.00\\nPixle\\n73.58\\n78.00\\n0.75\\n75.00\\nPIFGSMPP\\n68.46\\n89.00\\n0.77\\n74.00\\nFigure 8: Detection accuracy of the SWM-AED algorithm under various adversarial attack methods using different\\nmask window sizes.\\nThe impact of mask window size on the SWM-ADE detection algorithm. Experiments were performed using the\\nsame model with three different mask sizes—3 × 3, 7 × 7, and 9 × 9—while evaluating their effects on detection\\naccuracy. The corresponding results are presented in Tables 2, 3, and 4. The evaluation involved ten distinct adversarial\\nattack algorithms and use four key performance metrics, including Precision, Recall, F1-score and Accuracy, for a\\ncomprehensive evaluation. For each attack algorithm, multiple classification thresholds were tested, and the optimal\\nresults were recorded.\\nBy synthesizing and analyzing the data presented in Tables 2, 3, and 4, Figure 8 is constructed. The figure distinctly\\nillustrates substantial variations in the detection accuracy of the SWM-AED algorithm across different mask sizes. The\\nexperimental findings underscore the necessity of dynamically adjusting the mask size to optimize detection performance\\nfor various adversarial attack strategies. Further comparative analysis reveals that the SWM-AED algorithm attains\\n15\\n\\narXiv Template\\nA PREPRINT\\nFigure 9: Detection accuracy of the SWM-AED algorithm across models with different classification accuracies under\\nvarious adversarial attack methods.\\noptimal performance in most adversarial example detection tasks when the mask size is configured to 7 × 7. This\\nfinding suggests that neither the smallest nor the largest mask necessarily yields optimal results; rather, the choice of\\nmask size should be tailored to the image dimensions and the specific characteristics of the detection task. A mask that\\nis too small may fail to effectively occlude local image features, reducing its ability to capture adversarial perturbations,\\nwhereas an excessively large mask may introduce unnecessary noise, compromising local feature detection.\\nThe Impact of Deep Neural Network Accuracy on the SWM-ADE Detection Algorithm. Table 5 presents the\\ndetection results of the SWM-AED algorithm applied to the ResNet-18 model, which achieves a training accuracy\\nof 80.8%. A comparative analysis with the results from Table 3 (used as a control variable) is conducted, leading\\nto the construction of Figure 9. This figure visually demonstrates a robust positive correlation between the model’s\\nclassification accuracy and the detection performance of the SWM-AED algorithm. Specifically, as the model’s\\nclassification accuracy increases, the algorithm demonstrates improved effectiveness in detecting adversarial examples\\ngenerated by different attack algorithms.\\nThis phenomenon can be explained from an algorithmic perspective: the SWM-AED detection mechanism relies on the\\nstability of image features, which is quantified using the confidence scores output by the model. When a model with\\nlower classification accuracy is used, even clean samples tend to receive lower confidence scores, leading to higher\\ncorresponding confidence entropy values. As a result, the confidence entropy distributions of adversarial and clean\\nsamples exhibit substantial overlap, making it difficult for the SWM-AED algorithm to establish an optimal decision\\nthreshold for effective differentiation. In contrast, high-accuracy models assign higher confidence scores and lower\\nconfidence entropy values to clean samples, creating a more pronounced distinction from the entropy distribution of\\nadversarial examples. This distinction significantly enhances the detection performance of the SWM-AED algorithm by\\nfacilitating more reliable threshold selection.\\nThe Impact of Deep Neural Network Layer Architecture on the SWM-ADE Detection Algorithm. Tables 6 and 5\\npresent a controlled experimental analysis designed to evaluate the impact of model depth on the performance of the\\nproposed SWM-AED algorithm. The experimental setup of Table 6 remains largely consistent with that of Table 5,\\nwith the sole difference being an increase in the depth of the ResNet architecture from 18 to 50 layers. The results from\\n16\\n\\narXiv Template\\nA PREPRINT\\nFigure 10: Detection accuracy of the SWM-AED algorithm using different model layer architectures under various\\nadversarial attack methods.\\nTables 6 and 5 are integrated in Figure 10, which illustrates that a deeper network architecture significantly enhances\\nthe detection performance of the SWM-AED algorithm.\\nThis improvement can be attributed to two key factors: the algorithmic principles underlying SWM-ADE and the\\nintrinsic characteristics of deep neural networks. First, the SWM-ADE detection mechanism is highly dependent on the\\nmodel’s classification performance, which benefits from increased network depth. Second, deeper networks exhibit\\nsuperior feature extraction capabilities, allowing them to capture more fine-grained representations and construct denser,\\nmore discriminative feature distributions. This leads to greater stability and reliability in classification outcomes. As a\\nresult, utilizing deeper network models for computing Sliding Mask Confidence Entropy significantly improves the\\neffectiveness of the SWM-ADE algorithm in distinguishing between adversarial and clean samples.\\nThe Impact of Different Neural Networks on the SWM-ADE Detection Algorithm. To further assess the detection\\ncapability of the SWM-AED algorithm across different model architectures, the experimental scope was expanded to\\ninclude the VGG network, with results summarized in Table 7. By leveraging Tables 5 and 7 as the experimental group,\\nwhile controlling variables such as mask size and model accuracy, Figure 11 was generated. The experimental findings\\nreveal that the SWM-AED algorithm demonstrates robust detection performance when the Sliding Mask Confidence\\nEntropy (SMCE) is computed using the VGG model. This underscores the critical role of selecting high-performance\\ndeep neural networks as the foundation for SWM-AED detection.\\nWith the continuous advancements in deep learning technology, the adoption of more powerful architectures can\\nsignificantly enhance the SWM-AED algorithm’s capacity to detect adversarial examples, reflecting its potential for\\nself-improvement alongside technological progress. By continuously updating advanced models for SMCE computation,\\nthe performance of the SWM-AED algorithm can be incrementally refined, ensuring broad applicability across diverse\\nscenarios. Notably, the effectiveness of the SWM-AED algorithm primarily stems from its core logic—the inherent\\nvulnerability of adversarial examples—rather than being dependent on the unique characteristics of a specific model\\narchitecture. This highlights its remarkable generalizability and suitability for widespread deployment in practical\\nenvironments.\\n17\\n\\narXiv Template\\nA PREPRINT\\nFigure 11: Detection accuracy of the SWM-AED algorithm using different models under various adversarial attack\\nmethods.\\nFigure 12: The figure presents a performance comparison between the SWM-AED algorithm and existing adversarial\\ndefense methods, including AT (Madry et al., 2017), FAST-AT (Wong et al., 2020), and FREE-AT (Shafahi et al., 2019).\\nIt evaluates the defense success rates of these four methods against adversarial examples generated by PGD attacks on\\nthe ResNet-18 network architecture.\\n18\\n\\narXiv Template\\nA PREPRINT\\n4.3.4\\nExperimental Comparison with Classical Adversarial Defense Algorithms\\nIn this section, we perform a comparative analysis between the SWM-AED algorithm and classical adversarial defense\\nmethods, namely Adversarial Training (AT) and its variants, FAST-AT and FREE-AT. As shown in Figure 12, the\\nexperiments utilize the benchmark PGD adversarial attack algorithm and are conducted on the ResNet-18 classifier.\\nThe AT and its variants aim to defend against adversarial examples by retraining a robust ResNet-18 model, achieving a\\nresistance rate of approximately 47% against adversarial examples generated by the PGD algorithm. In contrast, the\\nSWM-AED algorithm leverages SMCE computation within a deep learning model, substantially improving the defense\\ncapability to 76%. These findings demonstrate that the SWM-AED algorithm achieves superior accuracy in detecting\\nand defending against adversarial examples.\\n5\\nDiscussion and Conclusion\\nThis study presents, for the first time, a systematic characterization of the intrinsic vulnerabilities of adversarial examples.\\nWhile adversarial attacks are typically understood as perturbation-based manipulations that induce misclassification,\\nwe demonstrate that such perturbations inherently disrupt the local stability of input images. Specifically, the added\\nnoise perturbs the pixel value distribution, resulting in fuzzier classification boundaries. Compared to clean inputs,\\nadversarial examples exhibit reduced semantic coherence, making classifiers more prone to prediction errors.\\nTo quantify this instability, we introduce a novel metric: Sliding Mask Confidence Entropy (SMCE), which evaluates\\nthe sensitivity of model predictions to localized occlusions. SMCE is computed by applying a sliding window across the\\nimage and aggregating the resulting confidence entropy values. This metric captures not only the structural consistency\\nof the input image but also the robustness of the classifier to spatially localized perturbations. Building on this insight,\\nwe propose a detection framework termed Sliding Window Mask–Adversarial Example Detection (SWM-AED), which\\nidentifies adversarial examples by assessing anomalies in their SMCE values.\\nThe SMCE metric and SWM-AED algorithm exemplify a broader principle of deriving essential properties from\\nobservable phenomena. By leveraging localized fluctuations in model confidence, our approach reveals distinctive\\nentropy patterns that differentiate adversarial examples from clean inputs. This capability enables robust adversarial\\ndetection without compromising classification accuracy, offering a pathway towards more secure and trustworthy AI\\nsystems. SWM-AED offers several significant innovations:\\n• High Practical Applicability: SWM-AED can be seamlessly integrated into a wide range of existing deep\\nneural network models without requiring architectural modifications. This makes it a lightweight and flexible\\ndefense mechanism that is readily deployable in real-world applications.\\n• Model-Aware Adaptability: The effectiveness of SWM-AED naturally improves with more advanced\\nclassifiers. As shown in our experiments, the SMCE metric—proposed for the first time in this work—becomes\\nmore discriminative when computed on stronger models, allowing the detection performance to scale with\\nmodel robustness and accuracy.\\n• Flexible and Future-Proof Design: Unlike traditional detection methods that rely on static heuristics, SWM-\\nAED leverages a dynamic detection mechanism grounded in a fundamental vulnerability of adversarial\\nexamples. This makes it inherently adaptable to future advances in model architecture and applicable across\\nvarious domains involving deep learning.\\nTheoretically, this study is the first to reveal and rigorously demonstrate that adversarial examples exhibit a heightened\\nsensitivity to occlusion compared to their corresponding benign counterparts. This finding addresses a previously\\noverlooked characteristic of adversarial examples and contributes new insights into their intrinsic weaknesses. We\\nbelieve this insight opens up new avenues for understanding the underlying mechanisms of adversarial vulnerability in\\ndeep neural networks (DNNs), potentially aiding in the development of more robust AI systems.\\nNotably, as demonstrated in this experiments, the detection performance of SMCE is closely tied to the robustness of\\nthe underlying classifier, making it a flexible and adaptive solution. This enables SWM-AED to be easily integrated into\\nexisting DNN-based systems across a wide range of domains. In contrast to traditional detection techniques, which often\\nrely on fixed statistical patterns or input transformations, our method dynamically benefits from advancements in model\\narchitecture—yielding increasingly discriminative detection signals as classifier quality improves. This adaptability\\nunderscores a crucial aspect often overlooked in current AI development: the overemphasis on accuracy at the cost of\\nsystemic security. True AI reliability requires a balanced approach that prioritizes both predictive performance and\\nrobustness. The SWM-AED algorithm effectively achieves this balance, enhancing both model robustness and security\\nwhile maintaining high accuracy.\\n19\\n\\narXiv Template\\nA PREPRINT\\nData availability\\nAll datasets used in this paper were obtained from public data sources and repositories. A complete list of the public data\\nsources with links is available via GitHub at https://github.com/dawei7777/SWM-AED/blob/master/README.\\nmd.\\nCode availability\\nThe Python code used to compare the SMCE values of adversarial examples and clean samples, visualize the vulnerability\\nof adversarial examples, and evaluate the detection accuracy of adversarial inputs using the SWM-AED algorithm is\\npublicly available on GitHub at https://github.com/dawei7777/SWM-AED, along with documentation, DOIs, and\\ncitations (Li et al., 2025a). The code is released under the MIT license without restriction.\\n20\\n\\narXiv Template\\nA PREPRINT\\nA\\nProof of Property 3.2\\nProperty 3.2. maximum value of SMCE\\nHSMCE(I) ≤log2 m\\n(A.1)\\nProof. In order to obtain the maximum value of HSMCE(I), We formulate the following constrained optimization\\nproblem:\\nmax\\npij\\n1\\nn\\nn\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed−\\nm\\nX\\nj=1\\npij log2(pij)\\n\\uf8f6\\n\\uf8f8\\n(A.2)\\ns.t.\\nm\\nX\\nj=1\\npij = 1\\nfor ∀i\\n(A.3)\\nLet us introduce Lagrange multipliers λi and construct the Lagrangian function as follows:\\nL(pij, λi) = 1\\nn\\nn\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed−\\nm\\nX\\nj=1\\npij log2(pij)\\n\\uf8f6\\n\\uf8f8+ 1\\nn\\nn\\nX\\ni=1\\nλi\\n\\uf8eb\\n\\uf8ed\\nm\\nX\\nj=1\\npij −1\\n\\uf8f6\\n\\uf8f8\\nSolve the equations:\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n∂L(pij, λi)\\n∂pij\\n= 0\\nm\\nX\\nj=1\\npij = 1\\nfor ∀\\ni\\nObtain:\\n∂L(pij, λi)\\n∂pij\\n= ∂[−pij log2(pij) + λipij]\\n∂pij\\n= −log2(pij) −pij ·\\n1\\npij ln 2 + λi\\n= −log2(pij) −\\n1\\nln 2 + λi\\n= 0\\nSo\\nlog2(pij) = λi −\\n1\\nln 2\\nThen\\npij = 2λi−\\n1\\nln 2\\n(A.4)\\nSubstituting pij = 2λi−\\n1\\nln 2 into Pm\\nj=1 pij = 1 gives:\\nm\\nX\\nj=1\\n2λi−\\n1\\nln 2 = m · 2λi−\\n1\\nln 2 = 1\\nSo\\n2λi−\\n1\\nln 2 = 1\\nm\\n(A.5)\\n21\\n\\narXiv Template\\nA PREPRINT\\nBy eq. (A.4) and (A.5),\\npij = 1\\nm\\n(A.6)\\nSubstituting pij = 1\\nm into HSMCE(I) yields:\\nmax HSMCE(I) = 1\\nn\\nn\\nX\\ni=1\\n\\uf8eb\\n\\uf8ed−\\nm\\nX\\nj=1\\n1\\nm log2\\n1\\nm\\n\\uf8f6\\n\\uf8f8= 1\\nn\\nn\\nX\\ni=1\\nlog2 m = log2 m\\nTherefore:\\nHSMCE(I) ≤log2 m\\n22\\n\\narXiv Template\\nA PREPRINT\\nReferences\\nBorkar, T., Heide, F., Karam, L., 2020. Defending against universal attacks through selective feature regeneration, in:\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 709–719.\\nBuades, A., Coll, B., Morel, J.M., 2005. A review of image denoising algorithms, with a new one. Multiscale modeling\\n& simulation 4, 490–530.\\nBui, M., Doan, T.P., Hong, K., Jung, S., 2025. Boosting black-box transferability of weak audio adversarial attacks\\nwith random masking, in: International Conference on Information Security Applications, pp. 96–108.\\nChakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., Mukhopadhyay, D., 2021. A survey on adversarial attacks and\\ndefences. CAAI Transactions on Intelligence Technology 6, 25–45.\\nCroce, F., Hein, M., 2020. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free\\nattacks, in: International conference on machine learning, PMLR. pp. 2206–2216.\\nFu, X., Ma, L., Zhang, L., 2025. Remove to regenerate: Boosting adversarial generalization with attack invariance. IEEE\\nTransactions on Circuits and Systems for Video Technology 35, 1999–2012. doi:doi:10.1109/TCSVT.2024.3487761.\\nGao, L., Zhang, Q., Song, J., Shen, H.T., 2020. Patch-wise++ perturbation for adversarial targeted attacks. arXiv arXiv:\\n2012.15503. URL: http://arxiv.org/abs/2012.15503.\\nGoodfellow, I.J., Shlens, J., Szegedy, C., 2014. Explaining and harnessing adversarial examples. arXiv URL:\\nhttps://arxiv.org/abs/1412.6572.\\nJi, Q., Wang, L., Shi, C., Hu, S., Chen, Y., Sun, L., 2023. Benchmarking and analyzing robust point cloud recognition:\\nBag of tricks for defending adversarial examples, in: Proceedings of the IEEE/CVF International Conference on\\nComputer Vision, pp. 4295–4304.\\nJin, Y., Zhang, X., Lou, J., Ma, X., Wang, Z., Chen, X., 2023. Explaining adversarial robustness of neural networks\\nfrom clustering effect perspective, in: Proceedings of the IEEE/CVF International Conference on Computer Vision,\\npp. 4522–4531.\\nKherchouche, A., Fezza, S.A., Hamidouche, W., Déforges, O., 2020. Detection of adversarial examples in deep neural\\nnetworks with natural scene statistics, in: 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–7.\\ndoi:doi:10.1109/IJCNN48605.2020.9206959.\\nKim, H., Kim, B.C., Lee, S., Kang, M., Nam, H., Park, S., Kwak, I.Y., Lee, J., 2024. Rapid: Robust multi-patch masker\\nusing channel-wise pooled variance with two-stage patch detection. Journal of King Saud University-Computer and\\nInformation Sciences 36, 102188.\\nKrizhevsky, A., Hinton, G., et al., 2009. Learning multiple layers of features from tiny images .\\nKurakin, A., Goodfellow, I.J., Bengio, S., 2018. Adversarial examples in the physical world, in: Artificial intelligence\\nsafety and security, pp. 99–112.\\nLe, Q., Chan, F., Ni, J., Yam, S., Lu, N., 2025. Defending against high-intensity adversarial perturbations in deep\\nneural networks: A robust swin transformer approach, in: 2025 International Conference on Artificial Intelligence in\\nInformation and Communication (ICAIIC), pp. 0270–0275. doi:doi:10.1109/ICAIIC64266.2025.10920873.\\nLee, B.K., Kim, J., Ro, Y.M., 2023. Mitigating adversarial vulnerability through causal parameter estimation by\\nadversarial double machine learning, in: Proceedings of the IEEE/CVF International Conference on Computer Vision,\\npp. 4499–4509.\\nLee, K., Lee, K., Lee, H., Shin, J., 2018. A simple unified framework for detecting out-of-distribution samples and\\nadversarial attacks, in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.),\\nAdvances in Neural Information Processing Systems.\\nLi, J., Xu, Y., Hu, Y., 2025a. Deep learning models are vulnerable, but adversarial examples are even more vulnerable.\\ndoi:doi:10.5281/zenodo.15508744.\\nLi, J., Xu, Y., Hu, Y., Ma, Y., Yin, X., 2025b. You only attack once: Single-step deepfool algorithm. Applied Sciences\\n15. doi:doi:10.3390/app15010302.\\nLi, W., Li, B., Nie, W., Wang, L., Liu, A.A., 2025c.\\nDiversified perturbation guided by optimal\\ntarget code for cross-modal adversarial attack.\\nInformation Processing & Management 62, 104214.\\ndoi:doi:https://doi.org/10.1016/j.ipm.2025.104214.\\nLiu, S., Lian, Z., Zhang, S., Xiao, L., 2025. Adversarial purification of information masking. Neurocomputing 621,\\n129214.\\nMa, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Schoenebeck, G., Song, D., Houle, M.E., Bailey, J., 2018.\\nCharacterizing adversarial subspaces using local intrinsic dimensionality. arXiv preprint arXiv:1801.02613 .\\n23\\n\\narXiv Template\\nA PREPRINT\\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A., 2017. Towards deep learning models resistant to\\nadversarial attacks. stat 1050.\\nMeng, D., Chen, H., 2017. Magnet: A two-pronged defense against adversarial examples, in: Proceedings of the 2017\\nACM SIGSAC Conference on Computer and Communications Security, Association for Computing Machinery, New\\nYork, NY, USA. p. 135–147. doi:doi:10.1145/3133956.3134057.\\nMoosavi-Dezfooli, S.M., Fawzi, A., Frossard, P., 2016. Deepfool: a simple and accurate method to fool deep neural\\nnetworks, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2574–2582.\\nMustafa, A., Khan, S.H., Hayat, M., Shen, J., Shao, L., 2020. Image super-resolution as a defense against adversarial\\nattacks. IEEE Transactions on Image Processing 29, 1711–1724. doi:doi:10.1109/TIP.2019.2940533.\\nNaseer, M., Khan, S., Hayat, M., Khan, F.S., Porikli, F., 2020. A self-supervised approach for adversarial robustness, in:\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 262–271.\\nPapernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z.B., Swami, A., 2017. Practical black-box attacks against\\nmachine learning, in: Proceedings of the 2017 ACM on Asia conference on computer and communications security,\\nAssociation for Computing Machinery, New York, NY, USA. pp. 506–519. doi:doi:10.1145/3052973.3053009.\\nPapernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.B., Swami, A., 2016. The limitations of deep learning\\nin adversarial settings, in: 2016 IEEE European symposium on security and privacy (EuroS&P), pp. 372–387.\\ndoi:doi:10.1109/EuroSP.2016.36.\\nPomponi, J., Scardapane, S., Uncini, A., 2022.\\nPixle:\\na fast and effective black-box attack based on re-\\narranging pixels, in:\\n2022 International Joint Conference on Neural Networks (IJCNN), IEEE. pp. 1–7.\\ndoi:doi:10.1109/IJCNN55064.2022.9892966.\\nQian, Y., He, S., Zhao, C., Sha, J., Wang, W., Wang, B., 2023. Lea2: A lightweight ensemble adversarial attack\\nvia non-overlapping vulnerable frequency regions, in: Proceedings of the IEEE/CVF International Conference on\\nComputer Vision, pp. 4510–4521.\\nRangwani, H., Aithal, S.K., Mishra, M., Jain, A., Radhakrishnan, V.B., 2022. A closer look at smoothness in domain\\nadversarial training, in: International conference on machine learning, PMLR. pp. 18378–18399.\\nRyu, G., Choi, D., 2024. Detection of adversarial attacks based on differences in image entropy. International Journal\\nof Information Security 23, 299–314. doi:doi:10.1007/s10207-023-00735-6.\\nShafahi, A., Najibi, M., Ghiasi, M.A., Xu, Z., Dickerson, J., Studer, C., Davis, L.S., Taylor, G., Goldstein, T., 2019.\\nAdversarial training for free!, in: Wallach, H., Larochelle, H., Beygelzimer, A., d\\'Alché-Buc, F., Fox, E., Garnett, R.\\n(Eds.), Advances in Neural Information Processing Systems.\\nSotgiu, A., Demontis, A., Melis, M., Biggio, B., Fumera, G., Feng, X., Roli, F., 2020. Deep neural rejection against\\nadversarial examples. EURASIP Journal on Information Security 2020, 1–10.\\nSui, C., Wang, A., Wang, H., Liu, H., Gong, Q., Yao, J., Hong, D., 2025.\\nIsdat:\\nAn image-semantic\\ndual adversarial training framework for robust image classification.\\nPattern Recognition 158, 110968.\\ndoi:doi:https://doi.org/10.1016/j.patcog.2024.110968.\\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R., 2013. Intriguing properties of\\nneural networks. arXiv preprint arXiv:1312.6199 .\\nTarchoun, B., Ben Khalifa, A., Mahjoub, M.A., Abu-Ghazaleh, N., Alouani, I., 2023. Jedi: Entropy-based localization\\nand removal of adversarial patches, in: Proceedings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, pp. 4087–4095.\\nTarchoun, B., Khalifa, A.B., Mahjoub, M.A., Abu-Ghazaleh, N., Alouani, I., 2024. An information-theoretic perspective\\nof physical adversarial patches. Neural Networks 179, 106590.\\nTian, C., Fei, L., Zheng, W., Xu, Y., Zuo, W., Lin, C.W., 2020. Deep learning on image denoising: An overview. Neural\\nNetworks 131, 251–275.\\nTramèr, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., McDaniel, P., 2017. Ensemble adversarial training:\\nAttacks and defenses. arXiv preprint arXiv:1705.07204 URL: http://arxiv.org/abs/1705.07204.\\nWest, M.T., Tsang, S.L., Low, J.S., Hill, C.D., Leckie, C., Hollenberg, L.C., Erfani, S.M., Usman, M., 2023. Towards\\nquantum enhanced adversarial robustness in machine learning. Nature Machine Intelligence 5, 581–589.\\nWong, E., Rice, L., Kolter, J.Z., 2020. Fast is better than free: Revisiting adversarial training. arXiv arXiv: 2001.03994.\\nURL: https://arxiv.org/abs/2001.03994.\\nWu, S., Wang, J., Zhao, J., Wang, Y., Liu, X., 2024. Napguard: Towards detecting naturalistic adversarial patches, in:\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24367–24376.\\n24\\n\\narXiv Template\\nA PREPRINT\\nXie, C., Tan, M., Gong, B., Yuille, A., Le, Q.V., 2020. Smooth adversarial training. arXiv preprint arXiv:2006.14536\\nURL: http://arxiv.org/abs/2006.14536.\\nXie, C., Wu, Y., Maaten, L.v.d., Yuille, A.L., He, K., 2019. Feature denoising for improving adversarial robustness, in:\\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 501–509.\\nXu, C., Zhang, C., Yang, Y., Yang, H., Bo, Y., Li, D., Zhang, R., 2023.\\nAccelerate adversarial training with\\nloss guided propagation for robust image classification. Information Processing & Management 60, 103143.\\ndoi:doi:https://doi.org/10.1016/j.ipm.2022.103143.\\nXu, W., Evans, D., Qi, Y., 2017. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv\\npreprint arXiv:1704.01155 .\\nYang, R., Sun, Q., Cao, H., Shen, C., Cai, J., Rong, D., 2025.\\n1 + 1 > 2: A dual-function defense frame-\\nwork for adversarial example mitigation.\\nIEEE Transactions on Information Forensics and Security , 1–\\n1.doi:doi:10.1109/TIFS.2025.3555186.\\nZhang, S., Song, Y., Wang, S., 2025. Fa-gan: Defense against adversarial attacks in automatic modulation recognition,\\nin: ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.\\n1–5. doi:doi:10.1109/ICASSP49660.2025.10890747.\\nZhang, X., Zhang, T., Zhang, Y., Liu, S., 2024a. Enhancing the transferability of adversarial attacks with stealth\\npreservation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\\n2915–2925.\\nZhang, Z., Li, X., Li, H., Dunkin, F., Li, B., Li, Z., 2024b. Dual-branch sparse self-learning with instance binding\\naugmentation for adversarial detection in remote sensing images. IEEE Transactions on Geoscience and Remote\\nSensing 62, 1–13. doi:doi:10.1109/TGRS.2024.3436841.\\nZhao, J., Xie, L., Gu, S., Qin, Z., Zhang, Y., Wang, Z., Hu, Y., 2025. Universal attention guided adversarial defense\\nusing feature pyramid and non-local mechanisms. Scientific Reports 15, 5237. doi:doi:10.1038/s41598-025-89267-8.\\nZhao, M., Zhang, L., Kong, Y., Yin, B., 2023. Fast adversarial training with smooth convergence, in: Proceedings of the\\nIEEE/CVF International Conference on Computer Vision, pp. 4720–4729.\\n25\\n',\n",
       " 'Self-adaptive weighting and sampling for physics-informed\\nneural networks\\nWenqian Chena, Amanda Howarda, Panos Stinisa,∗\\naAdvanced Computing, Mathematics and Data Division\\nPacific Northwest National Laboratory\\nRichland, WA 99354, USA\\nAbstract\\nPhysics-informed deep learning has emerged as a promising framework for solving\\npartial differential equations (PDEs). Nevertheless, training these models on com-\\nplex problems remains challenging, often leading to limited accuracy and efficiency.\\nIn this work, we introduce a hybrid adaptive sampling and weighting method to en-\\nhance the performance of physics-informed neural networks (PINNs). The adaptive\\nsampling component identifies training points in regions where the solution exhibits\\nrapid variation, while the adaptive weighting component balances the convergence\\nrate across training points. Numerical experiments show that applying only adaptive\\nsampling or only adaptive weighting is insufficient to consistently achieve accurate\\npredictions, particularly when training points are scarce. Since each method empha-\\nsizes different aspects of the solution, their effectiveness is problem dependent. By\\ncombining both strategies, the proposed framework consistently improves prediction\\naccuracy and training efficiency, offering a more robust approach for solving PDEs\\nwith PINNs.\\nKeywords:\\nSelf-adaptive weighting, Self-adaptive sampling, Physics-informed\\n∗Corresponding author\\nEmail address: panos.stinis@pnnl.gov (Panos Stinis )\\n1\\narXiv:2511.05452v1  [stat.ML]  7 Nov 2025\\n\\nneural networks\\n1. Introduction\\nPhysics-informed neural networks (PINNs) embed the governing partial differ-\\nential equation (PDE) into the loss function of a deep neural network, enforcing\\nphysical consistency alongside data fidelity [1]. In a standard PINN formulation, the\\nobjective is composed of a data term (for initial and boundary conditions) and a PDE\\nresidual term evaluated at collocation points in the domain. While this approach has\\nshown promise on a variety of forward and inverse PDE problems, training can be\\nchallenging: competing loss components may have wildly different scales, and uni-\\nform sampling of collocation points often fails to focus learning on regions where the\\nsolution exhibits sharp gradients or singular behavior [2–5].\\nTo address the multi-objective nature of PINN training due to competing loss\\nterms, a range of self-adaptive weighting strategies have been proposed. Gradient-\\nbased methods include learning rate annealing, where weights are updated inversely\\nto back-propagated gradients [6]. Time-structured approaches, such as causal train-\\ning, assign weights in a temporally ordered fashion for time-dependent problems [7].\\nResidual-based strategies are also popular: Liu and Wang [8] proposed a minimax\\nmethod with gradient descent for parameters and ascent for weights; McClenny and\\nBraga-Neto [5] extended this to point-wise weights; and auxiliary networks have been\\nused for point-wise weighting [9, 10]. Anagnostopoulos et al. [11] updated weights by\\nnormalized residuals. Lagrangian approaches adapt weights as multipliers for con-\\nstraints, including augmented Lagrangian methods (ALM) [12], adaptive ALM [13],\\ndual problem formulations [14], and point-wise multipliers [15]. Finally, kernel-based\\nstrategies such as neural tangent kernel (NTK) weighting [16, 17] and the conjugate\\nkernel (CK) [18] update weights according to kernel eigenvalue information.\\n2\\n\\nIn parallel, adaptive sampling methods in physics-informed machine learning\\nrefine training point distributions to better capture the structure of the solution.\\nResidual-based approaches [19, 20] place points in regions of large residuals or select\\ninformative samples during training. Importance sampling [21] chooses points ac-\\ncording to a proposal distribution derived from the loss function to improve efficiency.\\nResidual/gradient-based strategies [22] enhance both accuracy and stability. Deep\\nadaptive sampling [23] employs generative models to sample high-residual regions,\\nwhile annealed adaptive importance sampling [24] applies expectation–maximization\\nto handle multimodal loss landscapes. For singular or sharp solutions, the expected\\nimprovement refinement [25] incorporates residual gradients and boundary sampling,\\nand Gaussian mixture distribution-based sampling [26] uses residual-informed distri-\\nbutions for adaptive selection. Collectively, these methods improve convergence and\\naccuracy, though iterative point addition increases computational cost.\\nIn this work, we propose a framework that integrates adaptive weighting and\\nadaptive sampling for physics-informed machine learning. The two strategies ad-\\ndress complementary aspects of the training process: adaptive weighting balances\\nthe contributions of different loss components, ensuring that no part of the solu-\\ntion dominates or is neglected, while adaptive sampling redistributes training points\\ntoward regions that are more challenging to approximate, such as areas with large\\nresiduals or sharp gradients. By combining these mechanisms, the framework pro-\\nvides a more comprehensive treatment of training, simultaneously stabilizing op-\\ntimization and enhancing data efficiency. Numerical experiments demonstrate that\\nthe proposed method consistently achieves high prediction accuracy, highlighting the\\nbenefits of this complementary interaction.\\nThe rest of the paper is structured as follows. Section 2 reviews the concept\\nof PINNs and summarizes our previous work on adaptive weighting based on the\\n3\\n\\nbalanced residual decay rate (BRDR) [27]. Section 3.2 introduces the self-adaptive\\nsampling method based on residuals and discusses how to combine it with the adap-\\ntive weighting method. In Section 4 the proposed adaptive weighting and sampling\\nmethod is tested on four benchmark problems. Finally, some conclusions will be\\ndrawn in Section 5. To promote reproducibility and further research, the code and\\nall accompanying data will be made available upon publication.\\n2. Background\\n2.1. Physics-Informed Neural Networks\\nPhysics-informed neural networks (PINNs) are designed to approximate the solu-\\ntion of PDEs by minimizing a loss function that includes physics-based terms derived\\nfrom the governing equations. Let’s assume we are solving a general PDE subject to\\nboundary conditions (BCs):\\nN(u(x)) = 0,\\nx ∈Ω\\n(1)\\nB(u(x)) = 0,\\nx ∈∂Ω,\\n(2)\\nwhere N is a differential operator and u(x) is the solution we seek. The boundary\\ncondition is enforced by a general boundary operator B.\\nThe goal of the PINN is to approximate u(x) by a neural network uθ(x), where\\nθ represents the network parameters (weights and biases). The PINN loss function\\nusually consists of two main components,\\nLtotal = LPDE + LBC.\\n(3)\\nThe first component is the residual loss, which ensures that the neural network\\napproximation uθ(x) satisfies the PDE:\\nLPDE = Ex∈Ω\\n\\x02\\n|N(uθ(x))|2\\x03\\n≈1\\nNr\\nNr\\nX\\ni=1\\n|N(uθ(xi))|2 ,\\n(4)\\n4\\n\\nwhere Nr is the number of collocation points (randomly selected points in the domain\\nwhere the PDE is enforced) and xi are the coordinates of these points. The second\\ncomponent is the boundary condition loss, which ensures that the solution satisfies\\nthe general boundary conditions imposed by B. This loss is formulated as:\\nLBC = Ex∈∂Ω\\n\\x02\\n|B(uθ(xi))|2\\x03\\n≈1\\nNb\\nNb\\nX\\ni=1\\n|B(uθ(xi))|2 ,\\n(5)\\nwhere Nb is the number of boundary points.\\n2.2. Adaptive Weighting based on balanced residual decay rate\\nIn our recent work [27], we introduced a self-adaptive weighting method based on\\nthe balanced residual decay rate (BRDR). This approach assigns a pointwise adaptive\\nweight to each residual term, including both the governing equation and boundary\\ncondition residuals. The method dynamically adjusts these weights during training\\nto balance the convergence rates across all training points, thereby improving both\\naccuracy and efficiency in solving PDEs with PINNs. The weighted loss function is\\ndefined as follows:\\nL(θ; w, s) = s\\n \\n1\\nNR\\nNR\\nX\\ni=1\\nwi\\nRR2(xi\\nR) + 1\\nNB\\nNB\\nX\\ni=1\\nwi\\nBB2(xi\\nB)\\n!\\n(6)\\ns.t.\\nmean(w) :=\\nPNR\\ni=1 wi\\nR + PNB\\ni=1 wi\\nB\\nNR + NB\\n= 1\\n(7)\\nwhere wi\\nR > 0 is the weight assigned to the residual of each residual collocation point,\\nwi\\nB > 0 is the weight assigned to each boundary point, and w is the collection of\\nthese weights. The scale factor s is employed to scale all the weights, so that the\\nformulation could cover all kinds of possible weight distributions.\\nThe basic idea behind BRDR is to update weights based on two critical observa-\\ntions:\\n5\\n\\n1. Residuals at different training points may vary significantly across the domain.\\n2. The point with the smallest residual decay rate often dominates the convergence\\nspeed of the global solution.\\nTo quantify the speed of residual decay, we use the inverse residual decay rate\\n(IRDR), defined as:\\nIRDR =\\nR2(t)\\nq\\nR4(t) + ϵ\\n(8)\\nwhere R(t) represents the residual at iteration t, R4(t) is the exponential moving\\naverage of R4(t), and ϵ is a small constant to avoid division by 0. The exponential\\nmoving average R4(t) is updated using:\\nR4(t) = βc R4(t −1) + (1 −βc) R4(t)\\n(9)\\nwhere βc is a smoothing constant that controls the influence of past residuals.\\nSince a larger IRDR indicates a slower residual decay, we assign higher weights\\nto loss terms with larger IRDR values. To manage these weights dynamically during\\ntraining, we compute reference weights at each iteration t based on the relative IRDR\\nvalues with respect to their global mean. This strategy ensures that the mean of the\\nweights remains at 1, keeping the weights bounded throughout the training process:\\nwref\\nt\\n=\\nIRDRt\\nmean(IRDRt)\\n(10)\\nwhere IRDRt is the vector of IRDR values for all the training items at iteration t.\\nTo minimize noise and stabilize weight updates during training, the weights are\\nadjusted using an exponential moving average:\\nwt = βw wt−1 + (1 −βw) wref\\nt\\n(11)\\n6\\n\\nwhere βw is a smoothing factor that helps to smooth out fluctuations in the weights.\\nFor more details on the BRDR method, please refer to our previous work [27].\\n3. Adaptive Training in Physics-Informed Machine Learning\\nWhile adaptive weighting adjusts the importance of different training points,\\nit often requires a high density of points in regions with large gradients, leading\\nto a large number of residual points when using random sampling and increasing\\nsignificantly the computing cost for training.\\nTo reduce the number of training\\npoints, we develop an adaptive sampling method so that we can use fewer training\\npoints while achieving a higher training accuracy.\\n3.1. Issues with overfitting\\nHere we take a 1D perturbation equation as an example to showcase the possible\\nissues that can arise when using the adaptive weighting method with a small number\\nof training points. The 1D perturbation equation we use as an example is given by\\n−ϵ2d2u\\ndx2 + u(x) = 1,\\nx ∈[0, 1]\\nu(0) = u(1) = 0\\n(12)\\nwhere the parameter ϵ is set to 10−4. The analytical solution of this problem is given\\nby:\\nu(x) = 1 −e−x/ϵ + e(x−1)/ϵ\\n1 + e−1/ϵ\\n.\\n(13)\\nThe solution contains two thin boundary layers at x = 0 and x = 1, with the\\nboundary layer thickness is proportional to ϵ. The stiff gradient within the boundary\\nlayers presents a challenge to PINN training.\\nTo solve this problem with PINNs, we uniformly sample 128 training points within\\nthe domain. The PINN prediction is shown in Fig. 1. The residuals at the training\\n7\\n\\npoints are minimized to a very small magnitude (less than 10−10), yet the PINN\\nprediction shows a large deviation from the ground truth. This discrepancy arises\\nbecause the residuals at unseen points remain significant. In other words, the fixed\\ntraining points underestimate the average residuals, which ultimately leads to the\\nlarge error of the PINN prediction.\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nExact solution\\nPINN solution\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n−14\\n−12\\n−10\\n−8\\n−6\\n−4\\n−2\\n0\\nlog10(|residual|2)\\nResiduals at a ﬁne grid\\nResiduals at 128 training points\\nFigure 1: (Left) PINN prediction for the 1D perturbation equation, and (right) residuals at the\\ntraining points compared with those on a fine grid.\\n3.2. Self-adaptive sampling based on residuals\\nConsidering the overfitting issue discussed in Section 3.1, a straightforward rem-\\nedy is to introduce additional training points in regions where the residuals are large.\\nTo achieve this, we first compute the residuals R(x) over a set of candidate points.\\nNew training points are then randomly selected from these candidates according\\nto a probability distribution, as shown in Fig. 2. The probability of selection is\\nproportional to the square of the corresponding residuals:\\np(x) =\\nR2(x)\\nP\\nx R2(x)\\n(14)\\n8\\n\\nTo prevent overemphasizing points with extremely large residuals and underem-\\nphasizing those with extremely small residuals, we clip the residuals using:\\nR2\\nclipped(x) = max\\n\\x10\\nˆR2, min\\n\\x10\\nR2(x), γ ˆR2\\x11\\x11\\n(15)\\nwhere ˆR2 is the median value of R2(x) over the candidate points, and γ = 100 is a\\nscaling factor unless stated otherwise.\\nFigure 2: Schematic illustration of the implementation process for self-adaptive sampling based on\\nresiduals.\\nResampling is performed after a specified number of optimization steps. To pre-\\nvent oscillations caused by abruptly replacing all training points, only a fraction pu of\\nthe existing points are updated in each resampling cycle, while the remaining points\\nare retained.\\n3.3. Combination of Adaptive Weighting and Sampling\\nTo effectively combine adaptive weighting with adaptive sampling, it is neces-\\nsary to determine the weights, or weight-related quantities, for the newly introduced\\ntraining points. As described in Section 2.2, both the weights and R4 are updated\\nat every training step. When new points are added through adaptive sampling, their\\ncorresponding weights and R4 must also be computed.\\n9\\n\\nTo estimate these quantities at the new points, we employ inverse distance weight-\\ning (IDW) interpolation. This interpolation allows us to estimate the weights and\\nR4 for new points based on the existing training points, ensuring a smooth transition\\nin the weighting scheme when training points are updated.\\nwnew =\\nP\\ni\\nwi\\nri\\nP\\ni\\n1\\nri\\n,\\nR4new =\\nP\\ni\\nR4i\\nri\\nP\\ni\\n1\\nri\\n(16)\\nwhere:\\n• wnew and R4new are the weight and exponential moving average of R4 at the\\nnew point,\\n• wi and R4i are the corresponding quantities at old points,\\n• ri is the distance between the new point and the i-th old point.\\nThe combined algorithm is summarized in Algorithm 1. For adaptive weighting,\\nkey parameters include βc (set to 0.999) for smoothing the exponential moving av-\\nerage of R4 and βw (set to 0.999) for weight updates, both of which perform well\\nacross various PINN problems [27]. For adaptive sampling, γ (set to 100) controls\\nresidual clipping, pu (set to 0.2) is the fraction of points updated per resampling,\\nand Ns (set to 100) determines the update frequency, namely the interval between\\nresampling steps. Default values are found empirically to be effective but may be\\ntuned for specific problems; detailed analyses are provided in Appendix B.\\n10\\n\\nAlgorithm 1 Adaptive Weighting and Sampling\\nInput: Initial training points X0\\n1: for each training iteration t = 1, 2, . . . , T do\\n2:\\nAdaptive Weighting:\\n3:\\nCompute residuals Rt(x) at current training points Xt−1\\n4:\\nUpdate R4t(x) = βc R4t−1(x) + (1 −βc) R4\\nt(x)\\n5:\\nCalculate IRDR: IRDRt(x) =\\nR2\\nt(x)\\nq\\nR4t(x) + ϵ\\n6:\\nCompute reference weights: wref\\nt (x) =\\nIRDRt(x)\\nmean(IRDRt(x))\\n7:\\nUpdate weights: wt(x) = βw wt−1(x) + (1 −βw) wref\\nt (x)\\n8:\\nAdaptive Sampling:\\n9:\\nif t mod Ns = 0 then\\n10:\\nGenerate candidate points Xcand\\n11:\\nCompute residuals Rt(x) for x ∈Xcand\\n12:\\nCompute selection probabilities with clipped residuals:\\np(x) =\\nR2\\nclipped(x)\\nP\\nx∈Xcand R2\\nclipped(x),\\nwhere\\nR2\\nclipped(x) = max\\n\\x10\\nˆR2, min\\n\\x10\\nR2\\nt(x), γ ˆR2\\x11\\x11\\n13:\\nSelect pu|Xt−1| new points Xnew based on p(x)\\n14:\\nSelect pu|Xt−1| old points Xreplace randomly.\\n15:\\nIDW Interpolation for New Points:\\n16:\\nwt(Xnew) = IDW(wt(Xt−1), Xt−1, Xnew )\\n17:\\nR4t(Xnew) = IDW(R4t(Xt−1), Xt−1, Xnew )\\n18:\\nUpdate training points: Xt = (Xt−1 \\\\ Xreplace) ∪Xnew\\n19:\\nend if\\n20:\\nAssemble the total loss and perform backward propagation\\n21:\\nUpdate the parameters with gradient descent\\n22: end for\\n11\\n\\n4. Numerical results\\nTo validate the performance of the proposed self-adaptive weighting and sampling\\napproach in training PINNs, we test on four benchmark problems: the perturbation\\nequation, the Allen–Cahn equation, the Burgers equation, and the lid-driven cavity\\nflow. The prediction accuracy is assessed using the L2 relative error defined as\\nϵL2 = ∥u −uE∥2\\n∥uE∥2\\n(17)\\nwhere u and uE are the vectors of the predicted and reference solutions on the test\\nset, respectively.\\nIn our experiments, we employ the mFCN network architecture (see Appendix A)\\nwhich consists of six hidden layers with 128 neurons each. The hyperbolic tangent\\nfunction is utilized as the activation function throughout the network.\\nNetwork\\nparameters are initialized using Kaiming Uniform initialization [28]; specifically, for\\na layer with shape (out_features, in_features), the weights and biases are sampled\\nfrom U\\n\\x10\\n−\\n√\\nk,\\n√\\nk\\n\\x11\\nwith k = 1/in_features. All implementations are carried out\\nin PyTorch [29] and executed on a GPU cluster using a 32-bit single-precision data\\ntype on an NVIDIA® Tesla P100 GPU.\\nTo assess the impact of adaptive sampling and weighting strategies, Fig.\\n3\\npresents the PINN prediction errors and training costs for the four representative\\nproblems. Across all training methods, increasing the number of residual points gen-\\nerally reduces the prediction error, but the rate of improvement varies notably among\\nmethods and problems. For the perturbation and Burgers equations, the error de-\\ncreases steadily as the number of residual points (i.e., the batch size) increases. In\\ncontrast, for the Allen–Cahn equation, the error under adaptive sampling remains\\nnearly unchanged with larger batch sizes, while for the lid-driven cavity flow problem,\\n12\\n\\nadaptive weighting shows little sensitivity to batch size. These results demonstrate\\nthat the effectiveness of adaptive sampling and weighting is strongly problem depen-\\ndent, making it difficult to rely solely on either strategy for robust performance. By\\ncontrast, the combined approach consistently achieves the best accuracy across all\\ntested cases.\\nIn terms of computational cost, the combined approach increases training time\\nby less than 20% relative to the non-adaptive PINN under the same batch size and\\nmaximum iteration settings. More importantly, for a given target prediction error,\\nthe proposed method achieves results with substantially less training time than the\\nother strategies.\\nTo visualize the distribution of training points and their associated weights for\\nthe Allen–Cahn, Burgers, and lid-driven cavity flow problems, we plot scatter dia-\\ngrams of the training points colored by their weights, as shown in Figs. C.9–C.11 in\\nAppendix C. While these plots allow for some preliminary observations, they do not\\nclearly reveal the focus of different methods, particularly for the combined approach.\\nTo provide a more quantitative assessment, we employ kernel density estimation\\n(KDE) to evaluate the relative importance of different regions. Specifically, we use\\ngaussian_kde in SciPy, which by default applies Scott’s rule for bandwidth selection\\n[30], and incorporate the adaptive weights by assigning them as kernel weights. The\\ndetailed results will be discussed in the following subsections.\\n4.1. Perturbation problem\\nWe revisit the 1D perturbation problem introduced in Section 3.1 to evaluate\\nthe effectiveness of the proposed adaptive weighting and sampling strategies. The\\ntraining setup is summarized in Table 1. Figure 3 presents the prediction accuracy of\\nPINN training with and without adaptive weighting and sampling. For this problem,\\n13\\n\\n128\\n256\\n512\\n1024\\n2048\\n4096\\n8192\\nnResi\\n10−4\\n10−3\\n10−2\\n10−1\\nerror\\n2.4 × 10−2\\n2.8 × 10−2\\n3.2 × 10−2\\nsec/step\\nSAweight+SAsample\\nSAsample\\nSAweight\\nNo adaptivity\\n(a) Perturbation\\n1000\\n2000\\n5000\\n10000\\n25000\\nnResi\\n10−4\\n10−3\\n10−2\\n10−1\\n100\\nerror\\n10−1\\n6 × 10−2\\n2 × 10−1\\nsec/step\\nSAweight+SAsample\\nSAsample\\nSAweight\\nNo adaptivity\\n(b) Allen Cahn\\n1000\\n2000\\n4000\\n800010000\\nnResi\\n10−4\\n10−3\\n10−2\\n10−1\\n100\\nerror\\n3 × 10−2\\n4 × 10−2\\nsec/step\\nSAweight+SAsample\\nSAsample\\nSAweight\\nNo adaptivity\\n(c) Burgers\\n512\\n1024\\n2048\\n4096\\nnResi\\n10−3\\n10−2\\n10−1\\nerror\\n8.6 × 10−28.8 × 10−2 9 × 10−2 9.2 × 10−29.4 × 10−29.6 × 10−\\nsec/step\\nSAweight+SAsample\\nSAsample\\nSAweight\\nNo adaptivity\\n(d) Lid Driven\\nFigure 3:\\nPINN prediction errors (left) and training time cost (right) for the perturbation,\\nAllen–Cahn, Burgers’ equations, and lid-driven flow. Shaded areas denote the mean ± 2 stan-\\ndard deviations, calculated from 5 independent runs for each case.\\n14\\n\\nTable 1: The choice of location of the training points and the BRDR training setup for solving\\ndifferent problems with PINNs.\\nProblems\\nAllen–Cahn\\nPerturbation\\nBurgers\\nLid-Driven\\nPDE points\\nLatin Hypercube\\nUnifrom\\nLatin Hypercube\\nRandom\\nIC points\\nUniform\\n–\\nUniform\\n–\\nBC points\\n–\\nUniform\\nRandom\\nUniform\\nNetwork\\n[21]+[128]×6+[1]\\nmFCN\\ntanh\\n[2]+[128]×6+[1]\\nmFCN\\ntanh\\n[2]+[128]×6+[1]\\nmFCN\\ntanh\\n[2]+[128]×6+[3]\\nmFCN\\ntanh\\nAdam steps\\n3e5\\n1e5\\n4e4\\n8E4\\nAdam Learning rate\\n0.001 × 0.99t/750\\n0.005 × 0.99t/250\\n0.001 × 0.99t/100\\n0.001 × 0.99t/400\\n(βc, βw,) in BRDR\\n(0.999, 0.999)\\n(0.999, 0.999)\\n(0.999, 0.999)\\n(0.999, 0.999)\\n(p, γ, Ns) in SAAR\\n(0.2, 100, 100)\\n(0.2, 100, 100)\\n(0.2, 100, 100)\\n(0.2, 100, 100)\\nadaptive weighting alone provides little improvement, whereas adaptive sampling\\nsignificantly enhances training accuracy. The combination of adaptive weighting and\\nsampling yields the best performance. In particular, the combined approach achieves\\nan L2 relative prediction error of 0.01% with 2048 residual points. For comparison,\\nthe prediction error reported in Ref. [31] is 0.43%, and the standard PINN produces\\nan error of about 12.56%, both using a much larger batch size of 10,000.\\nFigure 4 shows the pointwise PINN prediction error together with the weight\\nand residual-point distributions obtained from the combined adaptive weighting and\\nsampling strategy with the batch size 2048. The PINN prediction closely matches the\\nexact solution, even within the thin boundary layers, where the absolute pointwise\\nerror remains below 10−3. Residual points are more densely concentrated in these\\nregions, demonstrating that adaptive sampling effectively allocates training efforts\\n15\\n\\nwhere they are most needed. Meanwhile, the weight distribution remains relatively\\nuniform across the domain, indicating that adaptive weighting successfully balances\\nthe contribution of each training point according to its residual decay rate. This\\nhomogeneous pattern confirms that the method mitigates the dominance of slow-\\nconverging points and promotes consistent convergence, consistent with the findings\\nof the previous study [27].\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nu\\n−0.0010\\n−0.0005\\n0.0000\\n0.0005\\n0.0010\\nExact\\nPrediction error\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nx\\n0\\n5\\n10\\n15\\nweight\\n0\\n20\\n40\\n60\\n80\\n100\\n120\\nPoint number\\nweight\\nPoint number\\nFigure 4: Perturbation problem: (top) PINN prediction errors and exact solution and (bottom)\\nweight distribution and residual point distribution.\\n4.2. Allen-Cahn equation\\nThe Allen-Cahn equation is defined as follows:\\n∂u\\n∂t −5(u −u3) −D∂2u\\n∂x2 = 0,\\n(x, t) ∈[−1, 1] × [0, 1]\\nu(x, 0) = x2 cos(πx),\\nx ∈[−1, 1]\\nu(±1, t) = 0,\\nt ∈[0, 1]\\n(18)\\nwhere the viscosity D = 1E −4 is considered.\\nAs adopted in reference [11, 27], we use a Fourier feature transformation on x\\nto enhance the network model’s ability to approximate periodic functions. While\\n16\\n\\nthe Allen-Cahn equation does not explicitly have periodic boundary conditions, this\\ntransformation helps improve the model’s expressiveness. With 10 Fourier modes,\\nthe two-dimensional input x = (x, t) is expanded into a 21-dimensional feature vector\\nbx, which is then fed into the network through the following mapping:\\nbx = γ(x) = [sin(πBx), cos(πBx), t]T ,\\n(19)\\nwhere B = [1, . . . , 10]T.\\nFigure 3 illustrates the prediction error from PINN training with and without\\nadaptive weighting and sampling. For this problem, increasing the batch size under\\nadaptive sampling alone yields no significant improvement. In contrast, adaptive\\nweighting leads to a clear reduction in training error as the batch size increases. The\\ncombined use of adaptive weighting and sampling consistently achieves the lowest\\nprediction error.\\nFigure 5 shows the weighted density estimation for the Allen–Cahn equation.\\nWith adaptive sampling, the training points become more concentrated in large-\\ngradient regions, enabling more effective learning, particularly for smaller batch sizes.\\nIn contrast, solely adaptive weighting fails to identify these large-gradient regions\\nwhen the batch size is small, requiring larger batch sizes to be effective. Moreover,\\nadaptive weighting places greater emphasis on the initial condition, as indicated by\\nthe roughly decreasing density along the t dimension for batch sizes n2–n4. While\\nadaptive sampling effectively captures large-gradient regions, the combined strategy\\nblends the strengths of both methods, focusing simultaneously on the initial condition\\nand the large-gradient regions, thereby achieving improved prediction accuracy.\\n17\\n\\nx\\ny\\nExact solution\\n-1.0\\n-0.8\\n-0.5\\n-0.2\\n0.0\\n0.2\\n0.5\\n0.8\\n1.0\\n(n1)\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.6\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n0.1\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\n(n2)\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.6\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.6\\n0.7\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n0.1\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\n(n3)\\n0.2\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.6\\n0.7\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\n0.7\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n0.1\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\nSAweight+SAsample\\n(n4)\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.6\\n0.7\\nSAsample\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nSAweight\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\nNo adaptivity\\n0.1\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\nFigure 5: Weighted density estimation for Allen-Cahn equation. “n1-n4\" denotes the number of\\ntraining points 2000, 5000, 10000, 25000, respectively.\\n18\\n\\n4.3. Burgers equation\\nThe Burgers equation is defined as follows:\\n∂u\\n∂t + u∂u\\n∂x −v∂2u\\n∂x2 = 0,\\n(x, t) ∈[−1, 1] × [0, 1]\\nu(x, 0) = −sin(πx),\\nx ∈[−1, 1]\\nu(±1, t) = 0,\\nt ∈[0, 1]\\n(20)\\nwhere u is the flow velocity, and the viscosity of the fluid v = 0.01/π is considered.\\nFigure 3 illustrates the prediction accuracy from PINN training with/without\\nadaptive weighting and with/without adaptive sampling. For this problem, no sig-\\nnificant improvements from increasing batch size is gained from solely adaptive sam-\\npling. On the contrary, the training error of adaptive weighing decreases with the\\nincrease of batch size. The combination of adaptive weighting and adaptive sampling\\nconsistently achieves the lowest prediction error.\\nFigure 6 presents the weighted density estimation for the Burgers’ equation.\\nFor solely adaptive sampling, the density pattern—concentrated near the shock re-\\ngion—remains largely unaffected by batch size. In contrast, under solely adaptive\\nweighting, the density distribution varies considerably with batch size: when the\\nbatch size is 2000, the density is concentrated near the initial condition, but as the\\nbatch size increases, it gradually shifts toward the shock region. Compared with\\nsolely adaptive sampling, the combined approach places greater emphasis on the ini-\\ntial condition, an effect inherited from adaptive weighting, which ultimately leads to\\nimproved prediction accuracy.\\n4.4. Steady lid-driven cavity flow problem\\nIn this subsection, the lid-driven cavity flow problem is used to test the per-\\nformance of the proposed MF approach and also the influence of several hyper-\\n19\\n\\nx\\nt\\nExact solution\\n-1.0\\n-0.8\\n-0.5\\n-0.2\\n0.0\\n0.2\\n0.5\\n0.8\\n1.0\\n(n1)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n0.1\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\n(n2)\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\n(n3)\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n1.8\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\n0.1\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\nSAweight+SAsample\\n(n4)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\nSAsample\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\nSAweight\\n0.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\nNo adaptivity\\n0.1\\n0.1\\n0.2\\n0.2\\n0.3\\n0.4\\n0.4\\n0.5\\n0.5\\n0.6\\nFigure 6: Weighted density estimation for Burgers equation. “n1-n4\" denotes the number of training\\npoints 2000, 4000, 8000, and 10000, respectively.\\n20\\n\\nparameters. The flow enclosed in a square cavity Ω= [0, 1]2 is described by the\\n(non-dimensional) incompressible Navier-Stokes equations\\n\\uf8f1\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f4\\n\\uf8f3\\n∇· u = 0,\\nx ∈Ω\\nu · ∇u = −∇p + 1\\nRe∇2u,\\nx ∈Ω\\nu(x) = (uw(x), 0),\\nx ∈Γ1\\nu(x) = 0,\\nx ∈Γ2\\n,\\n(21)\\nwhere u = (u, v) is the velocity in the Cartesian coordinate system x = (x, y), p is\\nthe pressure and Re is the Reynolds number. The boundary is ∂Ω= Γ1 ∪Γ2, where\\nΓ1 represents the top moving lid and Γ2 represents the other three static non-slip\\nwalls. uw is the driving velocity of the moving lid. To overcome the singularity at\\nthe two upper corner points where the moving lid meets the two stationary vertical\\nwalls, a zero-corner-velocity profile uw is employed [32]:\\nuw(x) = 16x2(1 −x)2\\n(22)\\nFigure 3 illustrates the prediction accuracy from PINN training with/without\\nadaptive weighting and with/without adaptive sampling. For this problem, no signif-\\nicant improvements from increasing batch size is gained from solely adaptive weight-\\ning. On the contrary, the training error of adaptive sampling decreases with the\\nincrease of batch size. The combination of adaptive weighting and adaptive sam-\\npling consistently achieves the lowest prediction error.\\nFigure 7 shows the weighted density estimation for the lid-driven cavity flow prob-\\nlem. Across different batch sizes, the solely adaptive sampling method concentrates\\non the top-right corner, where the moving lid meets the stationary wall and gener-\\nates large gradients. In contrast, adaptive weighting places greater emphasis on the\\ntwo bottom corners, where resolving the secondary vortices is crucial for accurately\\n21\\n\\ncapturing the flow structures. The combined strategy effectively incorporates both\\naspects, focusing on all critical regions and thereby improving overall performance.\\nx\\ny\\nExact solution: u\\n-0.4\\n-0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n(n1)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n0.0\\n0.2\\n0.5\\n0.8\\n1.0\\n1.2\\n1.5\\n1.8\\n2.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\n(n2)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n0.0\\n0.2\\n0.5\\n0.8\\n1.0\\n1.2\\n1.5\\n1.8\\n2.0\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\n1.5\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n(n3)\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n0.0\\n0.4\\n0.8\\n1.2\\n1.6\\n2.0\\n2.4\\n2.8\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\n1.5\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\nSAweight+SAsample\\n(n4)\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\n1.4\\nSAweight\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\nSAsample\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\n1.4\\n1.6\\n1.8\\nNo adaptivity\\n0.2\\n0.3\\n0.5\\n0.6\\n0.8\\n0.9\\n1.1\\n1.2\\nFigure 7: Weighted density estimation for the lid-driven flow.\\n“n1-n4\" denotes the number of\\ntraining points 512, 1024, 2048, 4096, respectively.\\n22\\n\\n5. Conclusion\\nIn plain PINNs, the loss function is defined as a linear combination of the squared\\nresiduals from the PDEs and boundary conditions evaluated at a set of training\\npoints. However, plain PINNs often struggle with problems involving large gradi-\\nents. Adaptive weighting methods have been shown to mitigate this issue, but their\\neffectiveness is limited, particularly when the training points are insufficient to resolve\\nlarge-gradient regions. Adaptive sampling offers a natural complement by directing\\ntraining points toward regions of the domain where the structure of the solution is\\nmore difficult to capture accurately.\\nIn this work, we propose a self-adaptive sampling method based on residuals as a\\ncomplement to our previously developed adaptive weighting approach. In the com-\\nbined framework, more training points are allocated to regions with large gradients,\\nwhile larger weights are assigned to points where residuals decay slowly. The ef-\\nfectiveness of the proposed strategy is evaluated on four benchmark problems. The\\nresults show that neither adaptive weighting nor adaptive sampling alone is sufficient\\nto ensure robust performance across all problems. By contrast, their combination\\nconsistently yields superior prediction accuracy.\\nAcknowledgments\\nThe work is supported by the U.S. Department of Energy, Advanced Scien-\\ntific Computing Research program, under the Scalable, Efficient and Accelerated\\nCausal Reasoning Operators, Graphs and Spikes for Earth and Embedded Systems\\n(SEA-CROGS) project (Project No. 80278). Pacific Northwest National Laboratory\\n(PNNL) is a multi-program national laboratory operated for the U.S. Department\\n23\\n\\nof Energy (DOE) by Battelle Memorial Institute under Contract No. DE-AC05-\\n76RL01830.\\nAppendix A. Network architectures\\nThe modified fully-connected network (mFCN) is introduced in the reference[6],\\nand has demonstrated to be more effective than the standard fully-connected neural\\nnetwork. A mFCN maps the input x to the output y. Generally, a mFCN consists\\nof an input layer, L hidden layers and an output layer. The l-th layer has nl neurons,\\nwhere l = 0, 1, ..L, L + 1 denotes the input layer, first hidden layer,..., L-th hidden\\nlayer and the output layer, respectively. Note that the number of neurons of each\\nhidden layer is the same, i.e., n1 = n2 = ... = nL. The forward propagation, i.e. the\\nfunction y = fθ(x), is defined as follows\\nU = ϕ(WUx + bU)\\nV = ϕ(WV x + bV )\\nH1 = ϕ(W1x + b1)\\nZl = ϕ(WlHl−1 + bl),\\n2 ≤l ≤L\\nHl = (1 −Zl) ⊙U + Zl ⊙V,\\n2 ≤l ≤L\\nfθ(x) = WL+1HL + bL+1\\n,\\n(A.1)\\nwhere ϕ(•) is a point-wise activation and ⊙denotes point-wise multiplication. The\\ntraining parameter in the network is θ = {WU, WV , bU, bV , W1:L+1, b1:L+1}.\\nAppendix B. Hyperparameter study for adaptive sampling\\nThe adaptive sampling method involves three key hyperparameters: the fraction\\nof points to be updated pu, the clipping parameter γ, and the update frequency Ns.\\n24\\n\\nTo investigate their influence on training performance, we conduct a hyperparameter\\nstudy using the Burgers equation as a test case, as described in Section 4.3. To study\\nthe effect of each hyperparameter independently, we vary one while keeping the others\\nfixed at pu = 0.2, γ = 100, and Ns = 100. The other training settings are consistent\\nwith those in Section 4.3. Note that all the training cases share the same random\\nseed to ensure that the initial network parameters and the initial set of residual\\npoints are identical.\\nThe prediction errors as well as the training time cost for different hyperparame-\\nter values are presented in Fig. B.8. For the update frequency Ns shown in Fig. B.8\\n(a), the prediction error roughly increases with Ns, while the training time per step\\ndecreases. This suggests that more frequent updates of training points are beneficial\\nfor improving prediction accuracy, albeit at a modest increase in computational cost.\\nFor a compromise between accuracy and efficiency, we select Ns = 100 as the default\\nvalue. For the clipping parameter γ shown in Fig. B.8 (b), the prediction error\\nexhibits an approximately concave trend, with the lowest error occurring at about\\nγ = 100. This suggests that clipping is necessary to prevent excessive focus on a\\nfew points with very large residuals, which can lead to overfitting. However, overly\\naggressive clipping (i.e., too small γ) will diminish the effectiveness of adaptive sam-\\npling, making it similar to random sampling. Therefore, we choose γ = 100 as the\\ndefault value. The training time per step remains relatively constant across differ-\\nent γ values with only slight decrease with increasing γ. For the fraction of points\\nupdated pu shown in Fig. B.8 (c), the prediction error is very large when pu is too\\nsmall (e.g., pu = 0.05), as it will get close to non-adaptive sampling. As pu increases,\\nit almost remains nearly unchanged with an approximately increasing trend with pu.\\nFor a detailed comparison, Fig. B.8 (d) shows the raw and smoothed prediction error\\nhistories for three selected pu. The raw error curves are quite noisy, making it diffi-\\n25\\n\\ncult to discern clear trends. To enhance visual clarity, we apply a moving minimum\\nfilter with a window size of 1500 training steps to smooth the error histories. The\\nsmoothed curves reveal that pu = 0.2 and pu = 0.6 yield similar performance, while\\npu = 1 results in significantly higher errors, especially at the final stage of training.\\nThis indicates that updating all points at each update step is not optimal, as it may\\nlead to excessive fluctuations in the training set, hindering convergence especially\\nwhen the prediction error is already low. The training time per step remains nearly\\nconstant across different pu values. Based on these observations, we recommend to\\nuse pu ∈[0.2, 0.6] and select pu = 0.2 as the default value.\\n26\\n\\nFigure B.8: Hyperparameter study of adaptive sampling for the Burgers equation. The prediction\\nerror and training time per step are shown as functions of (a) the update frequency Ns, (b) the\\nclipping parameter γ, and (c) the fraction of points updated pu. Subfigure (d) compares the raw\\n(left) and smoothed (right) prediction error histories for different pu. The smoothing is performed\\nusing a moving minimum filter with a window size of 1500 training steps to improve visual clarity\\nand facilitate comparison.\\n27\\n\\nAppendix C. Point and weight distribution after training\\nThe distributions of training points and weights after applying both adaptive\\nsampling and adaptive weighting are illustrated in Figs. C.9, C.10, and C.11 for the\\nAllen–Cahn equation, Burgers equation, and lid-driven cavity flow problem, respec-\\ntively. The results reveal distinct patterns in relation to the exact solutions. For the\\nAllen–Cahn and Burgers equations, the training points are concentrated in regions\\nwith large gradients, while for the lid-driven cavity flow, they are concentrated near\\nthe walls. In contrast, the weight distributions appear nearly uniform across the\\ndomains for all three problems.\\n−1.00\\n−0.75\\n−0.50\\n−0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n2\\n4\\n6\\n8\\nFigure C.9: Allen-Cahn equation: (left)Exact solution and (right) weight distribution (color) over\\nthe residual points (scatter).\\n−1.00\\n−0.75\\n−0.50\\n−0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\nFigure C.10: Burgers equation: (left)PINN prediction errors and exact solution and (right) weight\\ndistribution (color) over the residual points (scatter).\\n28\\n\\nExact solution: p\\n−0.050\\n−0.025\\n0.000\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\nExact solution: u\\n−0.4\\n−0.2\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nExact solution: v\\n−0.5\\n−0.4\\n−0.3\\n−0.2\\n−0.1\\n0.0\\n0.1\\n0.2\\n0.3\\nWeight distribution\\n2\\n4\\n6\\n8\\n10\\nFigure C.11: Lid-driven cavity flow: (left)PINN prediction errors and exact solution and (right)\\nweight distribution (color) over the residual points (scatter).\\nReferences\\n[1] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-informed neural networks:\\nA deep learning framework for solving forward and inverse problems involving\\nnonlinear partial differential equations, Journal of Computational physics 378\\n(2019) 686–707.\\n[2] S. Wang, B. Yue, T. Xu, G. E. Karniadakis, Understanding and mitigating\\ngradient pathologies in physics-informed neural networks, Journal of Computa-\\ntional Physics 423 (2020) 109381.\\n[3] L. Gao, M. Wang, L. Zhang, Adaptive loss balancing for effective physics-\\ninformed neural network training, Proceedings of the AAAI Conference on Ar-\\ntificial Intelligence 35 (5) (2021) 456–464.\\n29\\n\\n[4] C. L. Wight, J. Zhao, Solving allen-cahn and cahn-hilliard equations using the\\nadaptive physics informed neural networks, arXiv preprint arXiv:2007.04542\\n(2020).\\n[5] L. McClenny, U. Braga-Neto, Self-adaptive physics-informed neural networks\\nusing a soft attention mechanism, arXiv preprint arXiv:2009.04544 (2020).\\n[6] S. Wang, Y. Teng, P. Perdikaris, Understanding and mitigating gradient flow\\npathologies in physics-informed neural networks, SIAM Journal on Scientific\\nComputing 43 (5) (2021) A3055–A3081.\\n[7] S. Wang, S. Sankaran, P. Perdikaris, Respecting causality for training physics-\\ninformed neural networks, Computer Methods in Applied Mechanics and Engi-\\nneering 421 (2024) 116813.\\n[8] D. Liu, Y. Wang, A dual-dimer method for training physics-constrained neural\\nnetworks with minimax architecture, Neural Networks 136 (2021) 112–125.\\n[9] Y. Song, H. Wang, H. Yang, M. L. Taccari, X. Chen, Loss-attentional physics-\\ninformed neural networks, Journal of Computational Physics 501 (2024) 112781.\\n[10] G. Zhang, H. Yang, F. Zhu, Y. Chen, et al., Dasa-pinns: Differentiable ad-\\nversarial self-adaptive pointwise weighting scheme for physics-informed neural\\nnetworks, SSRN (2023).\\n[11] S. J. Anagnostopoulos, J. D. Toscano, N. Stergiopulos, G. E. Karniadakis,\\nResidual-based attention in physics-informed neural networks, Computer Meth-\\nods in Applied Mechanics and Engineering 421 (2024) 116805.\\n30\\n\\n[12] S. Basir, I. Senocak, Physics and equality constrained artificial neural networks:\\nApplication to forward and inverse problems with multi-fidelity data fusion,\\nJournal of Computational Physics 463 (2022) 111301.\\n[13] S. Basir, I. Senocak, An adaptive augmented lagrangian method for train-\\ning physics and equality constrained artificial neural networks, arXiv preprint\\narXiv:2306.04904 (2023).\\n[14] S. Basir, Investigating and mitigating failure modes in physics-informed neu-\\nral networks (pinns), Communications in Computational Physics 33 (5) (2023)\\n1240–1269.\\n[15] H. Son, S. W. Cho, H. J. Hwang, Enhanced physics-informed neural networks\\nwith augmented lagrangian relaxation method (al-pinns), Neurocomputing 548\\n(2023) 126424.\\n[16] S. Wang, X. Yu, P. Perdikaris, When and why pinns fail to train: A neural tan-\\ngent kernel perspective, Journal of Computational Physics 449 (2022) 110768.\\n[17] S. Wang, H. Wang, P. Perdikaris, Improved architectures and training algo-\\nrithms for deep operator networks, Journal of Scientific Computing 92 (2) (2022)\\n35.\\n[18] A. A. Howard, S. Qadeer, A. W. Engel, A. Tsou, M. Vargas, T. Chiang, P. Stinis,\\nThe conjugate kernel for efficient training of physics-informed deep operator\\nnetworks, in: ICLR 2024 Workshop on AI4DifferentialEquations In Science.\\n[19] L. Lu, X. Meng, Z. Mao, G. E. Karniadakis, Deepxde: A deep learning library\\nfor solving differential equations, Journal of Computational Physics 429 (2021)\\n109926.\\n31\\n\\n[20] W. Gao, C. Wang, Active learning based sampling for high-dimensional nonlin-\\near partial differential equations, Journal of Computational Physics 475 (2023)\\n111848.\\n[21] M. A. Nabian, R. J. Gladstone, H. Meidani, Efficient training of physics-\\ninformed neural networks via importance sampling, Computer-Aided Civil and\\nInfrastructure Engineering 36 (8) (2021) 962–977.\\n[22] Z. Mao, X. Meng, Physics-informed neural networks with residual/gradient-\\nbased adaptive sampling methods for solving partial differential equations with\\nsharp solutions, Applied Mathematics and Mechanics 44 (7) (2023) 1069–1084.\\n[23] K. Tang, X. Wan, C. Yang, Das-pinns: A deep adaptive sampling method\\nfor solving high-dimensional partial differential equations, Journal of Computa-\\ntional Physics 476 (2023) 111868.\\n[24] Z. Zhang, J. Li, B. Liu, Annealed adaptive importance sampling method in pinns\\nfor solving high dimensional partial differential equations, Journal of Computa-\\ntional Physics 521 (2025) 113561.\\n[25] Y. Liu, L. Chen, J. Ding, Y. Chen, An adaptive sampling method based on\\nexpected improvement function and residual gradient in pinns, IEEE Access 12\\n(2024) 92130–92141.\\n[26] Y. Jiao, D. Li, X. Lu, J. Z. Yang, C. Yuan, A gaussian mixture distribution-based\\nadaptive sampling method for physics-informed neural networks, Engineering\\nApplications of Artificial Intelligence 135 (2024) 108770.\\n[27] W. Chen, A. A. Howard, P. Stinis, Self-adaptive weights based on balanced\\n32\\n\\nresidual decay rate for physics-informed neural networks and deep operator net-\\nworks, Journal of Computational Physics (2025) 114226.\\n[28] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectifiers: Surpassing human-\\nlevel performance on imagenet classification, in: Proceedings of the IEEE inter-\\nnational conference on computer vision, 2015, pp. 1026–1034.\\n[29] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,\\nZ. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imperative style, high-\\nperformance deep learning library, Advances in neural information processing\\nsystems 32 (2019).\\n[30] D. W. Scott, Multivariate Density Estimation: Theory, Practice, and Visualiza-\\ntion, John Wiley & Sons, New York, 1992.\\n[31] Z. Fang, S. Wang, P. Perdikaris, Ensemble learning for physics informed neu-\\nral networks: A gradient boosting approach, arXiv preprint arXiv:2302.13143\\n(2023).\\n[32] W. Chen, Y. Ju, C. Zhang, A multidomain multigrid pseudospectral method\\nfor incompressible flows, Numerical Heat Transfer, Part B: Fundamentals 74 (1)\\n(2018) 415–431.\\n33\\n',\n",
       " 'Multimodal Deep Learning for Prediction of\\nProgression-Free Survival in Patients with\\nNeuroendocrine Tumors Undergoing 177Lu-based\\nPeptide Receptor Radionuclide Therapy\\nSimon Baur1, Tristan Ruhwedel2, Ekin B¨oke1, Zuzanna Kobus3,4, Gergana\\nLishkova5, Christoph Wetz2, Holger Amthauer2, Christoph Roderburg6, Frank\\nTacke3, Julian M. Rogasch2, Wojciech Samek1,7,8, Henning Jann3, Jackie Ma1,\\nand Johannes Eschrich3,9\\n1 Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, 10587\\nBerlin, Germany\\n2 Department of Nuclear Medicine, Charit´e—Universit¨atsmedizin Berlin\\n3 Department of Hepatology and Gastroenterology, Charit´e—Universit¨atsmedizin\\nBerlin\\n4 Division of Interventional Radiology, Department of Radiology, Memorial Sloan\\nKettering Cancer Center\\n5 Department of Endocrinology and Metabolism, Charit´e—Universit¨atsmedizin\\nBerlin\\n6 Clinic for Gastroenterology, Hepatology and Infectious Diseases, University\\nHospital D¨usseldorf, Medical Faculty of Heinrich Heine University D¨usseldorf\\n7 BIFOLD −Berlin Institute for the Foundations of Learning and Data\\n8 Department of Electrical Engineering and Computer Science, Technische\\nUniversit¨at Berlin\\n9 Berlin Institute of Health at Charit´e – Universit¨atsmedizin Berlin\\nAbstract. Peptide receptor radionuclide therapy (PRRT) is an estab-\\nlished treatment for metastatic neuroendocrine tumors (NETs), yet long-\\nterm disease control occurs only in a subset of patients. Predicting progression-\\nfree survival (PFS) could support individualized treatment planning.\\nThis study evaluates laboratory, imaging, and multimodal deep learn-\\ning models for PFS prediction in PRRT-treated patients.\\nMethods In this retrospective, single-center study 116 patients with\\nmetastatic NETs undergoing [177Lu]Lu-DOTATOC were included. Clin-\\nical characteristics, laboratory values, and pretherapeutic somatostatin\\nreceptor positron emission tomography/computed tomographies (SR-\\nPET/CT) were collected. Seven models were trained to classify low- vs.\\nhigh-PFS groups, including unimodal (laboratory, SR-PET, or CT) and\\nmultimodal fusion approaches. Performance was assessed via repeated 3-\\nfold cross-validation with area under the receiver operating characteristic\\ncurve (AUROC) and area under the precision-recall curve (AUPRC). Ex-\\nplainability was evaluated by feature importance analysis and gradient\\nmaps.\\nResults Forty-two patients (36%) had short PFS (≤1 year), 74 patients\\nlong PFS (>1 year). Groups were similar in most characteristics, except\\narXiv:2511.05169v1  [cs.LG]  7 Nov 2025\\n\\n2\\nBaur, Eschrich et al.\\nfor higher baseline chromogranin A (p = 0.003), elevated γ-GT (p =\\n0.002), and fewer PRRT cycles (p < 0.001) in short-PFS patients. The\\nRandom Forest model trained only on laboratory biomarkers reached an\\nAUROC of 0.59±0.02. Unimodal three-dimensional convolutional neural\\nnetworks using SR-PET or CT performed worse (AUROC 0.42±0.03 and\\n0.54 ± 0.01, respectively). A multimodal fusion model laboratory values,\\nSR-PET, and CT -augmented with a pretrained CT branch - achieved\\nthe best results (AUROC 0.72 ± 0.01, AUPRC 0.80 ± 0.01).\\nConclusion Multimodal deep learning combining SR-PET, CT, and\\nlaboratory biomarkers outperformed unimodal approaches for PFS pre-\\ndiction after PRRT. Upon external validation, such models may support\\nrisk-adapted follow-up strategies.\\n1\\nIntroduction\\nNeuroendocrine tumors (NETs) arise from neuroendocrine cells and represent a\\nheterogeneous group of neoplasms with variable biological behavior and clinical\\npresentation [1]. Although classified as rare, the reported incidence of NETs has\\nbeen steadily increasing over recent decades [2]. Most frequently, NETs originate\\nin the gastrointestinal tract or pancreas, collectively referred to as gastroen-\\nteropancreatic neuroendocrine tumors (GEP-NETs) [3]. In a subset of patients,\\nthe primary tumor site remains unknown despite extensive diagnostic work-up,\\nreferred to as NETs of unknown primary (CUP-NETs) [4]. For patients with ad-\\nvanced disease, available treatment options are limited and include somatostatin\\nanalogues, targeted therapies, chemotherapy, and peptide receptor radionuclide\\ntherapy (PRRT). PRRT with [177Lu]Lu-DOTATATE or [177Lu]Lu-DOTATOC\\nhas emerged as an effective treatment strategy for patients with metastatic NETs\\nthat express high levels of somatostatin receptors [5]. Clinical trials have demon-\\nstrated that 177Lu-based PRRT significantly prolongs progression-free survival\\n(PFS) [6]. More recently, the NETTER-2 trial evaluated [177Lu]Lu-DOTATATE\\nas a first-line therapy for patients with advanced grade 2 and 3 gastroenteropan-\\ncreatic (GEP) NETs, demonstrating encouraging outcomes that support its ex-\\npanded role in earlier lines of treatment [7]. Nonetheless, meta-analyses have\\ndemonstrated that PRRT achieves objective response rates in patients with ad-\\nvanced NETs ranging between 25.0% and 35.0%, depending on the response\\nassessment criteria applied, and disease control rates between 79.0% and 83.0%\\n[8]. Identifying patients who will not achieve long-term disease control or re-\\nmission in advance is a clinical need and defines the rationale of the present\\nstudy. As of today, histological Ki-67 proliferation index and serum chromo-\\ngranin A (CgA) remain the most established prognostic biomarkers in patients\\nundergoing PRRT. Elevated Ki-67 and CgA levels have been consistently associ-\\nated with shorter PFS [9]. The multigene transcriptomic assay NETest has been\\nproposed as a predictive tool for PRRT outcomes, showing promising initial\\nresults, however, its high cost and limited availability currently restrict clini-\\ncal implementation [10]. Recent work by Ruhwedel et al. identified the De Ri-\\ntis ratio (AST/ALT) as a prognostic biomarker in patients undergoing PRRT,\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n3\\nwith elevated values associated with shorter progression-free survival and overall\\nsurvival [11,12].Imaging-derived parameters have also been proposed to predict\\nPRRT outcomes. Somatostatin receptor (SR) heterogeneity, high lesional SR ex-\\npression - as assessed by the Krenning score - and the metastases-to-liver ratio\\n(M/L ratio) have been investigated as potential predictors of therapy response\\n[13,14]. Furthermore, radiomic signatures extracted from baseline somatostatin\\nreceptor PET (SR-PET) or CT imaging have demonstrated potential for strat-\\nifying patients [15]. However, the predictive value of these parameters remains\\nlimited, thus restricting their clinical applicability. In recent years, artificial in-\\ntelligence (AI) – and particularly deep learning (DL) – has emerged as a po-\\ntent tool to extract high-dimensional patterns from heterogeneous biomedical\\ndata, including imaging, genomics, and clinical variables. In oncology, multi-\\nmodal DL approaches leverage specialized architectures that process each data\\ntype in dedicated branches before combining them into a joint representation\\nfor prediction. Typically, convolutional neural networks are employed for im-\\nage analysis, while feed-forward networks handle structured clinical data, and\\nsequence models such as recurrent or transformer architectures are used for ge-\\nnomic or temporal inputs. These modality-specific encoders are integrated via\\nfusion strategies ranging from simple concatenation to attention-based trans-\\nformers enabling end-to-end optimization across all modalities [16,17]. Recent\\nreviews highlight that such architectures can capture complementary informa-\\ntion, mitigate modality specific biases, and improve generalization across diverse\\npatient populations [16,18]. This paradigm is particularly relevant for PRRT,\\nwhere reliable biomarkers for predicting durable response remain scarce. Apply-\\ning multimodal AI frameworks to NETs could therefore facilitate more accurate\\npatient selection and personalized therapeutic strategies.\\n2\\nMaterial and Methods\\n2.1\\nPatient Cohort\\nThis retrospective, single-center study included 116 consecutive patients with\\nhistologically confirmed NETs who received PRRT with [177Lu]Lu-DOTATOC\\nbetween 2015 and 2022 at Charit´e—Universit¨atsmedizin Berlin. Eligibility cri-\\nteria comprised: (1) metastatic, progressive disease; (2) sufficient SR expression\\nconfirmed by pretherapeutic [68Ga]Ga-DOTATOC PET/CT; (3) availability of\\nboth pretherapeutic laboratory data and imaging (SR-PET and CT scan); and\\n(4) availability of clinical follow-up data. Baseline laboratory values had to in-\\nclude liver function parameters - namely aspartate transaminase (AST), alanine\\ntransaminase (ALT), and gamma-glutamyl transferase (GGT) - as well as the\\nneuroendocrine tumor marker chromogranin A (CgA), all measured within four\\nweeks before initiation of PRRT. Patients were excluded if they had undergone\\nprior PRRT, had incomplete clinical records, or insufficient follow-up to assess\\ndisease progression. A majority of the patient cohort analyzed in this study was\\npreviously included in earlier publications [11,19]. The present study comprises\\n\\n4\\nBaur, Eschrich et al.\\nadditional, more recently treated patients. Furthermore, it differs methodologi-\\ncally by employing a multimodal deep learning framework that integrates labo-\\nratory values and imaging data (SR-PET/CT) for predictive modeling of PFS.\\nThus, while the patient cohort overlaps with previous studies, the methodolog-\\nical approach of the current work is distinct. Table\\n1 illustrates all patient\\ncharacteristics.\\n2.2\\nImaging Characteristics\\nPrior to PRRT initiation, all patients underwent a pretherapeutic [68Ga]Ga-\\nDOTA-based PET. The median interval between the pretherapeutic SR-PET\\nand initiation of the first PRRT cycle was 36 days (IQR 44; Q1 15.5 – Q3 59.5\\ndays). PET/CT examinations were performed in our center with either a Philips\\nGemini TF 16 scanner with time-of-flight capability and a 16-row CT scanner\\n[20] or a GE Discovery MI scanner with silicon photomultipliers and time-of-\\nflight capability and a 64-row CT scanner [21]. The CT scans included in our\\nmodel were exclusively those acquired simultaneously with the pretherapeutic\\nSR-PET to guarantee spatial and temporal alignment between anatomical and\\nfunctional imaging data. Among the 116 patients included, 74 underwent whole-\\nbody contrast-enhanced CT, while the remaining patients received whole-body\\nnon-contrast CT. The contrast-enhanced CT images were acquired during the\\nvenous contrast phase with a slice thickness of 3 mm. We deliberately used whole-\\nbody CTs to ensure that all lesions detected on SR-PET could be anatomically\\ncorrelated with the corresponding CT scan across the entire field of view.\\n2.3\\nPeptide Receptor Radionuclide Therapy and Response\\nAssessment\\nPatients received [177Lu]Lu-DOTATOC PRRT with a median of 3 cycles (range:\\n1–7), each administered at a standard dose of 200 mCi (7.40 GBq). Treatment\\ncycles were scheduled at intervals of 10 to 12 weeks. Interim response assessment\\nwas performed using [68Ga]Ga-DOTATOC PET/CT after every two cycles, with\\nthe first evaluation following the second treatment cycle. To minimize the risk of\\nmisinterpreting radiogenic edema as disease progression (pseudo-progression), in-\\nterim staging was conducted at least two months after the most recent PRRT cy-\\ncle [22]. Disease progression was determined by an interdisciplinary tumor board.\\nIn patients showing progressive disease, no additional PRRT cycles were admin-\\nistered. Following completion of therapy, patients underwent routine follow-up\\nimaging every 3 to 6 months. Morphological evaluation was primarily based on\\nCT.\\n2.4\\nProgression-Free Survival\\nPFS was defined as the time from the initiation of the first PRRT cycle until\\nthe date of documented disease progression or death from any cause. Disease\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n5\\nprogression was assessed according to RECIST 1.1 criteria as determined by the\\nlocal interdisciplinary tumor board. Patients without a progression event were\\nnot included in the analysis. For the purpose of this study, we defined the PFS\\nthreshold at 1 year, as progression within the first year after PRRT initiation\\nis generally considered to indicate insufficient therapeutic benefit and has been\\nused previously in clinical studies [23]. This threshold was applied to dichotomize\\npatients into low-PFS (≤1 year) and high-PFS (> 1 year) groups for subsequent\\nanalyses.\\n2.5\\nDeep Learning Models\\nWe evaluated seven predictive models for PFS classification, exploring the pre-\\ndictive value of laboratory parameters, imaging, and multimodal data. First we\\ntrained a Random Forest classifier using only laboratory biomarkers. The lab-\\noratory biomarkers included are AST, ALT, CgA, and GGT. Subsequently, we\\ndeveloped two separate 3D convolutional neural networks (3D-CNNs) to pre-\\ndict PFS based on PET and CT imaging data, respectively. To investigate the\\nbenefit of multimodal integration, we constructed fusion models that combined\\n(1) PET imaging with laboratory biomarkers, (2) CT imaging with laboratory\\nbiomarkers, and (3) PET and CT imaging with laboratory biomarkers. Finally,\\nwe assessed an advanced fusion model that integrates PET, CT, and labora-\\ntory data, where the CT branch was initialized with a pretrained MONAI 3D\\nCT segmentation model [24], that we finetuned rather than being trained from\\nscratch. Laboratory biomarkers were fused via concatenation to a flattened vec-\\ntor of image embeddings, and passed to 3 layers of MLPs. An overview of all\\nour evaluated models is given in Figure 1. All models were trained using a learn-\\ning rate of 0.01. We applied a dropout rate of 0.1. Changing dropout to higher\\nrates did not significantly change results. To prevent overfitting and employed\\nthe ADAM optimizer with a weight decay of 0.2. Again, different weight decay\\ndid not significantly influence results. We used Binary Cross Entropy Loss for all\\nmodels. All imaging data underwent preprocessing prior to model training and\\nstatistical analysis. Visual artifacts were removed, voxel slopes were harmonized\\nusing dicom metadata, and finally the 3D volumes of all scans were normalized\\nacross training data. Each scan was subsequently resized to 75 × 50 × 50 voxels.\\nAs our architectural design is fairly simple, our models are easily reproducable,\\ngiven access to the data.\\n2.6\\nStatistical Analysis\\nTo make sure our evaluation was reliable and not dependent on one specific data\\nsplit, we used a repeated cross-validation approach. Therefore, we divided the\\ndataset into three parts of equal size (3-fold cross-validation). In each round,\\ntwo parts were used to train the model and the remaining part was used to test\\nit. This rotation continued until each part had served once as the test set. We\\nthen repeated this entire 3-fold process five times, each time reshuffling the data.\\n\\n6\\nBaur, Eschrich et al.\\nPET\\nCT\\nLaboratory values\\nInput Setup Description\\n✓\\nLaboratory values only\\n✓\\nPET only\\n✓\\nCT only\\n✓\\n✓\\nPET Fusion\\n✓\\n✓\\nCT Fusion\\n✓\\n✓\\n✓\\nPET CT Fusion\\n✓\\n✓\\n✓\\nPET CT Fusion (pretrained CT)\\nFig. 1: (Top) Overview of the proposed deep learning pipeline for PRRT response\\nprediction. The model integrates 3D PET and 3D CT scans processed through\\nseparate 3D convolutional networks, along with laboratory biomarkers, via a\\nconcatenation-based fusion layer. The fused features are passed through fully\\nconnected layers to generate the prediction output. Interpretability is provided\\nthrough backward analyses, including gradient maps, feature importance, and\\nUMAP-based feature space analysis. Solid arrows represent forward pass data\\nflow, dashed arrows backward pass dervied post hoc outputs. (Bottom) Overview\\nof input modality combinations evaluated in our experiments.\\nThis repetition reduced the chance that the results were influenced by a partic-\\nular way of splitting the data, giving us a more robust estimate of performance.\\nFinally, we reported the average performance across all 5 repetitions, along with\\nthe standard error to indicate how much the results varied. Performance was\\nmeasured using two widely applied metrics: the area under the receiver oper-\\nating characteristic curve (AUROC), and the area under the precision–recall\\ncurve (AUPRC). To assess whether the observed performance differences be-\\ntween model families were statistically significant, we conducted nonparametric\\nsignificance testing. Each model was assigned to one of three predefined groups\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n7\\nbased on its input modality: unimodal (PET-only, CT-only, or Random Forest on\\ntabular data), one-image fusion (PET or CT combined with clinical variables),\\nand dual fusion (both PET and CT combined, with or without pretraining). We\\nfirst compared one-image fusion models against unimodal models, and then dual-\\nfusion models against one-image fusion models. For each comparison, we used\\nthe Mann–Whitney U test (two-sided) to evaluate whether one group achieved\\nhigher performance than the other. To control for multiple testing, p-values were\\nadjusted using the Bonferroni correction. In addition, we quantified the effect\\nsize with Cliff’s delta, which indicates how strongly one group tends to outper-\\nform the other. According to conventional interpretation, values of |δ| < 0.147,\\n< 0.33, < 0.474, and ≥0.474 correspond to negligible, small, medium, and large\\neffects, respectively.\\n2.7\\nModel Analysis and Explainability\\nTo elucidate the decision-making process of our deep learning framework, we\\nperformed both representation space analysis and biomarker relevance assess-\\nment as well as qualitative explainability through saliency maps. The aim was\\nto provide transparency on how the model integrates multimodal information—\\nnamely, PET imaging and laboratory biomarkers—to arrive at its predictions.\\nUmap Analysis of Feature Embeddings Uniform Manifold Approximation\\nand Projection (UMAP)[25] was employed to visualize the high-dimensional fea-\\nture embeddings learned by the network. UMAP is a non-linear dimensionality\\nreduction technique that projects complex feature spaces into two dimensions\\nwhile preserving both local and global structural relationships. This makes it\\nparticularly suitable for identifying separable patient subgroups in latent space.\\nFeature Importance Analysis of Laboratory Biomarkers Feature im-\\nportance analysis was performed to quantify the relative contribution of each\\nlaboratory biomarker to the predictive performance of different fusion strate-\\ngies. For this, we evaluated the influence of gradients of laboratory values in the\\nfusion layer on the model predictions.\\nQualitative Explainability Gradient-based saliency maps were computed to\\nlocalize regions within the PET scans that most strongly influenced the model’s\\nclassification decisions. For each patient, voxel-wise gradients from the PET Fu-\\nsion and PET Only models were backpropagated and mapped to the input PET\\nvolume to generate saliency heatmaps. These were overlaid on the original scans\\nin three anatomical planes (axial, coronal, and sagittal) to visually highlight\\nspatial patterns of model attention.\\n\\n8\\nBaur, Eschrich et al.\\n3\\nResults\\n3.1\\nClinical Characteristics\\nA total of 116 patients were included in the final study cohort with a median\\nage of 66 years (range: 36–87). 41% of patients were female. The most common\\nprimary sites were the small intestine (42%) and pancreas (29%), with 17% of\\npatients having a cancer of unknown primary (CUP). Most patients presented\\nwith hepatic metastases (73%), frequently accompanied by lymphonodal or os-\\nseous spread. The majority of tumors were G2 (71%) with a median Ki-67 index\\nof 5% (range: 1–40). Patients received a median of 3 PRRT cycles (range: 1–7).\\nKey baseline laboratory values, including chromogranin A (CgA) and γ-GT, are\\nsummarized in Table 1.\\n3.2\\nProgression Free-Survival\\nTo evaluate treatment outcomes in the study cohort, we first analyzed the PFS.\\nFigure 2 illustrates the PFS distribution of the patient cohort. The median\\nPFS for the total cohort was 15.7 months (interquartile range [IQR]: 9.1–26.7\\nmonths), indicating that half of the patients remained progression-free for at\\nleast this duration. No censored patients were included, as target values are\\nnecessary for a sample to be used in our deep learning setup. Our cohort in-\\ncludes only patients that eventually had a progress. Patients were stratified into\\nshort-PFS (≤1 year) and long-PFS (> 1 year) group. When comparing these\\ntwo subgroups most clinical characteristics did not differ significantly between\\nthe groups, including age, sex distribution, primary tumor location, metastatic\\npattern, tumor functionality, and histological grading (all p > 0.05). Notably,\\nbaseline chromogranin A (CgA) level was significantly higher in patients with\\nshorter PFS (p = 0.003), and elevated γ-GT levels (p = 0.02). In addition, pa-\\ntients with early progression had received fewer PRRT cycles (p < 0.01), which\\nis expected, as treatment is typically discontinued in the event of disease pro-\\ngression prior to completion of the planned cycles.\\n3.3\\nDeep Learning Predictive Model for Progression-Free Survival\\nWe applied a series of unimodal and multimodal deep learning architectures to\\nassess the added value of integrating imaging and laboratory data for predicting\\nPFS. Our results demonstrate that integrating multiple data modalities consis-\\ntently improves model performance in progression-free survival (PFS) classifica-\\ntion (Table 2). The baseline Random Forest model, trained solely on laboratory\\nbiomarkers, achieved moderate performance (AUROC: 0.59 ± 0.02, AUPRC:\\n0.67 ± 0.01, Accuracy: 0.61 ± 0.02). In contrast, unimodal 3D convolutional\\nneural networks trained on PET or CT data alone yielded lower discrimina-\\ntive performance, particularly for the PET-only model (AUROC: 0.42 ± 0.03).\\nThe CT-only model performed slightly better (AUROC: 0.54 ± 0.01), though\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n9\\nMetric\\nTotal\\nPFS ≤1 year\\nPFS > 1 year\\np-value\\nPatient Statistics\\nPatient Count\\n116 (100%)\\n42 (36%)\\n74 (64%)\\nAge in years\\n66 (36–87)\\n66 (36–87)\\n66 (36–80)\\n0.945\\nMale\\n68 (59%)\\n23 (55%)\\n45 (61%)\\n0.560\\nFemale\\n48 (41%)\\n19 (45%)\\n29 (39%)\\n0.560\\nPrimary Location\\nSmall intestine\\n49 (42%)\\n19 (45%)\\n30 (41%)\\n0.697\\nPancreas\\n34 (29%)\\n8 (19%)\\n26 (35%)\\n0.090\\nColon/Rectum\\n12 (10%)\\n4 (10%)\\n8 (11%)\\n1.000\\nStomach\\n1 (1%)\\n0 (0%)\\n1 (1%)\\n1.000\\nCUP\\n20 (17%)\\n11 (26%)\\n9 (12%)\\n0.073\\nMetastatic Spread\\nHepatic\\n85 (73%)\\n28 (67%)\\n57 (77%)\\n0.276\\nLymphonodal\\n75 (65%)\\n26 (62%)\\n49 (66%)\\n0.689\\nOsseous\\n35 (30%)\\n12 (29%)\\n23 (31%)\\n0.836\\nPeritoneal\\n19 (16%)\\n6 (14%)\\n13 (18%)\\n0.796\\nPulmonal\\n5 (4%)\\n1 (2%)\\n4 (5%)\\n0.652\\nFunctionality\\nYes\\n40 (34%)\\n19 (45%)\\n21 (28%)\\n0.072\\nNo\\n75 (65%)\\n22 (52%)\\n53 (72%)\\n0.045\\nUnknown\\n1 (1%)\\n1 (2%)\\n0 (0%)\\n0.362\\nGrading\\nG1\\n23 (20%)\\n8 (19%)\\n15 (20%)\\n1.000\\nG2\\n82 (71%)\\n29 (69%)\\n53 (72%)\\n0.833\\nG3\\n6 (5%)\\n2 (5%)\\n4 (5%)\\n1.000\\nUnknown\\n5 (4%)\\n3 (7%)\\n2 (3%)\\n0.351\\nKi67-Index %\\n5 (1–40)\\n5 (1–25)\\n5 (1–40)\\n0.501\\nLaboratory Parameters\\nCgA in µg/l\\n419 (24–99590)\\n821 (25–99590)\\n262 (24–15100)\\n0.001\\nAST in U/l\\n28 (14–139)\\n32 (14–123)\\n28 (14–139)\\n0.123\\nALT in U/l\\n28 (7–132)\\n27 (7–96)\\n28 (10–132)\\n0.774\\nγ-GT in U/l\\n61 (9–691)\\n95 (21–688)\\n50 (9–691)\\n0.014\\nDe Ritis ratio\\n1.12 (0.46–3.43) 1.16 (0.46–3.43) 1.07 (0.53–2.87)\\n0.223\\nPRRT Cycles\\n4 (1–7)\\n2 (1–4)\\n4 (1–7)\\n< 0.001\\nTable 1: Summary of patient characteristics stratified by PFS. Categorical vari-\\nables were compared with Fisher’s exact test; continuous variables with the\\nWilcoxon rank-sum (Mann–Whitney) test. Values are counts with percentages,\\nor median (min–max). “Functionality” refers to the presence of clinically rel-\\nevant hormone secretion by NETs. Parts of the presented cohort overlap with\\npreviously published studies [11,19]. See the Methods section for further details.\\n\\n10\\nBaur, Eschrich et al.\\nFig. 2: Kaplan-Meier curve for progression-free survival (PFS) in the total study\\ncohort (n = 116) of patients with neuroendocrine tumors treated with [177Lu]Lu-\\nDOTATOC PRRT. Vertical dashed red line indicates our split into high and low\\ntherapy response. No censored patients are included, and all patients eventually\\nhad progress.\\nboth remained inferior to the model using only laboratory parameters. Intro-\\nducing laboratory features into imaging-based models led to improvements: the\\nPET-laboratory fusion model achieved an AUROC of 0.68 ± 0.01 and the high-\\nest accuracy overall (0.65 ± 0.01), suggesting strong complementarity between\\nPET imaging and laboratory data. Similarly, the CT-laboratory fusion model\\nimproved over the CT-only model across all metrics, reaching an AUROC of\\n0.62 ± 0.03. Further combining all three modalities—PET, CT, and laboratory\\ndata—resulted in additional performance gains. The PET-CT-laboratory fusion\\nmodel achieved an AUROC of 0.69 ± 0.01 and matched the highest AUPRC\\nscore (0.80 ± 0.01), reinforcing the value of multimodal integration. Finally, ini-\\ntializing the CT branch with a pretrained model further boosted the AUROC\\nto 0.72±0.01, indicating that leveraging pretrained representations can enhance\\npredictive performance. Figure 3 gives a visual overview of predictive perfor-\\nmance of all models. For additional insights into predictive performance, Figure\\n4 displays ROC and Precision-Recall curves of a single exemplary cross vali-\\ndation fold for our PET CT Fusion model. We can clearly see the improved\\npredictive performance compared to the Random Forest model and a random\\nbaseline. Statistical testing applied as described in 2.6 suggests that adding it-\\neratively more modalities improves predictive model performance significantly\\n(p < 0.01).\\nTo further explore the clinical relevance of our model predictions, we performed\\na Kaplan–Meier analysis stratified by the model-derived probability of long\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n11\\nprogression-free survival (PFS) (see Figure 5. The plot shows a representative\\nsingle cross validation of the PET Fusion model. Patients predicted by our model\\nto have a high therapy response (probability ≥0.5) demonstrated a markedly\\nprolonged PFS compared with those in the low-response group (probability ¡\\n0.5).The median PFS was 17.25 months in the high-response group versus 12.43\\nmonths in the low-response group, corresponding to a statistically significant\\ndifference (log-rank p = 0.0001; test statistic 15.18). As all patients in our co-\\nhort eventually experienced progression, no censoring occurred, and the survival\\ncurves therefore display the proportion of patients who had progressed at a given\\ntime point. These findings indicate that our multimodal model is capable of clin-\\nically meaningful risk stratification, separating patients into distinct prognostic\\ngroups based solely on baseline data prior to therapy initiation.\\nFig. 3: Barplot comparison of AUROC and AUPRC across models.\\n3.4\\nResults Model Analysis and Explainability\\nUMAP Analysis Figure 6 displays the UMAP projections of embeddings de-\\nrived from different fusion strategies, PET imaging combined with laboratory\\nbiomarkers (PET Fusion), CT imaging combined with laboratory biomarkers\\n(CT Fusion), joint PET and CT imaging fused with laboratory biomarkers\\n(PET-CT Fusion), and the same PET-CT fusion model with the CT branch ini-\\ntialized from a pretrained network (PET-CT Fusion, Pretrained). PET-derived\\nembeddings alone show weak class separation. In contrast, the fused PET-CT\\nembeddings—particularly when incorporating pretrained CT features—exhibit\\nmarkedly improved clustering, with tighter intra-class grouping and greater\\ninter-class separation. This suggests multimodal fusion enriches the feature space,\\nenabling the model to capture subtle features associated with PRRT response.\\n\\n12\\nBaur, Eschrich et al.\\nModel\\nAUROC\\nAUPRC\\nRF (Laboratory values only)\\n0.59 ± 0.02\\n0.67 ± 0.01\\nPET only\\n0.42 ± 0.03\\n0.58 ± 0.03\\nCT only\\n0.54 ± 0.01\\n0.57 ± 0.03\\nPET Fusion\\n0.68 ± 0.01∗\\n0.80 ± 0.01∗\\nCT Fusion\\n0.62 ± 0.03∗\\n0.72 ± 0.04∗\\nPET CT Fusion\\n0.69 ± 0.01†\\n0.80 ± 0.01\\nPET CT Fusion (pretrained CT)\\n0.72 ± 0.01†\\n0.80 ± 0.02\\nTable 2: Performance metrics (AUROC and AUPRC) for all models. Values are\\nreported as mean ± standard error. Bolded values indicate the best performance\\nin each metric. Significance markers denote statistical improvement over the next\\nlower model family: ∗p < 0.01 (vs. unimodal); † p < 0.01 (vs. one-image fusion).\\nAll significant differences correspond to large effect sizes (Cliff’s δ > 0.8).\\nFig. 4: Comparison of predictive performance between the Random Forest base-\\nline (laboratory values only) model and the PET CT Fusion model. The plot is\\nshowing the example of a single representative cv fold. Left: ROC curves showing\\nTrue Positive Rate versus False Positive Rate; the dashed gray line represents a\\nrandom baseline. Right: Precision-Recall curves illustrating the trade-off between\\nprecision and recall; the dashed gray line indicates the all-positive baseline. The\\nPET CT Fusion model consistently outperforms the Random Forest baseline,\\nas reflected in higher AUROC and AUPRC values. For cross-validation metrics,\\nrefer to Table 2.\\nFeature Importance Our evaluation of feature importance of laboratory val-\\nues is displayed in Figure 7. Across all model configurations, ALT, AST, CgA,\\nand Gamma-GT consistently emerged as key discriminative variables. Notably,\\nCgA exhibited the highest importance in both PET-CT fusion approaches, high-\\nlighting its strong association with the target outcome when combined with\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n13\\nFig. 5: Kaplan-Meier Curve of study cohort, stratified by our model prediction\\noutput probabilities ˆy (low PFS: ˆy < 0.5, high PFS: ˆy >= 0.5). Note that as\\nwe did not include censored patients, and all patients in our cohort eventually\\nhad progression, the y axis represents the proportion of progression patients at\\na given time. Log-Rank Test: p = 0.0001.\\nimaging- related features. The pretrained PET-CT fusion model generally pre-\\nserved or enhanced biomarker relevance compared to the non-pretrained variant,\\nsuggesting that the integration of well-learned CT representations can strengthen\\nthe interpretive value of specific laboratory measures. The Random Forest base-\\nline, while ranking the same biomarkers highly, demonstrated lower absolute\\nimportance values, underscoring the advantage of deep multimodal learning in\\ncapturing non-linear relationships between biochemical and imaging features.\\nQualitative Explainability Figures 8 and 9 show the results of our qualitative\\nexplainability analysis. Firstly, we analyzed the global distributions of gradient\\nmagnitudes across all test sets of an entire cross validation (Figure 8 (a)) for PET\\nOnly and PET Fusion models. The PET Only model exhibits an irregular, noisy\\ndistribution with substantial density fluctuations across the gradient range. In\\ncontrast, PET Fusion displays a smooth, unimodal distribution centered around\\nmoderate gradient magnitudes. Notably, PET Only shows a pronounced shift\\ntoward higher gradient values (spanning the full [0, 1] range with significant\\ndensity beyond 0.6), which—coupled with the model’s poor generalization per-\\nformance—is indicative of exploding gradients, a well-known phenomenon in\\ndeep learning that can hinder stable learning and meaningful representation\\nformation [26,27,28,29]. Quantitative comparisons using multiple statistical dis-\\ntance metrics confirmed substantial divergence between the distributions of the\\ntwo models: Wasserstein Distance (0.090), Kolmogorov–Smirnov Statistic (0.241,\\n\\n14\\nBaur, Eschrich et al.\\nFig. 6: U-MAP projection of learned feature embeddings from different fusion\\nstrategies. Row 1: Embeddings derived from PET imaging alone. Embeddings\\nare mixed up between classes, therefore PET imaging alone is not capable of\\ngood discrimination. Row 2: Embeddings derived from CT imaging alone. Sim-\\nilar to PET imaging only, embeddings are scattered with no clear distinction.\\nNotably, the pretrained CT model displays better clustered embeddings, due\\nto prior exposure to CT imaging. Row 3: Fusion embeddings combining PET,\\nCT, and laboratory biomarkers reveal markedly improved class separation, with\\ntighter intra-class clustering and clearer inter-class boundaries. This illustrates\\nthe synergistic effect of multimodal integration in capturing disease-related vari-\\nation that is not apparent in single-modality embeddings.\\np < 0.001), Jensen–Shannon Divergence (0.418), Energy Distance (0.183), Bhat-\\ntacharyya Distance (0.207), and Histogram Overlap (0.518) (Figure 8(b)). We\\nfurther illustrate these differences at the individual-case level. An example of a\\nraw PET scan is shown in Figure 9 (a). Figure 9 (b) and (c) compare the corre-\\nsponding saliency maps of the PET Only and PET Fusion models for the same\\npatient. Brighter colors indicate voxels with stronger contributions to the model’s\\nprediction. The PET Fusion model predominantly focuses on relevant tumorous\\nregions, while the PET Only model assigns high importance to the bladder. In\\naddition to visual inspection, we compared the gradient magnitude distributions\\nof both models (Figure 9 (d) and (e)). The PET Only model exhibits numerous\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n15\\nFig. 7: Feature importance of selected laboratory biomarkers across different fu-\\nsion strategies and a Random Forest baseline. Importance values were computed\\nusing a permutation-based approach, with higher values indicating stronger con-\\ntribution to model predictions. Four biomarkers—ALT, AST, CgA, and Gamma-\\nGT—consistently ranked among the most relevant features across all model con-\\nfigurations. CgA exhibited the highest importance in both PET-CT fusion mod-\\nels, suggesting a strong association with the target outcome when combined with\\nimaging-derived features.\\nlarge gradients and an irregular, fragmented distribution, whereas the PET Fu-\\nsion model produces a smoother, more coherent gradient distribution with fewer\\nextreme values. In total, our gradient analyses support our earlier findings from\\nmodel performance (Table 2) and internal feature representations (Figure 6):\\nthe PET Fusion model learns more physiologically meaningful signal patterns\\nassociated with PRRT effectiveness, while the PET Only model fails to capture\\nrelevant information. The contrasting distributional shapes underscore funda-\\nmental differences in training stability: PET Fusion’s concentrated, bell-shaped\\nprofile reflects well-regulated gradient flow, while PET Only’s diffuse, erratic\\npattern signals optimization instability.\\n4\\nDiscussion\\nIn this study, we developed and evaluated a multimodal deep learning model\\nintegrating somatostatin receptor PET, CT imaging, and laboratory biomarkers\\nto predict progression-free survival in patients undergoing [177Lu]Lu-DOTATOC\\nPRRT. Our results show that unimodal imaging models alone — whether based\\non SR-PET or CT — were insufficient to provide clinically meaningful predictive\\nperformance and, in fact, performed worse than the model using only laboratory\\ndata. In contrast, combining complementary imaging modalities with laboratory\\nbiomarkers in a fusion architecture substantially enhanced predictive accuracy\\nand robustness. Importantly, we incorporated explainability into the model by\\nleveraging three-dimensional gradient maps and biomarker relevance analyses,\\n\\n16\\nBaur, Eschrich et al.\\n(a) Comparison of gradient magnitude distributions between PET Only and PET Fu-\\nsion models.\\nMetric\\nValue\\nRange & Interpretation\\nWasserstein Distance\\n0.090\\n[0, ∞)\\n↑(9% of range)\\nKS Statistic (p < 0.001)\\n0.241\\n[0, 1]\\n↑(moderate)\\nJensen–Shannon Divergence\\n0.418\\n[0, 1]\\n↑(moderate)\\nEnergy Distance\\n0.183\\n[0, ∞)\\n↑(18% of range)\\nBhattacharyya Distance\\n0.207\\n[0, ∞)\\n↑(moderate)\\nHistogram Overlap\\n0.518\\n[0, 1]\\n↓(52% overlap)\\n(b) Quantitative comparison of gradient distributions between PET Only and PET\\nFusion models across multiple distance metrics.\\nFig. 8: Visual and quantitative comparison of global gradient distributions for\\nPET only and PET Fusion models. (a) Histograms of global gradient distribu-\\ntions for both models. (b) Quantitative comparison of global gradient distribu-\\ntions for both models.\\nenabling interpretation of decision-driving features.\\nAs reported previously, analyses of the cohort characteristics showed that base-\\nline levels of CgA and gamma-GT were higher in patients with shorter PFS\\n[11]. Consistent with this finding, earlier studies have demonstrated an inverse\\nassociation between baseline CgA and clinical outcome in NET patients under-\\ngoing PRRT [30]. A similar pattern was observed for gamma-GT, which has been\\nlinked to hepatic tumor burden and poorer prognosis in NET [31], and which in\\nour analysis was consistently higher in patients with early progression. Interest-\\ningly, in the explainability analysis of our multimodal model, CgA also emerged\\nas a parameter with notable contribution to prediction (see Figure 4). This sug-\\ngests that despite its limited value as a stand-alone biomarker — being strongly\\ninfluenced by non-tumor–related factors such as proton-pump inhibitor therapy,\\nrenal dysfunction, or other comorbidities — CgA can still provide complemen-\\ntary prognostic information when integrated with imaging and other laboratory\\nfeatures. Although previous authors have applied machine learning approaches\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n17\\n(a) Example of a three-dimensional 68Ga-DOTA-PET scan.\\n(b) Gradient heatmap PET Only.\\n(c) Gradient heatmap PET Fusion.\\n(d) Gradient distribution for PET Only.\\n(e) Gradient distribution for PET Fusion.\\nFig. 9: Example PET scan (a) and comparison of gradient maps for a single sam-\\nple between PET Only and PET Fusion models. Panels (b)–(c) show the gradient\\nheatmap overlays, while (d)–(e) show the corresponding gradient distributions.\\nGradient maps were filtered with vmin = 0.3, omitting smaller gradients.\\nto predict the prognosis of patients with NETs [32,33], these models have typ-\\nically relied on single data modalities, predominantly clinical and laboratory\\n\\n18\\nBaur, Eschrich et al.\\ndata. For example, Jiang et al. used deep learning on population-based data\\nfrom the SEER registry—containing only demographic, clinical, and pathologi-\\ncal variables—to predict survival in pancreatic NETs [32]. Likewise, Gao et al.\\ndeveloped a machine learning model for prognosis estimation in gastroenteropan-\\ncreatic NET patients with liver metastases using solely clinical parameters [33].\\nIn addition to clinical and laboratory parameters, imaging information has in-\\ncreasingly been explored as a means of predicting outcomes after PRRT in NET\\npatients. SR-PET/CT provide essential information on tumor burden and re-\\nceptor expression, and several groups have investigated whether quantitative or\\nradiomic features could be used for prognostic modeling. For example, in a re-\\ncent study, Opali´nska et al. found that a significant decrease in liver-normalized\\nSUVmax in NET lesions on [68Ga]Ga-DOTA-TATE PET/CT following PRRT\\nwas associated with a lower risk of disease progression over a 20-month follow-up\\n[34]. This suggests that PET/CT-derived SUVlmax in NET lesions may serve as\\nan additional and independent predictor of treatment outcome. Further, Laudi-\\ncella et al. reported that the [68Ga]Ga-DOTA-TATE PET/CT radiomic features\\nHISTO Skewness and HISTO Kurtosis predicted PRRT response for individual\\nlesions of both primary and metastatic GEP-NETs, regardless of tumor origin,\\nwith AUCs of 0.745 and 0.722, respectively [15]. Importantly, in the CLARINET\\ntrial, Pavel et al. reported that deep learning models based on CT imaging alone\\nfailed to outperform conventional laboratory markers such as chromogranin A\\nand specific growth rate (SLDr) [35]. Similarly, in our study, the model based\\nsolely on CT scans or SR-PET showed no meaningful prognostic value and per-\\nformed worse than a baseline model using laboratory biomarkers. Only when\\nlaboratory and imaging data were combined in a multimodal fusion model did\\nwe observe a relevant increase in predictive performance. These results further\\nunderscore the complementary nature of PET and CT imaging in capturing\\ndistinct yet clinically relevant aspects of disease biology. While SR-PET empha-\\nsizes functional and metabolic activity, CT provides higher resolution anatomi-\\ncal detail. From a clinical perspective, this implies that radiomic signatures from\\ncombined PET and CT imaging—augmented by biochemical markers—may re-\\nflect pathological differences more accurately than any modality alone. When\\nintegrated within a shared feature space alongside laboratory biomarkers, the\\ncombined modality seems to offer a richer and more complete representation of\\npatient status. This multimodal synergy enables the network to detect patterns\\nthat may be too subtle to discern in either modality alone, thereby improving\\nthe robustness and generalizability of the learned representations.\\nWe recognize several limitations of our study. First, the sample size was rel-\\natively small, which raises concerns about the robustness and generalizability of\\nthe findings. Training deep learning models on limited data can lead to over-\\nfitting; although we employed cross-validation and regularization techniques,\\na larger dataset would be needed to ensure the model’s performance is con-\\nsistent and not an artifact of our particular cohort. Second, our analysis was\\nretrospective. This inherently carries risks of selection bias (e.g. only patients\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n19\\nwho completed PRRT were included) and confounding factors that prospective\\nstudies could better control. A key limitation of our study is the absence of\\nan external validation cohort, which restricts the generalizability of our find-\\nings. Nonetheless, the PET/CT images were acquired using different scanner\\nsystems, potentially introducing variability due to differences in reconstruction\\nalgorithms. Given that all required inputs—laboratory parameters as well as SR-\\nPET and CT imaging—are routinely obtained as part of the standard diagnostic\\nwork-up in patients scheduled for PRRT, prospective validation in larger multi-\\ncenter cohorts appears feasible. Another methodological limitation concerns the\\nheterogeneity of CT acquisition protocols. Among the 116 patients included, 42\\nunderwent non-contrast CT, whereas the remaining patients received contrast-\\nenhanced whole-body scans acquired during the venous contrast phase. This\\nensured complete anatomical coverage and optimal alignment between PET and\\nCT images. However, arterial phase imaging could have improved the visualiza-\\ntion of certain lesions, especially hepatic metastases, and might have enhanced\\nthe accuracy of image-based analyses. A further limitation of our study is that\\nwe considered only a relatively small fraction of the potentially available clinical\\ninformation. Additional data such as genetic profiles, advanced laboratory pa-\\nrameters, histological images, or multiplex staining might have provided further\\npredictive value. At the same time, novel biomarkers such as the NETest are\\ngaining increasing attention. Future models that integrate such high-specificity\\nbiomarkers with deep learning predictions could further enhance the accuracy\\nand clinical utility of prognostic tools in NET patients undergoing PRRT.\\nIn contrast to many previous studies in this field, our work makes a contribution\\nwith respect to explainability, moving beyond the paradigm of “black-box” deep\\nlearning models. While gradient-based visualization did highlight tumor regions,\\nas expected, it also consistently emphasized areas such as kidneys, spleen, and\\nurinary bladder. In line with the observed AUROC of 0.42 for the SR-PET–only\\nmodel, these findings indicate that SR-PET data alone did not provide predictive\\nvalue for PFS. Consistent with the inferior predictive performance, PET-only\\nmodels produced noisier and less structured gradient maps, with strong activa-\\ntions concentrated in medically irrelevant regions (Figure 9). In contrast, the\\nPET Fusion model—though not entirely free of spurious correlations, which are\\nexpected to some extent in any explainability method—yielded clearer, more co-\\nherent gradient patterns that tend to focus more on clinically relevant tumorous\\nregions. In general, saliency in non-tumor regions likely reflects a mix of relevant\\nand spurious correlations inherent to the imaging data. Importantly, these corre-\\nlations are not necessarily harmful in our setting: predictive performance emerges\\nonly after fusion with laboratory features, as supported by our UMAP analysis\\nof embedding space, suggesting that the model leverages clinically meaningful\\ninteractions rather than relying solely on non-medically relevant image cues.\\nFrom a clinical perspective, the ability to stratify patients by likely PFS has\\nsignificant implications. PRRT is an expensive and resource-intensive therapy,\\n\\n20\\nBaur, Eschrich et al.\\nand not without toxicity, therefore, optimizing patient selection is critical. If a\\nmodel identifies a patient as high risk for early progression, clinicians might con-\\nsider adapting the treatment strategy. Such patients could benefit from closer\\nmonitoring during therapy and earlier response evaluation. The multimodal deep\\nlearning framework presented in this study builds on routinely available labo-\\nratory and imaging data, which may facilitate integration into interdisciplinary\\ntumor board discussions and clinical workflows. Moreover, the architecture is\\ndesigned to flexibly incorporate additional data sources in the future, such as\\ngenetic profiling, thereby further enhancing its predictive potential.\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n21\\nReferences\\n1. Sergio Pedraza-Ar´evalo, Manuel D Gahete, Emilia Alors-P´erez, Ra´ul M Luque, and\\nJusto P Casta˜no. Multilayered heterogeneity as an intrinsic hallmark of neuroen-\\ndocrine tumors.\\nReviews in Endocrine and Metabolic Disorders, 19(2):179–192,\\n2018.\\n2. Eric H. Liu. Neuroendocrine Tumors: Epidemiology, pages 37–50. Springer Inter-\\nnational Publishing, Cham, 2024.\\n3. James C. Yao, Manal M. Hassan, Alexandria T. Phan, Cecile G. Dagohoy,\\nColleen C. Leary, Jeannette E. Mares, Eddie K. Abdalla, Jason B. Fleming, Jean-\\nNicolas Vauthey, Asif Rashid, and Douglas B. Evans. One hundred years after\\n”carcinoid”: epidemiology of and prognostic factors for neuroendocrine tumors in\\n35,825 cases in the united states. Journal of clinical oncology : official journal of\\nthe American Society of Clinical Oncology, 26 18:3063–72, 2008.\\n4. John D Hainsworth, F Anthony Greco, and Jonathan R Strosberg. Neuroendocrine\\nneoplasms of unknown primary site, 2024.\\n5. Anna Pellat, Anne S´egol`ene Cottereau, Benoit Terris, and Romain Coriat. Neu-\\nroendocrine carcinomas of the digestive tract: what is new? Cancers, 13(15):3766,\\n2021.\\n6. Jonathan Strosberg, Ghassan El-Haddad, Edward Wolin, Andrew Hendifar, James\\nYao, Beth Chasen, Erik Mittra, Pamela L Kunz, Matthew H Kulke, Heather\\nJacene, et al. Phase 3 trial of 177lu-dotatate for midgut neuroendocrine tumors.\\nNew England Journal of Medicine, 376(2):125–135, 2017.\\n7. Simron Singh, Daniel Halperin, Sten Myrehaug, Ken Herrmann, Marianne Pavel,\\nPamela L Kunz, Beth Chasen, Salvatore Tafuto, Secondo Lastoria, Jaume Capdev-\\nila, et al. [177lu] lu-dota-tate plus long-acting octreotide versus high-dose long-\\nacting octreotide for the treatment of newly diagnosed, advanced grade 2–3, well-\\ndifferentiated, gastroenteropancreatic neuroendocrine tumours (netter-2): an open-\\nlabel, randomised, phase 3 study. The lancet, 403(10446):2807–2817, 2024.\\n8. Ying et al. Wang. The therapeutic efficacy of 177lu-dotatate/dotatoc in advanced\\nneuroendocrine tumors: A meta-analysis.\\nMedicine (Baltimore), 99(10):e19304,\\n2020.\\n9. Kosmas Daskalakis, Marina Tsoli, G¨oran Wallin, Angelika Kogut, Raj Srirajaskan-\\nthan, Christopher Harlow, Georgios Giovos, Martin O Weickert, Beata Kos-Kudla,\\nand Gregory Kaltsas. Modified histopathological grading optimizes prediction of\\nsurvival outcomes in small intestinal neuroendocrine tumors. The Journal of Clin-\\nical Endocrinology & Metabolism, 109(12):e2222–e2230, 2024.\\n10. U Knigge, J Capdevila, DK Bartsch, E Baudin, Jenny Falkerby, R Kianmanesh,\\nB Kos-Kudla, B Niederle, E Nieveen van Dijkum, D O’Toole, et al. Enets consensus\\nrecommendations for the standards of care in neuroendocrine neoplasms: follow-up\\nand documentation. Neuroendocrinology, 105(3):310–319, 2017.\\n11. T. Ruhwedel, J. M. Rogasch, K. Huang, H. Jann, I. Schatka, C. Furth,\\nH. Amthauer, and C. Wetz.\\nThe prognostic value of the de ritis ratio for\\nprogression-free survival in patients with net undergoing [177lu]lu-dotatoc-prrt:\\na retrospective analysis. Cancers, 13:635, 2021.\\n12. Tristan Ruhwedel, Julian Rogasch, Imke Schatka, Markus Galler, Peter Steinhagen,\\nChristoph Wetz, and Holger Amthauer. Beyond similarities: overall survival and\\nprognostic insights from [177lu] lu-dotatoc therapy in neuroendocrine tumors. Eu-\\nropean Journal of Nuclear Medicine and Molecular Imaging, pages 1–10, 2025.\\n\\n22\\nBaur, Eschrich et al.\\n13. Rudolf A Werner, Constantin Lapa, Harun Ilhan, Takahiro Higuchi, Andreas K\\nBuck, Sebastian Lehner, Peter Bartenstein, Frank Bengel, Imke Schatka, Dirk O\\nMuegge, et al.\\nSurvival prediction in patients undergoing radionuclide therapy\\nbased on intratumoral somatostatin-receptor heterogeneity. Oncotarget, 8(4):7039,\\n2016.\\n14. Christoph Wetz, Philipp Genseke, Ivayla Apostolova, Christian Furth, Sammy\\nGhazzawi, Julian MM Rogasch, Imke Schatka, Michael C Kreissl, Frank Hofheinz,\\nOliver S Grosser, et al. The association of intra-therapeutic heterogeneity of so-\\nmatostatin receptor expression with morphological treatment response in patients\\nundergoing prrt with [177lu]-dotatate. PLoS One, 14(5):e0216781, 2019.\\n15. Riccardo Laudicella, Albert Comelli, Virginia Liberini, Antonio Vento, Alessandro\\nStefano, Alessandro Spataro, Ludovica Croc`e, Sara Baldari, Bambaci Michelangelo,\\nDesiree Deandreis, et al.\\n[68ga] dotatoc pet/ct radiomics in the prediction of\\nresponse in gep-nets undergoing [177lu] dotatoc prrt: the “theragnomics” concept,\\n2022.\\n16. Juli´an N. Acosta, Guido J. Falcone, Pranav Rajpurkar, and Eric J. Topol. Multi-\\nmodal biomedical AI. Nature Medicine, 28(9):1773–1784, 2022.\\n17. Simon Baur, Alexandra Benova, Emilio Dolgener Cant´u, and Jackie Ma. On the\\neffectiveness of multimodal privileged knowledge distillation in two vision trans-\\nformer based diagnostic applications. arXiv preprint arXiv:2508.06558, 2025.\\n18. Pranav Rajpurkar, Emma Chen, Oishi Banerjee, and Eric J. Topol. Ai in health\\nand medicine. Nature Medicine, 28(1):31–38, 2022.\\n19. Christoph Wetz, Tristan Ruhwedel, Imke Schatka, Jane Grabowski, Henning Jann,\\nGiulia Metzger, Markus Galler, Holger Amthauer, and Julian MM Rogasch. Plasma\\nmarkers for therapy response monitoring in patients with neuroendocrine tumors\\nundergoing peptide receptor radionuclide therapy. Cancers, 15(24):5717, 2023.\\n20. Suleman Surti, Austin Kuhn, Matthew E Werner, Amy E Perkins, Jeffrey Koltham-\\nmer, and Joel S Karp. Performance of philips gemini tf pet/ct scanner with spe-\\ncial consideration for its time-of-flight imaging capabilities.\\nJournal of Nuclear\\nMedicine, 48(3):471–480, 2007.\\n21. Delphine Vandendriessche, Jorge Uribe, Hugo Bertin, and Frank De Geeter. Per-\\nformance characteristics of silicon photomultiplier based 15-cm afov tof pet/ct.\\nEJNMMI Physics, 6(1):8, 2019.\\n22. Tessa Brabander, Wouter A van der Zwan, Jaap JM Teunissen, Boen LR Kam,\\nWouter W de Herder, Richard A Feelders, Eric P Krenning, and Dik J Kwekke-\\nboom. Pitfalls in the response evaluation after peptide receptor radionuclide ther-\\napy with [177 lu-dota 0, tyr 3] octreotate. Endocrine-related cancer, 24(5):243–251,\\n2017.\\n23. E. Baudin, T. Walter, C. Docao, M. Haissaguerre, J. Hadoux, D. Taieb, C. Ansquer,\\nL. Dierickx, L. De Mestier, E. Deshayes, E. Quak, and S. Foulon. First multicentric\\nrandomized phase ii trial investigating the antitumor efficacy of peptide receptor\\nradionuclide therapy with 177lutetium–octreotate (oclu) in unresectable progres-\\nsive neuroendocrine pancreatic tumor: Results of the oclurandom trial. Annales\\nd’Endocrinologie, 83(5):289–290, 2022. On behalf of the ENDOCAN RENATEN\\nnetwork and GTE.\\n24. Jakob Wasserthal, Hanns-Christian Breit, Manfred T Meyer, Maurice Pradella,\\nDaniel Hinck, Alexander W Sauter, Tobias Heye, Daniel T Boll, Joshy Cyriac, Shan\\nYang, et al. Totalsegmentator: robust segmentation of 104 anatomic structures in\\nct images. Radiology: Artificial Intelligence, 5(5):e230024, 2023.\\n\\nMultimodal Deep Learning for PFS prediction in NET Patients\\n23\\n25. Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approx-\\nimation and projection for dimension reduction. arXiv preprint arXiv:1802.03426,\\n2018.\\n26. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep\\nfeedforward neural networks. In Proceedings of the thirteenth international confer-\\nence on artificial intelligence and statistics, pages 249–256. JMLR Workshop and\\nConference Proceedings, 2010.\\n27. Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term depen-\\ndencies with gradient descent is difficult. IEEE transactions on neural networks,\\n5(2):157–166, 1994.\\n28. Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma,\\nTechnische Universit¨at M¨unchen, 91(1):31, 1991.\\n29. Andrea Ceni.\\nRandom orthogonal additive filters: a solution to the vanish-\\ning/exploding gradient of deep neural networks.\\nIEEE Transactions on Neural\\nNetworks and Learning Systems, 2025.\\n30. E. A. Aalbersberg, D. M. V. d. Vries–Huizing, I. Walraven, B. J. d. W. d. Veen,\\nH. Kulkarni, A. Singh, M. P. M. Stokkel, and R. P. Baum. Parameters to predict\\nprogression-free and overall survival after peptide receptor radionuclide therapy: a\\nmultivariate analysis in 782 patients. Journal of Nuclear Medicine, 60:1259–1265,\\n2019.\\n31. Benjamin Christopher Schmidt, Miriam Theresa Leiderer, Tania Amin, Fabrice\\nViol, Samuel Huber, Frank Oliver Henes, and J¨org Schrader.\\nDoes gamma-\\nglutamyltransferase correlate with liver tumor burden in neuroendocrine tumors?\\nEndocrine, 83(2):511–518, 2024.\\n32. Chen Jiang, Kan Wang, Lizhao Yan, Hailing Yao, Huiying Shi, and Rong Lin.\\nPredicting the survival of patients with pancreatic neuroendocrine neoplasms us-\\ning deep learning: A study based on surveillance, epidemiology, and end results\\ndatabase. Cancer Medicine, 12(11):12413–12424, 2023.\\n33. Fuli Gao, Jian Chen, and Xiaodan Xu. Machine learning predicts prognosis in\\npatients with gastroenteropancreatic neuroendocrine tumors with liver metastases.\\nDiscover Oncology, 16(1):743, 2025.\\n34. Marta\\nOpali´nska,\\nKarolina\\nMorawiec-S lawek,\\nAdrian\\nKania-Kuc,\\nIbraheem\\nAl Maraih, Anna Sowa-Staszczak, and Alicja Hubalewska-Dydejczyk. Potential\\nvalue of pre-and post-therapy [68ga] ga-dota-tate pet/ct in the prognosis of re-\\nsponse to prrt in disseminated neuroendocrine tumors. Frontiers in Endocrinology,\\n13:929391, 2022.\\n35. Marianne Pavel, Clarisse Dromain, Maxime Ronot, Niklaus Schaefer, Dalvinder\\nMandair, Delphine Gueguen, David Elvira, Simon J´egou, F´elix Balazard, Olivier\\nDehaene, et al. The use of deep learning models to predict progression-free survival\\nin patients with neuroendocrine tumors. Future Oncology, 19(32):2185–2199, 2023.\\nStatements and Declarations\\nFunding\\nThis work was supported by the Senate of Berlin and the European Commision’s\\nDigital Europe Programme (DIGITAL) as grant TEF-Health (101100700). Jo-\\nhannes Eschrich is a participant in the BIH Charit´e Junior Digital Clinician\\nScientist Program funded by the Charit´e – Universit¨atsmedizin Berlin, and the\\nBerlin Institute of Health at Charit´e.\\n\\n24\\nBaur, Eschrich et al.\\nCompeting Interests\\nThe authors have no relevant financial or non-financial interests to disclose.\\nAuthor Contributions\\nAll authors contributed to the conception and design of the study. Data collec-\\ntion was performed by Tristan Ruhwedel, Zuzanna Kobus, Gergana Lishkova\\nand Johannes Eschrich. Imaging evaluation was supervised by Julian M. M. Ro-\\ngasch, Christoph Wetz, and Holger Amthauer. Model development and compu-\\ntational analysis were conducted by Simon Baur, Ekin B¨oke, Jackie Ma, and\\nWojciech Samek. Manuscript review and editing were performed by Christoph\\nRoderburg, Frank Tacke, Holger Amthauer, Christoph Wetz, Henning Jann, Ju-\\nlian M. M. Rogasch, Jackie Ma, and Wojciech Samek. Study conception, clinical\\noversight, and supervision were provided by Johannes Eschrich. The first draft of\\nthe manuscript was written by Johannes Eschrich and Simon Baur. All authors\\nread and approved the final version of the manuscript.\\nData Availability\\nThe datasets generated and analysed during the current study are available from\\nthe corresponding author on reasonable request. Due to institutional and ethical\\nrestrictions, data are not publicly available.\\nEthics Approval\\nThis study was performed in accordance with the ethical standards of the in-\\nstitutional research committee and with the 1964 Helsinki Declaration and its\\nlater amendments. Approval was granted by the Ethics Committee of Charit´e\\n– Universit¨atsmedizin Berlin (Approval No.: EA1/016/23; Date: 24 February\\n2023).\\nConsent to Participate\\nInformed consent was obtained from all individual participants included in the\\nstudy.\\nConsent to Publish\\nNot applicable. This manuscript does not contain any individual person’s data\\nin any form (including individual details, images, or videos); therefore, consent\\nfor publication was not required.\\n',\n",
       " 'PRECIPITATION NOWCASTING OF SATELLITE DATA USING\\nPHYSICALLY-ALIGNED NEURAL NETWORKS\\nAntônio Catão ∗\\nantonio.catao@impa.br\\nMelvin Poveda∗\\nmelvin.poveda@impa.br\\nLeonardo Voltarelli∗\\nleonardo.voltarelli@impa.br\\nPaulo Orenstein∗\\npauloo@impa.br\\nABSTRACT\\nAccurate short-term precipitation forecasts predominantly rely on dense weather-radar networks,\\nlimiting operational value in places most exposed to climate extremes. We present TUPANN\\n(Transferable and Universal Physics-Aligned Nowcasting Network), a satellite-only model trained\\non GOES-16 RRQPE. Unlike most deep learning models for nowcasting, TUPANN decomposes\\nthe forecast into physically meaningful components: a variational encoder–decoder infers motion\\nand intensity fields from recent imagery under optical-flow supervision, a lead-time-conditioned\\nMaxViT evolves the latent state, and a differentiable advection operator reconstructs future frames.\\nWe evaluate TUPANN on both GOES-16 and IMERG data, in up to four distinct climates (Rio de\\nJaneiro, Manaus, Miami, La Paz) at 10–180-min lead times using the CSI and HSS metrics over 4–64\\nmm/h thresholds. Comparisons against optical-flow, deep learning and hybrid baselines show that\\nTUPANN achieves the best or second-best skill in most settings, with pronounced gains at higher\\nthresholds. Training on multiple cities further improves performance, while cross-city experiments\\nshow modest degradation and occasional gains for rare heavy-rain regimes. The model produces\\nsmooth, interpretable motion fields aligned with numerical optical flow and runs in near real time due\\nto the low latency of GOES-16. These results indicate that physically aligned learning can provide\\nnowcasts that are skillful, transferable and global.\\nKeywords precipitation nowcasting, neural networks, physical conditioning, satellite data\\n1\\nIntroduction\\nExtreme precipitation events are projected to become more frequent and intense under climate change, increasing\\nthe risk of floods and landslides, particularly in vulnerable regions (K. et al., 2023). Nowcasting—forecasting the\\natmosphere on time horizons up to 6 h at high spatial resolution—is critical for early warnings and disaster management.\\nWhile numerical weather prediction has improved steadily, its finite resolution and latency limit the accuracy of\\nshort-term precipitation forecasts. Radar-based nowcasting methods provide detailed observations but often require\\ndense and well-maintained radar networks that are absent or degraded in much of the world. For example, Rio de\\nJaneiro experiences recurrent flood-induced disasters yet lacks reliable radar coverage due to topographic blocking and\\nlimited infrastructure.\\nRecent advances in machine learning have shown that deep networks can outperform traditional numerical models\\nin precipitation nowcasting when trained on high-resolution radar data. However, reliance on radar restricts their\\napplicability to radar-rich regions, leaving large parts of South America, Africa and Asia underserved. Furthermore,\\npurely data-driven architectures often struggle with physical interpretability: they may produce realistic-looking\\nprecipitation maps while neglecting physically consistent motion fields, hindering forecasters’ trust and operational\\nuptake.\\nThis paper addresses both accessibility and interpretability by leveraging geostationary satellites, which provide\\nglobal coverage with near real-time latency, and by incorporating explicit physical structure into the neural network.\\nWe present TUPANN (Transferable and Universal Physics-Aligned Nowcasting Network), a model that uses only\\nsatellite-derived precipitation fields and decomposes the forecasting problem into physically motivated submodules.\\n∗Instituto Nacional de Matemática Pura e Aplicada, Rio de Janeiro, RJ, Brazil\\narXiv:2511.05471v1  [cs.LG]  7 Nov 2025\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nTUPANN comprises a variational encoder–decoder trained under optical-flow supervision to recover motion and\\nintensity fields, a lead-time-conditioned transformer to evolve latent states, and a differentiable advection operator to\\nreconstruct future frames. A strong physical alignment is done by explicitly penalizing the encoder-decoder output to\\nmatch results from optical flows algorithms, which numerically infers motion fields. We evaluate TUPANN on data\\nfrom four climate regimes — tropical rainforest (Manaus), subtropical highland (La Paz), tropical savanna with coastal\\ninfluence (Rio de Janeiro) and tropical monsoon (Miami) — and report critical success indices (CSI) and Heidke skill\\nscores (HSS) across lead times from 10 to 180 minutes and various precipitation thresholds. We benchmark against\\nstate-of-the-art baselines, including optical–flow methods (PySTEPS), deep learning models (Earthformer, CasCast),\\nand hybrid approaches (NowcastNet).\\nOur main contributions are:\\n• We develop a physically aligned satellite-only nowcasting model that separates motion inference, latent\\ndynamics and advection. Unlike prior works that learn motion implicitly from final frame loss, our variational\\nencoder–decoder is directly supervised by numerical optical flow, yielding smooth and interpretable motion\\nfields.\\n• We leverage a lead-time-conditioned MaxViT transformer to evolve the latent representation and allow long\\nlead-time prediction with a single network, reducing memory requirements compared with recurrent decoding.\\n• We perform extensive experiments on GOES-16’s Rain Rate Quantitative Precipitation Estimation (RRQPE)\\nand IMERG datasets in up to four cities with different climate regimes, comparing TUPANN with well-\\nestablished and operational baselines. We demonstrate state-of-the-art CSI and HSS scores at multiple\\nthresholds, analyze the effect of adding a generative adversarial network, and evaluate cross-city and multi-city\\ntraining for transferability.\\n• We discuss operational considerations, including runtime and latency, and outline limitations and future\\ndirections for satellite-based nowcasting.\\nThe remainder of this paper is structured as follows. Section 2 summarizes related work in numerical, optical-flow,\\ndeep learning and satellite-only nowcasting. Section 3 describes the datasets and study regions. Section 4 details the\\nTUPANN architecture and its components. Section 5 presents our experimental design, baselines and results. Section 6\\ndiscusses limitations and future work, and Section 7 concludes our work.\\n2\\nRelated work\\n2.1\\nNumerical nowcasting methods\\nPrecipitation nowcasting emerged in the late 1980s (Browning and Collier, 1989) and remains a fundamental tool\\nto mitigate the impacts of extreme precipitation events (An et al., 2025). Early approaches relied primarily on\\nLagrangian extrapolation of radar echoes (Germann and Zawadzki, 2002), while later developments incorporated\\nphysical constraints and stochastic perturbations to enable ensemble-based probabilistic forecasts (Seed et al., 2013).\\nAmong these, PySTEPS (Pulkkinen et al., 2019) has become a widely adopted open-source Python library providing a\\nreproducible platform for numerical nowcasting. It integrates multiple optical-flow algorithms, including Lucas–Kanade\\n(Lucas and Kanade, 1981) and DARTS (Ruzanski et al., 2009), to estimate motion fields and applies the STEPS model\\n(Bowler et al., 2006) for probabilistic extrapolation enhanced with downscaled numerical weather prediction input.\\nDespite their interpretability and operational maturity, these numerical schemes typically experience a rapid decline in\\nforecast skill with increasing lead time, and ensemble configurations such as STEPS can incur substantial computational\\ncost.\\n2.2\\nDeep Learning models\\nDeep learning (DL) approaches have recently achieved strong performance in precipitation nowcasting, often surpassing\\ntraditional numerical methods in both accuracy and scalability while enabling faster inference once trained. Early\\nmodels include ConvLSTM (Shi et al., 2015) and PredRNN (Wang et al., 2023), which introduced convolutional and\\nrecurrent architectures to capture spatiotemporal dependencies in radar imagery. Transformer-based architectures\\nhave since extended this line of work. Earthformer (Gao et al., 2022) adapts the Transformer framework for general\\nEarth-system forecasting through a modified Cuboid Attention mechanism that models three-dimensional spatial\\ninteractions. Similarly, Rainformer (Bai et al., 2022) extracts local and global features by combining a window-based\\nmulti-head self-attention with a gating mechanism component. These deterministic models, typically trained with\\npixel-wise L1 or L2 losses, target mean or median intensities and therefore tend to produce overly smooth forecasts.\\n2\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nTo address this limitation, generative DL models have been proposed to better reproduce fine-scale variability by\\nlearning the underlying data distribution. The first class comprises Generative Adversarial Networks (GANs), including\\nDGMR (Ravuri et al., 2021) and NowcastNet (Zhang et al., 2023), which have shown competitive performance on\\nradar-based benchmarks. NowcastNet augments the GAN structure with an Evolution Network that estimates motion\\nand intensity fields used to generate intermediate predictions before adversarial refinement. Despite its physics-inspired\\ndesign, NowcastNet does not explicitly encode physical constraints and inherits known GAN instabilities, including\\nmode collapse and artifact generation (Saxena and Cao, 2021).\\nMore recently, diffusion-based generative models have emerged as stable alternatives to GANs, inspired by advances in\\ncomputer vision. PreDiff (Gao et al., 2023) employs a Latent Diffusion Model (LDM) with a knowledge-alignment\\nmechanism that enforces domain-specific physical consistency during the sampling process. Evaluated on the SEVIR\\ndataset (Veillette et al., 2020)—a combination of GOES-16 satellite imagery and NEXRAD radar data over the United\\nStates—PreDiff guides denoising steps toward physically plausible predictions by aligning generated intensities with\\nthose estimated from a time-series model applied to context-frame averages. CasCast (Gong et al., 2024a) extends this\\napproach through a cascaded LDM framework: it first conditions on deterministic Earthformer predictions and then\\nrefines them in latent space to generate high-resolution, small-scale structures. CasCast achieves superior results on\\nSEVIR and other benchmarks, though it remains limited to radar data, short lead times (up to one hour), and lacks\\nexplicit physical regularization—sometimes producing noisy outputs and incurring high inference costs typical of\\ndiffusion models (Salimans and Ho, 2022).\\n2.3\\nPhysically conditioned Deep Learning\\nDeep learning models are often regarded as black boxes, offering limited interpretability of the processes guiding their\\npredictions. In geophysical applications, incorporating domain-specific physical knowledge can regularize training and\\npromote physically consistent outputs. Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019) exemplify this\\napproach and have inspired numerous extensions and applications (McClenny and Braga-Neto, 2023; Kovacs et al.,\\n2022; Wang et al., 2022; Karniadakis et al., 2021). Their core principle is to augment the loss function with a term\\nenforcing that the network outputs satisfy a governing Partial Differential Equation (PDE).\\nIn precipitation nowcasting, several DL models include such physical conditioning. PID-GAN (Yin et al., 2024)\\ncombines a GAN framework with a physics-informed loss derived from the moisture conservation equation, trained on\\nradar data. FourCastNet (Kurth et al., 2023) employs an Adaptive Fourier Neural Operator architecture (Guibas et al.,\\n2021), a physics-inspired design widely used for PDE solutions, and applies it to global-scale forecasting with ERA5\\nreanalysis inputs. The previously discussed PreDiff and NowcastNet also incorporate elements of physical conditioning:\\nNowcastNet’s design draws inspiration from the continuity equation without numerical constraints, whereas PreDiff\\nexplicitly penalizes deviations from numerical-model intensities. The conditioning proposed in this work builds on both\\nideas by enforcing the continuity equation through an explicit loss between predicted physical terms and those derived\\nfrom a numerical optical-flow method, applied to a specific module of the architecture. As shown later, this formulation\\nachieves competitive predictive skill while improving interpretability and physical plausibility.\\n2.4\\nUse of satellite data\\nThe use of satellite observations for precipitation nowcasting remains relatively limited. Several studies have explored\\nthis direction. Shukla et al. (2025) evaluated the use of PySTEPS with satellite imagery under different optical flow\\nmethods. Lebedev et al. (2019) employed a variant of the U-Net architecture (Ronneberger et al., 2015) to predict\\nprecipitation up to two hours ahead over Russia using EUMETSAT data. Rahimi et al. (2024) proposed a hybrid\\nU-Net–ConvLSTM model evaluated on IMERG and Global Forecast System (GFS) data, using GFS as ground truth but\\nwithout comparison against baseline models. More recently, Park et al. (2025) introduced a two-phase neural network\\nthat first predicts future satellite imagery using a video prediction model and then performs image-to-image translation\\nto obtain radar reflectivity. Their approach leverages the Sat2Rdr dataset, derived from the Korean GK2A geostationary\\nsatellite and ten ground-based radar stations. While effective for light precipitation, reported results are limited to low\\ncritical success index (CSI) thresholds (below 8 mm/h), leaving high-intensity events mostly unassessed. Agrawal et al.\\n(2025) also leverage geostationary satellite mosaics, providing global skillful forecasts up to 12 hours into the future.\\nThe model uses an encoder-decoder architecture with multiple high-dimensional inputs, including numerical weather\\nprediction (NWP), leading to an intensive hardware use of more than 500 TPU cores during training.\\nOther studies, including Niu et al. (2024), Zheng et al. (2024), and Andrychowicz et al. (2023), integrate both satellite\\nand radar data as inputs, which limits their applicability in radar-sparse regions. In contrast, the present work relies\\nexclusively on satellite imagery, enabling global scalability. Comparisons are conducted against established baselines\\nand evaluated across a range of precipitation intensities, including high and extreme-rate events.\\n3\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\n3\\nData and study regions\\nWe train and evaluate TUPANN using precipitation data from satellite products. For each dataset we identify rain events\\nand split them into training, validation and test sets as described below.\\n3.1\\nGOES-16 RRQPE\\nThe primary data source is the GOES-R Advanced Baseline Imager Rain Rate Quantitative Precipitation Estimation\\n(RRQPE) (GOES-R Algorithm Working Group and GOES-R Program Office, 2018). RRQPE provides precipitation\\nestimates over the Americas every 10 min at 2 km spatial resolution with a latency of approximately 5 min, enabling\\nreal-time nowcasting. This product is highly correlated with rain-related bands and has been validated against ground\\nradars and the GPM CORRA dataset (Agrawal et al., 2025), highlighting the value of predicting geostationary satellite\\nobservations. We use RRQPE from January 2020 to December 2023. Rain events are defined as contiguous periods\\nwhen the precipitation rate exceeds a chosen threshold (see Section A.5 for details); we sample events uniformly\\nat random and allocate 70% to training, 15% to validation and 15% to testing. Figure 1 shows the proportion of\\nobservations above various thresholds for four different cities, while Figure 2 displays the accumulated precipitation\\nand dataset splits.\\n>1\\n>2\\n>4\\n>8\\n>16\\n>32\\n>64\\nRainfall Threshold (mm/h)\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.10\\nProportion of Pixels\\nProportion of pixels exceeding thresholds\\nManaus\\nLa Paz\\nRio de Janeiro\\nMiami\\nFigure 1: Proportion of observations above different precipitation thresholds in the GOES-16 RRQPE dataset for each\\nof the four study regions. Manaus exhibits the highest frequency of heavy rainfall across thresholds, while Rio de\\nJaneiro and La Paz show intermediate levels\\n3.2\\nIMERG\\nTo test generalization across data sources we also use the Integrated Multi-satellitE Retrievals for GPM (IMERG) Final\\nRun product (Huffman et al., 2014). IMERG provides precipitation estimates every 30 min at 10 km resolution and is\\nwidely used in remote sensing research. Its latency is about 3.5 months (the Early Run version has 4 h latency), which\\nprecludes real-time use but offers an independent validation dataset. We extract IMERG data from January 2020 to\\nDecember 2023, split it using the same event-based procedure, and consider only the Rio de Janeiro region. Figure 3\\ncompares the proportion of heavy-rain observations and accumulated precipitation for IMERG.\\n3.3\\nStudy regions\\nTo evaluate model performance across different climates we select four 512 km × 512 km subregions of the GOES-16\\ndomain centered on Rio de Janeiro (Brazil), La Paz (Bolivia), Manaus (Brazil) and Miami (USA). These regions span\\ncoastal, high-altitude, rainforest and subtropical environments. For IMERG we consider only a 2560 km × 2560 km\\narea surrounding Rio de Janeiro. The dominant precipitation processes include orographic and mesoscale convection in\\nRio (Luiz-Silva and Oscar-Júnior, 2022), high-altitude convective storms in La Paz (Garreaud, 2001), monsoon-driven\\n4\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nFigure 2: Accumulated precipitation in GOES-16 RRQPE from January 2020 to December 2023 over each study region.\\nShaded areas denote training, validation and test splits. Seasonal variability differs markedly between regions, with\\npronounced dry and wet seasons in La Paz and Rio de Janeiro\\nconvection in Manaus (Oliveira et al., 2016) and sea-breeze thunderstorms in Miami (Burpee, 1979). Figure 2 illustrates\\nthe seasonal cycle and dataset splits for GOES-16 across regions.\\n>1\\n>2\\n>4\\n>8\\n>16\\n>32\\nRainfall Threshold (mm/h)\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\nProportion of Pixels\\nProportion of pixels exceeding thresholds\\nManaus\\nLa Paz\\nRio de Janeiro\\nMiami\\n(a) Proportion of observations above different precipitation\\nthresholds in the IMERG dataset for each study region. Un-\\nlike RRQPE, Manaus only dominates the lowest thresholds.\\n(b) Estimated precipitation from IMERG (2020–2023) over\\nRio de Janeiro. The data splits are color-coded; low-level\\nprecipitation events are excluded from training and evalua-\\ntion.\\nFigure 3: Statistics of IMERG data, highlighting differences with respect to the RRQPE dataset\\n5\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\n4\\nMethods\\nThe proposed model, TUPANN, forecasts precipitation fields from a sequence of past satellite images 𝑋−𝑇:0 ∈\\nR(𝑇+1)×𝑛×𝑛, where 𝑛denotes the spatial resolution (i.e., number of pixels per dimension) and 𝑇+ 1 is the number of\\npast observations. It produces predicted fields b𝑋1:𝑇𝑓∈R𝑇𝑓×𝑛×𝑛, where 𝑇𝑓is the forecast horizon. The architecture\\ncomprises two modules: (i) a variational encoder–decoder (VED) that learns a latent representation 𝐿𝑘, 𝑘= 1, . . . ,𝑇𝑓\\nof the evolution of the precipitation fields under optical flow supervision; (ii) a visual transformer (MaxViT) that evolves\\nthe latent representation 𝐿𝑘so that the resulting application of a differentiable advection operator (warp) on the decoded\\nmotion and intensity fields, b𝑠𝑘−1→𝑘and b𝑣𝑘−1→𝑘, closely match the ground truth frame. See Figure 4. The training\\nprocedure is sequential: the VED module is initially trained to infer the first set of motion and intensity fields, then\\nits weights are fixed and used for the training of the MaxVit module. Details on the VED and MaxViT training are\\ndiscussed in Sections 4.1 and 4.2, respectively.\\nEven though MaxViT offers linear complexity in the image size, the choice of such encoder-decoder architecture is\\nguided by the idea that, apart from compressing the images, the VED is also responsible for learning the dynamics of a\\nsingle step evolution. MaxViT, on the other hand, learns to extrapolate the dynamics to further lead times.\\nFigure 4: TUPANN architecture. The VED and MaxViT modules displayed are learned; motion fields and the final\\npredictions are extrapolated through a warp function\\n4.1\\nVariational encoder–decoder\\nWe use a variational encoder–decoder to learn an efficient representation of the precipitation evolution. Instead of\\nreconstructing the input images as in classical variational autoencoders, our VED outputs motion fields b𝑣0→1 ∈R2×𝑛×𝑛\\nand intensity corrections b𝑠0→1 ∈R𝑛×𝑛given the past sequence 𝑋−𝑇:0. To enforce physically plausible motion we\\ncompute the ground truth motion fields 𝑣0→1 applying an optical flow algorithm to 𝑋−˜𝑇+2:1, where ˜𝑇is the context\\nlength provided to the optical flow algorithm. After that, the ground truth intensity correction 𝑠0→1 is obtained by\\nsubtracting the advected frame ˜𝑋1, obtained using 𝑣0→1, from the true frame 𝑋1 (see Figure 5). Thus, the estimated\\nfields b𝑣0→1 and b𝑠0→1 can be used to extrapolate the last observed frame 𝑋0 to an estimate b𝑋1 of the next frame via an\\nadvection operator (see Section 4.2.2).\\n4.1.1\\nTarget loss\\nTo supervise the predicted b𝑣0→1, ℓ1 and cosine-similarity losses are used with respect to 𝑣0→1. The intensity discrepan-\\ncies between 𝑠0→1 and b𝑠0→1 are penalized via ℓ1 loss. Finally, a Kullback-Leibler divergence term is added to ensure\\nthe regularity of the learned latent space. The VED loss is\\nLossVED(b𝑠0→1,b𝑣0→1, 𝑠0→1, 𝑣0→1) := 𝜆int ℓ1(𝑠0→1,b𝑠0→1) + 𝜆motion ℓ1(𝑣0→1,b𝑣0→1)\\n+ 𝜆cos CosSimilarity(𝑣0→1,b𝑣0→1) + 𝜆KL 𝐾𝐿(𝑝, b𝑝𝜃).\\n(1)\\nHere, b𝑝𝜃is the latent distribution inferred by the encoder, 𝑝is a standard normal distribution and 𝜆int, 𝜆motion, 𝜆cos, 𝜆KL\\nare hyperparameters tuned on the validation set. This loss encourages accurate motion fields, intensity corrections and\\nlatent regularity.\\n4.1.2\\nOptical flow\\nAn optical flow algorithm is able to infer motion fields between two images, and is thus essential to obtain the derived\\nground-truth motion and intensity fields (e.g., 𝑣0→1, 𝑠0→1) used in (1). We consider two options: Lucas–Kanade (LK),\\n6\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nOptical\\xa0\\nFlow\\nAdvect\\nSubtract\\nFigure 5: Ground truth motion fields are obtained using an optical flow method from a pair of past and future images.\\nThe past image is advected to obtain an intermediate one, ˜𝑋1. Finally, ground truth intensity is the subtraction of ˜𝑋1\\nfrom the future image\\nwhich solves a local least-squares problem under the assumption of small displacements, and DARTS, a spectral method\\ntailored to radar imagery that solves the optical-flow equation in Fourier space. For TUPANN, we have taken the choice\\nof optical flow method as a hyperparameter; below we use DARTS for GOES-16 and LK for IMERG results.\\n4.2\\nMaxViT\\nGiven the latent representation 𝐿1 from the VED, a visual transformer evolves the latent state forward in time. We\\nadopt MaxViT (Tu et al., 2022), which combines local and grid attention to efficiently capture global context while\\navoiding quadratic attention cost.\\n4.2.1\\nLead time conditioning\\nTo predict the latent state at lead time 𝑘, we condition the transformer on 𝑘via one-hot encoding and linear embedding,\\nyielding 𝐿𝑘= VT(𝐿1, 𝑘), 𝑘= 2, . . . ,𝑇𝑓, where where VT(·) represents the MaxViT model. Unlike recurrent\\ndecoding, this conditioning enables the same transformer to produce all lead times while reducing memory overhead\\n(Andrychowicz et al., 2023). Applying the VED decoder to 𝐿𝑘yields motion and intensity fields \\x00b𝑣𝑘−1→𝑘,b𝑠𝑘−1→𝑘\\n\\x01 =\\n𝐷(𝐿𝑘), where 𝐷(·) is the decoder module of the VED. Thus, all the necessary elements to predict the sequence\\nrecursively are obtained.\\n4.2.2\\nWarp function\\nFollowing NowcastNet (Zhang et al., 2023), we implement a fixed differentiable advection operator that can reconstruct\\nfuture precipitation frames using the predicted motion and intensity fields. Thus, given a frame b𝑋𝑘−1, motion field\\nb𝑣𝑘−1→𝑘and intensity field b𝑠𝑘−1→𝑘, the extrapolated frame b𝑋𝑘is\\nb𝑋𝑘= warp\\n\\x10\\nb𝑣𝑘−1→𝑘,b𝑠𝑘−1→𝑘, b𝑋𝑘−1\\n\\x11\\n.\\n(2)\\n4.2.3\\nTarget loss\\nWe compute the target loss in the original image space. We assume that b𝑋𝑘−1 = 𝑋𝑘−1 in equation (2) to avoid a costly\\nrecursive loss and calculate the ℓ1 loss between the warped frame b𝑋𝑘and the observed frame 𝑋𝑘. Thus,\\nLossMaxViT = ℓ1 (warp (b𝑣𝑘−1→𝑘,b𝑠𝑘−1→𝑘, 𝑋𝑘−1) , 𝑋𝑘) .\\n(3)\\n7\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nThe gradients of this loss will flow through the VED decoder module and the MaxViT transformer. The VED is\\npre-trained separately, thus optimizing this loss only affects the MaxViT modules.\\n5\\nExperiments and results\\nWe compare TUPANN against four nowcasting benchmarks using several evaluation metrics and across regions with\\ndifferent climates. We also run several ablation experiments.\\n5.1\\nEvaluation framework\\nWe evaluate TUPANN and baselines using the critical success index (CSI) and Heidke skill score (HSS). Both metrics\\ndepend on a choice of threshold 𝑡, so that pixel values above the threshold are assigned as positive and, otherwise,\\nnegative. CSI and HSS are given by\\nCSI𝑡=\\nTP𝑡\\nTP𝑡+ FN𝑡+ FP𝑡\\n,\\nHSS𝑡=\\n2 (TP𝑡× TN𝑡−FN𝑡× FP𝑡)\\n(TP𝑡+ FN𝑡) (TN𝑡+ FN𝑡) −(TP𝑡+ FP𝑡) (TN𝑡+ FP𝑡) ,\\nwhere TP stands for true positive, TN for true negatives, FN for false negatives and FP for false positives. Thus, CSI\\nquantifies the overlap between forecasted and observed precipitation, ignoring true negatives (i.e., disregarding correct\\npredictions of no precipitation) while HSS compares the forecast against random chance.\\nWe report CSI and HSS at thresholds of 4, 8, 16, 32 and 64 mm/h and compute both pixel-wise scores (POOL1) and\\nmax-pooled scores over 4 × 4 blocks (POOL4). For aggregated metrics we denote CSI–M and HSS–M as the mean\\nacross all thresholds. CSI values are reported in Section 5.3, and HSS ones are included in Appendix A.\\n5.2\\nBaseline models and tuning\\nIn our experiments, TUPANN is compared with four baselines from different nowcasting paradigms:\\n• PySTEPS (LK) and PySTEPS (DARTS) (Pulkkinen et al., 2019): optical-flow baselines that estimate a motion\\nfield (LK: local Lucas–Kanade; DARTS: DFT-based spectral) from recent frames and then semi-Lagrangianly\\nadvect the precipitation field forward. As purely physical extrapolation methods, they are strong for very short\\nlead times;\\n• Earthformer (Gao et al., 2022): a space-time Transformer for Earth-system data that uses Cuboid Attention\\n(local block attention with global tokens) in a hierarchical encoder–decoder to predict future frames;\\n• NowcastNet (Zhang et al., 2023): a hybrid model combining a U-Net–based learnable semi-Lagrangian advec-\\ntion (Evolution Network) with a physics-conditioned generative network trained with a temporal discriminator\\nto inject high-resolution convective structure;\\n• CasCast (Gong et al., 2024b): a cascaded scheme that first uses a deterministic predictor (e.g., Earthformer) to\\ncapture mesoscale evolution, then conditions a latent-space diffusion transformer on that coarse forecast to\\ngenerate small-scale features and improve extreme-precipitation skill.\\nFor TUPANN and Earthformer, we tune learning rate, dropout rate and loss weights on the validation set by maximizing\\nmean CSI in the city of Rio de Janeiro, and use these for the other cities. Hyperparameters for NowcastNet and CasCast\\nare mostly those presented in their original paper (see Appendix A). The optimizer for all models is Adam. After\\nselecting the best values, we retrain on the combined training and validation data and evaluate on a held-out test set.\\nTraining uses a single NVIDIA A100 GPU, and inference typically takes under two seconds per forecast (for all 18 lead\\ntimes).\\n5.3\\nGOES-16 results\\nTable 1 presents CSI scores across the four study regions and thresholds. TUPANN consistently ranks first or second\\nfor each metric. In Rio de Janeiro, it achieves the highest CSI at all thresholds; NowcastNet is the closest competitor,\\nfollowed by Earthformer. In Miami, TUPANN again dominates most metrics, but now CasCast performs well. In\\nManaus and La Paz, Earthformer and NowcastNet obtain slightly better CSI for low thresholds, but TUPANN leads\\nfor higher thresholds and is therefore better at forecasting extreme rainfall. The 64 mm/h CSI values are small across\\nmodels, reflecting the rarity of such intense events, yet TUPANN’s scores remain the highest. Overall, the table\\nhighlights TUPANN’s performance across different climate regimes and precipitation thresholds. Similar results are\\ntrue for HSS (see Table 7), although by that metric Earthformer is much more competitive.\\n8\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nTable 1: Aggregated CSI metrics for GOES-16 data across cities. Bold values denote the best, underlined values\\nthe second best. TUPANN obtains state-of-the-art performance at most thresholds and regions, particularly for high\\nrain-rate events.\\nModel\\nCSI-M ↑\\nCSI4 ↑\\nCSI8 ↑\\nCSI16 ↑\\nCSI32 ↑\\nCSI64 ↑\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nRio de Janeiro\\nEarthformer\\n0.237\\n0.222\\n0.326\\n0.320\\n0.287\\n0.236\\n0.326\\n0.312\\n0.238\\n0.237\\n0.009\\n0.006\\nNowcastNet\\n0.244\\n0.269\\n0.313\\n0.374\\n0.282\\n0.293\\n0.318\\n0.325\\n0.247\\n0.278\\n0.059\\n0.074\\nPySTEPS (LK)\\n0.165\\n0.169\\n0.242\\n0.262\\n0.212\\n0.195\\n0.226\\n0.226\\n0.142\\n0.156\\n0.005\\n0.008\\nPySTEPS (DARTS)\\n0.166\\n0.166\\n0.231\\n0.243\\n0.216\\n0.191\\n0.229\\n0.228\\n0.140\\n0.152\\n0.013\\n0.015\\nCasCast\\n0.170\\n0.187\\n0.308\\n0.343\\n0.205\\n0.249\\n0.164\\n0.162\\n0.159\\n0.156\\n0.016\\n0.027\\nTUPANN (ours)\\n0.259\\n0.277\\n0.330\\n0.384\\n0.289\\n0.289\\n0.330\\n0.336\\n0.274\\n0.287\\n0.072\\n0.090\\nMiami\\nEarthformer\\n0.141\\n0.126\\n0.274\\n0.270\\n0.180\\n0.160\\n0.154\\n0.122\\n0.097\\n0.078\\n0.000\\n0.000\\nNowcastNet\\n0.137\\n0.160\\n0.248\\n0.299\\n0.170\\n0.207\\n0.128\\n0.139\\n0.097\\n0.106\\n0.040\\n0.047\\nPySTEPS (LK)\\n0.113\\n0.116\\n0.188\\n0.202\\n0.133\\n0.136\\n0.120\\n0.111\\n0.079\\n0.079\\n0.045\\n0.053\\nPySTEPS (DARTS)\\n0.112\\n0.113\\n0.189\\n0.202\\n0.135\\n0.138\\n0.118\\n0.107\\n0.073\\n0.071\\n0.044\\n0.046\\nCasCast\\n0.146\\n0.170\\n0.258\\n0.298\\n0.188\\n0.229\\n0.144\\n0.167\\n0.117\\n0.135\\n0.020\\n0.028\\nTUPANN (ours)\\n0.169\\n0.187\\n0.267\\n0.312\\n0.189\\n0.211\\n0.177\\n0.177\\n0.135\\n0.141\\n0.079\\n0.094\\nManaus\\nEarthformer\\n0.276\\n0.256\\n0.355\\n0.341\\n0.323\\n0.297\\n0.316\\n0.292\\n0.265\\n0.245\\n0.124\\n0.104\\nNowcastNet\\n0.253\\n0.278\\n0.323\\n0.366\\n0.296\\n0.324\\n0.283\\n0.303\\n0.233\\n0.258\\n0.130\\n0.137\\nPySTEPS (LK)\\n0.200\\n0.196\\n0.258\\n0.266\\n0.237\\n0.233\\n0.218\\n0.212\\n0.160\\n0.156\\n0.125\\n0.112\\nPySTEPS (DARTS)\\n0.197\\n0.194\\n0.259\\n0.268\\n0.239\\n0.235\\n0.219\\n0.213\\n0.158\\n0.154\\n0.109\\n0.099\\nCasCast\\n0.265\\n0.286\\n0.344\\n0.377\\n0.303\\n0.333\\n0.295\\n0.307\\n0.260\\n0.269\\n0.126\\n0.141\\nTUPANN (ours)\\n0.290\\n0.293\\n0.339\\n0.367\\n0.316\\n0.321\\n0.315\\n0.312\\n0.278\\n0.274\\n0.200\\n0.193\\nLa Paz\\nEarthformer\\n0.303\\n0.270\\n0.337\\n0.312\\n0.329\\n0.281\\n0.359\\n0.319\\n0.323\\n0.291\\n0.167\\n0.146\\nNowcastNet\\n0.291\\n0.301\\n0.330\\n0.376\\n0.303\\n0.321\\n0.321\\n0.315\\n0.300\\n0.296\\n0.202\\n0.197\\nPySTEPS (LK)\\n0.212\\n0.208\\n0.248\\n0.250\\n0.243\\n0.230\\n0.247\\n0.237\\n0.197\\n0.195\\n0.126\\n0.131\\nPySTEPS (DARTS)\\n0.225\\n0.218\\n0.263\\n0.262\\n0.262\\n0.244\\n0.264\\n0.250\\n0.206\\n0.201\\n0.127\\n0.132\\nCasCast\\n0.228\\n0.235\\n0.309\\n0.337\\n0.251\\n0.270\\n0.245\\n0.237\\n0.232\\n0.222\\n0.101\\n0.111\\nTUPANN (ours)\\n0.314\\n0.317\\n0.336\\n0.363\\n0.327\\n0.323\\n0.350\\n0.340\\n0.327\\n0.322\\n0.232\\n0.239\\nThe graphs in Figure 6 show mean CSI (averaged across thresholds) versus lead time. TUPANN maintains the highest\\nor second-highest skill across all lead times; the advantage over NowcastNet grows for early lead times, reflecting the\\nbenefit of explicit motion supervision and the efficiency of lead-time conditioning (see also Figure 7).\\nBeyond aggregated metrics, Figure 10 illustrates a TUPANN prediction for a rain event in Manaus, compared with\\nNowcastNet, CasCast and Earthformer. Generative models such as NowcastNet and CasCast produce detailed textures\\nbut may introduce artifacts, whereas TUPANN and Earthformer yield smoother predictions. Despite the blurred\\nappearance, TUPANN captures the timing and location of heavy rain more accurately, leading to higher CSI values.\\n5.4\\nAblation results\\nWe will study how different experiments affect TUPANN: investigating its motion fields against another physics-DL\\nhybrid model, adding a GAN head, evaluating TUPANN’s cross-city generalization and training the model jointly on all\\ncities.\\n5.4.1\\nInterpretability and motion fields\\nTUPANN’s interpretability stems from its explicitly learned motion fields. Figure 7 compares motion fields predicted\\nby TUPANN and NowcastNet (which relies on an Evolution Network submodule for its motion fields). The TUPANN\\nfields are smooth and closely resemble the numerical optical flow computed by DARTS, whereas the baselines’ fields\\nexhibit unrealistic patterns. This underscores the benefit of supervising motion fields directly.\\n9\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nRio de Janeiro\\nManaus\\nEarthformer\\nNowcastNet\\nTUPANN (ours)\\nPySTEPS (LK)\\nPySTEPS (DARTS)\\nCasCast\\n30\\n60\\n90\\n120\\n150\\n180\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nMiami\\n30\\n60\\n90\\n120\\n150\\n180\\nLa Paz\\nMean CSI value\\nLead Time (min)\\nCSI-M\\nFigure 6: Mean CSI (CSI–M) versus lead time for the four study regions using GOES-16 data. TUPANN consistently\\noutperforms baselines across lead times\\n5.4.2\\nGAN-TUPANN\\nGenerative adversarial networks can improve visual realism at the cost of evaluation metrics. To study its impact on\\nTUPANN predictions, we evaluate the variant GAN-TUPANN, which adds a GAN head to TUPANN outputs.\\nFigure 8 shows that GAN-TUPANN produces significantly sharper images. Still, Table 2 shows this does not always\\nlead to improvements in CSI scores. While in Rio de Janeiro GAN-TUPANN increases low-threshold CSI, in Miami\\nthe gains are either non-existent or negative. Given the computational overhead and mixed impact on metrics, there is\\nno clear advantage in including this module unless visual fidelity is paramount.\\n5.4.3\\nCross-city generalization\\nTo assess generalization we train TUPANN on one city and evaluate on others. Table 3 compares TUPANN trained on\\nRio de Janeiro (TUPANN–Rio) against models trained separately on each city. In Manaus and La Paz, TUPANN–Rio\\nyields lower CSI at all thresholds, as expected due to climate differences. Surprisingly, in Miami the Rio-trained\\nmodel performs comparably or better at high thresholds, possibly because heavy-rain events are rare in Miami (see\\nFigure 1) and the Rio model may bias towards such events. Overall, cross-city degradation is modest (at most 20%) and\\nTUPANN–Rio still outperforms or matches baselines trained on the target city, highlighting the transferability of the\\narchitecture.\\n10\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nmm/h\\nFigure 7: Comparison of motion fields. Top row: ground-truth DARTS motion fields for future frames. Second row:\\nmotion fields from TUPANN. Subsequent rows: motion fields estimated by NowcastNet/Evolution Network. TUPANN\\nyields smoother fields that align with physical intuition\\n11\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nmm/h\\nFigure 8: Visual comparison of TUPANN and GAN-TUPANN for the same Manaus event as in Figure 10.\\nGAN-TUPANN reduces blur but yields mixed changes in CSI (Table 2)\\n5.4.4\\nMulti-city training\\nRather than training a model for each city, training on multiple cities can further improve skill. Table 4 compares\\nTUPANN trained separately on each city with a multi-city model trained jointly on all regions (including Toronto,\\nsee Appendix A). The multi-city model (TUPANN–Multicity) yields higher CSI across most thresholds and regions.\\nAccess to diverse climates helps the network learn more generalizable features, especially for extreme rainfall events.\\n5.5\\nIMERG results\\nTable 5 compares TUPANN to baselines on the IMERG dataset for Rio de Janeiro. Without pooling (POOL1), TUPANN\\nachieves the best CSI across all thresholds. With pooling (POOL4), generative models (NowcastNet, GAN-TUPANN)\\nslightly outperform TUPANN at low thresholds but TUPANN remains competitive and leads at higher thresholds.\\nFigure 9 plots CSI–M versus lead time, showing TUPANN’s superior performance at most lead times and small gaps\\nonly at 150 min. These results demonstrate that TUPANN generalizes to coarser spatial resolution and longer latency\\ndatasets, where its physics-aligned architecture can become slightly less beneficial.\\n12\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nTable 2: CSI metrics comparing TUPANN and its GAN variant (GAN-TUPANN) on GOES-16 data. Bold values denote\\nthe best, underlined values the second best. GAN-TUPANN improves low-threshold performance in Rio de Janeiro but\\ndegrades or yields marginal improvements in other cities.\\nModel\\nCSI-M ↑\\nCSI4 ↑\\nCSI8 ↑\\nCSI16 ↑\\nCSI32 ↑\\nCSI64 ↑\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nRio de Janeiro\\nNowcastNet\\n0.244\\n0.269\\n0.313\\n0.374\\n0.282\\n0.293\\n0.318\\n0.325\\n0.247\\n0.278\\n0.059\\n0.074\\nTUPANN\\n0.259\\n0.277\\n0.330\\n0.384\\n0.289\\n0.289\\n0.330\\n0.336\\n0.274\\n0.287\\n0.072\\n0.090\\nGAN-TUPANN\\n0.265\\n0.290\\n0.350\\n0.413\\n0.296\\n0.319\\n0.342\\n0.348\\n0.270\\n0.286\\n0.070\\n0.086\\nMiami\\nNowcastNet\\n0.137\\n0.160\\n0.248\\n0.299\\n0.170\\n0.207\\n0.128\\n0.139\\n0.097\\n0.106\\n0.040\\n0.047\\nTUPANN\\n0.169\\n0.187\\n0.267\\n0.312\\n0.189\\n0.211\\n0.177\\n0.177\\n0.135\\n0.141\\n0.079\\n0.094\\nGAN-TUPANN\\n0.152\\n0.174\\n0.252\\n0.300\\n0.180\\n0.211\\n0.160\\n0.164\\n0.116\\n0.128\\n0.052\\n0.066\\nManaus\\nNowcastNet\\n0.253\\n0.278\\n0.323\\n0.366\\n0.296\\n0.324\\n0.283\\n0.303\\n0.233\\n0.258\\n0.130\\n0.137\\nTUPANN\\n0.290\\n0.293\\n0.339\\n0.367\\n0.316\\n0.321\\n0.315\\n0.312\\n0.278\\n0.274\\n0.200\\n0.193\\nGAN-TUPANN\\n0.274\\n0.285\\n0.331\\n0.359\\n0.318\\n0.329\\n0.310\\n0.313\\n0.258\\n0.267\\n0.156\\n0.156\\nLa Paz\\nNowcastNet\\n0.291\\n0.301\\n0.330\\n0.376\\n0.303\\n0.321\\n0.321\\n0.315\\n0.300\\n0.296\\n0.202\\n0.197\\nTUPANN\\n0.314\\n0.317\\n0.336\\n0.363\\n0.327\\n0.323\\n0.350\\n0.340\\n0.327\\n0.322\\n0.232\\n0.239\\nGAN-TUPANN\\n0.306\\n0.312\\n0.336\\n0.367\\n0.327\\n0.333\\n0.344\\n0.336\\n0.308\\n0.305\\n0.214\\n0.218\\nTable 3: Cross-city CSI comparison between TUPANN trained on each city and TUPANN trained on Rio de Janeiro\\n(TUPANN–Rio). Bold values denote the best, underlined values the second best. Cross-city performance declines in\\nManaus and La Paz but remains competitive; in Miami TUPANN–Rio improves high-threshold scores.\\nModel\\nCSI-M ↑\\nCSI4 ↑\\nCSI8 ↑\\nCSI16 ↑\\nCSI32 ↑\\nCSI64 ↑\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nMiami\\nTUPANN\\n0.169\\n0.187\\n0.267\\n0.312\\n0.189\\n0.211\\n0.177\\n0.177\\n0.135\\n0.141\\n0.079\\n0.094\\nTUPANN–Rio\\n0.166\\n0.185\\n0.248\\n0.289\\n0.182\\n0.199\\n0.178\\n0.182\\n0.138\\n0.152\\n0.085\\n0.103\\nManaus\\nTUPANN\\n0.290\\n0.293\\n0.339\\n0.367\\n0.316\\n0.321\\n0.315\\n0.312\\n0.278\\n0.274\\n0.200\\n0.193\\nTUPANN–Rio\\n0.236\\n0.249\\n0.283\\n0.319\\n0.257\\n0.276\\n0.254\\n0.264\\n0.214\\n0.221\\n0.172\\n0.167\\nLa Paz\\nTUPANN\\n0.314\\n0.317\\n0.336\\n0.363\\n0.327\\n0.323\\n0.350\\n0.340\\n0.327\\n0.322\\n0.232\\n0.239\\nTUPANN–Rio\\n0.279\\n0.288\\n0.305\\n0.339\\n0.288\\n0.295\\n0.306\\n0.304\\n0.279\\n0.279\\n0.216\\n0.221\\n5.6\\nOperational deployment\\nDownloading and processing GOES-16 data and making predictions can be done in less than three minutes. Due to its\\nlow latency, this enables an efficient near real-time 10 to 180 minutes nowcast with 10 minutes temporal resolution.\\nThe whole pipeline, including the predictions generated by TUPANN, can be run in a machine with relatively modest\\nrequirements: 16GB of memory, GeForce RTX 3080 GPU. Predictions can also include the derived motion fields to help\\nmeteorologists interpret the results. This system is currently operational and has been deployed to aid Rio de Janeiro’s\\nInstituto Estadual do Ambiente (INEA) agency to prepare for floods and other extreme precipitation disasters. Due to\\nTUPANN’s reliance on widely available geostationary satellite data, we expect that this solution can be broadened to\\nother regions as well.\\n6\\nLimitations and future work\\nOur study has important limitations. First, although TUPANN may use globally available satellite products, it currently\\nrelies on GOES-16 coverage for real-time nowcasting. The architecture should be retrained and validated on other\\ngeostationary satellites (e.g., Himawari, Meteosat) to ensure generalization across platforms. Second, the optical-flow\\nsupervision requires additional computation and may not perfectly capture complex convection dynamics; using the\\n13\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nTable 4: Comparison between single-city TUPANN and a multi-city version (TUPANN–Multicity) trained on all regions\\nsimultaneously. Bold denotes the best, underlined the second best. The multi-city model improves performance in most\\ncases, demonstrating the benefit of pooled training.\\nModel\\nCSI-M ↑\\nCSI4 ↑\\nCSI8 ↑\\nCSI16 ↑\\nCSI32 ↑\\nCSI64 ↑\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nRio de Janeiro\\nTUPANN\\n0.259\\n0.277\\n0.330\\n0.384\\n0.289\\n0.289\\n0.330\\n0.336\\n0.274\\n0.287\\n0.072\\n0.090\\nTUPANN–Multicity\\n0.271\\n0.286\\n0.337\\n0.390\\n0.303\\n0.297\\n0.353\\n0.353\\n0.293\\n0.304\\n0.071\\n0.086\\nMiami\\nTUPANN\\n0.169\\n0.187\\n0.267\\n0.312\\n0.189\\n0.211\\n0.177\\n0.177\\n0.135\\n0.141\\n0.079\\n0.094\\nTUPANN–Multicity\\n0.178\\n0.190\\n0.273\\n0.313\\n0.196\\n0.211\\n0.188\\n0.182\\n0.151\\n0.151\\n0.082\\n0.095\\nManaus\\nTUPANN\\n0.290\\n0.293\\n0.339\\n0.367\\n0.316\\n0.321\\n0.315\\n0.312\\n0.278\\n0.274\\n0.200\\n0.193\\nTUPANN–Multicity\\n0.291\\n0.296\\n0.340\\n0.369\\n0.316\\n0.324\\n0.314\\n0.314\\n0.278\\n0.275\\n0.208\\n0.198\\nLa Paz\\nTUPANN\\n0.314\\n0.317\\n0.336\\n0.363\\n0.327\\n0.323\\n0.350\\n0.340\\n0.327\\n0.322\\n0.232\\n0.239\\nTUPANN–Multicity\\n0.324\\n0.325\\n0.339\\n0.363\\n0.334\\n0.327\\n0.359\\n0.348\\n0.335\\n0.331\\n0.251\\n0.254\\nTable 5: Aggregated CSI metrics for Rio de Janeiro using IMERG data. Bold denotes the best, underlined the second\\nbest. Without pooling TUPANN is clearly superior; with pooling, generative baselines perform slightly better at low\\nthresholds, but TUPANN remains competitive overall.\\nModel\\nCSI-M ↑\\nCSI4 ↑\\nCSI8 ↑\\nCSI16 ↑\\nCSI32 ↑\\nCSI64 ↑\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nEarthformer\\n0.153\\n0.141\\n0.361\\n0.343\\n0.270\\n0.243\\n0.130\\n0.114\\n0.006\\n0.006\\n0.000\\n0.000\\nNowcastNet\\n0.209\\n0.271\\n0.387\\n0.453\\n0.322\\n0.384\\n0.229\\n0.304\\n0.103\\n0.201\\n0.001\\n0.013\\nPySTEPS (LK)\\n0.114\\n0.099\\n0.300\\n0.269\\n0.191\\n0.164\\n0.075\\n0.061\\n0.002\\n0.003\\n0.000\\n0.000\\nPySTEPS (DARTS)\\n0.107\\n0.096\\n0.287\\n0.262\\n0.179\\n0.157\\n0.069\\n0.057\\n0.002\\n0.003\\n0.000\\n0.000\\nCasCast\\n0.180\\n0.229\\n0.362\\n0.420\\n0.288\\n0.339\\n0.188\\n0.248\\n0.061\\n0.139\\n0.000\\n0.000\\nTUPANN (ours)\\n0.218\\n0.248\\n0.414\\n0.454\\n0.344\\n0.379\\n0.241\\n0.280\\n0.087\\n0.123\\n0.005\\n0.006\\nGAN-TUPANN\\n0.210\\n0.274\\n0.391\\n0.461\\n0.327\\n0.393\\n0.234\\n0.313\\n0.095\\n0.199\\n0.002\\n0.004\\nproposed scheme with additional covariates could yield further improvements under a more detailed physical modeling.\\nThird, CSI and HSS values decline at longer lead times (>2h), reflecting inherent predictability limits; integrating\\nuncertainty quantification and ensemble approaches may better characterize forecast skill. Finally, GAN-based\\nenhancements improve visual realism but degrade or inconsistently affect skill metrics; stabilizing adversarial training\\nand assessing perceptual quality remain open challenges.\\nFuture work includes coupling TUPANN with probabilistic post-processing to provide calibrated uncertainty estimates,\\nextending the model to include additional inputs such as cloud imagery or microwave precipitation retrievals, and\\ninvestigating transfer learning across continents.\\n7\\nConclusions\\nWe have presented TUPANN, a physically aligned neural network for precipitation nowcasting using satellite imagery.\\nTUPANN’s modular design — combining a variational encoder–decoder supervised by optical flow, and a transformer\\ncapable of evolving the latent representation according to physical constraints — yields interpretable motion fields\\nand competitive forecast skill. Extensive experiments on GOES-16 and IMERG data across four climates show that\\nTUPANN matches or surpasses state-of-the-art baselines, particularly at high precipitation thresholds. Training on\\nmultiple cities improves performance and cross-city evaluations reveal modest degradation, highlighting the model’s\\ntransferability. With its low latency and reliance on globally available satellites, TUPANN supports equitable access to\\nshort-term rainfall forecasts and provides a foundation for operational applications in radar-sparse regions.\\nAcknowledgements\\nThe authors thank Adriana Monteiro, João Vitor Romano, Lucas Nissenbaum, and Thiago\\nRamos as well as the help and support from Google, in particular, Shreya Agrawal, Boris Babenko and Samier Merchant.\\n14\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\n30\\n60\\n90\\n120\\n150\\n180\\n0.05\\n0.10\\n0.15\\n0.20\\n0.25\\n0.30\\n0.35\\n0.40\\nCSI-M for Rio de Janeiro\\nEarthformer\\nNowcastNet\\nTUPANN (ours)\\nPySTEPS (LK)\\nPySTEPS (DARTS)\\nCasCast\\nMean CSI value\\nLead Time (min)\\nFigure 9: Mean CSI versus lead time for IMERG data in Rio de Janeiro. TUPANN outperforms baselines at most lead\\ntimes; NowcastNet overtakes slightly at 150 min but lags at shorter lead times\\nFunding Statement\\nA.C. was supported by a “FAPERJ Nota 10\" grant (SEI-260003/004731/2025) from Fun-\\ndação de Amparo à Pesquisa do Estado do Rio de Janeiro (FAPERJ). M.P. and L.V. were supported by scholarships\\nfrom Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES). P.O. was supported by grant SEI-\\n260003/001545/2022 from FAPERJ.\\nCompeting Interests\\nThe authors declare no competing interests.\\nData Availability Statement\\nGOES-16 RRQPE data were obtained from the NOAA GOES-R Product Distribution\\nand Access (PDA) system. IMERG Final Run data were obtained from the NASA GPM data portal. Processed datasets,\\nevent splits and TUPANN code are available at https://github.com/acataos/tupann.\\nEthical Standards\\nThe research meets all ethical guidelines, including adherence to the legal requirements of Brazil\\nand the United States.\\nAuthor Contributions\\nConceptualization: A.C., M.P., L.V., P.O.; Methodology: A.C., M.P., L.V.; Data curation and\\nvisualization: A.C.; Writing—original draft: A.C., M.P., L.V.; Writing—review and editing: A.C., M.P., L.V., P.O.. All\\nauthors approved the final manuscript.\\nReferences\\nAgrawal, S., Hassen, M. A., Brempong, E. A., Babenko, B., Zyda, F., Graham, O., Li, D., Merchant, S., Potes, S. H.,\\nRussell, T., Cheresnick, D., Kakkirala, A. P., Rasp, S., Hassidim, A., Matias, Y., Kalchbrenner, N., Gupta, P., Hickey,\\nJ., and Bell, A. (2025). An operational deep learning system for satellite-based high-resolution global nowcasting.\\nAn, S., Oh, T.-J., Sohn, E., and Kim, D. (2025). Deep learning for precipitation nowcasting: A survey from the\\nperspective of time series forecasting. Expert Systems with Applications, 268:126301.\\nAndrychowicz, M., Espeholt, L., Li, D., Merchant, S., Merose, A., Zyda, F., Agrawal, S., and Kalchbrenner, N. (2023).\\nDeep learning for day forecasts from sparse observations.\\nBai, C., Sun, F., Zhang, J., Song, Y., and Chen, S. (2022). Rainformer: Features extraction balanced network for\\nradar-based precipitation nowcasting. IEEE Geoscience and Remote Sensing Letters, 19:1–5.\\nBowler, N. E., Pierce, C. E., and Seed, A. W. (2006). Steps: A probabilistic precipitation forecasting scheme which\\nmerges an extrapolation nowcast with downscaled nwp. Quarterly Journal of the Royal Meteorological Society,\\n132(620):2127–2155.\\nBrowning, K. A. and Collier, C. G. (1989). Nowcasting of precipitation systems. Reviews of Geophysics, 27(3):345–370.\\n15\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nBurpee, R. W. (1979).\\nPeninsula-scale convergence in the south florida sea breeze.\\nMonthly Weather Review,\\n107(7):852–865.\\nGao, Z., Shi, X., Han, B., Wang, H., Jin, X., Maddix, D., Zhu, Y., Li, M., and Wang, Y. (2023). Prediff: precipitation\\nnowcasting with latent diffusion models. In Proceedings of the 37th International Conference on Neural Information\\nProcessing Systems, NIPS ’23, Red Hook, NY, USA. Curran Associates Inc.\\nGao, Z., Shi, X., Wang, H., Zhu, Y., Wang, Y., Li, M., and Yeung, D.-Y. (2022). Earthformer: exploring space-time\\ntransformers for earth system forecasting. In Proceedings of the 36th International Conference on Neural Information\\nProcessing Systems, NIPS ’22, Red Hook, NY, USA. Curran Associates Inc.\\nGarreaud, R. D. (2001). Interannual rainfall variability over the south american altiplano and related circulation\\nanomalies. Journal of Climate, 14(12):2779–2789.\\nGermann, U. and Zawadzki, I. (2002). Scale-dependence of the predictability of precipitation from continental radar\\nimages. part i: Description of the methodology. Monthly Weather Review, 130(12):2859 – 2873.\\nGOES-R Algorithm Working Group and GOES-R Program Office (2018). Noaa goes-r series advanced baseline imager\\n(abi) level 2 rainfall rate / qpe. https://doi.org/10.7289/V5W66J21.\\nGong, J., Bai, L., Ye, P., Xu, W., Liu, N., Dai, J., Yang, X., and Ouyang, W. (2024a). Cascast: skillful high-resolution\\nprecipitation nowcasting via cascaded modelling. In Proceedings of the 41st International Conference on Machine\\nLearning, ICML’24. JMLR.org.\\nGong, J., Bai, L., Ye, P., Xu, W., Liu, N., Dai, J., Yang, X., and Ouyang, W. (2024b). Cascast: skillful high-resolution\\nprecipitation nowcasting via cascaded modelling. In Proceedings of the 41st International Conference on Machine\\nLearning, ICML’24. JMLR.org.\\nGuibas, J., Mardani, M., Li, Z.-Y., Tao, A., Anandkumar, A., and Catanzaro, B. (2021). Adaptive fourier neural\\noperators: Efficient token mixers for transformers. ArXiv, abs/2111.13587.\\nHuffman, G., Bolvin, D., Braithwaite, D., Hsu, K., Joyce, R., and Xie, P. (2014). Integrated multi-satellite retrievals for\\ngpm (imerg), version 4.4.\\nK., M., Su, W., Delgado, R., Aarons, S., Chatterjee, A., Garcia, M., Hausfather, Z., Hayhoe, K., Hence, D., Jewett,\\nE., Robel, A., Singh, D., Tripati, A., Vose, R., Khan, A., Crimmins, A., Avery, C., Easterling, D., Kunkel, K., and\\nMaycock, T. (2023). Fifth National Climate Assessment: Chapter 2 Climate Trends.\\nKarniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., and Yang, L. (2021). Physics-informed machine\\nlearning. Nat. Rev. Phys., 3(6):422–440.\\nKovacs, A., Exl, L., Kornell, A., Fischbacher, J., Hovorka, M., Gusenbauer, M., Breth, L., Oezelt, H., Yano, M.,\\nSakuma, N., Kinoshita, A., Shoji, T., Kato, A., and Schrefl, T. (2022). Conditional physics informed neural networks.\\nCommun. Nonlinear Sci. Numer. Simul., 104(106041):106041.\\nKurth, T., Subramanian, S., Harrington, P., Pathak, J., Mardani, M., Hall, D., Miele, A., Kashinath, K., and Anandkumar,\\nA. (2023). Fourcastnet: Accelerating global high-resolution weather forecasting using adaptive fourier neural\\noperators. In Proceedings of the Platform for Advanced Scientific Computing Conference, PASC ’23, New York, NY,\\nUSA. Association for Computing Machinery.\\nLebedev, V., Ivashkin, V., Rudenko, I., Ganshin, A., Molchanov, A., Ovcharenko, S., Grokhovetskiy, R., Bushmarinov,\\nI., and Solomentsev, D. (2019). Precipitation nowcasting with satellite imagery. In Proceedings of the 25th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, page 2680–2688, New York,\\nNY, USA. Association for Computing Machinery.\\nLucas, B. D. and Kanade, T. (1981). An iterative image registration technique with an application to stereo vision. In\\nProceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’81, page 674–679,\\nSan Francisco, CA, USA. Morgan Kaufmann Publishers Inc.\\nLuiz-Silva, W. and Oscar-Júnior, A. C. (2022). Climate extremes related with rainfall in the state of rio de janeiro ,\\nbrazil: a review of climatological characteristics and recorded trends. Natural Hazards, 114(1):713–732.\\nMcClenny, L. D. and Braga-Neto, U. M. (2023). Self-adaptive physics-informed neural networks. J. Comput. Phys.,\\n474(111722):111722.\\nNiu, D., Li, Y., Wang, H., Zang, Z., Jiang, M., Chen, X., and Huang, Q. (2024). Fsrgan: A satellite and radar-\\nbased fusion prediction network for precipitation nowcasting. IEEE Journal of Selected Topics in Applied Earth\\nObservations and Remote Sensing, 17:7002–7013.\\nOliveira, R. et al. (2016). Characteristics and diurnal cycle of gpm rainfall estimates over the central amazon region\\n(around manaus). Remote Sensing, 8(7):544.\\n16\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nPark, Y.-J., Kim, D., Seo, M., Jeon, H.-G., and Choi, Y. (2025). Data-driven precipitation nowcasting using satellite\\nimagery. In Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference\\non Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial\\nIntelligence, AAAI’25/IAAI’25/EAAI’25. AAAI Press.\\nPulkkinen, S., Nerini, D., Pérez Hortal, A. A., Velasco-Forero, C., Seed, A., Germann, U., and Foresti, L. (2019).\\nPysteps: an open-source python library for probabilistic precipitation nowcasting (v1.0). Geoscientific Model\\nDevelopment, 12(10):4185–4219.\\nRahimi, R., Ravirathinam, P., Ebtehaj, A., Behrangi, A., Tan, J., and Kumar, V. (2024). Global precipitation nowcasting\\nof integrated multi-satellite retrievals for gpm: A u-net convolutional lstm architecture. Journal of Hydrometeorology,\\n25(6):947 – 963.\\nRaissi, M., Perdikaris, P., and Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework\\nfor solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys.,\\n378:686–707.\\nRavuri, S., Lenc, K., Willson, M., Kangin, D., Lam, R., Mirowski, P., Fitzsimons, M., Athanassiadou, M., Kashem,\\nS., Madge, S., Prudden, R., Mandhane, A., Clark, A., Brock, A., Simonyan, K., Hadsell, R., Robinson, N., Clancy,\\nE., Arribas, A., and Mohamed, S. (2021). Skilful precipitation nowcasting using deep generative models of radar.\\nNature, 597(7878):672–677.\\nRonneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In\\nNavab, N., Hornegger, J., Wells, W. M., and Frangi, A. F., editors, Medical Image Computing and Computer-Assisted\\nIntervention – MICCAI 2015, pages 234–241, Cham. Springer International Publishing.\\nRuzanski, E., Wang, Y., and Chandrasekar, V. (2009). Development of a real-time dynamic and adaptive nowcasting\\nsystem. 25th Conference on International Interactive Information and Processing Systems (IIPS) for Meteorology,\\nOceanography, and Hydrology, 2(12).\\nSalimans, T. and Ho, J. (2022). Progressive distillation for fast sampling of diffusion models. In International\\nConference on Learning Representations.\\nSaxena, D. and Cao, J. (2021). Generative adversarial networks (gans): Challenges, solutions, and future directions.\\nACM Comput. Surv., 54(3).\\nSeed, A. W., Pierce, C. E., and Norman, K. (2013). Formulation and evaluation of a scale decomposition-based\\nstochastic precipitation nowcast scheme. Water Resources Research, 49(10):6624–6641.\\nShi, X., Chen, Z., Wang, H., Yeung, D.-Y., Wong, W.-k., and Woo, W.-c. (2015). Convolutional lstm network: a\\nmachine learning approach for precipitation nowcasting. In Proceedings of the 29th International Conference on\\nNeural Information Processing Systems - Volume 1, NIPS’15, page 802–810, Cambridge, MA, USA. MIT Press.\\nShukla, B. P., Vyas, J., Chhari, A., Shah, S., Panda, S. K., and Varma, A. K. (2025). Satellite based rainfall nowcasting\\nusing geospatial techniques. Meteorology and Atmospheric Physics, 137(1):3.\\nTu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., and Li, Y. (2022). MaxViT: Multi-axis Vision\\nTransformer. In Computer Vision – ECCV 2022, pages 459–479. Springer, Cham, Switzerland.\\nVeillette, M. S., Samsi, S., and Mattioli, C. J. (2020). Sevir: a storm event imagery dataset for deep learning applications\\nin radar and satellite meteorology. In Proceedings of the 34th International Conference on Neural Information\\nProcessing Systems, NIPS ’20, Red Hook, NY, USA. Curran Associates Inc.\\nWang, S., Yu, X., and Perdikaris, P. (2022). When and why PINNs fail to train: A neural tangent kernel perspective. J.\\nComput. Phys., 449(110768):110768.\\nWang, Y., Wu, H., Zhang, J., Gao, Z., Wang, J., Yu, P. S., and Long, M. (2023). Predrnn: A recurrent neural\\nnetwork for spatiotemporal predictive learning. IEEE Transactions on Pattern Analysis and Machine Intelligence,\\n45(2):2208–2225.\\nYin, J., Meo, C., Roy, A., Cher, Z., Lic˘a, M., Wang, Y., Imhoff, R., Uijlenhoet, R., and Dauwels, J. (2024). Precipitation\\nnowcasting using physics informed discriminator generative models. In 32nd European Signal Processing Confer-\\nence, EUSIPCO 2024 - Proceedings, European Signal Processing Conference, pages 1967–1971. European Signal\\nProcessing Conference, EUSIPCO. Green Open Access added to TU Delft Institutional Repository ‘You share, we\\ntake care!’ – Taverne project https://www.openaccess.nl/en/you-share-we-take-care Otherwise as indicated in the\\ncopyright section: the publisher is the copyright holder of this work and the author uses the Dutch legislation to make\\nthis work public. ; 32nd European Signal Processing Conference, EUSIPCO 2024, EUSIPCO 2024 ; Conference\\ndate: 26-08-2024 Through 30-08-2024.\\nZhang, Y., Long, M., Chen, K., Xing, L., Jin, R., Jordan, M. I., and Wang, J. (2023). Skilful nowcasting of extreme\\nprecipitation with nowcastnet. Nature, 619(7970):526–532.\\n17\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nZheng, K., He, L., Ruan, H., Yang, S., Zhang, J., Luo, C., Tang, S., Zhang, J., Tian, Y., and Cheng, J. (2024). A\\ncross-modal spatiotemporal joint predictive network for rainfall nowcasting. IEEE Transactions on Geoscience and\\nRemote Sensing, 62:1–23.\\n18\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nA\\nAdditional results\\nSome additional results related to metrics and evaluation are presented below, such as HSS and CSI scores for the city\\nof Toronto. Prediction samples in both GOES-16 and IMERG data are also included.\\nA.1\\nModel Hyperparameters\\nThe hyperparameters for Earthformer and TUPANN (both VED and MaxViT) are shown below. Hyperparameters\\nEvolution Network/NowcastNet were the ones selected in the original paper (Zhang et al., 2023). Since CasCast uses a\\nDenoising Transformer (DiT) model trained with a different image size (384x384), slight adaptations were made to\\nsupport the 256x256 images used in this work. The hyperparameters modified were input_size to 32 and hidden_size to\\n512; other hyperparameters were chosen as in their original paper (Gong et al., 2024b). All remaining hyperparameters\\nand early stopping criteria were selected by maximizing the mean CSI value in the validation set during training.\\nTable 6: Model hyperparameters for VED, Earthformer, and MaxViT. Architectural and loss parameters are detailed in\\nSection 4.\\n(a) VED\\nParameter\\nValue\\nbatch_size\\n8\\nlearning_rate\\n0.0001\\nchannels\\n128\\nembed_dim\\n4\\nreduc_factor\\n4\\n𝜆cos\\n0.00165\\n𝜆KL\\n1.0e-06\\n𝜆motion\\n0.0033\\n𝜆int\\n0.995\\ndropout\\n0.2\\n(b) Earthformer\\nParameter\\nValue\\nbatch_size\\n4\\nlearning_rate\\n0.0001\\nnum_global_vectors\\n6\\nnum_heads\\n2\\nbase_units\\n64\\n(c) MaxViT\\nParameter\\nValue\\nbatch_size\\n8\\nlearning_rate\\n0.0001\\nMaxViT_depth\\n4\\nMaxViT_dim\\n64\\nA.2\\nHSS results\\nTable 7 summarizes aggregated HSS metrics for the four cities using GOES-16 data. TUPANN achieves the best scores\\nfor high thresholds and remains competitive for lower thresholds, mirroring the CSI behaviour.\\nA.3\\nVisual inspection of model predictions\\nFigure 10 includes predictions for several models at a given moment in time. While TUPANN and EarthFormer show\\nrelatively blurred predictions, note NowcastNet and CasCast add several artifacts to its predictions. Overall, TUPANN\\nachieves a reasonable trade-off between good evaluation metrics and reasonable precipitation plots. Figure 11 also\\nincludes precipitation plots for the IMERG dataset, where the temporal and spatial resolution of the image is coarser.\\nA.4\\nResults for Toronto\\nFor completeness Table 8 reports CSI scores for the city of Toronto using GOES-16 data. This city was excluded\\nfrom the main text once it did not present as many extreme precipitation events as the other selected alternatives. The\\nmulti-city TUPANN model performs best across most thresholds, while the single-city TUPANN model ranks second.\\nAlthough extreme precipitation events are rare in Toronto, these results demonstrate that TUPANN generalizes to\\nadditional regions.\\nA.5\\nRain events dataset selection\\nThe datasets used to train and evaluate our models comprise a subsample of rainy windows drawn from either the\\nGOES-16 RRQPE or the IMERG products. We define a rainy window as follows. For each 10-min timestamp 𝑡from\\n2020-01-01 00:00 UTC to 2023-12-31 23:50 UTC, we (i) form a symmetric 60-min window [𝑡−30 min, 𝑡+ 30 min];\\n(ii) compute the spatiotemporal precipitation accumulation over that window (10-min steps over all grid points); and\\n19\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nmm/h\\nFigure 10: Predictions up to 3h ahead for a rain event in Manaus, starting at 2021-11-20 22:00 UTC. Rows show the\\nground truth, TUPANN, and other models; columns represent lead times. TUPANN displays greater skill in predicting\\nthe movement and intensity of the rain event, while generative models produce visually sharper but less accurate\\npredictions\\n20\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nmm/h\\nFigure 11: Prediction sample from the IMERG test dataset centered in Rio de Janeiro, starting from 2023-01-08 07:30\\nUTC. The red square represents the same area as in the GOES-16 figures, demonstrating the difference in spatial\\nresolution\\n21\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nTable 7: Aggregated HSS metrics for GOES-16 data across cities. Bold values denote the best, underlined values the\\nsecond best. TUPANN excels at high thresholds and is second or first at lower thresholds.\\nModel\\nHSS-M ↑\\nHSS4 ↑\\nHSS8 ↑\\nHSS16 ↑\\nHSS32 ↑\\nHSS64 ↑\\nRio de Janeiro\\nEarthformer\\n0.360\\n0.473\\n0.438\\n0.488\\n0.382\\n0.017\\nNowcastNet\\n0.373\\n0.454\\n0.429\\n0.478\\n0.394\\n0.112\\nPySTEPS (LK)\\n0.266\\n0.369\\n0.341\\n0.365\\n0.247\\n0.010\\nPySTEPS (DARTS)\\n0.268\\n0.356\\n0.347\\n0.369\\n0.244\\n0.025\\nCasCast\\n0.266\\n0.438\\n0.321\\n0.271\\n0.270\\n0.032\\nTUPANN (ours)\\n0.393\\n0.473\\n0.439\\n0.492\\n0.428\\n0.135\\nMiami\\nEarthformer\\n0.230\\n0.412\\n0.299\\n0.265\\n0.176\\n0.000\\nNowcastNet\\n0.224\\n0.368\\n0.277\\n0.223\\n0.177\\n0.076\\nPySTEPS (LK)\\n0.195\\n0.299\\n0.227\\n0.213\\n0.147\\n0.087\\nPySTEPS (DARTS)\\n0.192\\n0.301\\n0.231\\n0.210\\n0.135\\n0.083\\nCasCast\\n0.237\\n0.388\\n0.304\\n0.247\\n0.208\\n0.039\\nTUPANN (ours)\\n0.277\\n0.398\\n0.309\\n0.298\\n0.237\\n0.146\\nManaus\\nEarthformer\\n0.417\\n0.502\\n0.476\\n0.472\\n0.414\\n0.220\\nNowcastNet\\n0.384\\n0.457\\n0.437\\n0.429\\n0.372\\n0.228\\nPySTEPS (LK)\\n0.319\\n0.385\\n0.369\\n0.349\\n0.271\\n0.222\\nPySTEPS (DARTS)\\n0.314\\n0.386\\n0.371\\n0.350\\n0.269\\n0.195\\nCasCast\\n0.401\\n0.486\\n0.447\\n0.444\\n0.406\\n0.222\\nTUPANN (ours)\\n0.435\\n0.479\\n0.464\\n0.469\\n0.430\\n0.333\\nLa Paz\\nEarthformer\\n0.454\\n0.487\\n0.486\\n0.523\\n0.485\\n0.285\\nNowcastNet\\n0.439\\n0.472\\n0.451\\n0.479\\n0.458\\n0.335\\nPySTEPS (LK)\\n0.340\\n0.378\\n0.381\\n0.390\\n0.326\\n0.223\\nPySTEPS (DARTS)\\n0.356\\n0.398\\n0.405\\n0.412\\n0.339\\n0.225\\nCasCast\\n0.351\\n0.441\\n0.381\\n0.381\\n0.370\\n0.183\\nTUPANN (ours)\\n0.468\\n0.482\\n0.481\\n0.512\\n0.490\\n0.376\\n(iii) if the accumulation exceeds a threshold 𝜏, label the larger window [𝑡−4 hours, 𝑡+ 4 hours] as a rainy window.\\nFinally, we merge rainy windows that intersect. The threshold 𝜏= 120,000 was chosen empirically to balance excluding\\nnear-dry periods against obtaining a dataset large enough for effective learning. Using a symmetric ±4-hour window\\nensures that events include both onset and dissipation phases (from no precipitation to mild or heavy precipitation and\\nback).\\n22\\n\\nPrecipitation nowcasting of satellite data using physically-aligned neural networks\\nTable 8: Aggregated CSI metrics for Toronto with GOES-16 data. Bold denotes the best, underlined the second best.\\nModel\\nCSI-M ↑\\nCSI4 ↑\\nCSI8 ↑\\nCSI16 ↑\\nCSI32 ↑\\nCSI64 ↑\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nPOOL1\\nPOOL4\\nEarthformer\\n0.209\\n0.187\\n0.288\\n0.280\\n0.233\\n0.203\\n0.154\\n0.133\\n0.102\\n0.084\\n0.000\\n0.000\\nNowcastNet\\n0.206\\n0.216\\n0.278\\n0.294\\n0.225\\n0.238\\n0.151\\n0.161\\n0.100\\n0.105\\n0.000\\n0.000\\nPySTEPS (LK)\\n0.179\\n0.173\\n0.250\\n0.240\\n0.200\\n0.186\\n0.130\\n0.120\\n0.078\\n0.070\\n0.000\\n0.000\\nPySTEPS (DARTS)\\n0.181\\n0.174\\n0.252\\n0.242\\n0.203\\n0.188\\n0.133\\n0.121\\n0.080\\n0.071\\n0.000\\n0.000\\nCasCast\\n0.192\\n0.203\\n0.270\\n0.286\\n0.217\\n0.231\\n0.145\\n0.158\\n0.096\\n0.105\\n0.000\\n0.000\\nTUPANN\\n0.219\\n0.222\\n0.295\\n0.295\\n0.239\\n0.231\\n0.159\\n0.158\\n0.106\\n0.105\\n0.000\\n0.000\\nTUPANN–Multicity\\n0.229\\n0.230\\n0.305\\n0.298\\n0.250\\n0.241\\n0.170\\n0.165\\n0.114\\n0.107\\n0.000\\n0.000\\n23\\n',\n",
       " 'Online motion recognition\\n.\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 1 of 8\\narXiv:2511.05250v1  [cs.CV]  7 Nov 2025\\n\\nHighlights\\nAccurate online action and gesture recognition system using detectors and Deep SPD Siamese\\nNetworks\\nMohamed Sanim AKREMI, Rim SLAMA, Hedi TABIA\\n• The \"SPD Siamese Network\" is efficient to resolve motion recognition problem\\n• An online system based on a detector and a classifier achieves well performances\\n• The proposed algorithm is accurate in different contexts and on challenging datasets\\n\\nAccurate online action and gesture recognition system using detectors\\nand Deep SPD Siamese Networks⋆\\nMohamed Sanim AKREMIa, Rim SLAMAb and Hedi TABIAa\\naIBISC, Univ Evry. Université Paris Saclay , Evry, Paris, France\\nbUniv. Eiffel, ENTPE, LICIT-ECO7, F-69518, Lyon, France\\nA R T I C L E I N F O\\nKeywords:\\nManifold approaches\\nSPD learning model\\nSiamese network\\nDeep learning\\nOnline gesture and action recognition\\n3D skeletal data\\nA B S T R A C T\\nOnline continuous motion recognition is a hot topic of research since it is more practical in\\nreal life application cases. Recently, Skeleton-based approaches have become increasingly popular,\\ndemonstrating the power of using such 3D temporal data. However, most of these works have focused\\non segment-based recognition and are not suitable for the online scenarios. In this paper, we propose\\nan online recognition system for skeleton sequence streaming composed from two main components:\\na detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a\\nSiamese network. The powerful statistical representations for the skeletal data given by the SPD\\nmatrices and the learning of their semantic similarity by the Siamese network enable the detector\\nto predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure\\nthe classifier capability to recognize the motion in each predicted interval. The proposed detector is\\nflexible and able to identify the kinetic state continuously. We conduct extensive experiments on both\\nhand gesture and body action recognition benchmarks to prove the accuracy of our online recognition\\nsystem which in most cases outperforms state-of-the-art performances.\\n1. Introduction\\nHuman activity recognition is an important research\\ntopic in pattern recognition field. It has been the subject\\nof many studies in the past two decades because of its\\nimportance in numerous areas such as security, health, daily\\nactivity, energy consumption and robotics. Recently, some\\nworks on the recognition of hand gestures or human ac-\\ntions from skeletal data are based on the modeling of the\\nskeleton’s movement as manifold-based representation and\\nproposed deep neural networks on this structure [1, 2, 3].\\nThese approaches demonstrated their potential in the pro-\\ncessing of skeletal data. Most of them are applied on offline\\nhuman action recognition which is useful in time-limited\\ntasks. However, in many applications, simply recognizing a\\nsingle gesture in a given segmented sequence is not enough,\\nespecially in monitoring systems and virtual-reality devices\\nwhich need to detect human movements moment by moment\\nin continuous videos.\\nIn these online recognition systems, it is important to\\ndetect the existence of an action as early as possible after\\nits beginning. It is also essential to determine the nature of\\nthe movement within a sequence of frames, without having\\ninformation about the number of gestures present within\\nthe video, their starting times or their durations, unlike the\\nsegmented action recognition.\\nIn this paper, we propose to use a manifold-based model\\nin order to build an online motion recognition system that\\ndetects and identifies different human activities in unseg-\\nmented skeletal sequences. As demonstrated in our previous\\nwork [4], the SPD Siamese network is accurate for hand\\nmohamedsanim.akremi@univ-evry.fr (M.S. AKREMI);\\nrim.slamasalmi@entpe.fr (R. SLAMA); hedi.tabia@univ-evry.fr (H.\\nTABIA)\\nORCID(s): 0000-0000-0000-0000 (M.S. AKREMI)\\ngesture classification. Thus, we decide to generalize our\\nwork on 3D body skeleton representation and adapt it for\\naction recognition problem. Besides, we upgrade our model\\nto make it as suitable as possible for online recognition\\ncriteria [5].\\nOur proposed online recognition system is composed\\nfrom (i) a detector which segments the sequence and (ii) a\\nclassifier which identifies the action or the gesture.\\nIn the following, we review both segmented and continu-\\nous human motion recognition methods. A focus is given to\\nmanifold deep learning approaches applied to 3D skeleton\\ndata in the offline recognition approaches. For the Online\\nrecognition system, we review the approaches applied for\\nsequence segmentation and online recognition, whatever the\\ntype of data. An overview of our approach is also given\\nto outline the main steps of the proposed solution. The\\npretrained models and the code are provided at the following\\nlink.\\n1.1. Related works\\nMany methods based on manifold learning approaches\\nare proposed in proceeding 3D skeletal data. Vemulapalli et\\nal. [3] worked on the special Euclidean Group denoted by\\nSE(3). They mapped the action curves from SE(3) to the lie\\nAlgebra se(3). Then, they computed a nominal curve using\\nDynamic Time Warping (DTW) and used Fourier Temporal\\nPyramid (FTP) representation to eliminate the noise issue\\nbefore the use of the classification with SVM. Huang et al.\\n[6] proposed a Lie group Network (LieNet) which based\\non an input rotation matrix, applies mapping transformation\\nin order to construct the best rotation matrix. Other studies\\nfocus on the SPD manifold-based approaches: a Riemannian\\nmetric learning for SPD Matrices was proposed by Huang\\net al.\\n[2]; a learning matrix neural network using mean\\nand covariance statistics was proposed by\\n[7] . A deep\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 1 of 8\\n\\nOnline motion recognition\\nneural network on Grassmann manifold, denoted by GrNet,\\nwas proposed by [1]. It is composed of 3 major blocks: a\\nprojection block used for the transformation of the orthonor-\\nmal input matrices, a pooling block designed to map the\\northonormal matrices and apply a mean pooling on them,\\nand an output block utilized for mapping and classification.\\nThese approaches, as proposed, work well on segmented\\nsequences. In order to adapt them to an online recognition\\ntask, introducing online approaches into these methods is\\nrequired. The well-known adopted strategy in identifying\\nonline motions is based mainly on determining the time\\ninterval for each motion, then brings the problem to an\\noffline recognition task. Some studies, such as [8], went\\nto monitor movements by studying the changes that occur\\nduring the transition from stagnation to active state. Then,\\nthey analyzed spatially limbs behavior in the stagnation.\\nHowever, this automatic segmentation did not generally\\nobtain the required efficacy. Some works have also gone\\naway from segmentation and are content with knowing the\\nmovement locally [5, 9]. Other approaches proposed various\\nsliding window strategies for online recognition purpose\\n[10, 11, 12]. A Temporal Recurrent Network (TRN) on fixed\\nlength windows was proposed by\\n[13] to anticipate the\\nfuture using a temporal decoder incorporating future infor-\\nmation with the historical information to improve online\\naction detection. Delamare et al. [10] used a sliding window\\napproach to execute Spatio-Temporal Graph Convolutional\\nNetwork (ST-GCN).\\n1.2. Contributions and method overview\\nThe major contributions of this paper are as follows: (1)\\nA new portioning for the human body skeleton is proposed,\\nenabling the \"SPD Siamese Network\" to operate on action\\nrecognition context. (2) A detector and a verification process\\nare introduced in order to upgrade our method from an offline\\nto an online system able to identify hand gestures or body ac-\\ntions in unsegmented sequences with high performance and\\nfast reaction. (3) The efficiency of the proposed algorithm is\\nproved in different contexts and datasets.\\nFirst, we propose to build a network that recognizes hand\\ngestures and body actions using a proposed partitioning of\\nbody/hand joints, SPD matrices spatio-temporal representa-\\ntions of the skeletal sequence and the characteristics of the\\nSiamese network.\\nIn a next stage, we build an online system based on\\ntwo main components: a detector and a classifier that use\\nthe proposed SPD Siamese network for the identification of\\nthe kinetic state and the recognition of the motion respec-\\ntively. The detector has as a role to identify continuously\\nthe kinetic state and find the segments of each performed\\nmotion throughout a long sequence: it predicts the starting\\nframe and the end frame of each performed motion by\\ndetecting a change in the kinetic state of the volunteer. As\\nillustrated in Figure 1, the change in the kinetic state may\\nrefer to an action beginning, an action end or an unexpected\\nidle period. In order to verify the nature of the state, we\\nrealize a Verification Process (VP). After each segmentation,\\nthe classifier is activated in order to recognize the motion\\nsegment.\\nThe rest of the paper is constructed as follows: section\\n2 presents the proposed network for motion recognition.\\nSection 3 presents the aspects of our online system. In\\nsection 4, the experiments results are reported. Finally, the\\nlast section is dedicated for the conclusion.\\n2. SPD Siamese neural network for motion\\nrecognition\\nThe SPD Siamese network aims to build a classifier\\ncapable of recognizing the actions performed by humans,\\nand a detector that identifies the kinetic state. It uses specific\\npartitioning for the body and hand joints. Then, it learns\\nan SPD matrix representing a motion sequence. Finally, a\\nSiamese network is used to finalize the classification of this\\nsequence.\\n2.1. Proposed architecture\\nThe proposed network, illustrated in Figure 2, is com-\\nposed of five principal components: preprocessing compo-\\nnent, partitioning component, SPD learning features com-\\nponent, SPD Siamese network component, and classification\\ncomponent.\\nHaving the 3D skeleton sequence, we divide the joint set\\ninto parts in order to perform the spatial analysis. The joints\\nfrom each part must have a correlation between each other.\\nIn the SPD learning component, we analyze the spatial-\\ntemporal evolution and the temporal-spatial evolution in\\norder to obtain the best SPD matrix representing the skeleton\\nsequence. For the SPD Siamese network component, we use\\nas a base model the network proposed by [2] without trans-\\nformation blocks since we realized the SPD learning in the\\nprevious component. We twin the two previous components\\nand use the contrastive loss function to train our model.\\nFinally, we use the K-NN algorithm on the learnt model\\nparameters applied to the base SPD network component for\\nthe classification.\\n2.2. Proposed representation for body/hand parts\\nHaving 3D skeletal joint coordinates, we divide the\\nhand/body skeleton into parts in order to study the evolution\\nof the coordinates locally. As explained in\\n[4], for the\\nhand, we consider that each finger represents a part (Figure\\n3(a)). As for the body, the proposed partitioning consists in\\ncombining each joint of the body to an adjacent joint, starting\\nfrom the head down to the other parts of the body. When a\\njoint has more than one adjacent, it produces ramifications\\nwhich we follow and divide the body into into four parts\\n(Figure 3(b)): (i) upper right part that runs from the head\\ndown to the palm of the right hand, (ii) upper left part that\\ngoes from the head part to the palm of the left hand, (iii)\\nlower right part which includes the spine connected to the\\nright leg, (iv) and lower left part which includes the spine\\nconnected to the left leg.\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 2 of 8\\n\\nOnline motion recognition\\n2.3. SPD matrix learning\\nIn order to learn the better SPD matrix representation, we\\nuse the ST-TS-HGR-Network inspired by [7]. This network\\nis composed from four principal phases. The first phase is the\\nconvolution layer. It highlights the correlation between the\\nneighboring joints, and it learns the filter weight associated\\nto each neighbor of a given joint. We apply 3×3 convolution\\non the matrix representation of the partitioning. Then, we\\ndivide the sequence into 6 sub-sequences: the first represents\\nthe whole sequence, the second and the third represent the\\ntwo halves of the sequence and the rest represents its three\\nthirds. We perform a spatial-temporal analysis in the ST-\\nGA-NET phase and a temporal-spatial analysis TS-GA-NET\\nphase of body/hand parts evolution. Each phase contains five\\nlayers. The first layer, Gaussian Aggregation (GA) outputs\\nthe SPD matrix 𝑌\\n=\\n[\\nΣ + 𝜇𝜇𝑇\\n𝜇\\n𝜇𝑇\\n1\\n]\\n, where 𝜇and Σ\\ndenote respectively the mean and the covariance of the joints\\npositions spatially in ST-GA-NET and temporally in TS-\\nGA-NET. Then, we apply a ReEig layer to rectify the SPD\\nmatrices, outputs of GA layer, by tuning up their small\\npositive eigenvalues using a chosen threshold. The LogEig\\nand VecMap layers are used to map the resulting SPD\\nmatrices into the euclidean space. Then, a second GA layer\\ncomputes the mean and the covariance along the temporal\\ndomain in ST-GA-NET and along the spatial domain (parts)\\nin TS-GA-NET. In the final phase, SPDC-NET has as a\\nfunction to generate more compact and discriminative SPD\\nmatrix. It takes as input the resulting SPD matrices from\\nboth ST-GA-NET and TS-GA-NET, returning as output 𝑌=\\n∑𝑊𝑖𝑋𝑖𝑊𝑇\\n𝑖, where its parameters {𝑊𝑖} are Stiefel weights\\nand {𝑋𝑖} are the outputs from both previous components.\\nFor more details on equations used in each layer, we refer\\nreaders to [4].\\n2.4. SPD Siamese network and motion recognition\\nThe SPD Siamese network consists of two identical\\nSPD sub-networks joined at their outputs and it is char-\\nacterized by its margin parameter 𝑔. We have as input,\\npair of two SPD matrices, with a binary label 𝑏. In each\\nsub-network of the SPD Siamese network, we map an\\nSPD matrix into a tangent space, and we extract a feature\\nvector using a fully connected layer. Then, we use the\\nContrastive Loss (𝐶𝐿) function to measure the distance\\nbetween the two extracted feature vectors and minimize\\ndistance between positive pairs using the following equation:\\n𝐶𝑙(𝑦1, 𝑦2, 𝑏) = 𝑏||𝑦1 −𝑦2||2 + (1 −𝑏)𝑚𝑎𝑥(0, 𝑔−||𝑦1 −𝑦2||2)\\nWhere 𝑦1 and 𝑦2 are the outputs of the two twin SPD\\nsub-networks and ||.||2 is the Euclidean distance. For the\\nmotion recognition, we apply K-Nearest Neighbor (K-NN)\\nalgorithm with 𝐾= 1.\\n3. Online action and gesture recognition using\\na detector\\nThe effectiveness of our online system is related to\\nthe performance of the proposed detector and its ability to\\npredict the motions intervals throughout a long sequence.\\nWe are also interested to study the efficiency of our system\\nwithin an early classification.\\n3.1. Proposed detector\\nThe detector preparation goes through two stages: train-\\ning loop and testing loop. As a first step, we train the\\ndetector with the segmented sequences. Then, we evaluate\\nits performance on unsegmented sequences.\\nTraining loop: In most sequences, there is a period of\\ninactivity between every two successive motions, called idle\\nperiod or stagnation state. In order to distinguish between\\nsuch an idle period (State \"0\") and a period of activity\\n(State \"1\") and detect the transition of state, we use a binary\\ndetector.\\nSometimes, we have sequences in which there is not nec-\\nessarily an idle separating between two actions. In this case,\\nwe use a multi-class object detector to detect the transition\\nof the kinetic states between one action and another. Let\\nN be the number of classes in the training dataset. While\\na subject is performing an action 𝑖, we attribute the state\\n𝑖to each possible subsequence extracted from its interval.\\nWhen a transition from a state 𝑖to another state 𝑗is detected\\nand verified, we consider that there is a transition from an\\naction to another. Actually, in the training process, we extract\\nfrom each sequence randomly subsequences with equal size:\\n𝑤𝑠frames. This size should be long enough for a person to\\nperform part from an action (longer than half a second). We\\nattribute to the subsequence the dominant state class, i.e.,\\nwhen the trained sample contains two different kinetic states,\\nit will be labeled according to the longer state within it. Same\\nas the classifier, we use the SPD Siamese Network to train\\nthe detector (see section 2).\\nTesting loop: In the testing loop, we adopt the sliding\\nwindow method proposed by [10], augmenting it by the\\ntrained detector and a Verification Process (V.P) that helps\\nour system in reducing False Positive (FP) rate and giving\\nmore accurate segmentation. Unlike the method proposed\\nby\\n[11] who relies on clustering sub-activities using his\\nproposed detector (that he calls SSD), we propose a su-\\npervised method to train our detector. Instead of having\\nunknown numerous groups of sub-activities, we consider\\nthese sub-activities belonging to more general classes (\"0\"\\nand \"1\" in the case of a binary detector). Besides, we use\\nmajority voting rather than Markov probability matrix for\\nsequence segmentation. since in case of high similarity of\\nsub-activities (like in \"Swipe X\" and \"Swipe V\"), the perfor-\\nmance of Markov probability matrix in predicting the classes\\nof the consecutive sub-activities is generally not accurate.\\nLet 𝑤𝑠be the size of the window (the same size of the\\ntraining subsequences), 𝑟be the refresh rate of the detector\\nand 𝑐𝑟be the capturing rate of the used sensor. Each 𝑟frames,\\nthe detector reveals the kinetic state on each window, i.e.,\\nthe segments on which the detector is applied are [0, 𝑤𝑠],\\n[𝑟, 𝑤𝑠+𝑟], [2𝑟, 𝑤𝑠+2𝑟]... As we are dealing with continuous\\nsequences, we have to set 𝑟≤0.3 ∗𝑐𝑟frames and ensure that\\nthe running time of the detector does not exceed 𝑟frames.\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 3 of 8\\n\\nOnline motion recognition\\nWhen the state of the detector moved from a state 𝐴to\\na different state 𝐵, we need to verify if it is a real change\\nof a kinetic state or a false detection. To this end, we realize\\na verification process based on a bunch of repetitive tests\\nto confirm this transition. Let 𝑡𝑒be the number of these\\ntests. The verification starts from the second test until the\\n𝑡𝑒𝑡ℎtest while retaining the states of the detector within\\neach window. We use the method of \"majority voting\" on\\nstates resulting from each test to make our decision on\\nthe transition to be checked. For example (see Figure 5),\\nwith 𝑡𝑒\\n=\\n5, when a binary detector detects a transi-\\ntion in the kinetic state from state \"0\" to \"1\" at the win-\\ndow [𝑁−𝑤𝑠, 𝑁], we realize 4 other tests on the segments\\n[𝑁+ 𝑘× 𝑟−𝑤𝑠, 𝑁+ 𝐾× 𝑟](𝑘=1,2,3,4). We suppose that the\\nstates detected are [0, 1, 1, 1, 0]. The change of state is con-\\nfirmed in 3 tests among 5. So, we can conclude that (𝑁−𝑟)𝑡ℎ\\nframe is the starting frame of an action. It is important to\\nnotice that the detected starting frame is recognized only\\nafter (𝑡𝑒× 𝑟) frames. So, we need to minimize 𝑡𝑒and 𝑟as\\nmuch as possible, relying on a high performing detector with\\nfast reaction time.\\n3.2. Early classification\\nIn the context of daily human activities\\n[9, 14] or\\nhand gesture context [15, 16], we can tolerate a delay of\\nno more than one second. However, in industrial context\\n[17] no delay is tolerated, the online recognition should\\nbe instantaneously even before the action is finished (Early\\nclassification is needed (see Figure 6).\\nWe define 𝑇as\\nthe maximum time used to recognize a motion. In order\\nto ensure that we will not exceed 𝑇seconds, we set 𝑡𝑒≤\\n𝑇\\n𝑟∗𝑐𝑟. Once a frame is recognized as the starting frame\\nof a motion, we wait until the next 𝑇frames coordinates\\nare provided. Then the classifier recognizes the motion. If\\nthe end frame of a motion is detected before reaching the\\ndeadline we set, the classifier will give the class of the\\nmotion between the two predicted boundaries, just like the\\nno-early classification case.\\n4. Experiments and results\\nOur method works for both online hand and action recog-\\nnition purpose. Thus, we evaluate it on four challenging\\ndatasets captured in different contexts. In the following, we\\nextensively provide details on the experimental settings and\\nresults obtained for each dataset. We also compare the state-\\nof-the-art methods to our approach using online metrics. Our\\nmodel is implemented on an octa-core CPU running at 3.2\\nGHz with 32GB RAM in python 3.9.7 environment. In the\\nwhole sequence, R.T denotes running time.\\n4.1. Datasets\\nOnline Dynamic Hand Gesture dataset (ODHG) [15]:\\nIt consists of 280 relatively long video clips taken with 28\\nvolunteers. Each video contains 10 hand gestures. The data\\nis captured by the depth camera (30 fps) and consists of 14\\nhand gesture sequences performed in two ways: using one\\nfinger and the whole hand.\\nSHREC 2021 gesture benchmark dataset\\n[16]: It\\nconsists of 180 unsegmented hand skeletal sequences cap-\\ntured by LeapMotion sensor at rate of 25 fps. 60% of these\\nsequences are used in the training loop and and the rest are\\ndedicated for the test. The dataset dictionary is made of 17\\ngestures divided in static gestures and dynamic gestures.\\nOnline Action Detection dataset (OAD)\\n[9]: It in-\\ncludes 59 long sequences representing 839 actions, captured\\nat 8 fps. The possible actions revolve around 10 action\\nclasses. It gives the coordinates of 25 joints from different\\nbody parts. We have 30 sequences for training and 20 se-\\nquences for testing.\\nUOW Online Action 3D dataset [14]: It consists of 48\\nskeleton sequences recorded using the kinect V2 sensor (20\\nfps). It has 21 action classes. Without a stagnation between\\ntwo consecutive actions, its actions are performed with\\ntwo manners: repeatably and continuously. The sequences\\ncontaining repeated actions are used in the training and the\\ncontinuous sequence is dedicated for the testing loop.\\nIndustrial Human Action Recognition Dataset (In-\\nHard) [17]: It is collected in industrial context of human\\nrobot collaboration. It contains 38 long sequences: 26 se-\\nquences used in the training and 12 sequences are dedicated\\nfor the test. The possible actions are 13 Meta actions. The\\nskeletal data comprises the 3D coordinates and the 3 rota-\\ntions around the axis of 21 body joints.\\n4.2. Evaluation of the classifier on segmented\\nsequences\\nThe experiments performed for the evaluation of the\\nclassifier follow the concept of the segmented motion recog-\\nnition. The training process and the validation of the classi-\\nfication are based on the proposed SPD Siamese network.\\nFirst of all, we start by data cleaning to remove corrupt or\\ninaccurate records. For InHard dataset, we remove two long\\nsequences P04_R01 and P04_R02 in which there is no de-\\nscription of the evolution of the 3D coordinates. Moreover,\\nfor this dataset, we remark that working with the derivates\\nof the sequences gives better performance comparing to that\\ngiven by the position evolution studying.\\nAfter data preprocessing, we prepare the segmented\\nsequences. Using the groundtruth information, we extract\\nthe actions sequences performed within the continuous se-\\nquences with respect to their train/test protocols mentioned\\nin their description. In the partitioning component (as illus-\\ntrated in Figure 3), we follow the matrix representation of\\nthe hand parts for ODHG and SHREC 2021 datasets and the\\nmatrix presentation of the body parts for the other datasets.\\nAs preprocessing, the input sequences are normalized\\nand interpolated to 500 frames for ODHG and SHREC 2021,\\nto 200 frames for OAD and UOW datasets and to 600 frames\\nfor InHard. The difference of the number of the interpolated\\nframes are due to the difference between the capturing rate\\nand the average duration of the motion in each dataset. For\\nthe rest of the components, we keep the same configurations\\nset on the experiment section of [4].\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 4 of 8\\n\\nOnline motion recognition\\nPartioning 1\\nPartioning 2\\nOurs\\n1\\n0\\n6\\n7\\n21\\n22\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n8\\n9\\n10\\n11\\n23\\n24\\n5\\n4\\n20\\n2\\n3\\nAcc: 94.46%\\nAcc: 94.81%\\nAcc: 96.19%\\nTable 1\\nSPD Siamese network performance with respect to partitioning\\nstrategy on OAD segmented sequences.\\nTable 2\\nSPD Siamese network performance on the datasets classifiers\\nDataset\\nODHG\\nSHREC\\nOAD\\nUOW\\nInHard\\nAccuracy(%)\\n95.60\\n95.92\\n96.19\\n98.84\\n79.38\\nRunning time(ms)\\n173\\n253\\n138\\n118\\n352\\nBody partitioning strategies: Different partitioning\\nstrategies are assessed and compared in order to select the\\nmore efficient one for body skeleton. As illustrated in Table\\n1, three strategies are considered. The first proposed by [18],\\ndivide the skeleton into 5 parts, the second, proposed by Ji\\net al. [19] divide the body into 10 parts and the third is\\nthe one we proposed. Experiments, conducted on segmented\\nsequences of OAD dataset have proven the efficiency of the\\nthird strategy we propose in term of classification accuracy.\\nThis can be explained by the fact that our strategy created\\nmore interconnection between adjacent joints and enhanced\\nit in the convolution component. In the following, we exploit\\nit for the action recognition context.\\nClassifiers performance: Using the settings described\\nin this section and the the body/hand partitioning mentioned\\nin section 2.2, we apply the SPD Siamese network on the\\ndifferent benchmarks: ODHG and SHREC 2021 for hand\\ngesture recognition and OAD, UOW and InHard datasets for\\nbody action recognition. According to the Table 2, the per-\\nformance of the proposed classifier are proved on both hand\\ngesture and action recognition scenarios. In fact, an accuracy\\nover 95% is achieved in almost experimented datasets except\\nfor InHard which is a new dataset very challenging one and\\nis collected in real industrial environment. The achieved per-\\nformances on action recognition datasets demonstrate that\\nwe have succeeded to generalize the SPD Siamese network\\non body skeleton sequences with an acceptable running time.\\n4.3. Ablation study of the detector and online\\nexperiments\\nThere are different levels of configurations to consider\\nfor the proposed detector: the one related to neural network\\nand classification performance and the configuration of slid-\\ning window and verification process parameters. The neural\\nnetwork of our detector, the SPD Siamese neural network,\\nis kept with the same parameters set for the classifier. Only\\ninterpolation step is considered with fewer number of frames\\nsince the detector training sequences has shorter. Besides,\\nmany experiments are conducted to evaluate our system in\\nterm of running speed. For the evaluation of the detection\\nprocess, we use the following metrics: Jaccard index, predic-\\ntion accuracy, prediction accuracy, F1-score, SL-score, EL-\\nscore. To better understand these metrics, we refer readers\\nto [9, 16].\\nInfluence of window size variation\\nWe recall that window size has to be long enough to\\ndetect the kinetic of human in continuous sequences but\\nnot too long that it exceeds some human action duration\\nor affects the detector rapidity. The kinetic state needs to\\nbe detected 4 or 5 times per second at least. We set 𝑟=\\n6, 5, 2, 4, 6 frames to be the refresh rates respectively of\\nODHG, SHREC 2021, OAD, UOW and InHard datasets.\\nThen, we vary 𝑤𝑠values corresponding to each dataset\\n(see Table 3). The different variations of 𝑤𝑠regarding each\\ndataset is due to the difference of the sensors capturing rate.\\nAccording to the results described in Table 3, the detector\\nrunning time did not exceed 120 ms even when varying 𝑤𝑠in\\ndifferent intervals. It enables us to work online and detect the\\nkinetic state 4 or 5 times per second, which is an acceptable\\nfrequency on human action recognition task.\\nRegarding the performance of the detector, we remark that\\nthe accuracy of the detector is enhanced by the elevation\\nof the window size in the datasets. This is expected since\\nwith this elevation, the detector is able to generate more\\ninformation about the motion evolution. However, the be-\\nhavior of the other metrics is different, as they continue\\nto rise until a peak value\\n̂𝑤𝑠is reached. ( ̂𝑤𝑠=18, 20, 6,\\n12, 18 are respectively the peak values of window sizes for\\nODHG, SHREC 2021, OAD, UOW and InHard datasets).\\nFor a longer window size, the reported performances shrink.\\nThis can be explained by to the fact that the exaggerated\\nincrease of windows size makes the window covering rapidly\\ndifferent transitions of kinetic status in the case of actions\\nwith short duration. This causes ambiguity in identifying\\nthe true kinetic state and affects the system performance,\\nespecially in motion boundaries prediction (SL-score and\\nEL-score remarkably decrease). Reducing the number of\\nframes excessively will lead to a lack of available informa-\\ntion about the ongoing state for the detector and thus will\\nlead to difficulty in the online recognition. It is remarkable\\nthat the best performances correspond to a window size\\ncompromise between 0.6s and 0.8s. This Time interval can\\nbe considered to be the interval of visibility for the detector.\\nInfluence of the V.P parameters variation\\nIn this experiment, we vary 𝑡𝑒: the number of tests\\nperformed by the detector during the verification process.\\nThe results are shown in the Table 4. We remark that the\\nbest performance is given by 𝑡𝑒= 3 for all the datasets except\\nOAD for which the peak is given by 𝑡𝑒= 2. This difference\\nis due to different reasons. Firstly, the detector performance\\nof OAD dataset is the highest performance comparing to\\nthe other detectors. Since the tests are performed in order to\\nreduce the error rate at the detector level, it is expected that\\nthe detector of the higher performance will need a smaller\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 5 of 8\\n\\nOnline motion recognition\\nTable 3\\nModel performance for different settings of 𝑤𝑠\\nODHG\\nSHREC 2021\\nOAD\\nUOW Online Action 3D\\n𝑤𝑠(frames)\\n12\\n18\\n24\\n15\\n20\\n30\\n4\\n6\\n10\\n8\\n12\\n20\\nDetector R.T (ms)\\n93\\n100\\n105\\n52\\n58\\n69\\n77\\n85\\n99.7\\n80\\n82\\n85\\nDetector accuracy (%)\\n73.11\\n75.89\\n77.12\\n92.76\\n92.82\\n93.31\\n86.97\\n88.34\\n89.34\\n61.13\\n76.09\\n77.5\\nDetection accuracy\\n0.723\\n0.771\\n0.739\\n0.737\\n0.770\\n0.615\\n0.619\\n0.901\\n0.801\\n0.802\\n0.84\\n0.812\\nSL-score\\n0.627\\n0.676\\n0.645\\n0.674\\n0.694\\n0.544\\n0.561\\n0.796\\n0.715\\n0.708\\n0.747\\n0.719\\nEL-score\\n0.601\\n0.661\\n0.632\\n0.658\\n0.691\\n0.526\\n0.593\\n0.803\\n0.715\\n0.73\\n0.733\\n0.725\\nGloabl F1-score\\n0.713\\n0.769\\n0.732\\n0.696\\n0.720\\n0.632\\n0.763\\n0.915\\n0.859\\n0.852\\n0.881\\n0.862\\nTable 4\\nEvolution of model performance according to 𝑡𝑒\\nODHG\\nSHREC 2021\\nOAD\\nUOW Online Action 3D\\nInHard\\nte\\n3\\n4\\n5\\n3\\n5\\n2\\n3\\n5\\n2\\n3\\n5\\n3\\nDetector accuracy\\n75.89%\\n92.82%\\n88.34%\\n76.09%\\n67.33%\\nDetection accuracy\\n0.771\\n0.752\\n0.742\\n0.770\\n0.737\\n0.901\\n0.829\\n0.801\\n0.74\\n0.84\\n0.414\\n0.675\\nSL-score\\n0.676\\n0.645\\n0.63\\n0.694\\n0.665\\n0.796\\n0.762\\n0.727\\n0.675\\n0.747\\n0.351\\n0.578\\nEL-score\\n0.661\\n0.645\\n0.65\\n0.691\\n0.66\\n0.803\\n0.775\\n0.762\\n0.664\\n0.733\\n0.341\\n0.564\\nGloabl F1-score\\n0.769\\n0.736\\n0.746\\n0.720\\n0.716\\n0.915\\n0.89\\n0.876\\n0.826\\n0.881\\n0.499\\n0.661\\nnumber of tests. More importantly, the actions durations are\\narbitrary in OAD dataset: some actions last less than half a\\nsecond and others last more than half a minute. The detector\\nextends the window size to 10 frames (1.25s) and may reach\\nthe idle period while performing 3 tests. In contrast, motions\\nin other datasets example are more regular and without\\nstagnation. Working with 𝑡𝑒> 5 gives performances slightly\\ninferior to those reported with 𝑡𝑒= 3 and with more time.\\nSo, we prefer to not exceed 3 tests.\\n4.4. Early classification experiments\\nWe set different deadlines for classification: 𝑇= [0.5s,\\n1s...3s]. We evaluate the performance of our system and to\\nwhat extent it can maintain its efficiency. The obtained F1-\\nscores are described in Figure 6.\\nObserving the obtained curves, we remark that the model\\ndidn’t show an efficiency in the lower deadlines. This can\\nbe explained that human activities generally need more\\nthan a second to be completely performed in the most\\ncases. Higher than one second, the curves show F1-scores\\namelioration especially for OAD, UOW and SHREC 2021\\nwhich approach rapidly to the ideal case. For ODHG and\\nInHard, it needs more time to move towards the F1-score\\nof the no-early classification case because the performances\\nof their detectors are lower than those of the other datasets.\\nAlso, they have some motions which are performed in the\\nsame manner at the beginning. For example, the first part of\\ngesture \"Tap\" and gesture \"Swipe Down\" in ODHG dataset\\nare too similar.\\n4.5. Comparison with the state-of-the art\\nIn order to assert the efficiency of our system, we need\\nto compare its performance with the previous proposed\\napproaches. Our experiments are reported on both online\\nhand gesture and online action recognition contexts.\\nComparison of Running Speed:\\nExperiments on answer time are carried out on SHREC\\n2021 challenge [16] and comparison is made with different\\napproaches proposed by four groups who give their perfor-\\nmances on this dataset. The average running time taken by\\nGroup\\nClass time (s)\\nTotal time (s)\\n1 [12, 16]\\n1.36\\n435.5\\n2 [20, 16]\\n0.41\\n48781.0\\n3 [21, 16]\\n0.6 × 10-4\\n66.7\\n4 [22, 16]\\n0.066\\n94.6\\n𝑂𝑢𝑟𝑠\\n0.038\\n3836.88\\nTable 5\\nExecution time corresponding to different approaches on\\nSHREC 2021\\nthe detector to identify the kinetic state in each window and\\nin the whole sequences are described in Table 5.\\nWe remark that the average running times per single\\nprediction (class time) are very discarded. Our system takes\\nabout 69 ms to give a single prediction, enabling it to\\nidentify the kinetic states of more than 12 windows per\\nsecond in average. This frequency is acceptable in real live\\napplications. It is faster than approaches used by groups 1, 2\\nand group 4. However, group 3 proposed a faster approach,\\nthanks to the simple complexity of its model which helps\\nfor better responding time but not highly efficient in term of\\nperformance 6.\\nFor the global classification, our system takes about 3837\\nseconds to compute all the test sequences. These sequences\\ncontain more than 119K frames, capturing at 25 fps. They\\nlast about 4700s, which is close to the total time taken by our\\nsystem because the refresh rate of the system is designed to\\nmake the system execution in phase with real streaming.\\nState-of-the-art on hand gesture context datasets:\\nFor hand gesture recognition, we evaluate and compare\\nthe performance of our online system on SHREC 2021\\ndataset\\n[16]. Best performances of research groups par-\\nticipating in SHREC 2021 challenge are reported in Table\\n6. Only skeletal based approaches are considered for this\\ncomparison. The proposed metrics for evaluation in this\\nchallenge are Detection rate, Jaccard index and FP rate.\\nAccording to Table\\n6, our system reveals interesting\\nperformance and shows its superiority over the majority\\nof the proposed models. Only adapted ST-GCN approach\\nseems to outperform our system performance. This can be\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 6 of 8\\n\\nOnline motion recognition\\nMethod\\nDetection\\nrate\\nJaccard\\nindex\\nFP\\nrate\\nDissimilarity-based Classification [12, 16]\\n0.3993\\n0.2566\\n0.764\\nTransformer module [20, 16]\\n0.7292\\n0.6029\\n0.257\\nuDeepGRU+TSGR [21, 16]\\n0.7431\\n0.6238\\n0.271\\nST-GCN [22, 16]\\n0.8993\\n0.8526\\n0.066\\nOurs\\n0.770\\n0.667\\n0.230\\nTable 6\\nPerformance comparison with the state-of-the-art online meth-\\nods on SHREC 2021 gesture benchmark dataset\\nTable 7\\nPerformance comparison with the state-of-the-art methods on\\nbody action context datasets\\nDataset\\nOAD\\nUOW\\nInHard\\nMethod\\nMetric\\nF1-\\nscore\\nPred.\\nAcc\\nF1-\\nscore\\nPred.\\nAcc\\nF1-\\nscore\\nPred.\\nAcc\\nRNN-SW [23]\\n0.600\\n-\\n-\\n-\\n-\\n-\\nJCR-RNN [9]\\n0.653\\n0.788\\n-\\n-\\n-\\n-\\nST-LSTM [24]\\n-\\n0.770\\n-\\n-\\n-\\n-\\nAttention Net [25]\\n-\\n0.783\\n-\\n-\\n-\\n-\\nRF+ST [26]\\n0.672\\n-\\n-\\n-\\n-\\n-\\nMM-MT-RNN [27]\\n0.795\\n-\\n-\\n-\\n-\\n-\\nFSNet [28]\\n-\\n0.800\\n-\\n-\\n-\\nSSNet [28]\\n-\\n0.820\\n-\\n-\\n-\\nVGG16+L1 [29]\\n-\\n0.868\\n-\\n-\\n-\\n-\\nSW-CNN [10]\\n-\\n-\\n0.680\\n0.680\\n-\\n-\\nSW-GCN [10]\\n-\\n-\\n0.750\\n0.755\\n-\\n-\\nOur model\\n0.915\\n0.901\\n0.881\\n0.840\\n0.661\\n0.631\\nexplained by the fact that the used approach is not only\\nbased on online ST-GCN but uses also Trajectory-based fine\\ntuning approach to simplify the recognition in some specific\\ngestures. Besides, energy-based detection module seems to\\nbe a good choice on such context.\\nState-of-the-art on body action context datasets:\\nComparison with state-of-the-art approaches on online\\naction recognition datasets regarding the commonly used\\nmetrics F1-score and prediction accuracy are reported in\\nTable 7. We can notice that our model outperforms other\\nmodels, especially in UOW Online Action 3D dataset, which\\nwitnessed an increase of 17%. Besides, we achieve the\\nbest performance on OAD dataset in term of F1-score and\\nprediction accuracy. This proves the efficiency of the de-\\ntector, the key role played by the SPD Siamese model in\\nensuring the quality of the results given by the system and\\nthe importance of the tests realized in the beginning and the\\nend of each action. Finally, promising results are highlighted\\nin the challenging InHard dataset which represents specific\\nindustiral context. In fact, actions in continuous sequences\\nare performed without stagnation periods and with human-\\nrobot collaboration scenarios in some cases.\\n5. Conclusion\\nWe proposed an accurate manifold-based approach which\\nworks either for hand gesture sequences or for human body\\naction recognition. We built an online system composed\\nfrom a detector and a classifier. The detector is used for\\nthe prediction of the action intervals using a verification\\nprocess and the sliding window approach. The classifier\\nuses the segmentation predicted by the detector for motion\\nrecognition. We provided the experimental evaluation on\\nfive benchmark datasets and compared our approach with\\nthe recent state of the art methods. However, our approach\\ncan be more optimized, especially the classifier architecture\\nwhich can help to obtain lower execution time. As future\\nwork, we plan to study more the human motion recognition\\nchallenges especially in industrial context such as human\\nrobot interaction in real case study and in virtual environ-\\nment.\\nReferences\\n[1] Zhiwu Huang, Jiqing Wu, and Luc Van Gool. Building deep networks\\non grassmann manifolds. In Proceedings of the AAAI Conference on\\nArtificial Intelligence, volume 32, 2018.\\n[2] Zhiwu Huang and Luc Van Gool.\\nA riemannian network for spd\\nmatrix learning. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 31, 2017.\\n[3] Raviteja Vemulapalli, Felipe Arrate, and Rama Chellappa. Human\\naction recognition by representing 3d skeletons as points in a lie\\ngroup. In Proceedings of the IEEE conference on computer vision\\nand pattern recognition, pages 588–595, 2014.\\n[4] Mohamed Akremi., Rim Slama., and Hedi Tabia. Spd siamese neural\\nnetwork for skeleton-based hand gesture recognition. In Proceedings\\nof the 17th International Joint Conference on Computer Vision,\\nImaging and Computer Graphics Theory and Applications - Volume\\n4: VISAPP,, pages 394–402. INSTICC, SciTePress, 2022.\\n[5] Okan Köpüklü, Ahmet Gunduz, Neslihan Kose, and Gerhard Rigoll.\\nReal-time hand gesture detection and classification using convolu-\\ntional neural networks. In 2019 14th IEEE International Conference\\non Automatic Face & Gesture Recognition (FG 2019), pages 1–8.\\nIEEE, 2019.\\n[6] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool.\\nDeep learning on lie groups for skeleton-based action recognition. In\\nProceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 6099–6108, 2017.\\n[7] Xuan Son Nguyen, Luc Brun, Olivier Lézoray, and Sébastien\\nBougleux.\\nA neural network based on spd manifold learning for\\nskeleton-based hand gesture recognition.\\nIn Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\\npages 12036–12045, 2019.\\n[8] Maxime Devanne, Panagiotis Papadakis, et al.\\nRecognition of\\nactivities of daily living via hierarchical long-short term memory\\nnetworks. In 2019 IEEE International Conference on Systems, Man\\nand Cybernetics (SMC), pages 3318–3324. IEEE, 2019.\\n[9] Yanghao Li, Cuiling Lan, Junliang Xing, Wenjun Zeng, Chunfeng\\nYuan, and Jiaying Liu. Online human action detection using joint\\nclassification-regression recurrent neural networks.\\nIn European\\nconference on computer vision, pages 203–220. Springer, 2016.\\n[10] Mickael Delamare, Cyril Laville, Adnane Cabani, and Houcine\\nChafouk. Graph convolutional networks skeleton-based action recog-\\nnition for continuous data stream: A sliding window approach. In\\nVISIGRAPP (5: VISAPP), pages 427–435, 2021.\\n[11] Farhood Negin, Abhishek Goel, Abdelrahman G Abubakr, Francois\\nBremond, and Gianpiero Francesca.\\nOnline detection of long-\\nterm daily living activities by weakly supervised recognition of sub-\\nactivities. In 2018 15th IEEE International Conference on Advanced\\nVideo and Signal Based Surveillance (AVSS), pages 1–6. IEEE, 2018.\\n[12] Ariel Caputo, Andrea Giachetti, Franca Giannini, Katia Lupinetti,\\nMarina Monti, Marco Pegoraro, and Andrea Ranieri.\\nSfinge 3d:\\nA novel benchmark for online detection and recognition of hetero-\\ngeneous hand gestures from 3d fingers’ trajectories. Computers &\\nGraphics, 91:232–242, 2020.\\n[13] Mingze Xu, Mingfei Gao, Yi-Ting Chen, Larry S Davis, and David J\\nCrandall. Temporal recurrent networks for online action detection. In\\nProceedings of the IEEE/CVF International Conference on Computer\\nVision, pages 5532–5541, 2019.\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 7 of 8\\n\\nOnline motion recognition\\n[14] Chang Tang, Pichao Wang, and Wanqing Li. Online action recogni-\\ntion based on incremental learning of weighted covariance descrip-\\ntors. arXiv preprint arXiv:1511.03028, 2015.\\n[15] Quentin De Smedt, Hazem Wannous, and Jean-Philippe Vandeborre.\\nSkeleton-based dynamic hand gesture recognition. In Proceedings of\\nthe IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR) Workshops, June 2016.\\n[16] Ariel Caputo, Andrea Giachetti, Simone Soso, Deborah Pintani,\\nAndrea D’Eusanio, Stefano Pini, Guido Borghi, Alessandro Simoni,\\nRoberto Vezzani, Rita Cucchiara, et al. Shrec 2021: Skeleton-based\\nhand gesture recognition in the wild. Computers & Graphics, 99:201–\\n211, 2021.\\n[17] M. DALLEL, V. HAVARD, D. BAUDRY, and X. SAVATIER. Inhard\\n- industrial human action recognition dataset in the context of indus-\\ntrial collaborative robotics. In 2020 IEEE International Conference\\non Human-Machine Systems (ICHMS), pages 1–6, 2020.\\n[18] Liafisu Sina Yekini, Tomasz Piotr Wisniewski, and Yuval Millo.\\nMarket reaction to the positiveness of annual report narratives. The\\nBritish Accounting Review, 48(4):415–430, 2016.\\n[19] Xiaopeng Ji, Jun Cheng, Wei Feng, and Dapeng Tao.\\nSkeleton\\nembedded motion body partition for human action recognition using\\ndepth sequences. Signal Processing, 143:56–68, 2018.\\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.\\nAt-\\ntention is all you need. Advances in neural information processing\\nsystems, 30, 2017.\\n[21] Mehran Maghoumi. Deep recurrent networks for gesture recognition\\nand synthesis. Ph.D. thesis, 2020.\\n[22] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph\\nconvolutional networks for skeleton-based action recognition.\\nIn\\nThirty-second AAAI conference on artificial intelligence, 2018.\\n[23] Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li,\\nLi Shen, and Xiaohui Xie. Co-occurrence feature learning for skeleton\\nbased action recognition using regularized deep lstm networks. In\\nProceedings of the AAAI conference on artificial intelligence, vol-\\nume 30, 2016.\\n[24] Jun Liu, Amir Shahroudy, Dong Xu, Alex C Kot, and Gang Wang.\\nSkeleton-based action recognition using spatio-temporal lstm network\\nwith trust gates. IEEE transactions on pattern analysis and machine\\nintelligence, 40(12):3007–3021, 2017.\\n[25] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C Kot. Global\\ncontext-aware attention lstm networks for 3d action recognition. In\\nProceedings of the IEEE conference on computer vision and pattern\\nrecognition, pages 1647–1656, 2017.\\n[26] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim.\\nReal-time\\nonline action detection forests using spatio-temporal contexts.\\nIn\\n2017 IEEE Winter Conference on Applications of Computer Vision\\n(WACV), pages 158–167. IEEE, 2017.\\n[27] Jiaying Liu, Yanghao Li, Sijie Song, Junliang Xing, Cuiling Lan, and\\nWenjun Zeng. Multi-modality multi-task recurrent neural network for\\nonline action detection. IEEE Transactions on Circuits and Systems\\nfor Video Technology, 29(9):2667–2682, 2018.\\n[28] Jun Liu, Amir Shahroudy, Gang Wang, Ling-Yu Duan, and Alex C\\nKot. Skeleton-based online action prediction using scale selection\\nnetwork. IEEE transactions on pattern analysis and machine intelli-\\ngence, 42(6):1453–1467, 2019.\\n[29] Nassim Mokhtari, Alexis Nédélec, and Pierre De Loor.\\nHuman\\nactivity recognition: A spatio-temporal image encoding of 3d skeleton\\ndata for online action detection. In VISIGRAPP (5: VISAPP), pages\\n448–455, 2022.\\nWindow size\\nDetector\\nqueue \\nSkeletal sequence\\nstream\\nDetector\\nKINETIC \\nSTATE\\nNo change\\nChange\\nVP\\nVP\\nSF\\nEF\\nClassifier\\nMotion\\nNext refresh\\nframe\\nFigure 1: The proposed online motion recognition system.\\nCONTRASTIVE LOSS \\nFUNCTION\\nST-TS-HGR-NET\\nPARTITIONING\\nSPDNET\\nST-TS-HGR-NET\\nPARTITIONING\\nSPDNET\\nLABEL 1\\nLABEL 2\\nBINARY \\nLABEL\\nPREPROCESSING\\nPREPROCESSING\\nKNN\\nLABEL 1\\nKNN\\nLABEL 2\\nFigure 2: The overview of the proposed SPD Siamese network.\\n16\\n17\\n16\\n17\\n17\\n18\\n16\\n17\\n16\\n17\\n19\\n20\\n16\\n17\\n16\\n17\\n13\\n14\\n16\\n17\\n16\\n17\\n15\\n16\\n16\\n17\\n16\\n17\\n9\\n10\\n16\\n17\\n16\\n17\\n11\\n12\\n16\\n17\\n16\\n17\\n5\\n6\\n16\\n17\\n16\\n17\\n7\\n8\\n16\\n17\\n16\\n17\\n1\\n2\\n16\\n17\\n16\\n17\\n3\\n4\\n(a)\\n(b)\\n4\\n3\\n2\\n1\\n16\\n15\\n14\\n13\\n12\\n11\\n10\\n9\\n8\\n7\\n6\\n5\\n20\\n19\\n18\\n17\\n16\\n17\\n16\\n17\\n24\\n23\\n16\\n17\\n16\\n17\\n11\\n10\\n16\\n17\\n16\\n17\\n19\\n18\\n16\\n17\\n16\\n17\\n17\\n16\\n16\\n17\\n16\\n17\\n15\\n14\\n16\\n17\\n16\\n17\\n13\\n12\\n16\\n17\\n16\\n17\\n22\\n21\\n16\\n17\\n16\\n17\\n7\\n6\\n16\\n17\\n16\\n17\\n9\\n8\\n16\\n17\\n16\\n17\\n20\\n2\\n16\\n17\\n16\\n17\\n0\\n1\\n16\\n17\\n16\\n17\\n20\\n2\\n16\\n17\\n16\\n17\\n0\\n1\\n16\\n17\\n16\\n17\\n20\\n2\\n16\\n17\\n16\\n17\\n5\\n4\\n16\\n17\\n16\\n17\\n20\\n2\\n17\\n17\\n3\\n17\\n17\\n3\\n17\\n17\\n3\\n17\\n17\\n3\\n1\\n0\\n6\\n7\\n21\\n22\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n8\\n9\\n10\\n11\\n23\\n24\\n5\\n4\\n20\\n2\\n3\\nFigure 3: Skeleton parts and matrix representation for (a) the\\nhand and (b) the body.\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 8 of 8\\n\\nOnline motion recognition\\nGA\\nREEIG\\nLOGEIG\\nVECMAP\\nGA\\nTS-GA-NET\\nGA\\nREEIG\\nLOGEIG\\nVECMAP\\nGA\\nTS-GA-NET\\nSPDAGG\\nLOGEIG\\nVECMAP\\nFC\\nCONVOLUTION\\nCONVOLUTION\\nSPDC\\nFigure 4: ST-TS-HGR-NET architecure\\nws\\nr\\nPossible \\nstart frame\\nFrame \\nof decision\\nTests performed\\nby the detector\\nKinetic state A\\n#\\nKinetic state B\\nFigure 5: Verification process with 𝑡𝑒= 5.\\n0.5\\n1\\n1.5\\n2\\n2.5\\n3\\nEF\\nDeadlines(seconds)\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nF1-score\\nODHG\\nSHREC2021\\nOAD\\nUOW\\nInHard\\nFigure 6: Model behavior toward early classification different\\ndeadlines.\\nM.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier\\nPage 9 of 8\\n',\n",
       " 'Peptide2Mol: A Diffusion Model for Generating Small\\nMolecules as Peptide Mimics for Targeted Protein\\nBinding\\nXinheng He1#, Yijia Zhang2,3#, Haowei Lin4, Xingang Peng4, Xiangzhe Kong5, Mingyu\\nLi6,7, and Jianzhu Ma2,3⋆\\n1 Lingang Laboratory, Shanghai, China\\n2 Department of Electronic Engineering, Tsinghua University, Beijing, China\\n3 Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China\\n4 Institute for Artificial Intelligence, Peking University, Beijing, China\\n5 Department of Computer Science and Technology, Tsinghua University, Beijing, China\\n6 Department of Pharmaceutical and Artificial-Intelligence Sciences, Shanghai Jiao\\nTong University School of Medicine, Shanghai, China\\n7 Institute of Medical Artificial Intelligence, Shanghai Jiao Tong University, Shanghai,\\nChina\\nmajianzhu@tsinghua.edu.cn\\nAbstract. Structure-based drug design has seen significant advancements\\nwith the integration of artificial intelligence (AI), particularly in the gener-\\nation of hit and lead compounds. However, most AI-driven approaches ne-\\nglect the importance of endogenous protein interactions with peptides, which\\nmay result in suboptimal molecule designs. In this work, we present Pep-\\ntide2Mol, an E(3)-equivariant graph neural network diffusion model that gen-\\nerates small molecules by referencing both the original peptide binders and\\ntheir surrounding protein pocket environments. Trained on large datasets and\\nleveraging sophisticated modeling techniques, Peptide2Mol not only achieves\\nstate-of-the-art performance in non-autoregressive generative tasks, but also\\nproduces molecules with similarity to the original peptide binder. Addition-\\nally, the model allows for molecule optimization and peptidomimetic design\\nthrough a partial diffusion process. Our results highlight Peptide2Mol as an\\neffective deep generative model for generating and optimizing bioactive small\\nmolecules from protein binding pockets.\\nKeywords: Small molecule design · Diffusion model · Structure based drug\\ndesign · Peptide mimicry\\n⋆Corresponding author.\\narXiv:2511.04984v1  [cs.LG]  7 Nov 2025\\n\\n2\\nX. He et al.\\n1\\nIntroduction\\nSmall molecules have long been the cornerstone of drug discovery due to their ease of\\nsynthesis, cell permeability, oral bioavailability, and cost-effectiveness in manufacturing\\n[1,2]. In contrast, peptides, despite their high affinity and specificity for protein targets,\\noften suffer from poor membrane permeability and metabolic instability, which severely\\nlimit their therapeutic application [3]. To combine the strength of both modalities, recent\\nstrategies aim to transform native peptide or protein binders into small molecules that\\npreserve key binding interactions [4]. This concept has been validated in several landmark\\ncases, such as the conversion of the snake-venom peptide Teprotide into the antihyper-\\ntensive drug Captopril and the rational design of the HIV protease inhibitor Saquinavir\\nthrough peptide bond isosteres [5,6]. However, these successes remain isolated, and no\\nsystematic or scalable framework exists for general peptide-to-small-molecule conversion.\\nWith the rapid advancement of artificial intelligence (AI), especially the remarkable\\nsuccess of generative models, drug design has entered a new era [7,8,9,10,11,12]. Early gen-\\nerative models learned structural distributions from known ligands [13,14], while recent\\nmethods incorporate pocket structures to generate target-specific molecules [10,15,16].\\nThis shift is motivated by the recognition that incorporating receptor-specific informa-\\ntion is vital for drug design, because only through precise binding to the target protein\\ncan a drug exert its therapeutic effect [17].\\nRecent advancements in predictive modeling have provided promising approaches for\\nmolecular generation. Modern all-atom models have demonstrated the capability to pre-\\ndict small molecule-protein complexes with atomic-level precision [18]. Furthermore, work\\nfrom the Baker group has shown has demonstrated that deep learning can be used to de-\\nsign diverse, high-affinity protein binders [19,20]. However, small molecule generation\\nencompasses a broader chemical space and presents challenges in terms of validation,\\nwhich can be costly and time-consuming [21].\\nDespite this, most existing models focus solely on small molecule-protein complex\\ndata, often overlooking the abundant and biologically significant protein-protein and\\nprotein-peptide interactions [22]. This narrow focus on protein-ligand complexes intro-\\nduces several challenges. This limited scope leads to a lack of diversity in the generated\\nsmall molecules, as the available protein-ligand complexes often represent similar scaf-\\nfolds, thereby constraining the exploration of novel molecular designs [23,24]. Moreover,\\ncurrent models fail to incorporate protein-protein or protein-peptide interaction structural\\ndata, despite the growing emphasis on mimicking peptide binders in small molecule de-\\nsign. Consequently, a significant gap exists in the ability to effectively link small molecules\\nand peptides/proteins at the atomic scale for generation.\\nTo address this gap, we propose Peptide2Mol, the first AI model that learns to\\ntranslate peptide or protein binding interfaces into small molecules directly in three-\\ndimensional space. Peptide2Mol is formulated as an E(3)-equivariant graph neural net-\\nwork (EGNN) diffusion model, trained on diverse datasets encompassing small-molecule\\nconformation ensembles, protein–ligand complexes, and both experimentally determined\\nand computationally predicted protein–protein interactions. This design enables Pep-\\ntide2Mol to generate target-aware small molecules that reliably mimic peptide-like bind-\\ning interactions while maintaining favorable drug-like properties towards the target pro-\\ntein. Such an algorithm does not conflict with existing diffusion based methods [10,11,25]\\nand can be combined to generate small molecule binders to mimic peptide behavior.\\nFinally, by analyzing antibody-antigen surface interactions, we identify preferred chemi-\\n\\nTitle Suppressed Due to Excessive Length\\n3\\ncal groups for replacing amino acids, which provides valuable insights into the design of\\npeptide mimicry.\\n2\\nMethods\\nFig. 1: Overview of the Peptide2Mol model. (a) Dataset composition, training, and in-\\nference workflow. The model is trained on peptide and small molecule structures, with\\ninference generating candidate ligands for target protein pockets. (b) Schematic repre-\\nsentation of edge for non-covalent interactions between ligands and the protein pocket.\\n(c) Model architecture of Peptide2Mol.\\n2.1\\nDataset Construction\\nTo construct our training dataset, we aggregated molecular structures from multiple\\nsources. Small molecules were obtained from the Geometric Ensemble Of Molecules\\n(GEOM) [26] drug dataset, while protein-ligand and protein-peptide complexes were\\nobtained from the PDBBind [27] and BioLip2 [28] databases. Protein-peptide interac-\\ntion were also derived from the monomeric models in the AlphaFold Database [29]. In\\nthese models, loops that are fully buried and exhibit interactions with other parts are\\ntreated as ligands, while the remaining parts are considered receptors. All molecular data\\nwere filtered using RDKit [30] to ensure successful parsing, yielding a final dataset com-\\nprising 304,322 drug-like small molecules, 38,860 protein-ligand complexes, and 39,499\\npeptide-protein interfaces.\\nFor evaluation, we assembled a test set comprising 10 protein-ligand complexes ran-\\ndomly selected from the CrossDock2020 dataset, consistent with prior publications [15,31].\\nThe selected complexes correspond to PDB IDs: 1BVR, 1ZYU, 2ATI, 4BNW, 5G3N,\\n1U0F, 2AH9, 2HW1, 4I91, and 5LVQ. Additionally, we included antibody-antigen pairs\\nsourced from the Structural Antibody Database (SAbDab) for showing the replacement\\nof residues by small molecule fragments [32].\\n\\n4\\nX. He et al.\\n2.2\\nModel overview\\nPeptide2Mol is a non-autoregressive diffusion based generative model designed to gener-\\nate and optimize small molecules within protein pockets, leveraging peptide-binder struc-\\ntural data. As shown in Figure 1a, the model is trained on a curated dataset combining\\nsmall-molecule conformation ensembles [26], protein-ligand/peptide complexes [27,28],\\nand protein-peptide models [29]. During training, ligand geometries undergo progressive\\ndisruption via a diffusion process, while peptide sidechains are partially diffused and\\nbinding pocket residues remain fixed. This framework establishes an invertible mapping\\nbetween the base Gaussian distribution and the ground truth graph. At inference, the\\nmodel iteratively transfers Gaussian noise into molecular structures at each step un-\\ntil convergence. Non-covalent interactions within 5Å are explicitly modeled to capture\\npocket–ligand contacts (Figure 1b).\\nAs depicted in Figure 1c, the model embeds ligand and pocket atoms into node and\\nedge features, augmented with timestep embeddings and pocket embedding to distinguish\\natomic contexts. These representations are processed by six E(3)-equivariant GNN layers\\nthat iteratively update atomic features and coordinates through rotation-equivariant con-\\nvolutions. Finally, the refined embeddings are decoded into molecular graphs. Pocket2Mol\\n[33] can be optionally used to resolve steric clashes to further refine ligand-pocket com-\\nplementarity.\\n2.3\\nMolecular Featurization\\nLigands and ligand–protein complexes were represented as undirected atomic graphs,\\ndenoted as M = (V, E). Each node vi ∈V corresponds to an atom and is associated\\nwith two attributes: its spatial coordinate ri ∈R3 and its element-type feature ai ∈R8,\\nwhere ai is implemented as a one-hot encoding over common atom symbols (C, N, O,\\nF, P, S, Cl, Br). Each edge eij ∈E corresponds to an atom pair and is described by a\\nbond feature vector bij ∈R6, encoding single, double, triple, aromatic, and non-bonded\\nproximity interactions, plus an absorbing state for no interaction.\\n2.4\\nDiffusion Model Architeture\\nA diffusion model is then employed in the generation process, characterized by two Markov\\nrandom processes. The forward process incrementally introduces noise to the data accord-\\ning to a predefined noise schedule, while the reverse process leverages neural networks\\nto denoise the data, ultimately reconstructing the original data from the noise. Let the\\nsuperscript t denote variables at time step t with M0 representing the 3D molecule or\\ncomplex drawn from the real distribution. At each step, Mt is sampled from the condi-\\ntional distribution q(Mt | Mt−1), dependent solely on Mt−1:\\nq(Mt | Mt−1, Mt−2, · · · , M0) := q(Mt | Mt−1)\\n(1)\\nFor atom positions ri, atom types ai and bond types bij, which are discrete, categorical\\ndistributions are used for their representation. The forward process is defined as:\\nq(rt\\ni | rt−1\\ni\\n) := N(rt\\ni |\\np\\n1 −βtrt−1\\ni\\n, βtI)\\n(2)\\nq(at\\ni | at−1\\ni\\n) := N(at\\ni |\\np\\n1 −βtat−1\\ni\\n, βtI)\\n(3)\\nq(bt\\nij | bt−1\\nij ) := N(bt\\nij |\\np\\n1 −βtbt−1\\nij , βtI)\\n(4)\\n\\nTitle Suppressed Due to Excessive Length\\n5\\nwith βt ∈[0, 1] denotes the predefined noise scaling schedule, I ∈R3×3 is the identity\\nmatrix. For atom position ri, atom type ai, and bond type bij, scaled standard Gaussian\\nnoise is incrementally added. In addition, time embedding and a binary pocket indicator\\n(0/1) were concatenated with the node embeddings, resulting in a unified representation\\nthat integrates atomic, temporal, and contextual information.\\nLeveraging the Markov property, M can be directly derived from the original sample\\nM0. By defining αt := 1 −βt, and ¯αt := Qt\\ns=1 αs, the sample can be expressed as\\nthe following equations: where ¯αt = Qt\\ns=1(1 −βs) denotes the fraction of information\\nretained at step t. From this formulation, ¯αt represents the proportion of information\\nfrom the original data retained at step t. We refer to ¯αt as the \"information level,\" which\\nis determined by the noise level βt.\\nq(rt\\ni | r0\\ni ) := N(rt\\ni |\\n√\\n¯αtr0\\ni , (1 −¯αt)I)\\n(5)\\nq(at\\ni | a0\\ni ) := N(at\\ni |\\n√\\n¯αta0\\ni , (1 −¯αt)I)\\n(6)\\nq(bt\\nij | b0\\nij) := N(bt\\nij |\\n√\\n¯αtb0\\nij, (1 −¯αt)I)\\n(7)\\nAs t →∞, the atom positions, types and bond types approximate a standard Gaussian\\ndistribution. These resulting prior distributions serve as the initial distributions for the\\nreverse process.\\nIn the reverse process, we invert the Markov chain to reconstruct the original sample\\nfrom prior distributions, employing E(3)-equivariant neural networks to parameterize the\\ntransition probability pθ(Mt−1 | Mt). Specifically, we model all the three predicted distri-\\nbutions as a Gaussian distribution N(Xt−1\\ni\\n| (µθ(Mt, t), Σt)), where X represents variable\\nand µθ is the neural network. The neural network is trained to recover Mt−1 from Mt by\\noptimizing the predicted distribution pθ(Mt−1 | Mt) to approximate the true posterior\\nq(Mt−1 | Mt, M0). During training, the loss function was defined in equations (8)–(11).\\nLt−1\\npos = 1\\nN\\nX\\ni\\n∥rt−1\\ni\\n−µθ(Mt, t)i∥2\\n(8)\\nLt−1\\natom = 1\\nN\\nX\\ni\\n∥at−1\\ni\\n−µθ(Mt, t)i∥2\\n(9)\\nLt−1\\nbond = 1\\nN\\nX\\ni\\n∥bt−1\\nij\\n−µθ(Mt, t)i∥2\\n(10)\\nLt−1 = Lt−1\\npos + λaLt−1\\natom + λbLt−1\\nbond\\n(11)\\nwhere λa and λb were set both 30 for atom and bond. A timestep t was randomly sampled\\nduring training and the neural network was applied to recover the unbiased molecule,\\nwhere its parameters was optimized by minimizing the loss Lt−1. In inference process,\\nwe sample atom position, type and symmetric bond type in Gaussian distribution and\\nrepeatedly sample from t = T, T −1, · · · , 1 to denoise the molecule.\\n2.5\\nMolecule Generation\\nInference process is used for the generation of small molecules. During inference, the model\\ntakes the receptor pocket originally defined by the peptide–protein interface but does\\nnot include the peptide backbone as a structural scaffold. Instead, the model initializes\\nfrom Gaussian noise and iteratively denoises the atomic positions, atom types, and bond\\nconnectivity to generate a small molecule directly within the peptide’s binding pocket.\\n\\n6\\nX. He et al.\\nIn this way, the generated molecules adopt drug-like geometries while preserving the\\nessential interaction pattern of the original peptide because of the diverse training data.\\nThis design allows Peptide2Mol to effectively translate peptide binding interfaces into\\nsmall molecule mimetics rather than reconstructing peptide structures. Peptide2Mol can\\nalso make molecule optimization and peptidomimetic design when pointing fixed atoms\\nduring diffusion process.\\nAfter generation, a pocket-aware refinement stage can be applied using the Pocket2Mol\\noptimization module [33]. This step performs local atom and bond adjustment to remove\\nsteric clashes, correct unreasonable geometries, and improve shape complementarity be-\\ntween the ligand and pocket. Such refinement is necessary because diffusion sampling\\nmay yield high-energy or overlapping conformations that are not physically realizable.\\nThe Pocket2Mol-based relaxation ensures that the final small molecules correspond to\\nchemically valid, energetically stable binding poses consistent with the protein pocket\\nenvironment.\\n2.6\\nFragmentation of Small Molecules\\nTo identify which fragments were most frequently used to replace residue side chains, we\\nfiltered the SabDab dataset for complementarity-determining region (CDR) domains in\\ncomplex with antigens. Antigens were defined as residues within 5Å of the CDR domain,\\nand only complexes where the number of antigen residues exceeded the number of CDR\\nresidues were retained. These CDRs were then converted into small molecules using Pep-\\ntide2Mol, and the resulting molecules were fragmented based on their rotatable bonds.\\nA fragment was defined as replacing an amino acid if it was located within 4 Å of any\\nheavy atom of the residue. To rank the likelihood of fragment–residue replacements, we\\ncomputed the Pointwise Mutual Information (PMI) as follows:\\nPMI(res, frag) = log\\n\\x12\\nP(res, frag)\\nP(res) · P(frag)\\n\\x13\\n(12)\\n3\\nResults\\n3.1\\nBenchmarking Molecular Properties of Peptide2Mol\\nWe first assessed the general properties of molecules generated by Peptide2Mol, focus-\\ning on evaluating their chemical validity and plausibility. To this end, we selected and\\ncomputed the following evaluation metrics, which have been widely adopted in previ-\\nous studies to characterize the properties of sampled candidates [33,34,35]: (1) QED\\n(Quantitative Estimation of Drug-likeness) [36], quantifies the likelihood of a molecule\\nbeing a viable drug candidate based on its physicochemical properties and conformity\\nto drug-like characteristics; (2) SA (Synthetic Accessibility) [37], measuring the ease of\\nmolecular synthesis, with higher scores indicating greater synthetic feasibility; (3) LogP\\n(Octanol–Water Partition Coefficient), a metric of molecular hydrophobicity derived from\\nthe distribution between octanol and aqueous phases; and (4) PBrate (PoseBusters pass-\\ning rate) [38] integrates 19 criteria to comprehensively assess docking quality, including\\nmolecular structural integrity and conformational validity, which provides a rigorous and\\ncomprehensive measure of docking plausibility and makes it a reliable benchmark for\\nassessing generative models.\\n\\nTitle Suppressed Due to Excessive Length\\n7\\nTable 1: The comparison of properties of the generated molecules in the test set.\\nMethod\\nQED(↑)\\nSA(↑)\\nLogP\\nPBrate(%. ↑)\\nLiGAN [31]\\n0.428\\n0.546\\n1.224\\n39.50\\nPocket2Mol [33]\\n0.587\\n0.758\\n1.063\\n71.60\\nTargetDiff [25]\\n0.430\\n0.550\\n1.249\\n36.90\\nPocketFlow [15]\\n0.497\\n0.769\\n3.521\\n46.00\\nPeptide2Mol\\n0.501\\n0.612\\n0.638\\n45.30\\nPeptide2Mol-Fixed\\n0.509\\n0.637\\n0.729\\n83.80\\nTable 1 summarizes the generative performance of Peptide2Mol compared with repre-\\nsentative molecular generation methods on the same benchmark used in LiGAN [31] and\\nPocketFlow [15]. In terms of QED, our model (0.501) already surpasses LiGAN (0.428)\\nand TargetDiff (0.430), and achieves a comparable level to PocketFlow (0.497), situating\\nit within a competitive range. For SA, while peptide-like molecules naturally score lower\\ncompared to small-molecule–oriented methods such as Pocket2Mol and PocketFlow, Pep-\\ntide2Mol maintains parity with LiGAN and TargetDiff, highlighting its balance between\\npeptide-specific features and overall synthetic feasibility. Regarding hydrophobicity, the\\nlower LogP values generated by Peptide2Mol reflect the intrinsic physicochemical prop-\\nerties of peptides, making the results consistent with the intended design space.\\nFig. 2: Waterfall diagram illustrating the stepwise evaluation of AI-generated\\nmolecules against the PoseBusters criteria. Each method was designed to generate\\n100 molecules per target across the testset targets. Panels show results for LiGAN (a),\\nPocket2Mol (b), TargetDiff (c), PocketFlow (d), Peptide2Mol (e), and Peptide2Mol-\\nFixed (f).\\n\\n8\\nX. He et al.\\nAlthough Peptide2Mol does not achieve the highest scores in QED or SA, its per-\\nformance is comparable to established approaches trained exclusively on small-molecule\\ndatasets. Importantly, when a partially masked autoregressive refinement step is applied\\n(Peptide2Mol-Fixed), the overall chemical validity is further improved, yielding the high-\\nest PBrate (83.80%). This indicates that molecules generated by Peptide2Mol, although\\nnot optimized exclusively for drug-likeness metrics, achieve competitive quality and can\\nbe effectively refined to ensure robust structural integrity and docking plausibility.\\nTo further dissect the structural quality of the generated molecules, we visualize the in-\\ndividual contributions to the PoseBusters score using a waterfall plot (Figure 2). This rep-\\nresentation highlights which specific geometric and conformational criteria most strongly\\ninfluence the PBrate for each method. For instance, Pocket2Mol demonstrates strong per-\\nformance in satisfying bond length distribution constraints and in generating molecules\\nwith favorable internal energies. In contrast with Pocket2Mol, Peptide2Mol achieves su-\\nperior control over intermolecular distance constraints with the target, thereby effec-\\ntively reducing steric clashes. Leveraging these complementary strengths, we employed\\nPocket2Mol for partially-refinement of our generated molecules, which yielded the most\\nfavorable overall evaluation outcomes.\\n3.2\\nBond Length Distribution Analysis\\nIn addition to the benchmark comparison, we further examined the bond length distri-\\nbutions of generated molecules. As shown in Figure 3a-i, Nine kinds of chemical bonds\\nare analyzed, including C-C, C=C, C-O, C=O, C-N, C=N, C-Cl, C-S, and C-F.\\nFig. 3: Geometric and property-based evaluation of generated molecules. (a–i)\\nBond length distributions of molecules generated by different AI-based methods compared\\nwith those in the training set. Nine representative bond types are analyzed: C–C (a), C=C\\n(b), C–O (c), C=O (d), C–N (e), C=N (f), C–Cl (g), C–S (h), and C–F (i).\\n\\nTitle Suppressed Due to Excessive Length\\n9\\nNotably, the results from Peptide2Mol closely match the overall distribution of the\\ntraining dataset, while also notably capturing the characteristic bond length patterns\\nspecific to peptides. This highlights the model’s ability to generate peptide-like molecules\\nthat are both chemically realistic and structurally consistent with experimental observa-\\ntions.\\n3.3\\nResidue replacement analysis\\nTo investigate the residue-level mimicry capability of Peptide2Mol, we applied the model\\nto an antibody–antigen dataset to generate small-molecule fragments substituting native\\nantibody side chains. Four representative residues—tyrosine (Y), aspartic acid (D), argi-\\nnine (R), and leucine (L)—were analyzed (Figure 4). Fragments were ranked by PMI\\nwith the corresponding residue, reflecting association strength. High-PMI fragments gen-\\nerally preserve key chemical features: tyrosine substitutes retain aromatic or hydroxyl\\ngroups; aspartic acid replacements are enriched in polar oxygen-containing groups; argi-\\nnine mimics maintain nitrogen-rich motifs; and leucine substitutes comprise carbon-rich\\nhydrophobic chains. These results indicate that Peptide2Mol generates chemically plau-\\nsible, residue-specific side-chain mimics while allowing structural diversity.\\nFig. 4: The histogram to show the top replacement fragment from small molecules with\\n4 representative residues (ARG, ASP, LEU and TYR), the color reflects the composition\\nproportion of elements (green: Carbon, Blue: Nitrogen, Red: Oxygen, Gray, others)\\n4\\nDiscussion\\nIn this work, we introduced Peptide2Mol, a structure-based generative framework de-\\nsigned to bridge the gap between peptides and small molecules in drug discovery. Un-\\nlike previous generative models that primarily focus on small-molecule–protein com-\\nplexes, Peptide2Mol integrates structural information from both protein–ligand and pro-\\ntein–protein (or protein–peptide) interactions. This enables the model to translate peptide\\nor antibody CDR binders into small molecules that mimic their native binding modes\\n(Fig. 5). This design enables the generation of peptide-mimicking small molecules that\\npreserve the functional essence of native residues while retaining drug-like chemical prop-\\nerties or generate peptidomimetics from original peptide.\\n\\n10\\nX. He et al.\\nFig. 5: Representative examples showing that Peptide2Mol can transform (a) a peptide\\nbinder (PDB ID: 7WXO) and (b) an antibody CDR (PDB ID: 3NGB) into corresponding\\nsmall molecules that mimic their binding interfaces.\\nOne strength of Peptide2Mol is its principled use of diverse structural datasets in train-\\ning. Existing models often inherit biases from protein–ligand complexes [15,33,34,35]. By\\nsystematically incorporating both experimentally determined and computationally pre-\\ndicted peptide and protein interaction data, Peptide2Mol broadens the generative chemi-\\ncal space. This approach improves the diversity of generated molecules, while still yielding\\ncompetitive performance in standard benchmarks. Importantly, refinement with a par-\\ntially masked autoregressive step significantly improved structural plausibility, achieving\\nthe highest PoseBusters passing rate, thereby demonstrating the potential of combining\\ncomplementary modeling strategies.\\nDespite these advances, several limitations remain. The generated molecules tend to\\ninherit physicochemical features closer to peptides than to conventional small molecules,\\nwhich may explain their modest performance on QED and SA relative to models opti-\\nmized exclusively for drug-likeness. While this aligns with the goal of peptide mimicry,\\npractical applications will require balancing peptide-like fidelity with pharmacokinetic\\nconstraints [39]. Moreover, although we demonstrated residue-level replacement analysis,\\nthe current model does not yet provide a quantitative metric for peptide–small molecule\\nsimilarity.\\nLooking forward, we envision several directions for extending this work. One is to\\ncouple Peptide2Mol with physics-based simulation pipelines to assess stability and bind-\\ning mechanisms beyond docking scores [40]. Moreover, systematic benchmarking across\\na broader range of “undruggable” protein–protein interaction targets will be critical to\\n\\nTitle Suppressed Due to Excessive Length\\n11\\nestablish the generality of this approach and to uncover patterns of residue substitution\\nthat may inform rational drug design [41].\\nIn conclusion, Peptide2Mol represents a step toward unifying peptide- and small-\\nmolecule–based design strategies. By capturing the structural logic of peptide binders\\nwhile ensuring drug-like feasibility, our framework highlights a new frontier for genera-\\ntive drug discovery. Just as the development of protein language models expanded the\\ninterpretability of sequence variation, the integration of peptide-derived binding infor-\\nmation into molecular generation holds promise to unlock new chemical modalities and\\naccelerate the translation of peptide insights into therapeutically viable small molecules.\\n5\\nCode Availability\\nThe source code, pretrained models, and a minimal test dataset for Peptide2Mol are\\npublicly available at https://github.com/BLUE-Flowing/Peptide2Mol/.\\nReferences\\n1. K. Wu, S. H. Kwon, X. Zhou, C. Fuller, X. Wang, J. Vadgama, and Y. Wu, Overcoming challenges in small-\\nmolecule drug bioavailability: A review of key factors and approaches, International Journal of Molecular\\nSciences, 2024, 25(23), 13121.\\n2. A. M. Vargason, A. C. Anselmo, and S. Mitragotri, The evolution of commercial drug delivery technologies,\\nNature Biomedical Engineering, 2021, 5(9), 951–967.\\n3. M. Muttenthaler, G. F. King, D. J. Adams, P. F. Alewood: Trends in peptide drug discovery. Nature Reviews\\nDrug Discovery 20(4), 309–325 (2021).\\n4. W. Brytan and L. Padrela, Structural modifications for the conversion of proteins and peptides into stable\\ndried powder formulations: A review, Journal of Drug Delivery Science and Technology, 2023, 89, 104992.\\n5. C. Odaka and T. Mizuochi, Angiotensin-converting enzyme inhibitor captopril prevents activation-induced\\napoptosis by interfering with T cell activation signals, Clinical & Experimental Immunology, 2000, 121(3),\\n515–522.\\n6. N. A. Roberts, J. A. Martin, D. Kinchington, A. V. Broadhurst, J. C. Craig, I. B. Duncan, S. A. Galpin, B. K.\\nHanda, J. Kay, A. Kröhn, et al., Rational design of peptide-based HIV proteinase inhibitors, Science, 1990,\\n248(4953), 358–361.\\n7. A. Gangwal and A. Lavecchia, Unleashing the power of generative AI in drug discovery, Drug Discovery Today,\\n2024, 29(6), 103992.\\n8. X.-h. He, J.-r. Li, J. Xu, H. Shan, S.-y. Shen, S.-h. Gao, and H. E. Xu, AI-driven antibody design with\\ngenerative diffusion models: current insights and future directions, Acta Pharmacologica Sinica, 2025, 46(3),\\n565–574.\\n9. S. Luo, Y. Su, X. Peng, S. Wang, J. Peng, and J. Ma, Antigen-specific antibody design and optimization with\\ndiffusion-based generative models for protein structures, Advances in Neural Information Processing Systems,\\n2022, 35, 9754–9767.\\n10. H. Lin, Y. Huang, O. Zhang, S. Ma, M. Liu, X. Li, L. Wu, J. Wang, T. Hou, and S. Z. Li, Diffbp: Generative\\ndiffusion of 3D molecules for target protein binding, Chemical Science, 2025, 16(3), 1417–1431.\\n11. L. Huang, T. Xu, Y. Yu, P. Zhao, X. Chen, J. Han, Z. Xie, H. Li, W. Zhong, K.-C. Wong, et al., A\\ndual diffusion model enables 3D molecule generation and lead optimization based on target pockets, Nature\\nCommunications, 2024, 15(1), 2657.\\n12. S. Luo, J. Guan, J. Ma, J. Peng: A 3D generative model for structure-based drug design. Advances in Neural\\nInformation Processing Systems 34, 6229–6239 (2021).\\n13. W. J. Godinez, E. J. Ma, A. T. Chao, L. Pei, P. Skewes-Cox, S. M. Canham, J. L. Jenkins, J. M. Young,\\nE. J. Martin, W. A. Guiguemde: Design of potent antimalarials with generative chemistry. Nature Machine\\nIntelligence 4(2), 180–186 (2022).\\n14. V. Bagal, R. Aggarwal, P. K. Vinod, U. D. Priyakumar: MolGPT: molecular generation using a transformer-\\ndecoder model. Journal of Chemical Information and Modeling 62(9), 2064–2076 (2021).\\n15. Y. Jiang, G. Zhang, J. You, H. Zhang, R. Yao, H. Xie, L. Zhang, Z. Xia, M. Dai, Y. Wu, et al.: Pocketflow is\\na data-and-knowledge-driven structure-based molecular generative model. Nature Machine Intelligence 6(3),\\n326–337 (2024).\\n16. S. Choi, S. Seo, B. J. Kim, C. Park, S. Park: PIDiff: Physics informed diffusion model for protein pocket-\\nspecific 3D molecular generation. Computers in Biology and Medicine 180, 108865 (2024).\\n\\n12\\nX. He et al.\\n17. R. Santos, O. Ursu, A. Gaulton, A. P. Bento, R. S. Donadi, C. G. Bologa, A. Karlsson, B. Al-Lazikani, A.\\nHersey, T. I. Oprea, et al.: A comprehensive map of molecular drug targets. Nature Reviews Drug Discovery\\n16(1), 19–34 (2017).\\n18. R. Roy, H. M. Al-Hashimi: AlphaFold3 takes a step toward decoding molecular behavior and biological\\ncomputation. Nature Structural & Molecular Biology 31(7), 997–1000 (2024).\\n19. J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R.\\nJ. Ragotte, L. F. Milles, et al., De novo design of protein structure and function with RFdiffusion, Nature,\\n2023, 620(7976), 1089–1100.\\n20. D. R. Fox, C. Taveneau, J. Clement, R. Grinter, and G. J. Knott, Code to complex: AI-driven de novo binder\\ndesign, Structure, 2025, in press.\\n21. X. Zeng, F. Wang, Y. Luo, S.-G. Kang, J. Tang, F. C. Lightstone, E. F. Fang, W. Cornell, R. Nussinov,\\nF. Cheng: Deep generative molecular design reshapes drug discovery. Cell Reports Medicine 3(12), 108865\\n(2022).\\n22. J. F. Greenblatt, B. M. Alberts, N. J. Krogan: Discovery and significance of protein-protein interactions in\\nhealth and disease. Cell 187(23), 6501–6517 (2024).\\n23. H. Zhu, X. Li, B. Chen, N. Huang: Augmented BindingNet dataset for enhanced ligand binding pose predic-\\ntions using deep learning. npj Drug Discovery 2(1), 1 (2025).\\n24. O. Zhang, H. Lin, H. Zhang, H. Zhao, Y. Huang, C.-Y. Hsieh, P. Pan, T. Hou: Deep lead optimization:\\nleveraging generative AI for structural modification. Journal of the American Chemical Society 146(46),\\n31357–31370 (2024).\\n25. J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, J. Ma: 3D equivariant diffusion for target-aware molecule\\ngeneration and affinity prediction. arXiv preprint arXiv:2303.03543 (2023).\\n26. S. Axelrod, R. Gomez-Bombarelli: GEOM, energy-annotated molecular conformations for property prediction\\nand molecular generation. Scientific Data 9(1), 185 (2022).\\n27. R. Wang, X. Fang, Y. Lu, C.-Y. Yang, S. Wang: The PDBbind database: methodologies and updates. Journal\\nof Medicinal Chemistry 48(12), 4111–4119 (2005).\\n28. C. Zhang, X. Zhang, L. Freddolino, Y. Zhang: BioLiP2: an updated structure database for biologically\\nrelevant ligand–protein interactions. Nucleic Acids Research 52(D1), D404–D412 (2024).\\n29. M. Varadi, S. Anyango, M. Deshpande, S. Nair, C. Natassia, G. Yordanova, D. Yuan, O. Stroe, G. Wood,\\nA. Laydon, et al., AlphaFold Protein Structure Database: massively expanding the structural coverage of\\nprotein-sequence space with high-accuracy models, Nucleic Acids Research, 2022, 50(D1), D439–D444.\\n30. G. Landrum: RDKit: A software suite for cheminformatics, computational chemistry, and predictive model-\\ning. Available at: https://www.rdkit.org (2013).\\n31. M. Ragoza, T. Masuda, D. R. Koes: Generating 3D molecules conditional on receptor binding sites with deep\\ngenerative models. Chemical Science 13, 2701–2713 (2022).\\n32. D. James, K. Konrad, L. Jinwoo, B. Terry, F. Angelika, G. Guy and S. Jiye and D. Charlotte, SAbDab: the\\nstructural antibody database, Nucleic acids research, 2014, 42(D1), D1140–D1146.\\n33. X. Peng, S. Luo, J. Guan, Q. Xie, J. Peng, J. Ma: Pocket2mol: efficient molecular sampling based on 3D\\nprotein pockets. In: International Conference on Machine Learning, pp. 17644–17655 (2022).\\n34. X. Peng, J. Guan, Q. Liu, J. Ma: MolDiff: addressing the atom-bond inconsistency problem in 3D molecule\\ndiffusion generation. arXiv preprint arXiv:2305.07508 (2023).\\n35. X. Zhou, J. Guan, Y. Zhang, X. Peng, L. Wang, J. Ma: Reprogramming pretrained target-specific diffusion\\nmodels for dual-target drug design. Advances in Neural Information Processing Systems 37, 87255–87281\\n(2024).\\n36. G. R. Bickerton, G. V. Paolini, J. Jérémy Besnard, S. Muresan, A. L. Hopkins: Quantifying the chemical\\nbeauty of drugs. Nature Chemistry 4(2), 90–98 (2012).\\n37. P. Ertl, A. Schuffenhauer: Estimation of synthetic accessibility score of drug-like molecules based on molecular\\ncomplexity and fragment contributions. Journal of Cheminformatics 1, 1–11 (2009).\\n38. M. Buttenschoen, G. M. Morris, C. M. Deane: PoseBusters: AI-based docking methods fail to generate\\nphysically valid poses or generalise to novel sequences. Chemical Science 15(9), 3130–3139 (2024).\\n39. A. Datta-Mannan: Mechanisms influencing the pharmacokinetics and disposition of monoclonal antibodies\\nand peptides. Drug Metabolism and Disposition 47(10), 1100–1110 (2019).\\n40. N. van Hilten, J. Methorst, N. Verwei, and H. J. Risselada, Physics-based generative model of curvature\\nsensing peptides; distinguishing sensors from binders, Science Advances, 2023, 9(11), eade8839.\\n41. Q. Sun, H. Wang, J. Xie, L. Wang, J. Mu, J. Li, Y. Ren, and L. Lai, Computer-Aided Drug Discovery for\\nUndruggable Targets, Chemical Reviews, 2025, in press.\\n',\n",
       " 'Code Review Automation using Retrieval Augmented\\nGeneration\\nQIANRU MENG, Leiden University, The Netherlands\\nXIAO ZHANG, University of Groningen, The Netherlands\\nZHAOCHUN REN, Leiden University, The Netherlands\\nJOOST VISSER, Leiden University, The Netherlands\\nCode review is essential for maintaining software quality but is labor-intensive. Automated code review\\ngeneration offers a promising solution to this challenge. Both deep learning-based generative techniques\\nand retrieval-based methods have demonstrated strong performance in this task. However, despite these\\nadvancements, there are still some limitations where generated reviews can be either off-point or overly general.\\nTo address these issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages Retrieval-\\nAugmented Generation (RAG) to combine retrieval-based and generative methods, explicitly incorporating\\nexternal domain knowledge into the code review process. RARe uses a dense retriever to select the most\\nrelevant reviews from the codebase, which then enrich the input for a neural generator, utilizing the contextual\\nlearning capacity of large language models (LLMs), to produce the final review. RARe outperforms state-of-\\nthe-art methods on two benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively. Its\\neffectiveness is further validated through a detailed human evaluation and a case study using an interpretability\\ntool, demonstrating its practical utility and reliability.\\nACM Reference Format:\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser. 2018. Code Review Automation using Retrieval\\nAugmented Generation. In Proceedings of Make sure to enter the correct conference title from your rights\\nconfirmation emai (Conference acronym ’XX). ACM, New York, NY, USA, 20 pages. https://doi.org/XXXXXXX.\\nXXXXXXX\\n1\\nIntroduction\\nCode review is an essential component of software quality assurance, requiring substantial human\\neffort. To reduce this effort, automated code review generation has been used to assist human\\nreviewers, for instance by alleviating their workload through the suggestion of review comments, or\\nto replace human reviewers by autonomously generating reviews that can be used immediately by\\ndevelopers. Various approaches have been employed to automate the code review process, evolving\\nfrom initial rule-based static analysis techniques [1, 4, 7, 22, 31] to more advanced deep learning\\n(DL) methods [14, 24, 27, 28, 39, 41]. These DL-based methods address the code review task as\\na generation task, employing pre-trained language models [24, 27, 41] or LLMs [28] to produce\\nreviews. Alternatively, code review can also be formulated as an information retrieval task, where\\na given code change is used as a query to search for the most relevant reviews from a code review\\nAuthors’ Contact Information: Qianru Meng, q.r.meng@liacs.leidenuniv.nl, Leiden University, The Netherlands; Xiao Zhang,\\nxiao.zhang@rug.nl, University of Groningen, The Netherlands; Zhaochun Ren, z.ren@liacs.leidenuniv.nl, Leiden University,\\nThe Netherlands; Joost Visser, j.m.w.visser@liacs.leidenuniv.nl, Leiden University, The Netherlands.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\\nConference acronym ’XX, Woodstock, NY\\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-XXXX-X/18/06\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\narXiv:2511.05302v1  [cs.SE]  7 Nov 2025\\n\\n2\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\ncorpus. The retrieval-based approach has demonstrated its effectiveness with greater computational\\nefficiency compared to DL-based generation methods [16].\\nAlthough both retrieval-based and DL-based generative methods have shown effectiveness in\\ncode review generation, each still comes with limitations. On one hand, retrieval-based methods,\\nas noted by prior research [16], heavily rely on existing reviews in the training dataset, which\\nrestricts their ability to generate new reviews. On the other hand, DL-based generative methods\\ncan compensate for this limitation by learning patterns from the given code-review context,\\nthereby generating new reviews. In recent years, LLMs have demonstrated impressive capabilities,\\nparticularly when fine-tuned with domain-specific data [28]. However, despite these advancements,\\nsignificant challenges still remain in applying LLMs effectively to code review tasks. LLM-generated\\nreviews often suffer from a lack of domain-specific knowledge, leading to outputs that are either\\noverly general or lacking in relevance [24]. Moreover, while fine-tuning can improve the specificity\\nof reviews by applying the model to learn patterns, it could results in reviews that are off-point\\ndue to the model’s overfitting on the limited training data. To mitigate these issues, integrating\\nretrieval-based methods alongside generative models offers a promising solution. By introducing a\\nwide-ranging external knowledge base, retrieval methods can provide relevant example reviews,\\nthereby enriching the generated content with insights from similar past cases. This hybrid approach\\nhas the potential to significantly improve the relevance, accuracy, and contextual understanding of\\nthe generated reviews.\\nMotivated by the limitations of both retrieval-based and generative approaches, we introduce\\nthe Retrieval-Augmented Reviewer (RARe), a novel method that combines the strengths of both\\nparadigms for automated code review generation. Fundamentally, RARe embodies the concept of\\nretrieval-augmented generation (RAG), which improves the quality and relevance of generative\\nmodels by incorporating external knowledge from a retrieval codebase. RARe is composed of two\\nmain components: the retriever and the generator. The retriever first uses the given code snippet\\nto search for the most relevant reviews within the codebase. These retrieved reviews, combined\\nwith the given code snippet and instructions, form a prompt that is then fed into the generator.\\nThe generator, utilizing this comprehensive prompt, produces a review tailored to the context.\\nExtensive experiments demonstrate that RARe outperforms state-of-the-art methods in the code\\nreview generation task across two benchmark datasets, achieving the best BLEU-4 scores of 12.31\\nand 12.96, respectively. Compared to its performance without retrieval augmentation, RARe shows\\nremarkable improvements, with a 153% increase in BLEU-4, a 240% rise in ROUGE-L, and a 228%\\ngain in METEOR. In addition, we conduct a comprehensive human evaluations and a case study\\nanalysis using an interpretability tool to further confirm the effectiveness of RARe. The detailed\\ninvestigation results show that by recommending relevant examples and leveraging the contextual\\nunderstanding capabilities of LLMs, RARe effectively reduce the number of overly general or\\noff-point reviews, providing more accurate and relevant reviews.\\nOur main contributions are as follows:\\n• We are the first to apply the RAG technique to the code review generation task, achieving\\nstate-of-the-art results across two datasets compared with the baselines.\\n• We conduct comprehensive comparisons among various retrievers (Normal Dense Retrieval\\nand Dense Passage Retrieval) and generators (Llama 3.1, Mistral, and CodeGemma), providing\\na detailed analysis of these components.\\n• We are the first to utilize an interpretability tool to provide a novel evaluation insight for\\ncode review generation.\\n• All of our code is available in anonymous repository: https://anonymous.4open.science/r/\\nGAR-9EE2\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n3\\nThe structure of the paper is as follows. We introduce the related work in Section 2. Details\\nof the RARe are provided in Section 3. In Section 4, we outline three research questions and\\ndetail the experiments conducted to investigate them. We then answer these questions in the\\nanalysis presented in Section 5 and Section 6. Implications and limitations are discussed in Section 7.\\nPotential threats to validity are listed in Section 8. Finally, Section 9 concludes the paper.\\n2\\nRelated Work\\n2.1\\nAutomation of Code Review Generation\\nStatic analysis tools are commonly used in early studies [1, 7, 22, 31] to integrate predefined rules\\nfor automating review processes. For instance, Balachandran et al. [1] developed a Review Bot\\n[1] to offer code modification suggestions by integrating outputs from multiple static analyzers.\\nRecently, Palvannan et al. [31] developed a Suggestion Bot for GitHub, using a Python static\\nanalyzer. Moreover, Chatley et al. [4] also employs data mining techniques to extract relevant rules\\nfrom history code commit information as predefined validation rules.\\nHowever, traditional rule-base methods lack flexibility and portability. To overcome these limi-\\ntations, some studies leverage deep learning techniques to enhance the relevance and semantic\\nunderstanding of code and reviews. Gupta et al. [14] introduced DeepCodeReviewer, which employs\\ndeep learning to determine the relevance of reviews to a code snippet and recommends the most\\nrelevant review from a repository of historical reviews. Siow et al. [39] presented a deep learning\\nmethod called CORE, which utilizes two-level embedding and an attention-based Long Short-Term\\nMemory (LSTM) model to generate relevant reviews.\\nWith the advent of pre-trained sequence-to-sequence models (transformer-based [42]), the\\nfocus in code review task has shifted towards model pre-training, particularly with the Text-to-\\nText Transfer Transformer (T5) framework [24, 27, 41]. Related studies tailor their pre-training\\nframework to achieve different task objectives. The AUGER method [24] was specifically designed\\nfor code review generation, while the other approaches by CodeReviewer [27] and Tufano et al. [41]\\nare adopted to various code review downstream tasks, including review necessity prediction, code\\nreview generation, and code refinement. Recently, the excellent contextual learning capabilities of\\nLLMs have been demonstrated in code review generation task. Lu et al. [28] compared different\\ntraining methods (LoRA [17] and prefix tuning [26]) for LLMs and proposed the LLaMA-Reviewer. It\\nleverages the capabilities of the LLM (LLaMA [40]) to automatically generate code reviews through\\nfine-tuning.\\nDifferent from above deep learning based methods that consume substantial computational\\nresources, to improve efficiency, Hong et al. [16] firstly proposed retrieval-based method for this\\ntask, which uses a given changed method as a query to retrieve the most similar changed methods\\nfrom the code-review corpus. They represented code token vectors utilizing the Bag-of-Words\\nmodel [46], and then employed cosine similarity along with a text similarity method known as\\nGestalt Pattern Matching (GPM) to identify the most relevant reviews.\\nIn conclusion, recent advanced methods addressing automatic code review generation as either\\na generation task or an information retrieval task, with both methods proving effective. However,\\nit is worth noting that there is currently no work that combines the two methods and no work\\nutilizes external knowledge to enhance the generation performance of the model. In this case,\\nwe propose to use the RAG method in code review generation task, which utilizes the in-context\\nlearning capabilities of LLMs to incorporate retrieval results into generation models.\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n4\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\n2.2\\nRetrieval-Augmented Generation\\nThe RAG technique has already been proposed in early development of neural models [20, 23]\\nand applied in various domains, such as question answering [8, 15, 23, 25] and dialogue systems\\n[5, 9, 21, 38], etc. Its expansion was previously limited because early neural models, lacking zero-\\nshot learning capabilities, were confined to specific datasets and tasks and required fine-tuning\\nfor optimal performance. Performance would often degrade significantly due to changes in input\\nformats. However, LLMs, such as GPT [3], Llama [40], Mistral [18], have reignited a broad interest in\\nRAG [11]. The excellent contextual learning and understanding abilities allow LLMs to understand\\nvarying inputs (also known as prompts) and provide relevant responses [3]. This breakthrough\\nmakes the LLMs feasible to incorporate with the input with external information. And as expected,\\nLLMs have shown to benefit from RAG, such as reducing the hallucinations[11].\\nFurthermore, the RAG method has also been employed in various code-related tasks. Parvez\\net al. [33] applied the RAG method to code generation and summarization tasks. They employed\\nthe Dense Passage Retriever (DPR) [20] to encode both code and text, using the similarity of\\nthese encoded embeddings for retrieval. Based on the retrieved results, they then generated the\\ncorresponding output. Lu et al. [29] presented ReACC, a retrieval-augmented method for code\\ncompletion, leveraging external context by retrieving codes that are semantically and lexically\\nsimilar in the codebase. Nashid et al. [30] introduced CEDAR, which uses two different retrieval\\naugmented strategies (embedding-based using SRoBERTA [36] and frequency-based using BM-25\\n[37]) for creating prompts, enabling the Codex [34] generator to produce targeted outputs for\\nassertion generation and program repair tasks. Geng et al. [12] introduced RAG method in multi-\\nintent code comment generation task, using two retrieval strategies (token-based and semantic-\\nbased) to retrieve the most similar code and comment pairs and then using Codex to generate code\\ncomments with corresponding intents. Zhang et al [45] applied RAG method into commit message\\ngeneration. Their framework employs a hybrid retriever that combines sparse and dense retrieval\\nmethods, to enhance the generation performance of Pre-traing language models (PLM) and LLMs.\\nThe task of code review generation, different with code summarization and comment generation,\\nrequires that the model possess not only an understanding of the code but also domain-specific\\nknowledge. Therefore, employing RAG for this task, which is capable of effectively leveraging\\nexternal knowledge, is fully-motivated.\\n3\\nMethodology\\nIn this section, we introduce the architecture of RARe, focusing on its two key components: retriever\\nand generator. We explain the method from a probabilistic perspective.\\nFigure 1 illustrates the methodologies employed in our proposed RARe: Retrieval, Generation\\nand RAG. In previous approaches, retrieval and generation were distinct processes. For example,\\nthe green pathway, labeled 1, represents the retrieval phase, where similar code snippets and their\\ncorresponding reviews are extracted from the codebase. The blue pathway, labeled 2, corresponds\\nto the generation phase, where the input code is directly fed into a generative model to produce a\\nreview. The RAG process, shown by the orange pathway labeled 3, acts as a “bridge” between these\\ntwo methods, integrating both retrieval and generation. The RAG methodology can be outlined\\nacross the following three stages:\\n(1) Use a retrieval system to identify the top 𝑘most relevant code-review pairs from the codebase\\nin relation to the target code.\\n(2) Construct a comprehensive prompt for the generative model by combining the target code\\nwith the retrieved reviews.\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n5\\nTarget Code\\nGenerator\\nReview\\nRetriever\\nTop k Reviews\\nPrompt\\nInstruction\\nInstruction\\n❖Target Code\\n❖Top k Review\\nQuery\\n# Code Snippet\\n# Review\\nTop 1\\ndef max(a, b):\\nif a > b:\\n \\n Return a\\nreturn is not complete\\n...\\nTop k\\nFig. 1. The overall architecture of RARe. Different colors distinguish the retrieval, generation, and RAG\\nprocesses. Within the review of the target code, the text related to generation is marked in blue, while those\\nrelated to retrieval are marked in green.\\n(3) Feed this augmented prompt into the generator, allowing it to produce a review that is both\\ncontextually informed and tailored to the target code.\\n3.1\\nRetriever\\nThe key of retrieval process is to acquire representations of both code and review, subsequently\\ncomparing their similarities. This distinction determines whether the retriever is Normal Dense\\nRetrieval (NDR) or Dense Passage Retrieval (DPR).\\nNDR for code review generation task follows a uni-encoder architecture, which means that both\\ncode and review representations are generated by a single encoder. The probability model of NDR\\nis shown in Equation 1:\\n𝑝(𝑧|𝑥) ∝exp(𝑐(𝑧)𝑇𝑐(𝑥)),\\n(1)\\n𝑐(𝑧) = encoder𝑐(𝑧),\\n(2)\\n𝑐(𝑥) = encoder𝑐(𝑥),\\n(3)\\n𝑜= 𝑚(𝑧),\\n(4)\\nwhere 𝑥denotes a input code, 𝑧represents the retrieved code and 𝑜is the output review. 𝑐sym-\\nbolizes the dense representation of the code, encoded by the 𝑒𝑛𝑐𝑜𝑑𝑒𝑟𝑐. The product between two\\nrepresentations represents their distance. The 𝑒𝑥𝑝function is used to ensure that the distance is\\npositive. This distance directly correlates to the likelihood 𝑝of the code being retrieved. Specifically,\\nNormal Dense Retrieval selects the code with closest representations from the codebase. Then the\\nreview associated with the retrieved code (store in the mapping 𝑚) is utilized as final output.\\nDifferently, Dense Passage Retrieval for code review generation follows a bi-encoder architecture,\\nindicating that code and review representations are generated by two independent encoders. The\\nprobability model of DPR is shown in Equation 5:\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n6\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\n𝑝(𝑜|𝑥) ∝exp(𝑟(𝑜)𝑇𝑐(𝑥)),\\n(5)\\n𝑟(𝑜) = encoder𝑟(𝑜),\\n(6)\\n𝑐(𝑥) = encoder𝑐(𝑥),\\n(7)\\nwhere 𝑥denotes a input code and 𝑜represents the output review. 𝑟(𝑜) represents the dense\\nrepresentation of a review, encoded by the 𝑒𝑛𝑐𝑜𝑑𝑒𝑟𝑟. DPR trains two encoders simultaneously, one\\nfor code (encoder𝑐) and another for reviews (encoder𝑟), using contrastive loss to enhance retrieval by\\nmaximizing the similarity between relevant pairs [20]. In this case, DPR allows to directly compute\\nthe similarity between representations 𝑟(𝑜) and 𝑐(𝑥), thereby facilitating the direct comparison of\\ncode and its corresponding review. Therefore, DPR does not need to retrieve the code and map it to\\nthe review, allowing for the direct retrieval of reviews instead, thus streamlining the process.\\n3.2\\nGenerator\\nTransformers-based sequence-to-sequence models have become predominant in generation tasks,\\nprimarily encompassing two architectures: decoder-only and encoder-decoder. Concisely, in the\\nencoder-decoder framework, the encoder is tasked with analyzing and understanding the input\\ndata, while the decoder is responsible for crafting output based on this acquired comprehension. On\\nthe other hand, the decoder-only architecture, prevalent in LLMs, specializes in generating content\\ndirectly from the given context. The underlying probabilistic models of these two architectures are\\nsimilar:\\n𝑝𝑒𝑛𝑐𝑜𝑑𝑒𝑟−𝑑𝑒𝑐𝑜𝑑𝑒𝑟(𝑜|𝑥) =\\n𝑛\\nÖ\\n𝑖=1\\n𝑝𝜃(𝑜𝑖|𝑜1:𝑖−1,𝑥),\\n(8)\\n𝑝𝑑𝑒𝑐𝑜𝑑𝑒𝑟−𝑜𝑛𝑙𝑦(𝑜|𝑥) =\\n𝑛\\nÖ\\n𝑖=1\\n𝑝𝜃(𝑜𝑖|𝑥,𝑜1:𝑖−1),\\n(9)\\nwhere 𝑥is the input code processed by the encoder, 𝑜1:𝑖−1 represents the review sequence generated\\nup to the current step, and 𝑜𝑖is the output being generated at the current step. 𝜃is the parameters\\nof the encoder and decoder. 𝑝denotes the probability (likelihood) of generating an output sequence\\n𝑜given an input sequence 𝑥.\\nThe encoder-decoder model (as shown in Equation 8) first encodes the input coder 𝑥into a\\nfixed-length vector representation using the encoder, and then the decoder uses this vector along\\nwith the previously generated outputs to generate the next output 𝑜𝑖. In contrast, for decoder-only\\nmodels, both 𝑥and 𝑜1:𝑖−1 are provided to the model as a continuous sequence, and the model\\ngenerates the next output 𝑜𝑖based on the entire context provided. In our work, we exclusively\\nutilize LLMs with a decoder-only architecture, which leads us to mainly rely on Equation 9.\\n3.3\\nRetrieval-Augmented Reviewer\\nThe integration of retrieval and generation serves to enhance the code review process by leveraging\\nexternal knowledge that directly informs the generation of output sequences. From a mathematical\\nstandpoint, the retrieval step introduces a probabilistic component, 𝑝(𝑜′|𝑥), that models the like-\\nlihood of retrieving relevant reviews 𝑜′ given the input sequence 𝑥. This retrieval probability is\\nthen combined multiplicatively with the generation probability, as shown in Equation 10. The prod-\\nuct of these probabilities indicates that both the retrieval and generation components contribute\\nmeaningfully to the final output.\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n7\\n𝑃𝑅𝐴𝑅𝑒(𝑜|𝑥) = 𝑝(𝑜′|𝑥)𝑝𝑑𝑒𝑐𝑜𝑑𝑒𝑟−𝑜𝑛𝑙𝑦(𝑜|𝑥,𝑜′)\\n= 𝑝(𝑜′|𝑥)\\n𝑁\\nÖ\\n𝑖=1\\n𝑝𝜃(𝑜𝑖|𝑥,𝑜′,𝑜1:𝑖−1)\\n(10)\\nBy integrating retrieved reviews, the model gains additional context beyond just the input\\nsequence and training data, allowing it to generate more accurate and contextually relevant reviews\\nby leveraging references.\\n4\\nExperiments\\nIn this section, we outline the research questions that guide our study and describe the experimental\\nsetup designed to address them. We detail the datasets used, the evaluation metrics employed, the\\nbaseline methods for comparison, the models involved, and the relevant experimental settings.\\n4.1\\nResearch Questions\\nWe list three research questions to guide the rest of experiments, each targeting a distinct aspect of\\nthe evaluation: the performance of individual components, the overall performance of the proposed\\nsystem, and a fine-grained analysis of the system’s performance.\\n• RQ1: What are the performances of different retrievers and generators?\\nWe aim to separately compare the effectiveness of different retrievers and generators.\\n• RQ2: What is the performance of RARe compared to the state-of-the-art method?\\nThis question is the main focus of the paper. As introduced in Section 1 and Section 3, the\\nretriever could offer external knowledge (similar reviews from the codebase) to the generator,\\nwhich make it have potential to outperform the current models.\\n• RQ3: What explanation can be provided for the performance of RARe?\\nWhile performance scores offer a quantitative measure of RARe’s effectiveness, they don’t\\ntell the whole story. We believe that human evaluation and interpretatbility of neural models\\nare also crucial to demonstrate the impact of RARe.\\n4.2\\nDataset\\nWe use two publicly available code review datasets for experiments: the CodeReviewer dataset\\n(CRer.) [27] and the Tufano dataset (Tuf.) [41]. The CRer dataset collects code from GitHub reposi-\\ntories across nine programming languages, including Java, Python, PHP, and others. In contrast,\\nthe Tuf dataset consists exclusively of Java code, sourced from both GitHub and Gerrit. Table 1\\ndetails the statistics of these two datasets. The code change inputs differ slightly between the two\\ndatasets, as illustrated in Table 1. The CRer dataset uses a code-difference format, where ′+′ and\\n′−′ symbols indicate the changes, whereas the Tuf dataset includes only the original code snippet.\\nTo give a clear insight of the formats, we provide two examples from these datasets in Table 2.\\nTable 1. Statistics of CodeReviewer and Tufano dataset.\\nDataset\\nTrain\\nTest\\nVal\\nTotal\\nCRer.\\n∼118k\\n∼10k\\n∼10k\\n∼138k\\nTuf.\\n∼134k\\n∼17k\\n∼17k\\n∼168k\\nWe do not apply any preprocessing to these datasets, since they have been already cleaned to\\nreduce duplicates. The data split ratios are consistent with those used in previous works [27, 41].\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n8\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\nTable 2. Two examples of CRer. dataset and Tuf. dataset.\\nCodeReviewer Code:\\n@@ -53,7 +53,7 @@\\npublic class ProtocGapicPluginGeneratorTest {\\nmodel.getFiles().stream()\\n.map(ProtoFile::getProto)\\n.collect(Collectors.toList()))\\n// Only the file to generate a client for (don\\'t generate dependencies)\\n.addFileToGenerate(multiple_services.proto)\\n-\\n.setParameter(language=java)\\n+\\n.setParameter(language=java,transport=grpc)\\n.build();\\nCodeGeneratorResponse response =\\nProtocGeneratorMain.generate(codeGeneratorRequest);\\nCodeReviewer Review: Can we also test for ‘transport=rest‘?\\nTufano Code:\\npublic static <T> TypeToken<T> getTypeToken(TypeToken<T> token, Class<? super T> raw) {\\nif (raw != null)\\nreturn TypeToken.of((Class<T>) raw);\\nif (token != null)\\nreturn token;\\nthrow new IllegalStateException(\"Both indicators of type are null\");\\n}\\nTufano Review: Should this not prefer the token over the raw?\\n4.3\\nEvaluation\\nFollowing prior works on code review generation [16, 24, 27, 28, 41], we apply three metrics\\nin our model evaluation: BLEU-4, ROUGE-L, and METEOR. These metrics together provide a\\ncomprehensive evaluation of the quality of generated reviews, capturing different aspects of text\\nsimilarity and fluency.\\nBLEU [32] is the most widely used metric to measure the quality of generated reviews. Specifically,\\nwe use BLEU-4, which calculates the overlap of n-grams, where n ranges from 1 to 4. It is computed\\nas:\\nBLEU = exp\\n 4\\n∑︁\\n𝑛=1\\n𝑤𝑛· log𝑝𝑛\\n!\\n(11)\\nwhere 𝑝𝑛is the precision of n-grams, 𝑤𝑛are the weights (typically uniform).\\nROUGE [6] measures text similarity by counting overlapping units such as n-grams, word pairs,\\nand sequences. Among its variants, ROUGE-L is the most popular [24], which uses the longest\\ncommon subsequence (LCS) for evaluation. The ROUGE-L score is computed as:\\nROUGE-L = 𝐿𝐶𝑆(𝐶, 𝑅)\\nlength(𝑅)\\n(12)\\nwhere 𝐿𝐶𝑆(𝐶, 𝑅) is the length of the longest common subsequence between the candidate text 𝐶\\nand the reference text 𝑅.\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n9\\nMETEOR [2] improves upon BLEU by considering synonyms, stemming, and alignment based\\non recall and precision. METEOR is designed to address some of the limitations of BLEU, particularly\\nin terms of word matching flexibility. The METEOR score is computed as:\\nMETEOR =\\n\\x12\\n1 −𝛾· 𝑐ℎ\\n𝑚\\n\\x13\\n· 𝐹mean\\n(13)\\nwhere 𝐹mean is the harmonic mean of precision and recall, 𝑚is the number of matches, 𝑐ℎis the\\nnumber of chunks, and 𝛾is a parameter that penalizes fragmentation in the matched words.\\nBLEU-4 and METEOR metrics use the implementations provided by the Python NLTK library1,\\nand ROUGE-L is sourced from the Python Rouge library2.\\n4.4\\nBaselines\\nAs mentioned in Section 2, we select two main types of state-of-the-art approaches: retrieval-based\\nmethods and DL-based generative methods. The retrieval-based method involves CommentFinder\\n[16], and the DL-based generative methods include Tufano et al. [41], CodeReviewer [27], AUGER\\n[24], and LLaMA-Reviewer [28]. In addition, we include the pre-trained language model CodeT5 as\\none of the baseline models, due to its capability in code generation fields [44].\\n4.5\\nModel Selection\\nAs described in Section 3, the RARe includes two components, with each process including several\\nmethods/models to choose. In this section, we describe the methods and models we use to compare.\\n4.5.1\\nRetriever. We compare three kinds of retrievers: the first two are based on Normal Dense\\nRetrieval (NDR), and the third is based on Dense Passage Retrieval (DPR).\\nThe first kind of retriever, termed as Normal Dense Retriever (NDR), utilizes CodeBERT [10] as\\nthe encoder, employing cosine distance as the similarity measurement. Inspired by CommentFinder\\n[16], the second kind of retriever improves upon the first by enhancing the similarity calculation,\\nwhich combines cosine and Gestalt Pattern Matching (GPM) similarity. And we name it as GPM\\ndense retriever (GDR). GPM [35] is a text similarity approach that takes into account the order of\\ncode tokens, allowing for a nuanced comparison of code snippets. Specifically, cosine similarity\\nis used to select the top k candidates, followed by reranking them using GPM to get the final top\\nk results. For the third kind of retriever, DPR, we follow the work on code summarization [19],\\nleveraging CodeBERT and GraphCodeBERT [13] as encoders for code and reviews, respectively,\\nand also use cosine similarity.\\n4.5.2\\nGenerator. Inspired by the work of Wang et al.[43], we also apply following two methods\\nin generation: inferencing and fine-tuning. We employ three open-source LLMs as our genera-\\ntion models: Llama 3.13, Mistral4, and CodeGemma5. These models have been pre-trained on code,\\nmaking them well-suited for our tasks. We compare four generation types to verify RARe’s effective-\\nness: direct inference, fine-tuning, retrieval-augmented direct inference, and retrieval-augmented\\nfine-tuning.\\n1https://www.nltk.org/\\n2https://pypi.org/project/rouge/\\n3https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\\n4https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\\n5https://huggingface.co/google/codegemma-7b-it\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n10\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\nTable 3. Prompts for direct inference and RARe.\\nDirect Inference Prompt:\\nRARe Prompt:\\nYour task is to write a concise code review for the\\ngiven code snippet. Your output should only be a\\nbrief code review, no extra information.\\nHere is an example code review: {retrieved reviews}.\\nYour task is to write a concise code review for the\\ngiven code snippet. Your output should only be a\\nbrief code review, no extra information.\\n4.5.3\\nPrompt Construction. For direct inference and fine-tuning, we followed the official prompt\\nexamples provided for each model. Since prompt tuning is not the focus of this paper, we kept the\\ntemplates simple and concise. The prompt template we designed for LLMs is shown in Table 3.\\n4.6\\nTraining Setting\\nAmong the retrievers, only DPR requires training. We train two encoders on the training data from\\nthe two datasets, following the well-adjusted hyperparameters provided by the original work [33].\\nFine-tuning LLMs can be inefficient for a single task due to their vast number of parameters.\\nTherefore, we use Low-Rank Adaptation (LoRA), which introduces trainable low-rank matrices\\nto modify the weights of the model’s layers, specifically targeting the attention and feedforward\\nlayers. LoRA modifies the weight matrix 𝑊0 as follows:\\n𝑊=𝑊0 + Δ𝑊,\\nwhere\\nΔ𝑊= 𝐴× 𝐵\\n(14)\\nHere, 𝐴and 𝐵are low-rank matrices with rank(𝐴) = rank(𝐵) ≪rank(𝑊0), reducing the number\\nof parameters needed for adaptation. While there are alternative strategies, such as prefix tuning,\\nwe follow the findings of the LLaMa Reviewer [28], which demonstrate the superiority of LoRA\\nover prefix tuning.\\nWe configure the training to run for 5 epochs with a learning rate of 10−4 and a batch size of 16.\\nWe employed 4 NVIDIA A100 GPUs to accelerate the training process. Additionally, to ensure the\\nreliability of our results, we averaged the results across three runs for each experiment.\\n5\\nResults and Analysis\\nIn this section, we compare the performance of different retrievers and generators, and present the\\nresults of our proposed RARe architecture in comparison with baseline methods.\\nTable 4. Comparison of three retrievers: NDR (Normal Dense Retrieval), GDR (GPM Dense Retrieval), DPR\\n(Dense Passage Retrieval).\\nMethod\\nCRer.\\nTuf.\\nBLEU-4\\nROUGE-L\\nMETEOR\\nBLEU-4\\nROUGE-L\\nMETEOR\\nNDR\\n9.53\\n5.65\\n5.54\\n12.06\\n8.45\\n8.17\\nGDR\\n9.55\\n5.66\\n5.55\\n12.33\\n8.67\\n8.43\\nDPR\\n9.71\\n5.70\\n5.60\\n11.80\\n7.98\\n7.74\\n5.1\\nComparisons of Retrievers\\nThe retrieval performance comparison involves three approaches: NDR, GDR, and DPR (introduced\\nin Section 4), as shown in Table 4. Notably, in this comparison, we primarily focus on the top-1\\nretrieval results. We observe that on the CRer. dataset, DPR attains the highest score in all metrics,\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n11\\nwith 9.71 in BLEU-4, 5.70 in ROUGE-L, and 5.60 in METEOR. And GDR demonstrates the best\\nperformance on the Tuf. dataset, with 12.33 in BLEU-4, 8.67 in ROUGE-L and 8.43 in METEOR.\\nThe results that GDR outperforms NDR aligns with expectations. GDR employs GPM, a specially\\ndesigned metric that reorganizes retrieval results while incorporating NDR, leading to superior\\nperformance on both datasets. These two methods excel when dealing with single-programming-\\nlanguage dataset (Tuf.), likely because in such a dataset, where all the code shares the same language\\nstructure and syntax, only comparing code similarity is sufficient for retrieving useful reviews.\\nHowever, both methods are less effective than DPR on a multi-programming dataset (CRer.) because\\nDPR assesses the similarity between code and review, thus avoiding potential issues that arise from\\ncomparing codes across different languages.\\nTable 5. Performance of models on CRer. dataset and Tuf. dataset with and without Retrieval Augmentation\\n(+RA). Note: DI=Direct Inference, FT=Fine Tuning, B=BLEU-4, R=ROUGE-L, M=METEOR.\\nModel\\nCRer.\\nTuf.\\nBLEU-4\\nROUGE-L\\nMETEOR\\nBLEU-4\\nROUGE-L\\nMETEOR\\nLlama3.1\\nDI\\n5.84\\n3.18\\n3.18\\n5.12\\n2.55\\n2.67\\n+RA\\n12.31 ↑110%\\n6.43 ↑102%\\n8.33 ↑161%\\n12.96 ↑153%\\n8.67 ↑240%\\n8.76 ↑228%\\nFT\\n8.82\\n6.75\\n5.29\\n8.06\\n5.87\\n4.57\\n+RA\\n9.05 ↑3%\\n6.90 ↑2%\\n5.42 ↑2%\\n9.19 ↑14%\\n7.00 ↑19%\\n5.66 ↑24%\\nMistral\\nDI\\n5.07\\n2.69\\n2.72\\n5.21\\n2.75\\n3.09\\n+RA\\n11.15 ↑114%\\n5.75 ↑114%\\n6.87 ↑153%\\n9.74 ↑87%\\n7.69 ↑179%\\n7.58 ↑145%\\nFT\\n9.07\\n6.56\\n5.32\\n8.42\\n6.21\\n4.91\\n+RA\\n9.59 ↑6%\\n7.40 ↑13%\\n6.59 ↑24%\\n9.58 ↑14%\\n7.40 ↑19%\\n6.05 ↑23%\\nCodeGemma\\nDI\\n3.34\\n1.53\\n1.40\\n5.27\\n3.02\\n2.95\\n+RA\\n4.76 ↑43%\\n2.56 ↑67%\\n2.49 ↑78%\\n5.51 ↑5%\\n3.02 —\\n3.53 ↑20%\\nFT\\n9.26\\n6.48\\n5.22\\n8.39\\n5.70\\n4.53\\n+RA\\n9.33 ↑1%\\n6.51 ↑1%\\n5.25 ↑1%\\n9.44 ↑13%\\n6.88 ↑16%\\n5.60 ↑24%\\n5.2\\nComparisons of Generators\\nAs introduced in Section 4, we compare three LLMs (LLama 3.1, Mistral and CodeGemma) with\\nemploying four generation approaches: direct inference (DI), fine-tuning (FT), retrieval-augmented\\ndirect inference, and retrieval-augmented fine-tuning. The results are shown in Table 5.\\nFirstly, we focus on the results without RA. Among three models, Llama 3.1 generally outper-\\nforms the other two models in direct inference, highlighting its strong code understanding ability.\\nCompared to direct inference, fine-tuning consistently improves the performance of all three models\\nacross both datasets. Overall, the performance of Mistral and CodeGemma is comparable, slightly\\noutperforming that of Llama 3.1. Llama 3.1 performs better in direct inference, likely because of\\nits broader pre-training. The similar performance of the three models after fine-tuning suggests\\nthat, despite differences in their architectures or pre-training, they can all effectively adapt to\\ntask-specific datasets when given additional training.\\nThen for direct inference (DI), retrieval augmentation (+RA) shows substantial performance\\nimprovements. Llama 3.1 demonstrates the highest improvement on both datasets, especially on\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n12\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\nthe Tuf. dataset, with a 153% increase in BLEU-4, a 240% rise in ROUGE-L, and a 228% increase in\\nMETEOR. Mistral shows considerable improvements across both datasets, notably on the CRer.\\ndataset, with a 114% increase in both BLEU-4 and ROUGE-L, and a 145% rise in METEOR. Although\\nCodeGemma also shows improvements, they are relatively modest compared to the other two\\nmodels.\\nFor fine-tuning (FT), retrieval also enhances generation performance, with Mistral showing the\\ngreatest improvement across both datasets. On the CRer. dataset, Mistral achieves a 6% increase in\\nBLEU-4, a 13% rise in ROUGE-L, and a 24% gain in METEOR. Similarly, on the Tuf. dataset, Mistral\\ndelivers a 14% boost in BLEU-4, a 19% improvement in ROUGE-L, and a 23% increase in METEOR.\\nObviously, retrieval can greatly improve model performance in the code review generation task,\\nespecially when applied in direct inference. However, this is not always the case. For example, in\\nCodeGemma, the benefits of retrieval are very limited. We attribute this to the model’s relatively\\nweaker ability to understand prompts compared to the other two models, as evidenced by its lower\\ndirect inference results. Notably, when retrieval augmentation is applied to direct inference, Llama\\n3.1 achieves the highest performance among the three models across both datasets. On the CRer.\\ndataset, it reaches a BLEU-4 score of 12.31, a ROUGE-L score of 6.43, and a METEOR score of 8.33.\\nOn the Tuf. dataset, it attains a BLEU-4 score of 12.96, a ROUGE-L score of 8.67, and a METEOR\\nscore of 8.76.\\nAnswer to RQ1:\\nFor retrievers, DPR and GDR achieve the best retrieval results on multi-programming-\\nlanguage and single-programming-language datasets, respectively.\\nFor generators, Llama 3.1 performs the best on both datasets, particularly in direct inference\\nwhen combined with retrieval augmentation.\\nTable 6. Overall comparison between baseline methods and RARe. −indicates that the previous study did\\nnot provide the predictions. For RARe, the number of retrieved reviews are in brackets.\\nMethod\\nCRer.\\nTuf.\\nBLEU-4\\nROUGE-L\\nMETEOR\\nBLEU-4\\nROUGE-L\\nMETEOR\\nTufano et al. [41]\\n-\\n-\\n-\\n12.31\\n8.72\\n7.83\\nCodeT5 [44]\\n7.34\\n7.41\\n5.86\\n7.10\\n6.61\\n5.13\\nCodeReviewer [27]\\n6.02\\n5.39\\n3.68\\n-\\n-\\n-\\nCommentFinder [16]\\n9.47\\n5.69\\n5.44\\n12.71\\n8.81\\n8.61\\nAUGER [24]\\n8.09\\n6.50\\n4.74\\n7.76\\n5.77\\n4.36\\nLLaMA-Reviewer [28]\\n8.23\\n6.12\\n5.34\\n7.85\\n5.82\\n4.38\\nRARe (random)\\n11.29\\n6.41\\n7.97\\n10.88\\n6.66\\n7.82\\nRARe (top1)\\n12.32\\n6.43\\n8.33\\n12.96\\n8.67\\n8.76\\nRARe (top3)\\n11.76\\n6.14\\n8.14\\n11.74\\n6.89\\n8.00\\nRARe (top5)\\n10.81\\n5.89\\n7.76\\n10.80\\n6.47\\n8.23\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n13\\n5.3\\nComparisons of Code Reviewers\\nBased on the previous comparison of retrievers and generators, we selected the best-performing\\nretriever and generator for RARe to compare against the baseline methods. Specifically, for the\\nCRer. dataset, we use the DPR retriever, while for the Tuf. dataset, we use the GDR retriever. And\\nLlama 3.1 serves as the generator for both datasets. We conducted performance comparisons among\\nRARe and baseline methods on both datasets, as shown in Table 6.\\nCompared to baselines, RARe achieves the best performance across both datasets in three metrics.\\nEspecially on the CRer. dataset, RARe significantly outperforms Commentfinder by 30% in BLEU-4\\nand 53% in METEOR, respectively. On the Tufano dataset, RARe shows a modest improvement,\\nsurpassing CommentFinder by 2% in both BLEU-4 and METEOR scores.\\nThese results highlight the superior performance of the RARe method compared to state-of-\\nthe-art retrieval-based and deep learning-based generative methods. To further understand how\\ndifferent retrieval options affect RARe’s performance, we implement different retrieval strategies in\\nRARe, as Table 6 presented. ’Random’ refers to the random selection of an example review for the\\nsource code.\\nThe results indicate that RARe with the top_1 retrieval setting attains the best performance. This\\nsuggests that the most relevant review contributes most significantly to the performance of the\\ngenerated code review. We also observe that with the increase of example reviews (from 1 to 5),\\nthe performance of RARe decreases. This implies that considering more reviews may introduce\\nirrelevant information, potentially confusing the generator.\\nAnswer to RQ2:\\nOur RARe system outperforms previous methods in the code review generation task,\\nachieving the best BLEU scores with 12.32 on the CRer. dataset and 12.96 on the Tuf.\\ndataset.\\nAnd retrieval quality is critical for the effectiveness of code review generation, where\\nselecting the most relevant review yields the best results rather than using more reviews.\\n6\\nCase Study\\nIn this section, we conduct three case studies: comparing different reviewers using a specific\\ninstance, evaluating the benefits of retrieval augmentation through human evaluation, and probing\\nthe models with an interpretable tool.\\nWe randomly select instances from the Tuf. dataset and compare the reviews generated by\\ndifferent reviewers on this code snippet, as shown in Table 7. Through these instances, we attribute\\nthe improvements of retrieval augmentation shown in Table 6 to two key factors: review writing\\nstyle and bug location.\\n6.1\\nComparative Analysis of Reviewers\\nFor review writing style, we observe that fine-tuned models are good at following the writing\\npatterns of original dataset, such as CodeT5, Tufano, AUGER and our FT model. In contrast, model\\nusing direct inference (DI) tends to produce general, long and summary-like reviews, resulting in\\nlower scores. The retrieval-augmented direct inference partly alleviates the problem of writing style\\nof DI, by just providing one retrieved review, which make it generate short and more review-like\\nreviews. For bug location, FT model does not locate the \"unnecessary synchronization\" problem,\\nbut after referring to the retrieved review (result for DPR retriever), RAFT model successfully locate\\nthis issue.\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n14\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\nTable 7. An example from Tuf. dataset comparing RARe (CodeGemma) with other baselines. We omit some\\nparticularly long reviews.\\nData/Models\\nCode/Reviews\\nBLEU-4\\nCode snippet\\npublic void onClose(Client client) {\\nsynchronized(clients) {\\nclients.remove(client);\\n}\\n}\\nGround Truth\\nThis synchronization seems unnecessary?\\nCodeT5\\nShould this be a WARN instead?\\n0.0839\\nCommentFinder\\nOff topic: I’ve been writing this as \"client == null\". I think this conveys just as much\\ninformation in fewer characters, so by Shipley’s law I should switch over. The drawback\\nis that \"client == null\" is a boolean expression, and is consistent with \"limit < 0\" etc.\\n0.0442\\nTufano\\nI think you should have done something like: if (clients!= null) {clients.remove(client);}\\n0.0255\\nAUGER\\nRemove client from the interface but keep it on the i imp\\n0.0530\\nGDR retriever\\nI don’t see any other method that this synchronization would avoid overlapping with.\\n0.2741\\nDI\\nThe code snippet synchronizes on the ‘clients‘ object before removing a client from\\nthe list. This is necessary to avoid race conditions where multiple threads attempt to\\nremove the same client from the list simultaneously...\\n0.1154\\nRADI\\nThe synchronized block is not necessary...\\n0.1228\\nFT\\nsynchronized (clients) clients.remove(client);\\n0.2328\\nRAFT\\nthis can be simplified to clients.remove(client)\\n0.0895\\nHowever, we notice that the BLEU score of RADI is lower than DI, due to its longer length, which\\nincreases the matches of overlap n-grams with the Ground Truth. Similarly, FT’s BLEU-4 is also\\nhigher than RAFT in some cases, even the latter is shown to be better after human check. This is\\nalso due to the limitation of BLEU’s semantic similarity. Other metrics (ROUGE, METEOR) also\\nhave this issue. Therefore, to better investigate the models’ performance, we further conduct a\\nhuman evaluation in following section.\\n6.2\\nHuman evaluation of RARe\\nBased on the classification criteria from prior work [16, 41], we randomly selected 100 samples\\nfrom the Tuf. dataset and the CRer. dataset, respectively. These samples are selected from Llama 3.1\\nmodel’s predictions, due to its overall superior performance. The quality of the code reviews can\\nbe evaluated by four criteria [16]:\\n• Perfect Prediction (PP): the review is syntactically equivalent to the actual one.\\n• Semantically Equivalent (SE): the review is semantically equivalent to the actual one.\\n• Alternative Solution (AS): a review that is different from the actual one but is also valuable.\\n• Incorrect (IN): there is no useful information in the review.\\nIn the annotation process, two expert annotators, each with 6 or 8 years of software engineering\\nexperience, use a 0-5 rating system to evaluate the review categories as follows: 0-1 as IN, 2-3 as AS,\\n4 as SE, and 5 as PP. To ensure reliable results, if there is a disagreement between the annotators, a\\ndiscussion is held to reach a consensus on the final rating.\\nThe human-checked results of direct inference and fine-tuning are shown in Table 8. We consider\\nreviews categorized as Perfect Prediction, Semantically Equivalent, and Alternative Solution to be\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n15\\nvaluable. On the Tuf. dataset, RARe significantly increases the proportion of valuable reviews, with\\nan increase of 266% (from DI to RADI) and an improvement of 8% (from FT to RAFT). Similarly, on\\nthe CRer. dataset, valuable reviews increase by 275% (from DI to RADI) and by 21% (from FT to\\nRAFT). This highlights the effectiveness of RARe in enhancing the models, particularly in direct\\ninference with LLMs, supporting our conclusion in RQ1.\\nTable 8. Human evaluation of RARe’s generated reviews on 100 samples from the CRer. and Tuf. datasets.\\nMetrics\\nCRer.\\nTuf.\\nDI\\nRADI\\nFT\\nRAFT\\nDI\\nRADI\\nFT\\nRAFT\\nPerfect Prediction\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n3\\nSemantically Equivalent\\n0\\n1\\n1\\n4\\n1\\n3\\n7\\n11\\nAlternative Solution\\n12\\n44\\n55\\n64\\n14\\n52\\n52\\n50\\nIncorrect\\n88\\n55\\n44\\n32\\n85\\n45\\n41\\n36\\nIn addition, a closer investigation of reviews generated through direct inference reveals that\\nthey often begin with a summary of the overall code but tend to be overly general, redundant,\\nand vague, resulting in a high rate of Incorrect reviews. After retrieval augmentation, the reviews\\nbecome more specific, leading to a sharp increase of AS reviews. While fine-tuning produces more\\nvaluable reviews overall, retrieval augmentation further enhances their accuracy, as evidenced by\\nthe increase of SE and PP reviews. This finding is consistent with our observation in Section 6.1.\\n6.3\\nInterpretative Analysis of RARe\\nWe use Inseq, an interpretability toolkit for sequence generation models, to make visible of the\\nmodels’ attention score. It can provide faithfulness, plausibility and usefulness of explanations for\\nthe output of decoder-only or encoder-decoder Transformer architectures. Simply, it generates a\\nattention score between each pair of the input token and output token to quantify the dependence\\nof the output on the input.\\nHere, we present an example to illustrate the enhancement of RARe, as depicted in Figure 2. Due\\nto extensive length the input, we exclude tokens with lower scores and only display the important\\ntokens.\\nThe performance of RARe is notably superior, as partly evidenced by its higher BLEU-4 score\\n(0.342 v.s. 0.042). Looking deeper into the generated output reveals additional evidence of RARe’s\\nenhancement. Specifically, the text retrieved (marked in green) receives considerable focus in\\nRARe’s predictions, as indicated by its high attention score. This demonstrates that the model\\neffectively leverages external information from retrieval, which is a suggestion to \"incorporate a\\nnull check function\" in this example. Moreover, RARe doesn’t simply copy the retrieved results,\\nit also concentrates on the code. This is evident from the attention scores assigned to the code\\n(highlighted in blue) from the output in Table (b), showing RARe conducts a balanced integration\\nof external knowledge with original code analysis.\\nGoing back to a broader perspective, it becomes apparent that retrieval results can significantly\\nenhance the model’s attention, preventing it from concentrating on less relevant code. For instance,\\nwithout RAG, the model focus on the getStartTime() function, diverging from our expected review.\\nHowever, with RAG, this deviation is corrected, redirecting the model’s attention to the null check\\nin volume.getAsyncTask() function. It is important to clarify that the output in Table (a) is not\\nnecessarily incorrect; rather, it deviates from the reviewer’s intended focus. This deviation suggests\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n16\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\nthat the output lacks specific domain knowledge necessary for concentrating on the aspects of the\\ncode that truly require review.\\n_I\\n_think _this\\n_is\\n_not\\n_need\\ned.\\n_The\\n_start _time\\n_is\\n_alrea\\ndy\\n_set\\n_in\\n_the\\n_const\\nrcucto\\nr\\n[instruction]\\n…\\n…\\n_private\\n0.02\\n0.01\\n0.01\\n0.01\\n0.01\\n0.01\\n0.01\\n0.00\\n0.00\\n0.00\\n0.00\\n0.01\\n0.01\\n0.01\\n0.01\\n[code…]\\n…\\n…\\n_volume.getAsync\\nTask\\n0.01\\n0.02\\n0.01\\n0.02\\n0.00\\n0.01\\n0.01\\n0.02\\n0.04\\n0.01\\n0.01\\n0.02\\n0.01\\n0.00\\n0.00\\n!stepslist. \\nisEmpty())\\n0.00\\n0.01\\n0.00\\n0.01\\n0.00\\n0.01\\n0.00\\n0.00\\n0.02\\n0.04\\n0.00\\n0.05\\n0.01\\n0.00\\n0.01\\n_entity.setStartTim\\ne(stepslist.get(0).ge\\ntStartTime\\n0.08\\n0.07\\n0.10\\n0.11\\n0.21\\n0.08\\n0.06\\n0.19\\n0.21\\n0.11\\n0.23\\n0.15\\n0.10\\n0.09\\n0.22\\n[…code]\\n…\\n…\\n_can\\n_you\\n_please\\n_add\\n_the\\n_check\\n_for\\n_volume.\\ngetAsync\\nTask()\\n_in\\n_this\\n_method\\n[instruction]\\n…\\n…\\n_private\\n0.01\\n0.00\\n0.00\\n0.01\\n0.01\\n0.00\\n0.01\\n0.00\\n0.00\\n0.00\\n0.00\\n[code…]\\n…\\n…\\n_volume.getAsyncTa\\nsk()\\n0.01\\n0.12\\n0.19\\n0.16\\n0.01\\n0.33\\n0.18\\n0.47\\n0.48\\n0.11\\n0.21\\n!stepslist. isEmpty( ))\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n_entity.setStartTime(\\nstepslist.get(0).getSta\\nrtTime( ))\\n0.01\\n0.03\\n0.01\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n0.00\\n[…code]\\n…\\n…\\n[retrieved review…]\\n…\\n…\\n_(asyncTask\\n0.01\\n0.04\\n0.04\\n0.22\\n0.23\\n0.44\\n0.10\\n0.33\\n0.31\\n0.12\\n0.22\\n_!=\\n0.04\\n0.04\\n0.07\\n0.12\\n0.12\\n0.56\\n0.19\\n0.12\\n0.11\\n0.23\\n0.28\\n_null)\\n0.01\\n0.08\\n0.17\\n0.19\\n0.23\\n0.29\\n0.22\\n0.19\\n0.11\\n0.01\\n0.01\\n[Instruction]\\nYour task is to write a concise code \\nreview for the given code snippet. Your \\noutput should only be a brief code review \\nwith no other information.\\n[code]\\nprivate GlusterVolumeTaskStatusEntity \\nfetchTaskStatusDetails() {\\n ...\\n GlusterAsyncTask asyncTask = \\nvolume.getAsyncTask();\\n if (asyncTask != null) {\\n List<Step> stepsList = ...;\\n if (stepsList != null \\n&& !stepsList.isEmpty()) {\\n entity.setStartTime(stepsList.get(0\\n).getStartTime());\\n }\\n }\\n return entity;\\n}\\n[retrieved review]\\nit could be more readable if (asyncTask != \\nnull && asyncTask.getTaskId() != null ) \\n{ ... }\\n[expected review]\\nnull check should done for \\nasyncTask.getTaskId() as well\\n(a) Prediction without RARe (BLUE-4: 0.042)\\n(b) Prediction with RARe (BLUE-4: 0.342)\\nFig. 2. An example from Tuf. dataset and the saliency heatmap comparison for fine-tuned Llama 3.1 with and\\nwithout retrieval augmentation. The horizontal rows of tables display the models’ output. Different input\\ncomponents are differentiated using colors, and higher attention scores are highlighted in red within the\\ntable.\\nAnswer to RQ3:\\nThrough these case studies, we explain how retrieval enhances the generation in details,\\nwhich are fine-grained evidences apart from simple scores. The retrieval is proven to be able\\nto explicitly supplies generative models with external domain knowledge by referencing\\nrelevant reviews, which allows RARe to produce more domain-appropriate reviews for the\\ncode snippet in the desired writing style and precise location.\\n7\\nDiscussion\\nThe findings from the research questions, along with the detailed analysis in Section 5 and Section\\n6 have demonstrated the effectiveness of our RARe approach in code review generation, compared\\nto previous work. In this section, we further discuss the implications, and limitations of RARe.\\nImplications. Firstly, in the domain of software engineering, the significant goal of automated\\ncode review generation approaches often lies in reducing human effort. Therefore, to a certain\\nextent, mimicking human behavior and thinking in the process of generating reviews is useful. In\\nour methodology, the retrieval process simulates the recall of prior professional knowledge and\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n17\\nexperience, allowing RARe to generate high-quality, relevant code reviews. By achieving state-of-\\nthe-art performance, RARe can effectively save developers time and effort. Secondly, RARe extends\\nthe generalization of Retrieval-Augmented Generation technique. As mentioned in Section2, RAG\\nhas been widely adopted for generation tasks across various domains. Our RARe further validates\\nits effectiveness in the field of software engineering and also offers insights for future research.\\nLimitations. There are some limitations in our experiments. First, our evaluation of RARe\\ncomponents is limited in diversity, focusing on only three LLMs for the generator and exclusively\\ndense retrieval methods. Future research could explore a broader range of models, from smaller\\nvariants like DistilBERT to larger ones like GPT, while incorporating diverse retrieval methods,\\nsuch as sparse or hybrid retrievers, to gain deeper insights. Second, the case study evaluation could\\nbenefit from including more cases in Section 6.1 or Section 6.3 to provide a more comprehensive\\nunderstanding of RARe’s performance. Additionally, the reliance on only two annotators may\\nintroduce bias and limit the diversity of human evaluations. Future studies should involve multiple\\nannotators to improve reliability.\\n8\\nTHREATS TO VALIDITY\\nAlthough we conducted comprehensive experiments and evaluations in our study, there are still\\nthe following threats that may affect our research.\\nInternal Validity. A notable threat to internal validity comes from the pre-training data of the\\nLLMs used in RARe. Because the details of the datasets used to pre-train these LLMs are often\\nnot publicly disclosed, there is uncertainty about the diversity, quality, and relevance of the data\\nthese models are exposed to. This lack of transparency can affect the performance of the models,\\nas biases or gaps in the pre-training data can lead to inappropriate code reviews. One solution\\ncould be to carefully select diverse and representative test datasets that cover a wide range of\\nprogramming languages and domain-specific scenarios. Additionally, cross-validation techniques\\ncan be employed to assess the robustness of the models across various tasks. It is also important to\\nconduct manual check of model outputs to identify potential biases or gaps in performance.\\nThe other potential threat to internal validity is the external knowledge base. A rich multilingual\\nknowledge base covering various programming languages, libraries, and coding practices can\\nsignificantly improve the effectiveness of RARe. In our study, we did not construct a new external\\ndatabase, but used the training dataset as the knowledge base. This is due to our main goal is to show\\nthe effectiveness of RARe in code review generation relative to retrieval-only and generation-only\\nmethods, rather than to build the best external knowledge base.\\nConstruct Validity. A potential threat to construct validity in this research lies in the evaluation\\nmetrics for assessing the quality of generated code reviews. Although we combine model evaluation\\nthat measure the text similarity and human evaluation that measure the valuable reviews. But there\\nis no quality metric to define what is a good code review. This oversight can result in measurements\\nthat are inconsistent with what was intended, affecting the accuracy and relevance of our findings.\\nAdditionally, the quality of datasets is also a threat. How to ensure that the quality of the reviews\\nin the dataset is good or bad. This issue also has a great impact on the generation performance.\\nTherefore, further work can explore specific quality metrics for reviews.\\nExternal Validity. An external threat in this research lies in the generalization of the results.\\nAlthough we used a multi-programming language dataset and a Java-specific dataset to validate\\nour approach, the data for each language in the multi-language dataset are relatively limited. This\\nraises concerns about the model’s ability to generalize effectively to other programming languages\\nor broader contexts. Consequently, larger and more diverse datasets are essential to improve the\\ngeneralization of our method in various programming environments.\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n18\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\n9\\nConclusion\\nIn this study, we have employed Retrieval-Augmented Generation (RAG) to incorporate exter-\\nnal domain knowledge into the input of LLMs for the task of code review generation. Through\\ncomprehensive evaluation on two datasets, our approach, RARe achieves superior performance\\nover the existing state-of-the-art methods, and on the other hand, it outperforms the retrieve-only\\nand generation-only methods. Notably, retrieval-augmented direct inference with LLMs yields\\nthe best results. Furthermore, detailed investigation through human evaluations and a case study\\noffers strong evidence that RARe can effectively improve the accuracy and relevance of reviews.\\nCompared to direct inference, it provides examples for reference and potential reviews, avoiding\\noverly general and vague reviews. Compared to fine-tuning, retrieval augmentation improves the\\nprecision by incorporating a set of broader and relevant examples, rather than relying solely on\\npatterns in a limited training dataset, thereby mitigating the risk of overfitting.\\nHowever, that doesn’t mean it’s perfect. For example, the dependency on the accuracy of retrieval\\nsignificantly impacts the generative performance of the model. In addition, retrieval significantly\\nincreases the input length of the generative model, posing challenges to the model’s long text\\nprocessing. Furthermore, incorporating retrieval does not have an efficiency advantage compared\\nto the retrieval-only method. On the other hand, it is essential to integrate our approach into the\\nreviewer workflow and conduct thorough testing with software developers and reviewers to assess\\nits practicality and effectiveness in real-world scenarios. Therefore, future work should focus not\\nonly on optimizing accuracy but also on improving efficiency to ensure that this method becomes\\nmore practical and adaptable to real-world applications.\\n10\\nData Availability\\nThe data and code that support the findings of this study are available in anonymous repository:\\nhttps://anonymous.4open.science/r/GAR-9EE2\\nReferences\\n[1] Vipin Balachandran. 2013. Reducing human effort and improving quality in peer code reviews using automatic static\\nanalysis and reviewer recommendation. In 2013 35th International Conference on Software Engineering (ICSE). 931–940.\\n[2] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation\\nwith human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine\\ntranslation and/or summarization. 65–72.\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural\\nInformation Processing Systems (NeurIPS) 33 (2020), 1877–1901.\\n[4] Robert Chatley and Lawrence Jones. 2018. Diggit: Automated code review via software repository mining. In 2018\\nIEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER). 567–571.\\n[5] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2024. Lift yourself up: Retrieval-augmented\\ntext generation with self-memory. Advances in Neural Information Processing Systems (NeurIPS) 36 (2024).\\n[6] Lin Chin-Yew. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of the Workshop on Text\\nSummarization Branches Out, 2004.\\n[7] Ewen Denney and Bernd Fischer. 2009. Generating code review documentation for auto-generated mission-critical\\nsoftware. In 2009 Third IEEE International Conference on Space Mission Challenges for Information Technology. 394–401.\\n[8] Alexander Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. 2020. Template-Based Question\\nGeneration from Retrieved Sentences for Improved Unsupervised Question Answering. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Linguistics (ACL). 4508–4513.\\n[9] Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. 2021. Augmenting transformers with KNN-based\\ncomposite memory for dialog. Transactions of the Association for Computational Linguistics (TACL) 9 (2021), 82–99.\\n[10] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu,\\nDaxin Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In\\nFindings of the Association for Computational Linguistics (EMNLP). 1536–1547.\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\nCode Review Automation using Retrieval Augmented Generation\\n19\\n[11] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.\\nRetrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023).\\n[12] Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao.\\n2024. Large language models are few-shot summarizers: Multi-intent comment generation via in-context learning. In\\nProceedings of the 46th IEEE/ACM International Conference on Software Engineering (ICSE). 1–13.\\n[13] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy,\\nShengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366\\n(2020).\\n[14] Anshul Gupta and Neel Sundaresan. 2018. Intelligent code reviews using deep learning. In Proceedings of the 24th ACM\\nSIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) Deep Learning Day.\\n[15] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language\\nmodel pre-training. In International Conference on Machine Learning (ICML). 3929–3938.\\n[16] Yang Hong, Chakkrit Tantithamthavorn, Patanamon Thongtanunam, and Aldeida Aleti. 2022. Commentfinder: a\\nsimpler, faster, more accurate code review comments recommendation. In Proceedings of the 30th ACM Joint European\\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). 507–519.\\n[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).\\n[18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las\\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint\\narXiv:2310.06825 (2023).\\n[19] Xin Jin, Jonathan Larson, Weiwei Yang, and Zhiqiang Lin. 2023. Binary code summarization: Benchmarking chatgpt/gpt-\\n4 and other large language models. arXiv preprint arXiv:2312.09601 (2023).\\n[20] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau\\nYih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP). 6769–6781.\\n[21] Brendan King and Jeffrey Flanigan. 2023. Diverse Retrieval-Augmented In-Context Learning for Dialogue State\\nTracking. In Findings of the Association for Computational Linguistics (ACL). 5570–5585.\\n[22] Markus Klinik, Pieter Koopman, and Rick van der Wal. 2021. Personal Prof: Automatic Code Review for Java\\nAssignments. In Proceedings of the 10th Computer Science Education Research Conference. 31–38.\\n[23] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler,\\nMike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation\\nfor knowledge-intensive NLP tasks. In Proceedings of the 34th International Conference on Neural Information Processing\\nSystems (NIPS). Article 793, 16 pages.\\n[24] Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan Hua, Geng Liang, and Chun Zuo. 2022. AUGER:\\nautomatically generating review comments with pre-training models. In Proceedings of the 30th ACM Joint European\\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). 1009–1021.\\n[25] Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023.\\nUnified Demonstration Retriever for In-Context Learning. In Proceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (ACL). 4644–4668.\\n[26] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of\\nthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\\nNatural Language Processing (ACL/IJCNLP). 4582–4597.\\n[27] Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svy-\\natkovskiy, Shengyu Fu, et al. 2022. Automating code review activities by large-scale pre-training. In Proceedings of the\\n30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering\\n(ESEC/FSE). 1035–1047.\\n[28] Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, and Chun Zuo. 2023. LLaMA-Reviewer: Advancing Code Review Automation\\nwith Large Language Models through Parameter-Efficient Fine-Tuning. In 2023 IEEE 34th International Symposium on\\nSoftware Reliability Engineering (ISSRE). 647–658.\\n[29] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval-\\nAugmented Code Completion Framework. 6227–6240. https://doi.org/10.18653/v1/2022.acl-long.431\\n[30] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based prompt selection for code-related few-shot learning.\\nIn 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 2450–2462.\\n[31] Nivishree Palvannan and Chris Brown. 2023. Suggestion bot: analyzing the impact of automated suggested changes\\non code reviews. In 2023 IEEE/ACM 5th International Workshop on Bots in Software Engineering (BotSE). 33–37.\\n[32] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of\\nmachine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics (ACL).\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n\\n20\\nQianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser\\n311–318.\\n[33] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented\\nCode Generation and Summarization. In Findings of the Association for Computational Linguistics (EMNLP). 2719–2734.\\n[34] Luca Pasquini, Stefano Cristiani, Ramón García López, Martin Haehnelt, Michel Mayor, Jochen Liske, Antonio Manescau,\\nGerardo Avila, Hans Dekker, Olaf Iwert, et al. 2010. Codex. In Ground-based and Airborne Instrumentation for Astronomy\\nIII, Vol. 7735. 957–968.\\n[35] John W Ratcliff, David E Metzener, et al. 1988. Pattern matching: The gestalt approach. Dr. Dobb’s Journal 13, 7 (1988),\\n46.\\n[36] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint\\nConference on Natural Language Processing (EMNLP-IJCNLP). 3982–3992.\\n[37] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations\\nand Trends® in Information Retrieval 3, 4 (2009), 333–389.\\n[38] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval Augmentation Reduces\\nHallucination in Conversation. In Findings of the Association for Computational Linguistics (EMNLP). 3784–3803.\\n[39] Jing Kai Siow, Cuiyun Gao, Lingling Fan, Sen Chen, and Yang Liu. 2020. Core: Automating review recommendation for\\ncode changes. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER).\\n284–295.\\n[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).\\n[41] Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota.\\n2022. Using pre-trained models to boost code review automation. In Proceedings of the 44th International Conference on\\nSoftware Engineering (ICSE). 2291–2302.\\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 30.\\n[43] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R Lyu. 2022. No more fine-\\ntuning? an experimental evaluation of prompt tuning in code intelligence. In Proceedings of the 30th ACM joint European\\nsoftware engineering conference and symposium on the foundations of software engineering (ESEC/FSE). 382–394.\\n[44] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-\\ndecoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).\\n[45] Linghao Zhang, Hongyi Zhang, Chong Wang, and Peng Liang. 2024. RAG-Enhanced Commit Message Generation.\\narXiv preprint arXiv:2406.05514 (2024).\\n[46] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. 2010. Understanding bag-of-words model: a statistical framework. International\\njournal of machine learning and cybernetics 1 (2010), 43–52.\\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\\n, Vol. 1, No. 1, Article . Publication date: November 2018.\\n',\n",
       " 'DGTN: Graph-Enhanced Transformer with Diffusive Attention\\nGating Mechanism for Enzyme ∆∆G Prediction\\nAbigail Lin*\\nDepartment of Computer & Information Science & Engineering, University of Florida\\nAbstract\\nPredicting the effect of amino acid mutations on enzyme thermodynamic stability (∆∆G) is fun-\\ndamental to protein engineering and drug design. While recent deep learning approaches have shown\\npromise, they often process sequence and structure information independently, failing to capture the in-\\ntricate coupling between local structural geometry and global sequential patterns. We present DGTN\\n(Diffused Graph-Transformer Network), a novel architecture that co-learns graph neural network (GNN)\\nweights for structural priors and transformer attention through a diffusion mechanism. Our key inno-\\nvation is a bidirectional diffusion process where: (1) GNN-derived structural embeddings guide trans-\\nformer attention via learnable diffusion kernels, and (2) transformer representations refine GNN message\\npassing through attention-modulated graph updates. We provide rigorous mathematical analysis showing\\nthis co-learning scheme achieves provably better approximation bounds than independent processing. On\\nProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson ρ = 0.87,\\nRMSE = 1.21 kcal/mol), with 6.2% improvement over best baselines. Ablation studies confirm the\\ndiffusion mechanism contributes 4.8 points to correlation. Our theoretical analysis proves the diffused\\nattention converges to optimal structure-sequence coupling, with convergence rate O(1/\\n√\\nT) where T\\nis diffusion steps. This work establishes a principled framework for integrating heterogeneous protein\\nrepresentations through learnable diffusion.\\n1\\nIntroduction\\nMotivation and Background\\nThe Gibbs free energy change upon mutation (∆∆G = Gmutant−Gwild-type) governs protein thermodynamic\\nstability, a critical determinant of enzyme function, disease pathogenesis, and therapeutic efficacy (Tokuriki\\nand Tawfik [2009]). Accurate ∆∆G prediction enables rational protein design for industrial biocatalysis\\n(Braun et al. [2024, 2025], Listov et al. [2025]), therapeutic antibody engineering (Jain et al. [2017]), and\\nunderstanding disease mechanisms (Yue et al. [2005], Bashour et al. [2024]).\\nTraditional computational approaches employ physics-based energy functions (FoldX [Schymkowitz\\net al., 2005a], Rosetta [Kellogg et al., 2011a]), achieving limited accuracy (Pearson ρ ≈0.5) due to approx-\\nimations in energy calculations and conformational sampling. Recent machine learning methods leverage\\neither sequence information via protein language models ([Meier et al., 2021b]) or structural data through\\n3D convolutional networks ([Zhou et al., 2023]), but typically process these modalities separately.\\nKey Challenges\\nThere are three key challenges when deploying a transformer-based learning algorithm to perform highly\\naccurate ∆∆G predictions. 1) Modal Heterogeneity. Sequence data (1D) and structural data (3D graph)\\n*Corresponding author: abigail.lin@ufl.edu\\n1\\narXiv:2511.05483v1  [cs.LG]  7 Nov 2025\\n\\nhave fundamentally different mathematical representations. Naive concatenation or late fusion fails to cap-\\nture cross-modal dependencies. 2) Local-Global Coupling. Mutation effects involve both local geometric\\nperturbations (captured by GNNs) and long-range sequential patterns (captured by Transformers). Exist-\\ning methods lack mechanisms for mutual refinement between these representations. 3) Attention Myopia.\\nStandard transformer attention is agnostic to 3D spatial relationships. Residues spatially proximal but se-\\nquentially distant receive inadequate attention, missing critical structural contacts.\\nOur Contributions\\nWe propose DGTN, which addresses these challenges through:\\n1. Diffused Graph-Transformer Architecture (§2.1): A novel co-learning framework where GNN\\nweights and Transformer attention are jointly optimized through bidirectional diffusion processes.\\n2. Learnable Diffusion Kernels (§2.2): We introduce structure-guided attention diffusion that prop-\\nagates spatial information into sequence attention weights, and attention-modulated graph diffusion\\nthat refines message passing using sequence context.\\n3. Rigorous Theoretical Analysis (§C): We prove:\\n• The diffused attention converges to the optimal structure-aware attention matrix (Theorem 1)\\n• The joint optimization has lower approximation error than independent models (Theorem 2)\\n• The diffusion rate achieves O(1/\\n√\\nT) convergence (Proposition 1)\\n4. State-of-the-Art Empirical Results (§3): DGTN achieves ρ = 0.87 on ProTherm, outperforming\\nESM-1v (ρ = 0.78), DeepDelta Delta G (ρ = 0.73), and MutFormer (ρ = 0.76).\\n2\\nMethodology\\nFormally, we formulate our problem as: given a protein sequence s = (s1, . . . , sL), where si ∈A (20 amino\\nacids), a 3D structure graph G = (V, E), where V = {v1, . . . , vL} represents residues and E contains edges\\n(i, j) if ∥ri −rj∥< rc (distance cutoff), and a mutation specification m = (p, swt\\np , where smut\\np ) indicating\\nposition p, wild-type residue, and mutant residue, predict ∆∆G value in kcal/mol. Essentially, our key\\nobjective is to learn a mapping fθ : (s, G, m) →∆∆G by minimizing L(θ) = E(s,G,m,∆∆G∗)[(∆∆G −\\n∆∆G∗)2] + λR(θ), where R(θ) is the regularization.\\n2.1\\nArchitecture Overview\\nStructure\\nGraph (G)\\nGeometric GNN\\nEncoder (ϕG)\\nSequence\\n(s)\\nSequence Transformer\\nEncoder (ϕT )\\nBidirectional Diffusion\\nModule (Ψ)\\nPrediction Head\\n(ϕP )\\n∆∆G\\nFigure 1: Architecture of our multi-modal framework with diffusively gated attention.\\n2\\n\\nDGTN consists of four key components (Figure 1): 1) Geometric GNN Encoder ϕG: Processes struc-\\nture graph G producing residue embeddings HG ∈RL×d.\\n2) Sequence Transformer Encoder ϕT :\\nProcesses sequence s producing embeddings HT ∈RL×d. 3) Bidirectional Diffusion Module Ψ: Co-\\nlearns GNN weights and Transformer attention through diffusion. 4) Prediction Head ϕP : Generates\\n∆∆G from fused representations. The forward pass is governed by i) HG, HT = Ψ(ϕG(G), ϕT (s)); ii)\\nhm = Aggregate(HG, HT , m); iii) ∆∆G = ϕP (hm).\\n2.2\\nBidirectional Diffusion Mechanism\\nThis is our key innovation. We introduce learnable diffusion processes that couple GNN and Transformer.\\nStructure-Guided Attention Diffusion. Our key innovation lies in introducing a learnable diffusion\\nprocess that couples the Graph Neural Network (GNN) and Transformer components into a unified ar-\\nchitecture. Standard self-attention (Eq. 33) captures dependencies along the sequence but disregards the\\nunderlying 3D geometry. To integrate spatial structure, we construct a graph-based affinity matrix S that\\nencodes geometric proximity between residues:\\nSij =\\n(\\nexp(−d2\\nij/σ2),\\nif (i, j) ∈E,\\n0,\\notherwise.\\n(1)\\nThe affinities are then symmetrically normalized as\\n˜S = D−1/2SD−1/2,\\n(2)\\nwhere D denotes the degree matrix. This normalization ensures numerically stable propagation across the\\ngraph.\\nWe inject geometric priors into the Transformer by diffusing the normalized structural affinity ˜S into the\\nattention maps. Starting from the vanilla attention A(ℓ), the diffusion proceeds iteratively as\\nA(t+1)\\ndiff\\n= (1 −β)A(t)\\ndiff + β ˜S A(t)\\ndiff,\\n(3)\\nwhere β ∈(0, 1) is a learnable diffusion rate controlling the degree of geometric influence. After T itera-\\ntions, the process yields the structure-aware attention matrix\\nAstruct = A(T)\\ndiff .\\n(4)\\nThis mechanism enables the Transformer to internalize 3D structural context while maintaining the expres-\\nsive flexibility of sequence-based attention.\\nLearnable Diffusion Kernel. To enable adaptive control over the diffusion strength, we introduce a\\nlearnable diffusion kernel that dynamically adjusts the diffusion rate across layers. Specifically, the diffusion\\ncoefficient is parameterized as\\nβℓ= σ(w⊤\\nβ LayerFeatures(ℓ)),\\n(5)\\nwhere LayerFeatures(ℓ) encodes properties such as layer depth, current loss, and attention entropy, and σ(·)\\nensures βℓ∈(0, 1). This learnable formulation allows the model to modulate how strongly structural infor-\\nmation propagates within each transformer layer, depending on the current training state and representational\\nneeds.\\nAttention-Modulated Graph Diffusion Conversely, we introduce an attention-modulated graph dif-\\nfusion mechanism, where the sequence-level attention informs and reshapes the GNN message-passing\\nprocess. A pseudo-graph is constructed from the averaged attention maps across heads:\\nGattn = 1\\nH\\nH\\nX\\nh=1\\nAh,\\n(6)\\n3\\n\\nwhere H is the number of attention heads. This attention-derived graph captures dynamic, context-dependent\\ninteractions that may not be explicit in the original structure. To refine the graph, we apply thresholding and\\nnormalization:\\n˜Gattn,ij =\\n(\\nGattn,ij,\\nif Gattn,ij > τ,\\n0,\\notherwise,\\n(7)\\nensuring that only meaningful attention-based connections influence subsequent graph diffusion.\\nWe enhance the structural encoder by introducing learnable graph diffusion, which refines the initial\\nadjacency matrix through iterative propagation of structural information. Starting from the original normal-\\nized adjacency matrix ˜S, we define a diffusion process over T steps:\\n˜S(0)\\ndiff = ˜S,\\n˜S(t+1)\\ndiff\\n= (1 −γ)˜S(t)\\ndiff + γ ˜Gattn˜S(t)\\ndiff,\\n(8)\\nwhere γ ∈(0, 1) is a learnable mixing coefficient and ˜Gattn is a geometric attention matrix encoding\\ndistance- and orientation-aware relationships between residues. This process effectively smooths and en-\\nriches the graph connectivity by allowing information to diffuse beyond immediate neighbors in a data-\\ndriven manner.\\nThe resulting diffused adjacency matrix ˜S(T)\\ndiff is then used to define updated neighborhoods Ndiff(i) for\\neach residue i. In the GNN message-passing layers, node representations are computed using these refined\\nneighborhoods:\\nh(ℓ+1)\\ni\\n= GNN-Layer\\n\\x00h(ℓ)\\ni , {h(ℓ)\\nj }j∈Ndiff(i)\\n\\x01\\n.\\n(9)\\nThis enables the GNN to aggregate information from structurally relevant but potentially non-adjacent\\nresidues, thereby capturing longer-range geometric dependencies critical for accurate stability prediction.\\nAlgorithm 1 Co-Learning via Bidirectional Diffusion\\n1: Input: Structure G, sequence s, mutation m\\n2: Initialize: GNN parameters θG, Transformer parameters θT\\n3: for layer ℓ= 1 to L do\\n4:\\n// GNN Forward\\n5:\\nHG\\nℓ= GNN-Layerℓ(G, θG)\\n6:\\n// Transformer Forward\\n7:\\nAℓ= Attentionℓ(s, θT )\\n8:\\n// Structure-guided attention diffusion\\n9:\\nAstruct,ℓ= Diffuse-Attention(Aℓ, S, βℓ, T)\\n(Eq. 3)\\n10:\\n// Attention-modulated graph diffusion\\n11:\\nSdiff,ℓ= Diffuse-Graph(S, Astruct,ℓ, γℓ, T)\\n12:\\n// Update with diffused structures\\n13:\\nHG\\nℓ+1 = GNN-Layerℓ+1(Sdiff,ℓ, HG\\nℓ)\\n14:\\nHT\\nℓ+1 = Transformer-Layerℓ+1(Astruct,ℓ, HT\\nℓ)\\n15: end for\\n16: ∆∆G = PredictionHead(HG\\nL, HT\\nL, m)\\n17: return ∆∆G\\nJoint Training Procedure. Our co-learning algorithm jointly refines structural and sequential representa-\\ntions through bidirectional diffusion. At each layer ℓ, the GNN computes geometric-aware structural embed-\\ndings HG\\nℓ, while the Transformer generates sequence embeddings with mutation-aware attention Aℓ. These\\nmodalities interact in two directions: (1) structural priors diffuse into the attention mechanism to produce\\ngeometry-guided attention weights Astruct,ℓ, and (2) these attention weights modulate the graph adjacency to\\n4\\n\\nyield a refined, attention-informed structure Sdiff,ℓ. Both the GNN and Transformer are then updated using\\nthese diffused representations. This layer-wise mutual refinement enables the model to iteratively align 3D\\nspatial context with evolutionary sequence patterns, culminating in a fused representation from which the\\nfinal ∆∆G prediction is made.\\nTo generate a precise and context-aware prediction, we perform mutation-specific aggregation of the\\nlearned structural and sequential representations. Let HG and HT denote the final node-level embeddings\\nfrom the GNN and Transformer, respectively. Around the mutation position p, we compute three comple-\\nmentary representations: (1) a local context vector hlocal, obtained by averaging the concatenated GNN and\\nTransformer embeddings over a window W(p) of size w centered at p; (2) a global context vector hglobal,\\nformed by concatenating the max-pooled structural representation and mean-pooled sequential represen-\\ntation across the entire protein; and (3) a mutation-specific encoding hmut, which embeds the wild-type\\nresidue, mutant residue, and normalized position p/L to capture the identity and location of the substitution.\\nFormally,\\nhlocal =\\n1\\n|W(p)|\\nX\\ni∈W(p)\\n[HG\\ni ; HT\\ni ],\\n(10)\\nhglobal = [MaxPool(HG); MeanPool(HT )],\\n(11)\\nhmut = [e(swt\\np ); e(smut\\np ); epos(p/L)].\\n(12)\\nThese three vectors are concatenated into a unified feature vector and passed through a dedicated pre-\\ndiction head implemented as a three-layer MLP. The first layer applies a GELU activation, the second adds\\ndropout for regularization followed by another GELU, and the final linear layer outputs the scalar ∆∆G\\nprediction:\\n∆∆G = MLP\\n\\x00[hlocal; hglobal; hmut]\\n\\x01\\n,\\n(13)\\nwhere the MLP architecture is defined as\\nz(1) = GELU(W(1)h + b(1)),\\n(14)\\nz(2) = Dropout\\n\\x00GELU(W(2)z(1) + b(2))\\n\\x01\\n,\\n(15)\\n∆∆G = w(3)⊤z(2) + b(3).\\n(16)\\nThis design enables accurate estimation of stability changes by jointly leveraging local perturbation effects,\\nglobal protein context, and explicit mutation identity.\\n3\\nExperiments\\nWe evaluate our framework on four widely used benchmark datasets for protein stability prediction. ProTherm (Ku-\\nmar et al. [2006]) contains 5,166 experimentally measured single-point mutations across 1,228 proteins; we\\nuse the standard split of 70% training (3,616), 15% validation (775), and 15% test (775) samples. To as-\\nsess generalization to protein–protein interfaces, we use SKEMPI 2.0 (Jankauskait˙e et al. [2019]), which\\nprovides 7,085 mutations in 319 protein complexes. For out-of-distribution evaluation on engineered pro-\\nteins, we include Ssym (Kellogg et al. [2011b]), a dataset of 628 symmetry-derived mutations. Finally,\\nFireProtDB (Stourac et al. [2021]) supplies 8,196 thermostability-focused mutations, enabling assessment\\non industrially relevant design tasks.\\nAll structures are processed uniformly: PDB files are parsed using Biopython, Cα coordinates are ex-\\ntracted, and residue–residue edges are constructed for pairs within a 10 A cutoff. Secondary structure assign-\\nments are computed using DSSP (Kabsch and Sander [1983]), and solvent-accessible surface areas (SASA)\\n5\\n\\nare calculated with NACCESS. This standardized pipeline ensures consistent structural feature extraction\\nacross all datasets.\\nWe compare our method against a comprehensive set of baselines spanning both physics-based and ma-\\nchine learning approaches. On the physics-based side, we include FoldX 5.0 (Schymkowitz et al. [2005b]),\\na widely used empirical force field method, and Rosetta ∆∆G monomer (Kellogg et al. [2011b]), which\\nemploys all-atom energy minimization with conformational sampling. Among machine learning methods,\\nwe evaluate DDGun3D (Montanucci et al. [2022]), which uses gradient boosting on handcrafted structural\\nfeatures; DeepDDG (Cao et al. [2019]), combining 3D convolutional networks with sequence informa-\\ntion; ThermoNet (Li et al. [2020a]), which applies graph convolutional networks (GCNs) to residue contact\\ngraphs; MutFormer (Zhou et al. [2020]), a graph-based Transformer architecture; and ESM-1v (Meier et al.\\n[2021a]), a state-of-the-art protein language model fine-tuned for stability prediction. This diverse set of\\nbaselines allows us to assess the relative contribution of structural modeling, sequence context, and archi-\\ntectural design in ∆∆G prediction.\\nTable 1: Model architecture and training configuration.\\nComponent\\nConfiguration\\nModel Architecture\\nGNN\\n4 layers, hidden dim 256, 8 attention heads, dropout 0.1\\nTransformer\\n6 layers, hidden dim 256, 8 heads, FFN dim 1024, dropout 0.1\\nDiffusion\\nT = 5 steps, learnable β, γ ∈[0.1, 0.5]\\nPrediction Head\\nMLP: 768 →384 →192 →1\\nTraining\\nOptimizer\\nAdamW, lr = 10−4, weight decay 10−2\\nBatch Size\\n32\\nEpochs\\n100 (early stopping, patience = 15)\\nLoss\\nMSE with gradient clipping (max norm = 1.0)\\nHardware\\nNVIDIA A100 40GB GPU\\nTraining Time\\n12 hours\\nWe assess model performance using four standard metrics: (1) Pearson correlation coefficient (ρ), which\\nmeasures the strength of the linear relationship between predicted and experimental ∆∆G values; (2) Spear-\\nman rank correlation (ρs), which evaluates the monotonic agreement in mutation rankings; (3) Root Mean\\nSquared Error (RMSE, in kcal/mol), which emphasizes larger errors; and (4) Mean Absolute Error (MAE, in\\nkcal/mol), which provides a robust measure of average prediction accuracy. Together, these metrics capture\\nboth correlation and calibration quality across the full range of stability effects.\\n4\\nResults\\nOur model, DGTN, achieves state-of-the-art performance on the ProTherm test set with a Pearson correla-\\ntion of 0.87, substantially outperforming the best prior method, ESM-1v (ρ = 0.78), and all physics-based\\nand machine learning baselines. This represents a 9% relative improvement in correlation and a 20% reduc-\\ntion in RMSE (from 1.52 to 1.21 kcal/mol), indicating not only better ranking of mutations but also more\\naccurate absolute ∆∆G predictions (critical for practical protein engineering). The ablation variant DGTN\\n(no diffusion) already surpasses all existing methods (ρ = 0.81), highlighting the strength of our multi-modal\\nGNN–Transformer architecture; however, the full model with diffusively gated attention provides an addi-\\ntional +0.06 gain in Pearson correlation, confirming that bidirectional structural–sequential diffusion is a key\\n6\\n\\nTable 2: Performance on ProTherm test set. Best results in bold, second best underlined.\\nMethod\\nPearson ρ\\nSpearman ρs\\nRMSE\\nMAE\\nPhysics-based\\nFoldX\\n0.46\\n0.43\\n2.83\\n1.92\\nRosetta\\n0.53\\n0.51\\n2.61\\n1.78\\nMachine Learning\\nDDGun3D\\n0.68\\n0.65\\n1.87\\n1.34\\nDeepDDG\\n0.73\\n0.71\\n1.65\\n1.21\\nThermoNet\\n0.71\\n0.69\\n1.72\\n1.27\\nMutFormer\\n0.76\\n0.74\\n1.58\\n1.15\\nESM-1v\\n0.78\\n0.76\\n1.52\\n1.12\\nOurs\\nDGTN (no diffusion)\\n0.81\\n0.79\\n1.38\\n1.02\\nDGTN (full)\\n0.87\\n0.85\\n1.21\\n0.94\\nImprovement over best baseline\\n+9%\\n+9%\\n-20%\\n-16%\\ndriver of performance. Together, these results demonstrate that explicitly modeling the interplay between\\n3D geometry and evolutionary sequence context (via a co-learned, diffused attention mechanism) yields\\nboth statistically and practically significant improvements in protein stability prediction.\\n4.1\\nCross-Dataset Generalization\\nFig. 2 shows cross-dataset generalization performance of four methods (DeepDDG, MutFormer, ESM-1v,\\nand DGTN) trained on ProTherm and evaluated on three unseen datasets: SKEMPI 2.0 (protein complexes),\\nSsym (symmetric proteins), and FireProtDB (thermostability-focused mutations). Performance is measured\\nby Pearson correlation (ρ) between predicted and experimental ∆∆G values. DGTN consistently outper-\\nforms all baselines across all three datasets, achieving ρ = 0.78 (SKEMPI), 0.73 (Ssym), and 0.80 (Fire-\\nProtDB). The consistent margin ( 0.05–0.07 points) over the strongest baseline (ESM-1v) demonstrates that\\nDGTN’s bidirectional diffusion mechanism enables more robust and transferable learning of fundamental\\nstability principles, rather than overfitting to dataset-specific biases. This superior generalization highlights\\nthe benefit of explicitly integrating 3D structural priors with sequence context in a co-learned framework.\\nAblation studies reveal that the bidirectional diffusion mechanism is central to DGTN’s performance,\\ncontributing a 6-point gain in Pearson correlation over simple cross-modal fusion. This improvement stems\\nfrom the complementary roles of its two components: attention diffusion (structure →sequence) alone\\nyields a 5-point boost, while graph diffusion (sequence →structure) adds 3 points, indicating that structural\\nguidance has a stronger immediate impact on sequence modeling than vice versa. Crucially, combining\\nboth directions produces a synergistic effect, surpassing the sum of individual contributions and achieving\\nthe highest accuracy. Remarkably, this substantial performance gain comes at minimal cost (only a 4%\\nincrease in model parameters) demonstrating the efficiency and effectiveness of our co-learning design.\\n4.2\\nDiffusion Step Analysis\\nOptimal at T = 5. Beyond this, performance plateaus (consistent with Theorem 1) while computational\\ncost increases.\\n7\\n\\nDeepDDG\\nMutFormer\\nESM-1v\\nDGTN\\nMethod\\n0.50\\n0.55\\n0.60\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\nPearson Correlation ( )\\n0.62\\n0.67\\n0.71\\n0.78\\n0.58\\n0.63\\n0.66\\n0.73\\n0.66\\n0.70\\n0.74\\n0.80\\nCross-Dataset Generalization (Trained on ProTherm)\\nSKEMPI 2.0\\nSsym\\nFireProtDB\\nFigure 2: Cross-dataset generalization performance. Models trained on ProTherm and evaluated on un-\\nseen datasets. DGTN consistently outperforms baselines, demonstrating superior generalization through\\nco-learned structural-sequential representations.\\nSequence only\\n(Transformer)\\nStructure only\\n(GNN)\\nGNN + Trans\\n(concat, no diffusion)\\nGNN + Trans\\n(learned fusion)\\n+ Attention\\ndiffusion only\\n+ Graph\\ndiffusion only\\n+ Bidirectional\\ndiffusion (full)\\nModel Configuration\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\nPearson Correlation ( )\\n0.74\\n0.70\\n0.79\\n0.81\\n0.84\\n0.82\\n0.87\\nAblation Study on ProTherm Test Set\\nPearson \\nRMSE\\nFull Model\\n1.2\\n1.3\\n1.4\\n1.5\\n1.6\\n1.7\\nRMSE (kcal/mol)\\n1.59\\n1.71\\n1.42\\n1.38\\n1.31\\n1.35\\n1.21\\nFigure 3: Ablation study showing the contribution of each component. Bidirectional diffusion yields the\\nlargest improvement in both correlation and error reduction.\\n8\\n\\nTable 3: Effect of diffusion steps T on performance.\\nDiffusion Steps T\\nPearson ρ\\nRMSE\\nTime (ms)\\nT = 1\\n0.83\\n1.35\\n12\\nT = 3\\n0.85\\n1.27\\n18\\nT = 5\\n0.87\\n1.21\\n26\\nT = 7\\n0.87\\n1.22\\n35\\nT = 10\\n0.86\\n1.23\\n48\\n4.3\\nLearned Diffusion Rates\\nObservations:\\n• Attention diffusion rate β increases from 0.15 (layer 1) to 0.42 (layer 6)\\n• Deeper layers rely more on structural guidance\\n• Graph diffusion rate γ stable around 0.25\\n• Supports hypothesis: Early layers learn local features, late layers integrate global structure-sequence\\ncoupling\\n4.4\\nAttention Visualization\\n0\\n10\\n20\\n30\\n40\\nResidue Index\\n0\\n10\\n20\\n30\\n40\\nResidue Index\\nEarly Layer\\n(Local Focus)\\n0\\n10\\n20\\n30\\n40\\nResidue Index\\n0\\n10\\n20\\n30\\n40\\nMiddle Layer\\n(Emerging Long-Range)\\n0\\n10\\n20\\n30\\n40\\nResidue Index\\n0\\n10\\n20\\n30\\n40\\nLate Layer\\n(Diffused Attention)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nAttention Weight\\nFigure 4: Attention weight matrices at early, middle, and late layers of the Transformer in DGTN. Early\\nlayers focus on local sequence context (diagonal dominance), while later layers (guided by bidirectional\\ndiffusion) develop strong attention between sequentially distant but spatially proximate residues (e.g., posi-\\ntions 5 and 45), demonstrating successful integration of 3D structural priors into the attention mechanism.\\nDiffused attention successfully identifies spatially proximal residues (12-18 ˚A in sequence, ¡8 ˚A in 3D)\\nthat vanilla attention misses.\\n4.5\\nCase Study: Antibody Stabilization\\nObjective: Stabilize therapeutic IgG1 antibody (PDB: 1HZH) while preserving binding.\\n9\\n\\nTable 4: Computational cost comparison (single mutation prediction).\\nMethod\\nTime (GPU)\\nTime (CPU)\\nFoldX\\n-\\n180 s\\nRosetta\\n-\\n300 s\\nDeepDDG\\n2.1 s\\n12 s\\nESM-1v\\n1.2 s\\n8 s\\nDGTN (ours)\\n1.8 s\\n15 s\\nApproach: Screen 2,000 single mutations in framework regions using DGTN.\\nTop predictions:\\n• L15V: ∆DeltaGpred = −1.8 kcal/mol\\n• T43A: ∆DeltaGpred = −1.5 kcal/mol\\n• S88L: ∆DeltaGpred = −1.3 kcal/mol\\nExperimental validation (differential scanning fluorimetry):\\n• L15V: ∆DeltaGexp = −1.6 kcal/mol, Tm +2.1°C\\n• T43A: ∆DeltaGexp = −1.4 kcal/mol, Tm +1.8°C\\n• S88L: ∆DeltaGexp = −1.1 kcal/mol, Tm +1.5°C\\n• Combined variant: Tm +4.9°C, binding affinity maintained (KD within 1.5-fold)\\nInterpretation: DGTN correctly identified hydrophobic core packing improvements detected by dif-\\nfused attention connecting framework residues to CDR-proximal positions.\\n4.6\\nComputational Efficiency\\nDGTN is 100× faster than physics-based methods while achieving higher accuracy. Comparable to other\\ndeep learning methods despite additional diffusion computation.\\n4.7\\nError Analysis\\nLarge errors (|∆| > 2 kcal/mol) occur in:\\n• Oligomeric interfaces (18% of errors): Model trained on monomers\\n• Cofactor binding sites (12%): Cofactors not explicitly modeled\\n• Large |∆DeltaG| values (>5 kcal/mol) (22%): Training data imbalance\\n• Flexible loops (15%): Static structure limitation\\nFuture improvements: Include oligomeric structures, model cofactors, use ensemble structures for\\nflexibility.\\n10\\n\\n5\\nDiscussion\\nKey Insights\\nOur analysis reveals four core findings. First, bidirectional diffusion is essential: unidirectional infor-\\nmation flow (e.g., structure to sequence alone) yields limited gains, whereas mutual refinement between\\nmodalities enables iterative co-adaptation that significantly boosts performance. Second, the model learns\\ndepth-dependent diffusion rates, placing greater emphasis on structural guidance in deeper layers, consis-\\ntent with a hierarchical integration strategy where early layers capture local features and later layers fuse\\nglobal structure–sequence relationships. Finally, attention visualization confirms the mechanism’s efficacy:\\nthe diffused attention maps consistently highlight spatially proximal residues that are sequentially distant,\\ndirectly demonstrating the model’s ability to bridge 3D geometry and sequence context as intended.\\nComparison with Related Approaches\\nOur method, DGTN, offers distinct advantages over existing approaches by design. Compared to ESM-1v\\n(a large protein language model trained on 250 million sequences with 650M parameters) DGTN achieves\\nsuperior prediction performance despite using 40× fewer parameters, primarily by explicitly integrating 3D\\nstructural information rather than relying solely on evolutionary sequence patterns. In contrast to Mut-\\nFormer, which applies Transformers to protein graphs but treats structural context uniformly across layers,\\nDGTN employs learnable, depth-adaptive diffusion rates that enable dynamic, context-sensitive coupling\\nbetween sequence and structure, strengthening their interaction in deeper layers where global dependencies\\nmatter most. Finally, while DeepDDG relies on 3D CNNs that assume translational invariance and operate\\non fixed voxel grids, DGTN leverages graph neural networks, which are inherently permutation-equivariant\\nand better suited to the irregular, graph-structured nature of protein residues and their spatial interactions.\\nThis architectural choice not only respects the native geometry of proteins but also enables more faithful\\nmodeling of long-range and orientation-dependent interactions critical for stability prediction.\\nStudy Limitations and Future Plan\\nDGTN operates on a single static conformation of the wild-type protein, which cannot capture dynamic\\nor entropic contributions to stability. Future work will incorporate ensembles of structures (either from\\nmolecular dynamics simulations or predicted conformational distributions) to better model flexibility and\\nallostery. Moreover, the current DGTN framework evaluates mutations in isolation and does not explicitly\\nmodel epistatic (non-additive) interactions between multiple substitutions. Extending the architecture to\\njointly represent combinatorial mutations (e.g., via graph-based mutation encoding or interaction-aware\\nattention) remains an important direction. Finally, the ProTherm dataset exhibits significant class imbalance,\\nwith certain protein families (e.g., lysozyme, constituting ˜8% of entries) overrepresented. This may limit\\ngeneralization to underrepresented folds. Strategies such as domain-adversarial training, balanced sampling,\\nor cross-family transfer learning could improve robustness and fairness.\\n6\\nRelated Work\\nRecent advances in protein stability prediction fall into three broad categories, each with notable limitations\\nthat our work addresses. Sequence-based methods, particularly protein language models like ESM-1v and\\nothers (Meier et al. [2021a], Rives et al. [2021]), leverage evolutionary information from millions of se-\\nquences and achieve strong performance (ρ ≈0.75 –0.78 ), but they operate solely on amino acid sequences\\nand lack explicit awareness of 3D structural constraints. In contrast, structure-based approaches incorporate\\n11\\n\\nspatial information: DeepDDG (Cao et al. [2019]) uses 3D convolutions (ρ=0.73 ), while ThermoNet (Li\\net al. [2020a]) applies graph convolutional networks (GCNs) to residue interaction graphs (ρ=0.71 ). Al-\\nthough effective, these methods often treat structure in isolation and do not fully exploit evolutionary se-\\nquence signals. Hybrid models (Zhou et al. [2020]) attempt to combine both modalities but typically rely\\non simple concatenation or late fusion of independently encoded features, missing opportunities for deep\\ncross-modal interaction during representation learning.\\nThis gap is especially pronounced in the architectural design of modern components. While Graph Neu-\\nral Networks (Kipf and Welling [2017], Veliˇckovi´c et al. [2018]) naturally model proteins as graphs (where\\nnodes are residues and edges encode spatial proximity) and recent geometric GNNs (Jing et al. [2021])\\nfurther incorporate distance and angular features, they remain disconnected from sequence-level context.\\nSimilarly, Transformers (Vaswani et al. [2017]) excel at capturing long-range dependencies in sequences,\\nand protein-specific variants (Rives et al. [2021], Rao et al. [2021]) learn rich evolutionary representations,\\nyet their attention mechanisms cannot natively integrate 3D geometric information. Although techniques\\nlike graph diffusion (Klicpera et al. [2019]) and attention smoothing (Li et al. [2020b]) have improved mes-\\nsage passing in deep GNNs, none enable bidirectional, co-adaptive information flow between structural and\\nsequential modalities. Our work is the first to introduce a bidirectionally diffused attention mechanism that\\ncouples GNN and Transformer representations during training, supported by formal convergence guarantees\\nand designed specifically for interpretable, multi-modal protein property prediction.\\n7\\nConclusion\\nWe present DGTN, a multi-modal framework that unifies geometric graph neural networks and mutation-\\naware Transformers through bidirectional diffusion, enabling mutual refinement of structural and sequential\\nrepresentations. By allowing GNN-derived structural priors to guide attention and Transformer-derived\\nsignals to modulate graph connectivity, DGTN effectively captures the coupling between 3D geometry and\\nevolutionary sequence context. This yields state-of-the-art performance on ProTherm (Pearson ρ = 0.87,\\nRMSE = 1.21 kcal/mol),surpassing ESM-1v with 40x fewer parameters, and ablation studies confirm the\\ndiffusion mechanism alone contributes +4.8 points to correlation. Attention visualizations further validate\\nthat DGTN correctly identifies spatially proximate yet sequentially distant residue interactions.\\nBeyond accuracy, these capabilities of DGTN enable practical protein design, as demonstrated in case\\nstudies on antibody thermostabilization, industrial enzyme engineering, and rescue of disease-causing mu-\\ntations (all experimentally validated). Although currently limited to static structures and single-point mu-\\ntations, DGTN establishes a principled, efficient, and interpretable paradigm for rational protein design,\\nproving that integration, interaction, and interpretability are as essential as predictive performance in bridg-\\ning AI with real-world biological discovery.\\nAcknowledgments\\nA\\nGeometric Graph Neural Network\\nA.1\\nNode Features\\nEach residue i is initialized as:\\nh(0)\\ni\\n= [eaa(si); epos(ri); fi]\\n(17)\\nwhere:\\n• eaa(si) ∈Rda: learnable amino acid embedding\\n12\\n\\n• epos(ri) ∈Rdp: coordinate encoding via MLP\\n• fi ∈Rdf : additional features (secondary structure, solvent accessibility, B-factor)\\nA.2\\nEdge Features\\nFor edge (i, j) ∈E:\\neij = ϕedge(dij, vij, θijk)\\n(18)\\nwhere:\\n• dij = ∥ri −rj∥: Euclidean distance\\n• vij = (rj −ri)/dij: unit direction vector\\n• θijk: bond angles (for backbone connectivity)\\nEdge encoder:\\nϕedge(d, v, θ) = MLP([RBF(d); v; RBF(θ)])\\n(19)\\nwhere RBF is radial basis function expansion:\\nRBF(x) = [exp(−γ(x −µk)2)]K\\nk=1\\n(20)\\nA.3\\nGeometric Message Passing\\nAt layer ℓ, messages are computed as:\\nm(ℓ)\\nij = ϕ(ℓ)\\nmsg(h(ℓ)\\ni , h(ℓ)\\nj , eij)\\n(21)\\nWe use geometric attention ([Jing et al., 2021]):\\nα(ℓ)\\nij =\\nexp(a(ℓ)\\nij )\\nP\\nk∈N(i) exp(a(ℓ)\\nik )\\n(22)\\na(ℓ)\\nij =\\n1\\n√\\nd\\nq(ℓ)⊤\\ni\\nk(ℓ)\\nij\\n(23)\\nk(ℓ)\\nij = W(ℓ)\\nk [h(ℓ)\\nj ; eij]\\n(24)\\nAggregation:\\nh(ℓ+1)\\ni\\n= σ\\n\\uf8eb\\n\\uf8edW(ℓ)\\ns h(ℓ)\\ni\\n+\\nX\\nj∈N(i)\\nα(ℓ)\\nij W(ℓ)\\nm [h(ℓ)\\nj ; eij]\\n\\uf8f6\\n\\uf8f8\\n(25)\\nAfter LG layers:\\nHG = [h(LG)\\n1\\n, . . . , h(LG)\\nL\\n]⊤\\n(26)\\nB\\nSequence Transformer\\nB.1\\nToken Embedding\\nX(0) = Eaa(s) + Epos([1, . . . , L])\\n(27)\\nwhere Epos uses sinusoidal encoding:\\nPE(p, 2i) = sin(p/100002i/d)\\n(28)\\nPE(p, 2i + 1) = cos(p/100002i/d)\\n(29)\\n13\\n\\nB.2\\nMulti-Head Self-Attention\\nStandard transformer attention at layer ℓ:\\nQ(ℓ) = X(ℓ)W(ℓ)\\nQ\\n(30)\\nK(ℓ) = X(ℓ)W(ℓ)\\nK\\n(31)\\nV(ℓ) = X(ℓ)W(ℓ)\\nV\\n(32)\\nA(ℓ) = softmax\\n \\nQ(ℓ)K(ℓ)⊤\\n√dk\\n!\\n(33)\\nX(ℓ+1) = FFN(A(ℓ)V(ℓ) + X(ℓ))\\n(34)\\nAfter LT layers:\\nHT = X(LT )\\n(35)\\nC\\nTheoretical Analysis\\nWe now provide rigorous mathematical analysis of the diffusion mechanism.\\nC.1\\nConvergence of Diffused Attention\\nDefinition 1 (Optimal Structure-Aware Attention) The optimal attention matrix A∗that best incorpo-\\nrates structural information is defined as:\\nA∗= argmin\\nA\\n∥A −Asemantic∥2\\nF + λ∥A −S∥2\\nF\\n(36)\\nsubject to row-stochasticity constraints, where Asemantic is vanilla attention and S is structural adjacency.\\nTheorem 1 (Convergence of Attention Diffusion) Let A(t)\\ndiff be the diffused attention at step t (Eq. 3) with\\ndiffusion rate β ∈(0, 1). Then:\\nlim\\nt→∞A(t)\\ndiff = A∗\\n(37)\\nwhere A∗= (1 −β)A(0) + β˜SA∗is the unique fixed point.\\nProof 1 The diffusion process (Eq. 3) can be written as:\\nA(t)\\ndiff = (1 −β)\\nt−1\\nX\\nk=0\\n(β˜S)kA(0) + (β˜S)tA(0)\\n(38)\\nSince ˜S is normalized and β < 1, the spectral radius ρ(β˜S) < 1. By Neumann series:\\nlim\\nt→∞A(t)\\ndiff = (1 −β)\\n∞\\nX\\nk=0\\n(β˜S)kA(0)\\n(39)\\n= (1 −β)(I −β˜S)−1A(0)\\n(40)\\n14\\n\\nLet A∗= (1 −β)(I −β˜S)−1A(0). Then:\\nA∗= (1 −β)\\n∞\\nX\\nk=0\\n(β˜S)kA(0)\\n(41)\\n= (1 −β)A(0) + (1 −β)β\\n∞\\nX\\nk=0\\n˜S(β˜S)kA(0)\\n(42)\\n= (1 −β)A(0) + β˜SA∗\\n(43)\\nThis shows A∗is a fixed point. Uniqueness follows from contraction mapping theorem since ∥β˜S∥< 1.\\nProposition 1 (Convergence Rate) The convergence rate of attention diffusion is:\\n∥A(t)\\ndiff −A∗∥F ≤C · ρ(β˜S)t ≤C · e−t/τ\\n(44)\\nwhere τ = −1/ log(βλmax(˜S)) is the time constant and C depends on initialization.\\nProof 2 From the error recurrence:\\nA(t)\\ndiff −A∗= (β˜S)t(A(0) −A∗)\\n(45)\\nTaking Frobenius norm:\\n∥A(t)\\ndiff −A∗∥F = ∥(β˜S)t(A(0) −A∗)∥F\\n(46)\\n≤∥(β˜S)t∥2∥A(0) −A∗∥F\\n(47)\\n≤[βλmax(˜S)]t · C\\n(48)\\nSince β < 1 and λmax(˜S) ≤1 (normalized), we have exponential convergence.\\nC.2\\nApproximation Error Bounds\\nDefinition 2 (Function Space) Define Fjoint as the space of functions learned by joint GNN-Transformer\\nwith diffusion, and Fsep as functions from separate GNN and Transformer without interaction.\\nTheorem 2 (Superior Approximation of Joint Model) For any target function f∗: (s, G) →R satisfying\\nstructural-sequential coupling, the joint model achieves better approximation:\\ninf\\nf∈Fjoint∥f −f∗∥2 ≤κ · inf\\nf∈Fsep∥f −f∗∥2\\n(49)\\nwhere κ < 1 depends on the coupling strength.\\nProof 3 Consider target function with cross-modal interaction:\\nf∗(s, G) = g(s) + h(G) +\\nc(s, G)\\n| {z }\\ncoupling term\\n(50)\\nSeparate Model: Can only approximate g(s) + h(G), ignoring c(s, G). Therefore:\\ninf\\nf∈Fsep∥f −f∗∥2 ≥∥c(s, G)∥2\\n(51)\\n15\\n\\nJoint Model: Through diffusion, structural information enters transformer (via Astruct) and sequence\\ninformation enters GNN (via Sdiff). The effective representation becomes:\\nfjoint = g(s, S) + h(G, A) + c′(s, G)\\n(52)\\nBy universal approximation theorem for GNNs and Transformers, there exist parameters such that:\\n∥c′(s, G) −c(s, G)∥2 ≤ϵ\\n(53)\\nTherefore:\\ninf\\nf∈Fjoint∥f −f∗∥2 ≤ϵ ≪∥c(s, G)∥2\\n(54)\\nThe ratio κ = ϵ/∥c∥2 is small when coupling is significant.\\nCorollary 1 (Sample Complexity) For ϵ-approximation with probability 1 −δ, the joint model requires:\\nNjoint = O\\n\\x12d log(1/δ)\\nϵ2\\n\\x13\\n(55)\\nsamples, compared to:\\nNsep = O\\n\\x12d log(1/δ)\\nκ2ϵ2\\n\\x13\\n(56)\\nfor separate models, where d is effective dimension.\\nC.3\\nInformation Flow Analysis\\nLemma 1 (Mutual Information Lower Bound) The diffusion mechanism ensures mutual information be-\\ntween structure and sequence representations satisfies:\\nI(HG; HT ) ≥I(HG\\nsep; HT\\nsep) + ∆I\\n(57)\\nwhere ∆I > 0 quantifies additional information flow from diffusion.\\nProof 4 By data processing inequality, standard separate encoding has:\\nI(HG\\nsep; HT\\nsep) ≤I(G; s)\\n(58)\\nWith diffusion, attention Astruct depends on G:\\nI(HT ; G) ≥I(HT\\nsep; G) + I(HT ; G|Astruct)\\n(59)\\n≥I(HT\\nsep; G) +\\nH(G|Astruct)\\n|\\n{z\\n}\\nstructural information in attention\\n> I(HT\\nsep; G)\\n(60)\\nSimilarly for I(HG; s) through graph diffusion. Therefore:\\nI(HG; HT ) = H(HG) + H(HT ) −H(HG, HT )\\n(61)\\nis larger due to increased shared information.\\n16\\n\\nC.4\\nGeneralization Bound\\nTheorem 3 (Generalization Error) With probability at least 1 −δ over training set S of size N:\\nEtest[L(fS)] ≤ˆLS(f) + O\\n \\nRdiff(F)\\n√\\nN\\n+\\nr\\nlog(1/δ)\\nN\\n!\\n(62)\\nwhere Rdiff(F) is the Rademacher complexity of the diffused function class.\\nProof 5 The diffusion operation adds smoothness to the function class. By Ledoux-Talagrand contraction\\nlemma:\\nRdiff(F) ≤(1 −β)T R(Foriginal)\\n(63)\\nSince diffusion is a contraction with rate (1 −β), the Rademacher complexity is reduced. Applying\\nstandard generalization bounds:\\nEtest[L(f)] −ˆLS(f)\\n(64)\\n≤2Rdiff(F) + 3\\nr\\nlog(2/δ)\\n2N\\n(65)\\n≤2(1 −β)T R(Foriginal) + 3\\nr\\nlog(2/δ)\\n2N\\n(66)\\nThis shows diffusion improves generalization by reducing complexity.\\nC.5\\nStability Analysis\\nProposition 2 (Lipschitz Continuity) The diffused attention is Lipschitz continuous with respect to input\\nperturbations:\\n∥Adiff(s) −Adiff(s′)∥F ≤L∥s −s′∥2\\n(67)\\nwhere L =\\n1\\n1−βλmax is the Lipschitz constant.\\nProof 6 From the fixed point equation:\\nA∗= (1 −β)A(0) + β˜SA∗\\n(68)\\nTaking difference for two inputs:\\nA∗(s) −A∗(s′) = (1 −β)[A(0)(s) −A(0)(s′)]\\n(69)\\n+ β˜S[A∗(s) −A∗(s′)]\\n(70)\\nRearranging:\\n(I −β˜S)[A∗(s) −A∗(s′)] = (1 −β)[A(0)(s) −A(0)(s′)]\\n(71)\\nSolving:\\nA∗(s) −A∗(s′) = (1 −β)(I −β˜S)−1[A(0)(s) −A(0)(s′)]\\n(72)\\nSince ∥(I −β˜S)−1∥2 ≤1/(1 −βλmax) and vanilla attention is L0-Lipschitz:\\n∥A∗(s) −A∗(s′)∥F ≤\\n1 −β\\n1 −βλmax\\nL0∥s −s′∥2\\n(73)\\n≤L∥s −s′∥2\\n(74)\\n17\\n\\nReferences\\nHabib Bashour, Eva Smorodina, Matteo Pariset, Jahn Zhong, Rahmad Akbar, Maria Chernigovskaya, Khang\\nLˆe Qu´y, Igor Snapkow, Puneet Rawat, Konrad Krawczyk, Geir Kjetil Sandve, Jose Gutierrez-Marcos,\\nDaniel Nakhaee-Zadeh Gutierrez, Jan Terje Andersen, and Victor Greiff. Biophysical cartography of\\nthe native and human-engineered antibody landscapes quantifies the plasticity of antibody developability.\\nCommunications Biology, 7(1):922, Jul 2024. ISSN 2399-3642. doi: 10.1038/s42003-024-06561-3. URL\\nhttps://doi.org/10.1038/s42003-024-06561-3.\\nMarkus Braun, Adrian Tripp, Morakot Chakatok, Sigrid Kaltenbrunner, Massimo Totaro, David Stoll, Alek-\\nsandar Bijelic, Wael Elaily, Shlomo Yakir Hoch, Matteo Aleotti, M´elanie Hall, and Gustav Oberdor-\\nfer. Computational design of highly active de novo enzymes. bioRxiv, 2024. doi: 10.1101/2024.08.02.\\n606416. URL https://www.biorxiv.org/content/early/2024/08/03/2024.08.02.\\n606416.\\nMarkus Braun, Adrian Tripp, Morakot Chakatok, Sigrid Kaltenbrunner, Celina Fischer, David Stoll, Alek-\\nsandar Bijelic, Wael Elaily, Massimo G. Totaro, Melanie Moser, Shlomo Y. Hoch, Horst Lechner,\\nFederico Rossi, Matteo Aleotti, M´elanie Hall, and Gustav Oberdorfer.\\nComputational enzyme de-\\nsign by catalytic motif scaffolding. bioRxiv, 2025. doi: 10.1101/2024.08.02.606416. URL https:\\n//www.biorxiv.org/content/early/2025/06/11/2024.08.02.606416.\\nHong Cao, Jielin Wang, Lin He, Yan Chen, Kaixian Chen, Hualiang Jiang, and Jian Li. Deepddg: Predicting\\nthe stability change of protein point mutations using neural networks. Journal of Chemical Information\\nand Modeling, 59(4):1508–1514, 2019.\\nTanya Jain, Tong Sun, St´ephane Durand, Andrew Hall, Nathan R. Houston, Jason H. Nett, Brian Sharkey,\\nBoguslaw Bobrowicz, Isaac Caffry, Yingda Yu, Yi Cao, Heather Lynaugh, Michael Brown, Himadri\\nBaruah, Laura T. Gray, Eric M. Krauland, Yan Xu, Martha V´asquez, and K. Dane Wittrup. Biophysical\\nproperties of the clinical-stage antibody landscape. Proceedings of the National Academy of Sciences\\nof the United States of America, 114(5):944–949, January 2017. doi: 10.1073/pnas.1616408114. URL\\nhttps://doi.org/10.1073/pnas.1616408114.\\nJovita Jankauskait˙e, Bruno Jim´enez-Garc´ıa, Justas Dapk¯unas, Joost Schymkowitz, Ignacio Ferreira,\\nFrancesco Luigi Gervasio, Johannes S´oding, Wim Vranken, Adri´an Velazquez-Campoy, and Yves De-\\nhouck. Skempi 2.0: an updated benchmark of changes in protein–protein binding energy, kinetics and\\nthermodynamics upon mutation. Bioinformatics, 35(3):462–469, 2019.\\nBowen Jing, Stephan Eismann, Patricia A. Suriana, Raphael J. L. Townshend, and Ron Dror. Learning from\\nprotein structure with geometric vector perceptrons. In International Conference on Learning Represen-\\ntations (ICLR), 2021.\\nWolfgang Kabsch and Chris Sander.\\nDictionary of protein secondary structure: pattern recognition of\\nhydrogen-bonded and geometrical features. Biopolymers, 22(12):2577–2637, 1983.\\nElizabeth H. Kellogg, Andrew Leaver-Fay, and David Baker. Role of conformational sampling in com-\\nputing mutation-induced changes in protein structure and stability. Proteins: Structure, Function, and\\nBioinformatics, 79(3):830–838, March 2011a. doi: 10.1002/prot.22921. URL https://doi.org/\\n10.1002/prot.22921.\\nElizabeth H Kellogg, Andrew Leaver-Fay, and David Baker. Role of conformational sampling in com-\\nputing mutation-induced changes in protein structure and stability. Proteins: Structure, Function, and\\nBioinformatics, 79(3):830–838, 2011b.\\n18\\n\\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In\\nInternational Conference on Learning Representations (ICLR), 2017.\\nJohannes Klicpera, Stefan Weißenberger, and Stephan G¨unnemann. Diffusion improves graph learning. In\\nAdvances in Neural Information Processing Systems (NeurIPS), volume 32, pages 13340–13352, 2019.\\nM D Kumar, K A Bava, M Michael Gromiha, N Srinivasan Prabhu, Shandar Ahmad, Harinder Singh, and\\nAkinori Sarai. Protherm and pronit: thermodynamic databases for proteins and protein–nucleic acid\\ninteractions. Nucleic Acids Research, 34(D1):D204–D206, 2006.\\nBo Li, Yung-Tsung Yang, John A Capra, and Mark B Gerstein. Predicting changes in protein thermodynamic\\nstability upon point mutation with deep 3d convolutional neural networks. PLoS Computational Biology,\\n16(11):e1008291, 2020a.\\nGuohao Li, Chen Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns.\\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2020b.\\nDina Listov, Eva Vos, Gyula Hoffka, Shlomo Yakir Hoch, Andrej Berg, Shelly Hamer-Rogotner, Orly Dym,\\nShina Caroline Lynn Kamerlin, and Sarel J. Fleishman. Complete computational design of high-efficiency\\nkemp elimination enzymes. Nature, 643(8074):1421–1427, Jul 2025. ISSN 1476-4687. doi: 10.1038/\\ns41586-025-09136-2. URL https://doi.org/10.1038/s41586-025-09136-2.\\nJoshua Meier, Roshan Rao, Raoul Verkuil, Jason Liu, Tom Sercu, Mihaly Varadi, Jinhe Lee, John Muschelli,\\nDima Kozakov, Andrej Sali, et al. Language models enable zero-shot prediction of the effects of mutations\\non protein function. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages\\n29287–29303, 2021a.\\nJoshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives.\\nLanguage\\nmodels enable zero-shot prediction of the effects of mutations on protein function.\\nIn M. Ran-\\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in\\nNeural Information Processing Systems, volume 34, pages 29287–29303. Curran Associates, Inc.,\\n2021b.\\nURL https://proceedings.neurips.cc/paper_files/paper/2021/file/\\nf51338d736f95dd42427296047067694-Paper.pdf.\\nLuca Montanucci, Emidio Capriotti, Yana Frank, Pier Luigi Martelli, and Rita Casadio. DDGun: an un-\\ntrained predictor of protein stability changes upon amino acid variants. Nucleic Acids Research, 50(W1):\\nW222–W227, 2022.\\nRoshan Rao, Joshua Meier, Tom Sercu, Saleh Manzoor, Wojciech Martini, Aditya Benites, Calvin Pan, Jesse\\nIngraham, Stephane Gobeil, Fatima Amanat, et al. Transformer protein language models are unsupervised\\nstructure learners. In International Conference on Learning Representations (ICLR), 2021.\\nAlexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Guo, Myle Ott,\\nC Lawrence Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function emerge from scal-\\ning unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of\\nSciences (PNAS), 118(15):e2016239118, 2021.\\nJoost Schymkowitz, Jan Borg, Florian Stricher, Robin Nys, Frederic Rousseau, and Luis Serrano. The foldx\\nweb server: an online force field. Nucleic Acids Research, 33(Web Server issue):W382–W388, July\\n2005a. doi: 10.1093/nar/gki387. URL https://doi.org/10.1093/nar/gki387.\\nJoost Schymkowitz, Jesper Borg, Francois Stricher, Remy Nys, Frederic Rousseau, and Luis Serrano. The\\nFoldX web server: an online force field. Nucleic Acids Research, 33(suppl 1):W382–W388, 2005b.\\n19\\n\\nJan Stourac, Jiri Dubrava, Milos Musil, David Bednar, Jiri Damborsky, and Zbynek Prokop. Fireprotdb:\\ndatabase of manually curated protein stability data. Nucleic Acids Research, 49(D1):D319–D324, 2021.\\nNobuhiko Tokuriki and Dan S Tawfik. Stability effects of mutations and protein evolvability. Current\\nOpinion in Structural Biology, 19(5):596–604, 2009. doi: 10.1016/j.sbi.2009.08.003.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Process-\\ning Systems (NeurIPS), volume 30, 2017.\\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio.\\nGraph attention networks. In International Conference on Learning Representations (ICLR), 2018.\\nPeiqi Yue, Zhen Li, and John Moult.\\nLoss of protein structure stability as a major causative factor in\\nmonogenic disease. Journal of Molecular Biology, 353(2):459–473, October 2005. doi: 10.1016/j.jmb.\\n2005.08.020.\\nGuangyu Zhou, Min Chen, Christopher J T Ju, Jie Xu, Haoyang Chen, and Zhiyong Lu. Mutation effect esti-\\nmation on protein–protein interactions using deep contextualized representation learning. NAR Genomics\\nand Bioinformatics, 2(2):lqaa015, 2020.\\nYunzhuo Zhou, Qisheng Pan, Douglas E V Pires, Carlos H M Rodrigues, and David B Ascher. Ddmut:\\npredicting effects of mutations on protein stability using deep learning. Nucleic Acids Research, 51(W1):\\nW122–W128, 06 2023. ISSN 0305-1048. doi: 10.1093/nar/gkad472. URL https://doi.org/10.\\n1093/nar/gkad472.\\n20\\n',\n",
       " 'Neural Image Abstraction Using Long Smoothing B-Splines\\nDANIEL BERIO, Goldsmiths, University of London, United Kingdom\\nMICHAEL STROH, University of Konstanz, Germany\\nSYLVAIN CALINON, Idiap Research Institute, Switzerland\\nFREDERIC FOL LEYMARIE, Goldsmiths, University of London, United Kingdom\\nOLIVER DEUSSEN, University of Konstanz, Germany\\nARIEL SHAMIR, Reichman University, Israel\\nStylized space filling\\nStroke-based image abstraction\\nStylized image vectorization\\nCalligrams and text stylization\\nFig. 1. Our method allows to optimize long and smooth B-Spline curves using DiffVG rasterization pipelines. Applications range from text stylization to image\\nabstraction and vectorization.\\nWe integrate smoothing B-splines into a standard differentiable vector graph-\\nics (DiffVG) pipeline through linear mapping, and show how this can be\\nused to generate smooth and arbitrarily long paths within image-based\\ndeep learning systems. We take advantage of derivative-based smoothing\\ncosts for parametric control of fidelity vs. simplicity tradeoffs, while also\\nenabling stylization control in geometric and image spaces. The proposed\\npipeline is compatible with recent vector graphics generation and vector-\\nization methods. We demonstrate the versatility of our approach with four\\napplications aimed at the generation of stylized vector graphics: stylized\\nspace-filling path generation, stroke-based image abstraction, closed-area\\nimage abstraction, and stylized text generation.\\nCCS Concepts: • Computing methodologies →Non-photorealistic ren-\\ndering; Rasterization; Parametric curve and surface models; Neural\\nnetworks; • Applied computing →Fine arts.\\nAdditional Key Words and Phrases: Differentiable vector graphics, B-splines,\\nDiffusion, CLIP, Long strokes\\nACM Reference Format:\\nDaniel Berio, Michael Stroh, Sylvain Calinon, Frederic Fol Leymarie, Oliver\\nDeussen, and Ariel Shamir. 2025. Neural Image Abstraction Using Long\\nAuthors’ Contact Information: Daniel Berio, Goldsmiths, University of London, Lon-\\ndon, United Kingdom, daniel.berio@gold.ac.uk; Michael Stroh, University of Konstanz,\\nKonstanz, Germany, michael.stroh@uni-konstanz.de; Sylvain Calinon, Idiap Research\\nInstitute, Martigny, Switzerland, sylvain.calinon@idiap.ch; Frederic Fol Leymarie, Gold-\\nsmiths, University of London, London, United Kingdom, ffl@gold.ac.uk; Oliver Deussen,\\nUniversity of Konstanz, Konstanz, Germany, oliver.deussen@uni-konstanz.de; Ariel\\nShamir, Reichman University, Herzliya, Israel, arik@runi.ac.il.\\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\\n© 2025 Copyright held by the owner/author(s).\\nACM 1557-7368/2025/12-ART225\\nhttps://doi.org/10.1145/3763345\\nSmoothing B-Splines. ACM Trans. Graph. 44, 6, Article 225 (December 2025),\\n12 pages. https://doi.org/10.1145/3763345\\n1\\nIntroduction\\nhe ability to produce long, smooth curves is cen-\\ntral to a variety of design and artistic tasks. These\\ninclude freehand drawing, sketching, calligraphy,\\ntypography, logo design as well as image abstrac-\\ntions into compositions of organic, flowing, or\\nblobby shapes. Our aim is to enable the gener-\\nation of vector graphic outputs that allow these types of designs,\\nwhile taking advantage of recent advances in gradient-based image\\ngeneration, stylization, and understanding.\\nDevelopments in differentiable vector graphics (DiffVG) raster-\\nization have enabled gradient-based optimization methods that\\nleverage complex image-space losses to drive image generation,\\nstylization and abstraction methods. Most existing approaches rely\\non the method of Li et al. [2020], which implements differentiable\\nrasterization for a large subset of elements of the Scalable Vector\\nGraphics (SVG) standard, including piecewise cubic and quadratic\\nBézier curves. Most of these methods directly optimize Bézier curves,\\nbut even with additional smoothing penalties they do not provide\\nguarantees of continuity across segments, which limits their ability\\nto represent long, smooth and expressive strokes.\\nOur work is based on two observations. First, alternative spline\\nparametrizations such as B-spline [De Boor 2001] or Catmull-Rom\\n[DeRose and Barsky 1988] provide inherent continuity constraints\\nin their definition. Second, the conversion of such curves to Bézier\\ncurves is a linear transformation, making their integration into exist-\\ning DiffVG pipelines a matter of an additional matrix multiplication.\\nAlthough these curve parameterizations are well established, to the\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\narXiv:2511.05360v1  [cs.GR]  7 Nov 2025\\n\\n225:2\\n•\\nBerio et al.\\nbest of our knowledge, their integration into DiffVG remains largely\\nunexplored.\\nIn our work we focus on uniform B-splines for their simplicity,\\nhigh-order continuity and analytic properties [Farin 2001]. This en-\\nables a straightforward implementation of derivative-based smooth-\\ning criteria that are well known in the fairing and motor-control/robo-\\ntics domains, but most importantly support our goals of generating\\nlong stylized curves within a DiffVG pipeline.\\nWe define B-splines with control-polygons consisting of series of\\n“key-points” and convert these to piecewise cubic Bézier curves for\\nrendering. Since this transformation is linear and rendering is differ-\\nentiable, gradients from image-space losses can be back-propagated\\nto the key-points. We treat stroke width as a third curve dimension,\\nwhere each curve control point can be assigned an independent\\nstroke radius, enabling smooth variations similar to that seen in\\nphysical brush strokes [Fujioka and Kano 2007]. Allowing the stroke\\nwidth to vanish also presents an effective way to alter the number\\nof visible strokes required for an image abstraction. Our method op-\\nerates with both open and closed curves, supporting the generation\\nof closed and organic-looking areas.\\nWe present four different applications for our method: abstract\\nspace filling curves (Section 4.1), sketch-based stylization (Section\\n4.2), abstract image vectorization with color quantization (Section\\n4.3) as well as text stylization and calligram generation with a novel\\nlegibility cost (Section 4.4). We provide a practical implementation\\nof smoothing B-splines that can be directly integrated into DiffVG\\npipelines and demonstrate how this enables long and expressive\\ncurves while maintaining flexible geometric and stylistic control.\\nWorking code and examples for our method are available at github.\\ncom/colormotor/calligraph.\\n2\\nRelated work\\n2.1\\nSmooth and stylized curve generation\\nLong and stylized strokes have been explored used in the litera-\\nture for applications including image stylization [Kaplan and Bosch\\n2005; Tong et al. 2025; Wong and Takahashi 2011], text-based styl-\\nization [Maharik et al. 2011], and fabrication [Liu et al. 2017; Yang\\net al. 2021]. To widen and enhance such applications, our method\\nalso enables the generation of long and smooth strokes through\\nthe use of neural-driven image-based costs. Smoothing is achieved\\nby minimizing the squared magnitude of higher-order positional\\nderivatives.\\nIn the motor control literature, it is well established that the kine-\\nmatics of hand and arm movements can be modeled by optimizing\\nperformance criteria [Flash and Hogan 1998]. The so-called mini-\\nmum square derivative models have been successfully applied to\\nhandwriting and curved motion by minimizing third-order deriva-\\ntives (jerk) [Flash and Hogan 1985] and fourth-order derivatives\\n(snap) [Edelman and Flash 1987]. Similar minimum principles are\\nwidely employed for smooth motion control in drones [Mellinger\\nand Kumar 2011; Ren and Kry 2019] and robots [Todorov 2004; Tou-\\nssaint 2017], as well as in statistics for smoothing noisy data [Eilers\\nand Marx 1996; Reinsch 1967].\\nSimilar principles of smooth motion and continuity have also\\nbeen used for curve fairing, where a “fair” curve is typically one that\\nexhibits a smooth variation of the curvature [Farin 2001]. In this\\ncontext, jerk has been adopted as an approximation for curvature\\nvariation [Lu 2015; Meier and Nowacki 1987; Pottmann 1990], while\\nsnap serves as an approximation for transverse distributed load\\n[Meier and Nowacki 1987].\\nIn an extensive body of work, Egerstedt and Martin [2009] de-\\nvelop “dynamic splines” that formulate polynomial splines through\\noptimal control of linear systems. Berio et al. [2017] use similar\\nprinciples for the interactive generation of stylized paths similar to\\nthe ones seen in graffiti art and calligraphy with applications similar\\nto ours. Kano et al. [2003] study the relations between dynamic\\nsplines and B-splines and in a collection of work, they develop an\\noptimal formulation of B-splines [Kano et al. 2005] applied to gen-\\nerate motion paths and curves similar to those found in Japanese\\ncalligraphy [Fujioka et al. 2006; Matsukida and Fujioka 2013]. Our\\napproach is strongly inspired by the B-spline construction initially\\nproposed by Kano et al. [2005], but we extend their formulation to\\nsupport DiffVG and demonstrate its flexibility for generative and\\nstylization settings.\\n2.2\\nDiffVG and applications\\nIn recent years, differentiable rendering has enabled the use of\\nlarge pretrained vision and generative imaging models with 3D\\n[Kato et al. 2020; Tewari et al. 2020; Worchel and Alexa 2023] and\\n2D [Li et al. 2020; Mihai and Hare 2021; Worchel and Alexa 2023]\\nparametric primitives . We adopt the method of Li et al. [2020], which\\nsupports a large subset of the SVG standard and cubic curves with\\nvarying width profiles. Our method leverages DiffVG’s support\\nfor cubics with varying width profiles, a feature yet to be used\\ncomprehensively, likely due to limited support in mainstream vector\\ngraphics tools and standards.\\nCLIP-driven graphics. One of the first applications of DiffVG to\\nlarge-pretrained models has been through the use of the Contrastive\\nLanguage–Image Pretraining (CLIP) model [Radford et al. 2021],\\na multimodal model that has been trained to share an embedding\\nspace between images and their textual descriptions. Frans et al.\\n[2022] demonstrate that together with DiffVG, the model is able\\nto generate vector images guided by a text caption or “prompt”.\\nGanz and Elad [2024] use an adversarial \"robustification\" method to\\nfine-tune CLIP in order to enable gradients that are better aligned\\nwith human perception. Vinker et al. [2022] introduce the idea\\nof using a loss on internal layers of CLIP to guide vector image\\nabstraction. A similar approach, combined with DiffVG, has enabled\\nthe generation of stroke-based stylization methods [Schaldenbrand\\net al. 2023; Vinker et al. 2023; Xing et al. 2023]. Our method provides\\nsimilar capacities, but we take advantage of the fine-tuned CLIPAG\\nmodel of Ganz and Elad [2024] and support long smooth strokes,\\nwhich was not possible with previous methods.\\nDiffusion-driven graphics. In the context of 3D asset generation,\\nPoole et al. [2023] pioneered the so-called Score Distillation Sam-\\npling (SDS), which enables gradient propagation from pre-trained\\ndiffusion models to parametric representations. While effective,\\nthe original method relies on high classifier-free guidance (CFG)\\nscales, often resulting in over-saturation and lack of detail [Katzir\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\nNeural Image Abstraction Using Long Smoothing B-Splines\\n•\\n225:3\\nFig. 2. Flow chart of our pipeline, all operations are differentiable.\\nFig. 3. Optimization procedure. From left to right: input image with an\\ninitial spline (quintic with multiplicity 3 on all keypoints) and subsequent\\noptimization steps 10, 150, 300.\\net al. 2024]. Recent methods, including variational methods [Wang\\net al. 2023], DDIM inversion [Liang et al. 2024] and noise-free score\\ndistillation (NFSD) [Katzir et al. 2024], address these challenges,\\nimproving fidelity and control. Our method is compatible with all\\nthese techniques, and we specifically adopt the approach of Liang\\net al. [2024], which proved to be the most effective for us.\\nIn the context of 2D asset generation, Jain et al. [2023b] pioneered\\nthe use of SDS in conjunction with the DiffVG method of Li et al.\\n[2020], demonstrating the expressive potential of diffusion for vector\\ngraphics generation. Iluz et al. [2023] use SDS for stylizing vector\\nfont outlines to resemble user-defined semantics. To name a few,\\nvariants of SDS have been used for prompt-based sketch generation\\n[Xing et al. 2023] and animation [Gal et al. 2024], 2D vector graphics\\n[Xing et al. 2024; Zhang et al. 2024] as well as 3D line art [Qu et al.\\n2024; Tojo et al. 2024]. None of these methods support the creation\\nof long, smooth strokes aligned with our objectives, except for\\nTojo et al. [2024], who also use B-splines to produce long single-\\nstroke outputs. We incorporate their proposed repulsion loss in our\\nmethod. However, their work does not cover high-order derivative\\nsmoothing, relies on curve discretization, uses a custom CUDA\\nrenderer, and does not support variable-width strokes.\\n3\\nMethod\\nOur approach works as follows: we specify one or more B-splines\\nthrough a series of 2D or 3D keypoints, where the third dimension\\ncan be used to describe width variation along a stroke. The B-splines\\nare converted to cubic piecewise Bézier curves that are then rendered\\nin a differentiable manner with the method of Li et al. [2020] (see also\\nFigure 2). Similarly to conventional DiffVG pipelines, this enables\\ngradient optimization of the key-points with costs that depend on\\ncurve geometry as well as on the rendered version of the curves.\\nFigure 3 shows the process: first, some initial points and an image\\nare given; then, during optimization a spline gradually represents\\nthe input image more explicitly, while the stroke widths are jointly\\nadapted.\\n3.1\\nUniform B-splines\\nWe use normalized uniform or “cardinal” B-splines that have uni-\\nformly spaced integer knots [De Boor 2001], which simplifies com-\\nputations and proves successful in our applications. A B-spline of\\ndegree 𝑝and order 𝑘= 𝑝+ 1 is a linear combination\\n𝒙(𝑢) =\\n𝑛−1\\n∑︁\\n𝑖=0\\nc𝑖𝑁𝑘(𝑢−𝑡𝑖)\\nwhere 𝑛control-points 𝑪= [c0, c1, . . . c𝑛−1] and shifted bases 𝑁𝑘\\nare associated with a non-decreasing sequence of 𝑚= 𝑛+ 𝑘knots.\\nIn our formulation we keep these fixed to\\n𝒕= [𝑡0, . . . ,𝑡𝑚−1] = [−𝑝, . . . , 0, . . . ,𝑛−𝑘\\n|       {z       }\\n𝑡𝑝,...,𝑡𝑚−𝑘\\n, . . .𝑛].\\nThe spline is defined by sampling 𝑢in the interval [𝑡𝑘−1,𝑡𝑚−𝑘].\\nIncreasing order derivatives 𝒙(𝑑) of a B-spline are easily computed as\\nweighted combinations of lower order B-splines. We refer the reader\\nto the supplement for details on the basis functions construction, but\\nthese are readily available in many modern scientific computation\\npackages [Virtanen et al. 2020]. The number of curves and control\\npoints is predefined, so that basis functions and knot sequences can\\nbe precomputed and remain fixed during optimization.\\n3.2\\nSpline construction\\nB-splines are approximating curves, and both periodicity and clamp-\\ning to endpoints require the repetition of either knots or control\\npoints. This is typically achieved with repeated knots, but we follow\\nFujioka et al. [2006] and use repeated control points. This maintains\\nstrict uniformity while enabling adaptive smoothing of corner-like\\nfeatures and simplifying integral computations, which is advanta-\\ngeous for our use-case. Instead of directly specifying control points,\\nwe let a user initially specify a spline through a series of 𝑀key-\\npoints 𝑸= 𝒒1, . . . , 𝒒𝑀and optimize these rather than the spline\\ncontrol points directly. The key-points are automatically adapted\\ninto a series of control points 𝑪depending on the curve’s desired\\nclamped or periodic behavior.\\nFor a clamped (open) spline the control points are given by the\\nkey-points 𝑸padded the first and last key-point repeated 𝑘−1 times.\\nThis results in a parametric motion that begins and ends with a rest.\\nFor periodic closed splines we construct 𝑪by appending the first\\n𝑘−1 keypoints to the initially specified key-point sequence 𝑸.\\nKey-points may optionally be repeated to create sharp corners, as\\neach repetition initially reduces the continuity of the curve by one\\ndegree [Farin 2001]. This strategy is useful to produce additional\\ndegrees of freedom for the subsequent optimization, where the cor-\\nners can be adaptively smoothed depending on the desired amount\\nof smoothing.\\n3.3\\nSmoothing B-splines\\nB-splines of order 𝑘are by definition 𝐶𝑘−2-continuous, but more im-\\nportantly their construction facilitates the formulation of smoothing\\ncriteria since they allow closed form computation of derivatives and\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\n225:4\\n•\\nBerio et al.\\nintegrals. In our method, we adopt a smoothing cost based on the\\nsquared magnitude of the curve derivatives, which is standard in the\\nsmoothing literature and is also known for its utility in curve fairing\\n[Pottmann 1990] and for modeling human arm movements [Todorov\\nand Jordan 1998]. These methods typically trade off smoothness\\nwith a geometric accuracy term, but in our work we consider a\\nvariety of image-space objectives instead of geometry and define a\\nsmoothing cost:\\nL𝑑\\nsmooth = 1\\n𝑇\\n∫𝑡𝑚−𝑘\\n𝑡𝑘−1\\n∥𝒙(𝑑) (𝑢)∥2d𝑢= 1\\n𝑇𝒄⊤¯𝑮𝒄\\n(1)\\nwhere𝑇= 𝑡𝑚−𝑘−𝑡𝑘−1 and 𝒄is a vector that concatenates all control\\npoints of the spline. The integral can be calculated exactly by setting\\n¯𝑮to a block Gram-matrix constructed from the inner products of\\nthe basis function derivatives [Fujioka and Kano 2007; Vermeulen\\net al. 1992], resulting in the standard spline smoothing criterion.\\nAlternatively, a finite difference approximation of ¯𝑮results in the\\npenalized-spline method of Eilers and Marx [1996]. Both methods\\nhave similar run-time performance because the matrix is precom-\\nputed for each stroke, and we refer the reader to the supplement\\nfor derivations. Most of our examples use quintic splines with a\\nsmoothing cost L3\\nsmooth on the third positional derivative (jerk).\\nWe do so on the basis that “minimum jerk” is a known criterion\\nthat has been used to model hand and arm movements [Flash and\\nHogan 1985; Todorov and Jordan 1998] as well as an approximant\\nfor curvature variation in curve fairing [Lu 2015]. Nevertheless, our\\nmethod generalizes to different curve and smoothing orders (Fig. 4).\\n3.4\\nConversion to Bézier and rendering\\nOur goal is to integrate smoothing B-splines into a DiffVG pipeline\\nby taking advantage of the linear relationship between B-splines and\\nBézier curves. B-splines can be converted exactly to piecewise Bézier\\ncurves of the same degree. To do so we use the method of Romani\\nand Sabin [2004], which reduces to a matrix multiplication between\\nthe flattened spline control points 𝒄and a block transformation\\nmatrix 𝑺.\\nOur method also supports smoothing costs on higher-order posi-\\ntional derivatives such as jerk (third derivative) and snap (fourth\\nderivative), which require polynomial curves of degree greater than\\nthree. Although native rendering of such higher-degree curves is\\nnot supported in DiffVG and remains a challenge, we observe that\\nreducing the degree of B-splines to three introduces negligible geo-\\nmetric error (less than 0.3% of the curve’s bounding box diagonal\\nin all our experiments), making the optimization of higher-degree\\nB-splines practical for image-based error calculations.\\nWe perform a degree reduction of Bézier curves using the multi-\\nreduction method of Sunwoo [2005], which involves a second block\\ntransformation matrix ¯𝑹. As a result, the control points for a cubic\\npiecewise Bézier curve compatible with DiffVG are computed from\\nthe (flattened) control points 𝒄with the linear map ¯𝑹¯𝑺𝒄. We refer\\nthe reader to the work of Romani and Sabin [2004] and Sunwoo\\n[2005] for details; we include in the supplement details and matrices\\nfor quintic Bézier and their reduction to cubic.\\nDiffVG rendering and optimization. The conversion procedure\\nresults in a sequence of Bézier control points ∈IR3 , where the third\\ndimension represents the stroke radius. Control points and associ-\\nated stroke and fill colors are all treated as differentiable parameters\\nto be optimized. Rendering the scene results in an image I, which is\\ndifferentiable with respect to all the underlying parameters.\\n4\\nApplications\\nThe proposed B-spline construction, smoothing and conversion to\\nBézier enables the optimization of long, expressive and optionally\\nperiodic curves, which would be challenging to produce with cur-\\nrently known methods leveraging DiffVG. All the results presented\\nhereafter are produced using a combined cost:\\nL = LI + LG\\n(2)\\nconsisting of an image-space term, LI, and a geometric term, LG.\\nWe construct each term as a combination of losses depending on\\nthe application objective. LI relies on differentiable rasterization,\\nwhich allows gradients to propagate from raster-based objectives to\\nthe geometry parameters. LG leverages the properties of B-splines\\nto enable smoothing, stylization objectives, and constraints while\\npreserving continuity. We denote the relative weights of any loss\\nL◦as 𝜆◦, e.g. the weight of a smoothing loss on the third derivative\\nis denoted as 𝜆smooth. If not specified, the weights are assumed to be\\n1. When also optimizing stroke widths, we clip these to a minimum\\nand maximum value at each iteration.\\nWe generate strokes using the Adam optimizer and use a cosine\\nannealing schedule on the learning rates. We run our experiments\\non a single NVIDIA GeForce RTX 3060 with 12 Gb of memory.\\nWe run most of the presented applications for 300 steps, which\\napproximately takes between 30 and 60 seconds on our system. One\\nexception is using diffusion-guidance, which takes approximately\\n0.6 to 1.0 second per step depending on the method used, leading\\nto an optimization time of up to 6 minutes.\\nFig. 4. Comparison of different spline degrees 𝑝(rows), smoothing deriv-\\native orders 𝑑and smoothing weight 𝜆smooth (columns). In each row, we\\nlet the smooting derivative to 𝑝−1. We quantify smoothness using the\\ndimensionless jerk measure [Hogan and Sternad 2009]. Lower is smoother.\\nWe use the stylized area fill method in Section 4.1 using the style image in\\nFig. 6, left.\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\nNeural Image Abstraction Using Long Smoothing B-Splines\\n•\\n225:5\\nFig. 5. Text combining areas generated with our stylized area filling method.\\nEach letter is generated separately.\\nFig. 6. Examples of stylized area filling for a letter “S”. The images on the\\nlower left are used to guide stylization.\\nFig. 7. Left, weighted Voronoi samples (black) for a bitmap of the letter ’A’\\nand an open TSP path connecting the points (blue). Middle, initial quintic B-\\nspline with key-points given by the Voronoi samples overlaid on the bitmap\\narea with 50% opacity. Right, result of an optimization with LG = L3\\nsmooth\\nand LI for the 50% opacity bitmap. Decreasing opacity results in sparser\\nand thinner strokes.\\n4.1\\nArea fillings and pattern generation\\nAs a baseline for our method we demonstrate how our pipeline\\ncan be used to create pattern fills of solid regions. It illustrates also\\nhow our approach can be flexibly used to control stylization while\\nmaintaining smoothness (Fig. 5 and 6).\\nInitialization. Stochastic gradient descent is well known to be\\nsensitive to initialization due to its susceptibility to local minima.\\nWe find good points using an initialization strategy based on so-\\ncalled weighted Voronoi stippling [Secord 2002]. For simplicity, we\\nadopt this method for different applications presented in this paper.\\nThe input can be an arbitrary bitmap (Fig. 7, Left) or a saliency map\\n(Fig. 13). To create a single stroke, we use a TSP route connecting\\nthe points in an open or looping path. This method is known in\\nthe literature as “TSP art” [Kaplan and Bosch 2005] (Fig. 7, Left).\\nFor open paths, we select the left-topmost point and the bottom-\\nrightmost as initial and final points, respectively.\\nImage coverage loss. We find that setting LI as a multiscale mean\\nsquared error (MSE) loss works particularly well to fill an area or\\nsilhouette defined as an image. This loss is computed between the\\ntarget and the rendered image, with each step corresponding to a\\nprogressively reduced scale and blurred version of the image. This\\napproach is similar to the shape-based losses used by Iluz et al.\\n(a)\\n(b)\\n(c)\\nFig. 8. Stylized coverage of a letter “S” using the image on the left as a\\ntarget. (a) Optimization using Bézier curves. (b) Optimization using quintic\\nsplines and smoothing on jerk with 𝜆smooth = 1. (c). Same procedure with\\n𝜆smooth = 10.\\nFig. 9. Examples combining image coverage with a patch-wise loss on CLIP\\nfeatures derived from an example image (top left) and using the same\\ninitialization from Fig. 7. From the left, the first two examples use a quintic\\nspline with a smoothing loss LG = L3\\nsmooth (jerk). For comparison, the right\\nexample uses a Catmull-Rom spline only enforcing 𝐶1 continuity. Allowing\\nzero stroke width results in the appearance of multiple strokes, but the\\noptimization is still performed on a single curve (left, dotted cyan).\\n[2023] and Tojo et al. [2024], but lower scales encourage alignment\\nwith broader intensity regions and faster convergence, while higher\\nscales promote a more accurate silhouette reconstruction. Reducing\\nthe opacity of the target image directly decreases the density of\\ncurves used to cover it (Fig. 7-right), allowing control over the\\nvisual result.\\nBounding box loss. For some of our optimization procedures, it\\nis useful to extend LG with a bounding box loss that keeps curve\\nkey-points within the bounding box of a given image:\\nLbox =\\n∑︁\\n𝑖\\n1⊤\\x02\\n𝜑\\x00𝒃min −𝒑𝑖\\n\\x01 + 𝜑\\x00𝒑𝑖−𝒃max\\n\\x01\\x03\\nwhere Lbox > 0 only if key-points fall outside of the bounding box\\n𝒃min, 𝒃max and where 𝜑can be either a Softplus or a ReLU function\\napplied element-wise to the vectors.\\nImage-space semantic-driven stylization. Together with geometry-\\nbased stylization costs, we can add a semantic stylization term Lstyle\\nto the image-space loss LI, which enables stylization based on a\\ntext prompt or features extracted from an example image ( Fig. 8 , 6\\nand 9). We apply the technique proposed by Kwon and Ye [2022] for\\nsemantic-driven image stylization and use a patch-wise directional\\nloss between the encoded features of an example image and the\\nencoded features of the rendered curves. We use the augmented\\nCLIPAG [Ganz and Elad 2024] ViT-B/32 transformer architecture\\nas we find it to be efficient while working well for our use vector\\nstylization use-case.\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\n225:6\\n•\\nBerio et al.\\n(a)\\n(b)\\n(c)\\nFig. 10. Diffusion driven stroke abstraction with ControlNet and IP-adapter\\nconditioning. (a) a variable width abstraction of “Spock”. (b) Allowing a\\nsingle strokes to reach zero width in regions results in an effective strategy\\nfor automatically determining the number of strokes for a multi-stroke\\nabstraction. (c) combining the diffusion cost with a stylization term that\\nfavors horizontal and vertical orientations.\\nFig. 11. Rendering stroke abstractions with a spray-like brush. The quintic\\nB-splines together with smoothing on jerk produce smooth motions that\\ntend to slow down where curvature is higher. This is a characteristic feature\\nof human hand motions [Viviani, Paolo and Flash, Tamar 1995] and results\\nin a lower deposition of paint particles where speed is higher.\\n4.2\\nSingle-stroke image abstraction\\nMost existing DiffVG-based methods that work with diffusion mod-\\nels rely on variants of Score Distillation Sampling (SDS) together\\nwith a text caption to guide the generation of parametric vector\\nprimitives. We follow a similar approach but enable image-conditi-\\noned stylization by integrating ControlNet [Zhang et al. 2023] with\\nCanny edge detection and IP-Adapter [Ye et al. 2023] into the diffu-\\nsion pipeline. ControlNet helps to preserve structural cues from the\\ninput image, while IP-Adapter encourages the strokes to align with\\nits global appearance and style (Fig. 10 and 11).\\nIn our experiments, we find that using a generic text prompt such\\nas “A black and white drawing” for stroke-based outputs is sufficient\\nto generate recognizable abstractions and stylizations of an input\\nimage. For a given condition 𝑦, the gradient of the SDS-like loss\\nwith respect to the optimizated parameters 𝜃has the form:\\n∇𝜃LSDS = E𝑡\\n\\x14\\n𝜔(𝑡) \\x00𝝐𝜙(𝑥𝑡,𝑡,𝑦) −𝝐\\x01 𝜕𝑔(𝜃)\\n𝜕𝜃\\n\\x15\\n,\\n(3)\\nwhere 𝝐𝜙(𝑥𝑡,𝑡,𝑦) is the predicted denoising direction for a latent\\n𝑥𝑡at time step 𝑡, 𝝐is the noise predicted by the model and 𝜔(𝑡) is a\\nweighting function dependent on the time-step.\\nWe employ the time-step schedule annealing procedure proposed\\nby Liang et al. [2024] and use their Interval Score Matching (ISM)\\nvariant of SDS, which helps convergence in our experiments and\\nenables a standard classifier-free guidance of 7.5.\\nIt is known that for diffusion models, higher time steps during\\ndenoising typically produce coarser features, while lower time steps\\nFig. 12. From left to right: varying the minimum time-step (100, 300, 500, 700)\\nfor 300 timesteps of the ISM [Liang et al. 2024] variant of SDS and a single\\nquintic stroke. The cyan line emphasizes the centerline of the stroke, which\\nreaches zero width in certain regions.\\nFig. 13. Left, initialization with a saliency map computed from the nor-\\nmalized logits of the last layer of the OneFormer panoptic segmentation\\nmodel [Jain et al. 2023a]. Right, stroke optimization using ISM with diffusion\\nconditioned on the edge map, using a minimum time step of 400 and with\\nan additional stylization loss 𝜆style guided by the same image as Fig. 8.\\nyield finer details [Hwang et al. 2023]. Given our goal of producing\\nsingle stroke image abstractions, the curves lack sufficient degrees\\nof freedom to capture these finer details, so we limit the time steps in\\nthe denoising process to a minimum of 500 (Fig. 12). With a similar\\nmotivation, we find that with diffusion-guided stroke abstraction it\\nis useful to initialize the strokes with a multiplicity > 1 (we use 3\\nwith quintic splines in our examples). Using a higher multiplicity\\nresults in smoother strokes, where fewer details are captured.\\n4.3\\nArea-based image abstraction\\nOur method allows for the generation of smooth closed areas, and\\nwe observe that this is useful to generate image abstractions similar\\nto what can be seen in certain designs consisting of overlapping\\nsmooth regions and a limited color palette. Examples include psych-\\nedelic designs, album covers, screen-printed graphics, or street-art\\ninspired fashion and graphic designs. We are interested in generating\\noutputs that aim to be printed or fabricated as collages with a limited\\nnumber of regions and colors. To guide stylization, we use filled\\nareas instead of strokes and set LI to a variant of the CLIP-driven\\ngeometric cost described by Vinker et al. [2022]\\nLCLIP =\\n∑︁\\n𝑙\\n\\r\\rCLIP𝑙(ˆI) −CLIP𝑙(I𝜃)\\n\\r\\r\\n1 ,\\n(4)\\nusing the 𝐿1 norm instead of 𝐿2 and omitting the semantic term\\noriginally proposed by the authors. We use layers 2 and 3 together\\nwith the CLIPAG [Ganz and Elad 2024] architecture. We use CLIP as\\nopposed to diffusion because we find this to be significantly faster,\\nwhile being effective for this kind of stylization task.\\nRepulsion loss. For applications using closed curves, we adopt\\nthe repulsion method for 3D wire fabrication proposed by Tojo\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\nNeural Image Abstraction Using Long Smoothing B-Splines\\n•\\n225:7\\nFig. 14. Quantized color vectorizations using an additional image-driven\\nstylization term Lstyle. The palette is extracted from the style image.\\nFig. 15. Increasing 𝜆style weight for an abstract vectorization of Bach. From\\nleft to right 𝜆style = 0, 𝜆style = 0.06 , 𝜆style = 0.1\\net al. [2024] to compute a geometric loss Lrepul, penalizing self-\\nintersections and overlaps based on a tangent-point energy kernel\\nfor a set of sampled points along the spline. For this application, we\\ncompute the loss for each area separately, thus allowing overlaps\\nand intersections among different areas.\\nOptimization with quantized coloring. Jang et al. [2017] use the\\nGumbel-Softmax trick to make discrete choices differentiable during\\ntraining. We apply the same idea to assign colors to image regions,\\nusing soft selections from a fixed color palette that can be optimized\\nwith backpropagation. To progressively transit from soft to discrete\\nassignments during the training process, we anneal the Gumbel-\\nSoftmax temperature using an exponential schedule.\\nGiven a set of 𝐾palette colors organized as a matrix 𝑽∈IR𝐾×3,\\nwe optimize the logits per area ℓ𝑖∈R𝐾using a soft assignment\\n𝒂𝑖= softmax\\n\\x12ℓ𝑖+ g𝑖\\n𝜏\\n\\x13\\nwith\\n𝒈𝑖∼Gumbel(0, 𝛽)𝐾,\\nwhere 𝛽is a scale parameter that we empirically set to 0.15 to avoid\\nexcessive noise during optimization [Huijben et al. 2023] and 𝜏is\\na temperature parameter that we anneal during optimization. We\\nuse these soft colors computed as v𝑖= 𝒂⊤\\n𝑖𝑽during the optimization.\\nAt the same time, for visualization, we obtain hard assignments\\nby taking the argmax over the optimized logits and selecting the\\ncorresponding palette color. To encourage a balanced use of all the\\nspecified palette colors, we add a regularization term:\\n𝜆𝐾\\n\\r\\rE𝑖[𝒂𝑖] −𝐾−11\\n\\r\\r2\\nto LI, which penalizes deviations from a uniform color assignment,\\nencouraging a balanced use of the palette. Figures 14 and 15 show\\nsome results.\\nArea initialization and optimization. We initialize a user-defined\\nnumber of areas using weighted Voronoi sampling on a saliency map\\nof the input image and create an initial series of closed curves with\\nkeypoints given by the vertices of each resulting Voronoi regions.\\nEach curve is then assigned random initial logit and the curves\\nare sorted by increasing saliency of the covered area. Optimization\\nproceeds with the inclusion of the repulsion loss in LG, which keeps\\nthe area outlines from intersecting.\\n4.4\\nText stylization\\nIn line with the smooth curve image abstractions, we aim to generate\\ntext abstractions made of smooth curves that fit inside a target area.\\nExamples of this approach can be seen in posters, graphic designs,\\nas well as in “calligrams”: renditions of text that is arranged to fit\\na specific silhouette, such as those seen in the methods of Xu and\\nKaplan [2007] and Zou et al. [2016] (c.f. Figure 17). Our pipeline\\nresults in a simple way to generate calligrams, such as “blobby” texts\\n(Fig. 16) and abstract monospace fonts (Fig. 18).\\nWe tackle text stylization with the tools we have covered so far\\nand start with a bitmap image ˆI representing the desired silhouette\\nand an initial text layout rendered as a second image Itxt. We uni-\\nformly sample the glyph outlines and produce key-point sequences\\nused in optimization. The optimization deforms the outlines based\\non a loss that balances silhouette coverage, outline smoothness, and\\nrepulsion between outline points. This procedure alone smooths\\nand fits the outlines into the target area, but this may compromise\\nlegibility (Figure 18c).\\nTo preserve legibility, we introduce a perceptual loss based on the\\nfeatures of a pretrained vision encoder, which we use to compute the\\nfeature-space distance between the rendered deformed image I and\\nthe original layout Itxt. We find that using the last-layer [CLS] token\\nas feature of the TrOCR model [Li et al. 2023] and calculating a loss\\nbased on the 𝐿1-norm of the embeddings produce robust results for\\nthis application (Figure 18).\\nThe placement of glyph can be manual or automatic. In the auto-\\nmatic case, we optimize a similarity transform per glyph to maxi-\\nmize silhouette coverage while avoiding overlaps and maintaining\\na readable text layout. We first offset each glyph by a user-specified\\namount to encourage padding around the text. At each optimization\\nstep, we render both a morphologically opened version of the silhou-\\nette and glyphs into two images using white with 50% opacity on a\\nblack background. We minimize a loss that combines (i) a coverage\\nterm LI (Section 4.1), (ii) an overlap cost given by Í ReLU(𝑣−0.5)\\nfor each pixel intensity 𝑣∈[0, 1] of the rendered image and (iii)\\n(a)\\n(b)\\n(c)\\n(d)\\nFig. 16. Automatic calligram production for a silhouette generated with\\nthe prompt “Silhouette of a BUNNY“. (a) initial text layout rendered with\\n50% opacity and overlayed on the silhouette. (b) intermediate step of the\\nlayout optimization displaying an image area that increases the overlap\\ncost. (c) Sampled glyphs placed according to the layout. (d) Result of the\\noptimization.\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\n225:8\\n•\\nBerio et al.\\n(a)\\n(b)\\nFig. 17. Calligram generation: comparison of (a) an example from Zou et al.\\n[2016] for a camel silhouette and (b) two runs of our method on the same\\nsilhouette with automatic initialization and two different fonts.\\n(f)\\n(a)\\n(b)\\n(c)\\n(d)\\n(e)\\nFig. 18. Monospace font generation. (a) A letter “R” (quintic B-splines with\\njerk cost) adapted to a triangle, using 𝜆repul = 666, 𝜆txt = 6.6 and 𝜆smooth =\\n200.0. (b) Setting 𝜆repul = 0 (no repulsion), still results in a readable letter but\\n(c) removing the legibility loss does not. (d) B-splines with legibility but no\\nsmoothing. (e) Catmull-Rom to enforce tangent continuity with legibility\\nloss (for comparison). (f) Combining glyphs optimized to fit a triangle, a\\nsquare and a circle.\\nFig. 19. More calligrams generated with our system. The seagull silhouette\\nis generated using the prompt “Silhouette of a SEAGULL”.\\nan alignment cost Í ∥𝜃∥that penalizes the absolute turning an-\\ngles 𝜃between consecutive glyph center-points and maintains text\\nordering. We note that image generation models such as DALL-E\\n3 [Betker et al. 2023] are particularly effective at generating sil-\\nhouettes with a prompt, which finally results in a fully automatic\\ncalligram generation pipeline.\\n5\\nDiscussion\\nIn our example applications, we have seen how a B-spline reparame-\\ntrization can be used to generate long and expressive strokes and\\ncurves in a DiffVG pipeline. B-splines enforce high-order continu-\\nity by design, which enables analytic smoothing losses that help\\nproducing more regular geometry when combined with different\\nstylization losses. This offers a considerable advantage compared\\nto using only Bézier curves or parametrizations with lower order\\ncontinuity, especially for applications like the ones demonstrated\\nin this paper. Qualitative examples of this can be seen in examples\\nFig. 20. Stroke abstraction of Thelonious Monk. Left, using a single stroke\\nand Voronoi with TSP initialzation. Middle, using multiple strokes with\\nmultiple key-points along vertical lines. Right, using facial features extracted\\nwith MediaPipe [Lugaresi et al. 2019].\\nFig. 21. Smooth speed and acceleration of a quntic spline covering a num-\\nber “5” and optimized using the method of Section 4.1. With appropriate\\nresampling the path kinematics can be safely tracked with a robot.\\nFig. 22. Our method produces smooth kinematics that facilitate reproduc-\\ntion with a robot, and the varying width can be used to control brush\\npressure. Left and center, the robot reproducing portraits. Right, the robot\\nreproducing a stylized area fill.\\nsuch as Figure 8 and Figure 18. For conciseness, we used a simi-\\nlar Voronoi-based initialization strategy in most of our examples.\\nHowever, our method performs well with different initializations\\n(Fig. 20), which can serve as an additional design parameter to be\\nexplored by users.\\nOne common challenge in stroke-based abstraction pipelines is\\ncontrolling the trade-off between visual fidelity and geometric sim-\\nplicity. Previous methods typically address this by pre-determining\\nthe number of curves [Vinker et al. 2022] or by integrating a learned\\ncomponent into the optimization loop [Vinker et al. 2023]. We find\\nthat our use of smoothing, combined with optimizable stroke width,\\nallows this trade-off to be controlled parametrically and with the\\nnumber of strokes emerging from the optimization. This results in\\na solution that is significantly simpler than previous methods.\\nWe investigate the utility of our representations and different\\nloss terms in different examples of our applications. In Figure 18\\nwe perform a small qualitative ablation showing the effectiveness\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\nNeural Image Abstraction Using Long Smoothing B-Splines\\n•\\n225:9\\nof the proposed legibility loss (Figure 18c) as well as the benefits\\nof B-splines and smoothing compared to Catmull-Rom splines (Fig-\\nure 18e), which only enforce 𝐶1 continuity. In Figure 8a we can\\nobserve that directly optimizing Bézier curves effectively captures\\nfeatures of the example style image. However, higher degrees of\\nfreedom produce results that capture finer details at the expense of\\na clear stroke structure. Although this additional detail may be de-\\nsirable in certain applications, it is not suitable for the applications\\nconsidered in our work.\\nInterestingly, the computational overhead of the proposed B-\\nspline to Bézier matrix conversion is lower than the one for the\\nadditional optimization parameters required for an equivalent multi-\\nBézier curve. We tested performance with a simple comparison\\nwhere we cover an area by optimizing the 290 key-points of a single\\nopen cubic B-spline. We compared this to a similar setup directly\\noptimizing the corresponding 874 Bézier control points. On our\\nhardware setup, the Bézier case is 4.5 times slower. This shows that\\nthe additional cost of the proposed matrix conversion is negligible\\nand suggests that our method is an efficient way to enforce output\\ncontinuity in DiffVG settings.\\nRobotic reproduction. Optimizing splines with degree greater than\\nthree results in smooth acceleration profiles (Fig. 21). This enables\\na safe reproduction of the resulting trajectory kinematics with an\\narticulated robot arm (Fig. 22), without requiring an intermediate\\nreparameterization step. We tested this by reproducing the trajec-\\ntories using a 7-axis Franka robot equipped with a brush. We first\\ntransformed the control points to a desired workspace coordinate\\nsystem, treating the stroke widths as perpendicular distances to the\\ndrawing plane. We then sampled the trajectories at a resolution that\\nproduced a maximum speed and accelerations within the robot’s\\nmechanical limits. The inverse kinematics for the resulting trajecto-\\nries are then computed with an iterative linear quadratic regulator\\n(iLQR) [Li and Todorov 2004].\\n6\\nConclusions and future work\\nWe have presented a framework for integrating high-order B-splines\\ninto DiffVG pipelines together with minimum-square derivative-\\nbased smoothing costs. We have explored different applications and\\ndemonstrated how this enables the generation of long, smooth, and\\nstylized strokes through a combination of geometric and image-\\nspace loss functions. While the combination of losses allows for a\\nlarge variety of creative outputs, a practical challenge is the necessity\\nto weigh different losses to achieve the desired result, which, given\\nthe iterative optimization procedure, can be slow and tedious.\\nAlthough our formulation draws on a large body of existing work\\non B-splines, an effective use of this tool together with DiffVG is\\nnovel, and we expect it to be a valuable tool for the community. We\\nused uniform B-splines because of their simplicity and effectiveness\\nfor our use cases. However, exploring non-uniform parameteriza-\\ntions, such as NURBs, presents an interesting direction for further\\nresearch, as it may unlock additional flexibility and control for styl-\\nized outputs.\\nAcknowledgments\\nThis work was funded by the EACVA (Embodied Agents in Contem-\\nporary Visual Art) Project, led by Goldsmiths (UKRI/AHRC grant\\nAH/X002241/1) and the University of Konstanz (grant 508324734,\\nDeutsche Forschungsgemeinschaft/DFG). Special thanks to Guil-\\nlaume Clivaz (Idiap Research Institute) for the technical support\\nand useful discussions.\\nA\\nB-Spline details\\nA B-spline (or basis-spline) or order 𝑘is a piecewise polynomial\\ncurve of degree𝑝= 𝑘−1 defined by a linear combination of𝑛weights\\nor control points c0, c1, . . . c𝑛−1 and a non-decrasing sequence of\\n𝑚= 𝑛+ 𝑘knots (or breakpoints) 𝑡0,𝑡1,𝑡2, . . . ,𝑡𝑚−1.\\n𝒙(𝑢) =\\n𝑛−1\\n∑︁\\n𝑖=0\\nc𝑖𝐵𝑖,𝑘(𝑢)\\nEach basis function 𝐵𝑖,𝑘defines 𝑘polynomial segments spanning\\n𝑘+ 1 knots 𝑡𝑖,𝑡𝑖+1, . . . ,𝑡𝑖+𝑘and is positive in the half-open domain\\n[𝑡𝑖,𝑡𝑖+ 𝑘). The knots between 𝑡𝑝and 𝑡𝑚−𝑘(not included) are called\\n“internal” or “interior” knots. From here: For 𝑛control points we\\nhave 𝑛+ 𝑘knots and 𝑛−𝑘interior knots.\\nB-spline bases can be defined through the “Cox-de Boor” recur-\\nsion starting from order 1 (degree 0):\\n𝐵𝑖,1(𝑢) =\\n(\\n1\\nif 𝑡𝑖≤𝑢< 𝑡𝑖+1\\n0\\notherwise\\nAnd with\\n𝐵𝑖,𝑘(𝑢) =\\n𝑢−𝑡𝑖\\n𝑡𝑖+𝑘−1 −𝑡𝑖\\n𝐵𝑖,𝑘−1(𝑢) + 𝑡𝑖+𝑘−𝑢\\n𝑡𝑖+𝑘−𝑡𝑖+1\\n𝐵𝑖+1,𝑘−1(𝑢)\\nThe number of control points 𝑛, order 𝑘and number of knots 𝑚\\nare related by 𝑛+ 𝑘−𝑚= 0. For nonrepeating knot sequences, the\\ncurve will be 𝐶𝑘−2 continuously differentiable.\\nA.1\\nDerivatives\\nThe derivative of a B-spline basis function of order 𝑘is given by\\nd\\nd𝑢𝐵𝑖,𝑘(𝑢) = 𝐵′\\n𝑖,𝑘(𝑢) =\\n𝑘−1\\n𝑡𝑖+𝑘−1 −𝑡𝑖\\n𝐵𝑖,𝑘−1(𝑢) −\\n𝑘−1\\n𝑡𝑖+𝑘−𝑡𝑖+1\\n𝐵𝑖+1,𝑘−1(𝑢).\\nIt is a linear combination of all the derivatives of the basis function.\\nAs a result, the derivative of a B-spline is equivalent to a B-spline\\nof order 𝑘−1 with a new set of control points given by weighted\\ndifferences of pairs of consecutive control points.\\nA.2\\nCardinal B-splines\\nA cardinal B-spline (not to be confused with cardinal/Catmull-Rom\\nsplines) is a “normalized uniform B-spline”. It has uniformly spaced\\nknots, with 𝑡𝑖+1 −𝑡𝑖= ℎ(uniform) with ℎ= 1 (normalized) so\\nthe knots are all integers (Fig. 23). Uniformity and normalization\\nsimplify the computations of a B-spline as all basis functions are\\ntranslated versions of the same basis function that we denote as\\n𝑁𝑘(𝑢). We then have\\n𝒙(𝑢) =\\n𝑛−1\\n∑︁\\n𝑖=0\\nc𝑖𝑁𝑘(𝑢−𝑡𝑖)\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\n225:10\\n•\\nBerio et al.\\nand the B-spline derivatives simplify to\\nd\\nd𝑢𝑁𝑘(𝑢) = 𝑁′\\n𝑘(𝑢) = 𝑁𝑘−1(𝑢) −𝑁𝑘−1(𝑢−1)\\nso\\n¤𝒙(𝑢) =\\n𝑛−1\\n∑︁\\n𝑖=0\\nc𝑖(𝑁𝑘−1(𝑢−𝑡𝑖) −𝑁𝑘−1(𝑢−𝑡𝑖−1))\\nA.3\\nSmoothing term\\nThe smoothing term can be computed exactly and is considerably\\nsimplified for the case of cardinal B-splines [Schumaker 1981]. While\\ndifferent approaches exist to calculate this kind of integral [de Boor\\net al. 1976; Vermeulen et al. 1992] to calculate this kind of integral,\\nwe follow Fujioka and Kano [2007] and Fujioka et al. [2017] to have\\nL𝑑\\nsmooth =\\n∫∞\\n−∞\\n𝐷(𝑢) d𝑢−\\n∫𝑡𝑘−1\\n−∞\\n𝐷(𝑢) d𝑢−\\n∫∞\\n𝑡𝑛\\n𝐷(𝑢) d𝑢\\nwith 𝐷(𝑢) = ∥𝒙(𝑑) (𝑢)∥2\\nThis can be computed explicitly by constructing a Gramian 𝑮\\nwith:\\n𝐺𝑖,𝑗=\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n∫𝑘\\n0 𝑁(𝑑)\\n𝑖,𝑗d𝑢−\\n∫𝑝−𝑖\\n0\\n𝑁(𝑑)\\n𝑖,𝑗d𝑢\\nif 𝑖< 𝑝and 𝑗< 𝑝\\n∫𝑘\\n0 𝑁(𝑑)\\n𝑖,𝑗d𝑢−\\n∫𝑝−𝑖\\n0\\n𝑁(𝑑)\\n𝑛+𝑝−𝑖,𝑛+𝑝−𝑗d𝑢\\nif 𝑖≥𝑛and 𝑗≥𝑛\\n∫𝑘\\n0 𝑁(𝑑)\\n𝑖,𝑗d𝑢\\notherwise\\nand\\n𝑁(𝑑)\\n𝑖,𝑗\\n= 𝑁(𝑑)\\n𝑘\\n(𝑢)𝑁(𝑑)\\n𝑘\\n(𝑢−𝑗+ 𝑖)\\n.\\nThen each 𝐺𝑖,𝑗can be computed exactly using quadrature [Ver-\\nmeulen et al. 1992].\\nIf we let 𝒄∈IR𝑛𝐷be a vector that concatenates 𝑛control points,\\neach of dimensions 𝐷we have\\nL𝑑\\nsmooth = 𝒄⊤¯𝑮𝒄,\\n¯𝑮= 𝑮⊗𝑰𝐷\\nwhere ⊗is the Kroenecker product and 𝑰𝐷is the identity matrix of\\ndimensions 𝐷.\\n2\\n0\\n2\\n4\\n6\\n8\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nDegree 3\\n4\\n2\\n0\\n2\\n4\\n6\\n8\\nu\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nDegree 4\\nFig. 23. B-Splines and their bases with degrees 3 and 4.\\nP-splines. A similar procedure can be efficiently approximated\\nwith discretization of the derivative cost, using penalized-splines\\n(P-splines) as described by Eilers and Marx [1996]. To do this, we\\ncan simply use 𝑮= 𝑫⊤\\n(𝑑)𝑫(𝑑) with 𝑫(𝑑) a matrix representing the\\nfinite difference operator of order 𝑑. The advantage of this method\\nis the simplicity of implementation and the possibility of achieving\\nsimilar smoothing results. We can arbitrarily combine the degree of\\ndiscrete differences with the degree of the curve. We expose both\\nmethods for completeness and to enable applications where the\\nintegral cost may be necessary (e.g., planning and robotics).\\nA.4\\nConversion to Bézier\\nWith the method of Romani and Sabin [2004], converting the 𝑝+ 1\\ncontrol points of a quintic B-spline of degree 𝑝to single Bézier\\nsegment of the same degree, can be done with a (𝑝+ 1) × (𝑝+ 1)\\nmatrix that we denote as 𝑺𝑝. To convert all the control points of a\\nB-spline we stack multiple shifted and overlapping copies of 𝑺𝑝into\\na larger matrix 𝑺, by shifting each copy by 𝑝rows and 1 column.\\nFor a quintic spline this can be visualized as:\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\nThe blocks for a quintic spline are given by:\\n𝑺5 =\\n1\\n120\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n26\\n66\\n26\\n1\\n0\\n0\\n16\\n66\\n36\\n2\\n0\\n0\\n8\\n60\\n48\\n4\\n0\\n0\\n4\\n48\\n60\\n8\\n0\\n0\\n2\\n36\\n66\\n16\\n0\\n0\\n1\\n26\\n66\\n26\\n1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nThe block matrix ¯𝑺used to compute the Bézier control points from\\nthe flattend B-spline control points 𝒄is given by the Kroenecker\\nproduct 𝑺⊗𝑰𝐷.\\nA.5\\nDegree reduction\\nWith the method of Sunwoo [2005], reducing a Bézier curve of\\ndegree 𝑝to one of degree 𝑞can be done with a (𝑞+ 1) × (𝑝+ 1)\\nmatrix that we denote as 𝑹𝑝,𝑞. To reduce the degree of all the control\\npoints of a Bézier chain we stack multiple shifted and overlapping\\ncopies of 𝑹𝑝,𝑞into a larger matrix 𝑹by shifting each copy by 𝑝rows\\nand 𝑞columns. For a reduction from quintic to cubic this can be\\nvisualized as:\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\nNeural Image Abstraction Using Long Smoothing B-Splines\\n•\\n225:11\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\nThe blocks of the quintic to cubic reduction matrix are given by\\n𝑹5,3 =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n0\\n0\\n0\\n0\\n0\\n−2\\n3\\n5\\n3\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n5\\n3\\n−2\\n3\\n0\\n0\\n0\\n0\\n0\\n1\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nThe block matrix ¯𝑹used to compute the reduced Bézier control\\npoints from is given by the Kroenecker product 𝑹⊗𝑰𝐷.\\nReferences\\nDaniel Berio, Sylvain Calinon, and Frederic Fol Leymarie. 2017. Generating Calligraphic\\nTrajectories with Model Predictive Control. In Proceedings of Graphics Interface.\\nCanadian Human-Computer Communications Society, Edmonton, Canada.\\nJames Betker, Gabriel Goh, Li Jing, Aditya Ramesh, et al. 2023. Improving Image\\nGeneration with Better Captions. Technical Report. OpenAI. https://cdn.openai.\\ncom/papers/dall-e-3.pdf\\nCarl De Boor. 2001. A Practical Guide to Splines (1 ed.). Springer, New York, NY.\\nCarl de Boor, Tom Lyche, and Larry L Schumaker. 1976. On Calculating with B-splines\\nII. Integration.\\nNumerische Methoden der Approximationstheorie/Numerical\\nMethods of Approximation Theory: Vortragsauszüge der Tagung über numerische\\nMethoden der Approximationstheorie vom 25. bis 31. Mai 1975 im Mathematischen\\nForschungsinstitut Oberwolfach (Schwarzwald) (1976), 123–146.\\nTony D. DeRose and Brian A. Barsky. 1988. Geometric Continuity, Shape Parameters,\\nand Geometric Constructions for Catmull-Rom Splines. ACM Transactions on\\nGraphics 7, 1 (Jan. 1988), 1–41. doi:10.1145/42188.42265\\nShimon Edelman and Tamar Flash. 1987.\\nA Model of Handwriting.\\nBiological\\nCybernetics 57, 1-2 (1987), 25–36.\\nMagnus Egerstedt and Clyde Martin. 2009. Control Theoretic Splines: Optimal Control,\\nStatistics, and Path Planning. Princeton University Press, Princeton. doi:10.1515/\\n9781400833870\\nPaul H. C. Eilers and Brian D. Marx. 1996. Flexible Smoothing with B-splines and\\nPenalties. Statist. Sci. 11, 2 (May 1996). doi:10.1214/ss/1038425655\\nGerald E. Farin. 2001. Curves and Surfaces for CAGD: A Practical Guide (5th ed ed.).\\nMorgan Kaufmann, San Francisco, CA.\\nT Flash and N Hogan. 1985. The Coordination of Arm Movements: An Experimentally\\nConfirmed Mathematical Model. The Journal of Neuroscience 5, 7 (July 1985), 1688–\\n1703. doi:10.1523/JNEUROSCI.05-07-01688.1985\\nTamar Flash and Neville Hogan. 1998. Optimization Principles in Motor Control. In\\nThe Handbook of Brain Theory and Neural Networks. MIT Press, Cambridge, MA,\\nUSA, 682–685.\\nKevin Frans, Lisa Soros, and Olaf Witkowski. 2022. CLIPDraw: Exploring Text-to-\\nDrawing Synthesis through Language-Image Encoders. In Advances in Neural\\nInformation Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,\\nK. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 5207–5218.\\nHiroyuki Fujioka and Hiroyuki Kano. 2007. Constructing Character Font Models\\nfrom Measured Human Handwriting Motion. In 2007 American Control Conference.\\nIEEE, New York, NY, USA, 1467–1472. doi:10.1109/ACC.2007.4282784\\nH. Fujioka, H. Kano, H. Nakata, and H. Shinoda. 2006. Constructing and Reconstruct-\\ning Characters, Words, and Sentences by Synthesizing Writing Motions. IEEE\\nTransactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 36,\\n4 (July 2006), 661–670. doi:10.1109/TSMCA.2005.851344\\nHiroyuki Fujioka, Wenli Zhu, Akinori Hidaka, and Hiroyuki Kano. 2017.\\nRecon-\\nstructing Dynamic Font-Based Chinese Characters Using Support Vector Machine.\\nIn 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC).\\n2408–2413. doi:10.1109/SMC.2017.8122983\\nRinon Gal, Yael Vinker, Yuval Alaluf, Amit Bermano, Daniel Cohen-Or, Ariel Shamir,\\nand Gal Chechik. 2024. Breathing Life into Sketches Using Text-to-Video Pri-\\nors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR). 4325–4336.\\nRoy Ganz and Michael Elad. 2024. CLIPAG: Towards Generator-Free Text-to-Image Gen-\\neration. In 2024 IEEE/CVF Winter Conference on Applications of Computer Vision\\n(WACV). IEEE, Waikoloa, HI, USA, 3831–3841. doi:10.1109/WACV57701.2024.00380\\nNeville Hogan and Dagmar Sternad. 2009. Sensitivity of smoothness measures to\\nmovement duration, amplitude, and arrests. J. Mot. Behav. 41, 6 (Nov. 2009), 529–\\n534.\\nIris A. M. Huijben, Wouter Kool, Max B. Paulus, and Ruud J. G. van Sloun. 2023. A\\nReview of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in\\nMachine Learning . IEEE Transactions on Pattern Analysis & Machine Intelligence\\n45, 02 (Feb. 2023), 1353–1371. doi:10.1109/TPAMI.2022.3157042\\nJuno Hwang, Yong-Hyun Park, and Junghyo Jo. 2023. Resolution Chromatography of\\nDiffusion Models. arXiv:2401.10247 [cs] doi:10.48550/arXiv.2401.10247\\nShir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, and Ariel Shamir.\\n2023. Word-as-Image for Semantic Typography. ACM Transactions on Graphics 42,\\n4, Article 151 (July 2023). doi:10.1145/3592123\\nAjay Jain, Amber Xie, and Pieter Abbeel. 2023b. VectorFusion: Text-to-SVG by Abstract-\\ning Pixel-Based Diffusion Models. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR). 1911–1920.\\nJitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey\\nShi. 2023a.\\nOneFormer: One Transformer To Rule Universal Image Segmen-\\ntation. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition. 2989–2998.\\nEric Jang, Shixiang Gu, and Ben Poole. 2017.\\nCategorical Reparametrization\\nwith Gumbel-Softmax. In Proceedings International Conference on Learning\\nRepresentations (ICLR). https://openreview.net/pdf?id=rkE3y85ee\\nHiroyuki Kano, Magnus Egerstedt, Hiroaki Nakata, and Clyde F. Martin. 2003. B-Splines\\nand Control Theory. Appl. Math. Comput. 145, 2 (2003), 263–288. doi:10.1016/S0096-\\n3003(02)00486-1\\nHiroyuki Kano, Hiroaki Nakata, and Clyde F. Martin. 2005. Optimal Curve Fitting\\nand Smoothing Using Normalized Uniform B-splines: A Tool for Studying Complex\\nSystems. Appl. Math. Comput. 169, 1 (Oct. 2005), 96–128. doi:10.1016/j.amc.2004.\\n10.034\\nCraig S Kaplan and Robert Bosch. 2005. TSP Art. In Renaissance Banff: Mathematics,\\nMusic, Art, Culture. 301–308.\\nHiroharu Kato, Deniz Beker, Mihai Morariu, Takahiro Ando, Toru Matsuoka,\\nWadim Kehl, and Adrien Gaidon. 2020.\\nDifferentiable Rendering: A Survey.\\narXiv:2006.12057 [cs] doi:10.48550/arXiv.2006.12057\\nOren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. 2024. Noise-Free Score\\nDistillation. In The Twelfth International Conference on Learning Representations.\\nGihyun Kwon and Jong Chul Ye. 2022.\\nCLIPstyler: Image Style Transfer with a\\nSingle Text Condition. In 2022 IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR). IEEE, New Orleans, LA, USA, 18041–18050. doi:10.\\n1109/CVPR52688.2022.01753\\nMinghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\\nZhoujun Li, and Furu Wei. 2023. TrOCR: Transformer-based Optical Character\\nRecognition with Pre-trained Models. In AAAI 2023.\\nTzu-Mao Li, Michal Lukáč, Michaël Gharbi, and Jonathan Ragan-Kelley. 2020. Differen-\\ntiable Vector Graphics Rasterization for Editing and Learning. ACM Transactions\\non Graphics 39, 6 (Dec. 2020), 1–15. doi:10.1145/3414685.3417871\\nW. Li and E. Todorov. 2004. Iterative Linear Quadratic Regulator Design for Nonlin-\\near Biological Movement Systems. In Proc. Intl Conf. on Informatics in Control,\\nAutomation and Robotics (ICINCO). 222–229.\\nYixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Ying-Cong Chen.\\n2024. LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval\\nScore Matching. In Proceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition.\\nChenxi Liu, Jessica Hodgins, and James McCann. 2017. Whole-cloth quilting pat-\\nterns from photographs. In Proceedings of the Symposium on Non-Photorealistic\\nAnimation and Rendering (Los Angeles, California) (NPAR ’17). Association for\\nComputing Machinery, New York, NY, USA, Article 7, 8 pages. doi:10.1145/3092919.\\n3092925\\nLizheng Lu. 2015. A Note on Curvature Variation Minimizing Cubic Hermite Inter-\\npolants. Appl. Math. Comput. 259 (May 2015), 596–599. doi:10.1016/j.amc.2014.11.\\n113\\nCamillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja,\\nMichael Hays, Fan Zhang, Chuo-Ling Chang, Ming Yong, Juhyun Lee, Wan-\\nTeh Chang, Wei Hua, Manfred Georg, and Matthias Grundmann. 2019. Medi-\\naPipe: A Framework for Perceiving and Processing Reality. In Third Workshop\\non Computer Vision for AR/VR at IEEE Computer Vision and Pattern Recognition\\n(CVPR) 2019.\\nhttps://mixedreality.cs.cornell.edu/s/NewTitle_May1_MediaPipe_\\nCVPR_CV4ARVR_Workshop_2019.pdf\\nRon Maharik, Mikhail Bessmeltsev, Alla Sheffer, Ariel Shamir, and Nathan Carr. 2011.\\nDigital micrography. ACM Transactions on Graphics 30, 4, Article 100 (July 2011),\\n12 pages. doi:10.1145/2010324.1964995\\nHirotsugu Matsukida and Hiroyuki Fujioka. 2013. Modeling and Reshaping Handwritten\\nCharacters Based on Dynamic Font Model. In 2013 International Joint Conference\\non Awareness Science and Technology & Ubi-Media Computing (iCAST 2013 &\\nUMEDIA 2013). IEEE, Aizuwakamatsu, Japan, 344–350. doi:10.1109/ICAwST.2013.\\n6765463\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n\\n225:12\\n•\\nBerio et al.\\nH. Meier and H. Nowacki. 1987. Interpolating Curves with Gradual Changes in Curva-\\nture. Computer Aided Geometric Design 4, 4 (Dec. 1987), 297–305. doi:10.1016/0167-\\n8396(87)90004-5\\nDaniel Mellinger and Vijay Kumar. 2011. Minimum Snap Trajectory Generation and\\nControl for Quadrotors. In 2011 IEEE International Conference on Robotics and\\nAutomation. 2520–2525. doi:10.1109/ICRA.2011.5980409\\nDaniela Mihai and Jonathon Hare. 2021.\\nDifferentiable Drawing and Sketching.\\narXiv:2103.16194 [cs]\\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Text-\\nto-3D Using 2D Diffusion. In The Eleventh International Conference on Learning\\nRepresentations.\\nH. Pottmann. 1990. Smooth Curves under Tension. Computer-Aided Design 22, 4 (May\\n1990), 241–245. doi:10.1016/0010-4485(90)90053-F\\nZhiyu Qu, Lan Yang, Honggang Zhang, Tao Xiang, Kaiyue Pang, and Yi-Zhe Song.\\n2024. Wired Perspectives: Multi-View Wire Art Embraces Generative AI. In 2024\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,\\nSeattle, WA, USA, 6149–6158. doi:10.1109/CVPR52733.2024.00588\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\\nKrueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From\\nNatural Language Supervision. In Proceedings of the 38th International Conference\\non Machine Learning. PMLR, 8748–8763.\\nChristian H. Reinsch. 1967. Smoothing by Spline Functions. Numer. Math. 10, 3 (Oct.\\n1967), 177–183. doi:10.1007/BF02162161\\nKejia Ren and Paul G. Kry. 2019.\\nSingle Stroke Aerial Robot Light Paint-\\ning. In Proceedings of the 8th ACM/Eurographics Expressive Symposium\\non Computational Aesthetics and Sketch Based Interfaces and Modeling and\\nNon-Photorealistic Animation and Rendering (Expressive ’19). Eurographics As-\\nsociation, Goslar, DEU, 61–67. doi:10.2312/exp.20191077\\nL Romani and M.A Sabin. 2004. The Conversion Matrix between Uniform B-spline\\nand Bézier Representations. Computer Aided Geometric Design 21, 6 (July 2004),\\n549–560. doi:10.1016/j.cagd.2004.04.002\\nPeter Schaldenbrand, James McCann, and Jean Oh. 2023. FRIDA: A Collaborative\\nRobot Painter with a Differentiable, Real2Sim2Real Planning Environment. In 2023\\nIEEE International Conference on Robotics and Automation (ICRA). 11712–11718.\\ndoi:10.1109/ICRA48891.2023.10160702\\nLarry L. Schumaker. 1981. Spline Functions: Basic Theory.\\nAdrian Secord. 2002.\\nWeighted Voronoi Stippling. In Proceedings of the 2nd\\nInternational Symposium on Non-photorealistic Animation and Rendering. ACM,\\nAnnecy France, 37–43. doi:10.1145/508530.508537\\nHasik Sunwoo. 2005. Matrix Representation for Multi-Degree Reduction of Bézier\\nCurves. Computer Aided Geometric Design 22, 3 (March 2005), 261–273. doi:10.\\n1016/j.cagd.2004.12.002\\nA. Tewari, O. Fried, J. Thies, V. Sitzmann, S. Lombardi, K. Sunkavalli, R. Martin-Brualla,\\nT. Simon, J. Saragih, M. Nießner, R. Pandey, S. Fanello, G. Wetzstein, J.-Y. Zhu, C.\\nTheobalt, M. Agrawala, E. Shechtman, D. B Goldman, and M. Zollhöfer. 2020. State\\nof the Art on Neural Rendering. Computer Graphics Forum 39, 2 (2020), 701–727.\\ndoi:10.1111/cgf.14022\\nE. Todorov. 2004. Optimality Principles in Sensorimotor Control. Nature Neuroscience\\n7, 9 (2004), 907–915.\\nEmanuel Todorov and Michael I. Jordan. 1998. Smoothness Maximization Along a\\nPredefined Path Accurately Predicts the Speed Profiles of Complex Arm Movements.\\nJournal of Neurophysiology 80, 2 (Aug. 1998), 696–714. doi:10.1152/jn.1998.80.2.696\\nKenji Tojo, Ariel Shamir, Bernd Bickel, and Nobuyuki Umetani. 2024.\\nFab-\\nricable 3D Wire Art. In Special Interest Group on Computer Graphics and\\nInteractive Techniques Conference Conference Papers ’24. ACM, Denver CO USA,\\n1–11. doi:10.1145/3641519.3657453\\nZhifang Tong, Bolei Zuo, Xiaoxia Yang, Shengjun Liu, and Xinru Liu. 2025. Continuous-\\nLine Image Stylization Based on Hilbert Curve. Computer Graphics Forum (2025).\\ndoi:10.1111/cgf.70169\\nM. Toussaint. 2017. A Tutorial on Newton Methods for Constrained Trajectory Opti-\\nmization and Relations to SLAM, Gaussian Process Smoothing, Optimal Control, and\\nProbabilistic Inference. In Geometric and Numerical Foundations of Movements.\\nSpringer International Publishing, Cham, 361–392.\\nAlan H Vermeulen, Richard H Bartels, and Glenn R Heppler. 1992. Integrating Products\\nof B-splines. SIAM journal on scientific and statistical computing 13, 4 (1992), 1025–\\n1038.\\nYael Vinker, Yuval Alaluf, Daniel Cohen-Or, and Ariel Shamir. 2023.\\nCLI-\\nPascene: Scene Sketching with Different Types and Levels of Abstraction. In\\n2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Paris,\\nFrance, 4123–4133. doi:10.1109/ICCV51070.2023.00383\\nYael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Haim\\nBermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. 2022.\\nCLIPasso:\\nSemantically-Aware Object Sketching. ACM Transactions on Graphics 41, 4, Article\\n86 (July 2022). doi:10.1145/3528223.3530068\\nPauli Virtanen et al. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing\\nin Python. Nature Methods 17 (2020), 261–272. doi:10.1038/s41592-019-0686-2\\nViviani, Paolo and Flash, Tamar. 1995. Minimum-Jerk, Two-Third Power Law, and\\nIsochrony, Converging Approaches to Motor Planning.\\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan LI, Hang Su, and Jun Zhu.\\n2023. ProlificDreamer: High-fidelity and Diverse Text-to-3D Generation with Vari-\\national Score Distillation. In Advances in Neural Information Processing Systems,\\nA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36.\\nCurran Associates, Inc., 8406–8441.\\nFernando J. Wong and Shigeo Takahashi. 2011. A Graph-based Approach to Continuous\\nLine Illustrations with Variable Levels of Detail. Computer Graphics Forum 30, 7\\n(2011), 1931–1939. arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-\\n8659.2011.02040.x doi:10.1111/j.1467-8659.2011.02040.x\\nMarkus Worchel and Marc Alexa. 2023. Differentiable Rendering of Parametric Ge-\\nometry. ACM Transactions on Graphics 42, 6, Article 232 (Dec. 2023), 18 pages.\\ndoi:10.1145/3618387\\nXiMing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian Yu, and Dong Xu. 2023.\\nDiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models.\\nIn Thirty-seventh Conference on Neural Information Processing Systems.\\nhttps:\\n//openreview.net/forum?id=CY1xatvEQj\\nXiming Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. 2024.\\nSVGDreamer: Text Guided SVG Generation with Diffusion Model. (June 2024),\\n4546–4555.\\nJie Xu and Craig S. Kaplan. 2007.\\nCalligraphic Packing. In Proceedings of\\nGraphics Interface 2007 on - GI ’07. ACM Press, Montreal, Canada, 43. doi:10.1145/\\n1268517.1268527\\nZhijin Yang, Pengfei Xu, Hongbo Fu, and Hui Huang. 2021. WireRoom: model-guided\\nexplorative design of abstract wire art. ACM Transactions on Graphics 40, 4, Article\\n128 (July 2021), 13 pages. doi:10.1145/3450626.3459796\\nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. IP-adapter: Text Compatible\\nImage Prompt Adapter for Text-to-Image Diffusion Models. arXiv:2308.06721 [cs.CV]\\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding Conditional Control\\nto Text-to-Image Diffusion Models. In 2023 IEEE/CVF International Conference on\\nComputer Vision (ICCV). IEEE, Paris, France, 3813–3824. doi:10.1109/ICCV51070.\\n2023.00355\\nPeiying Zhang, Nanxuan Zhao, and Jing Liao. 2024. Text-to-Vector Generation with\\nNeural Path Representation. ACM Transactions on Graphics 43, 4 (July 2024), 1–13.\\ndoi:10.1145/3658204\\nChangqing Zou, Junjie Cao, Warunika Ranaweera, Ibraheem Alhashim, Ping Tan, Alla\\nSheffer, and Hao Zhang. 2016. Legible Compact Calligrams. ACM Transactions on\\nGraphics 35, 4 (July 2016), 1–12. doi:10.1145/2897824.2925887\\nACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.\\n',\n",
       " 'A Hybrid Deep Learning based Carbon Price Forecasting\\nFramework with Structural Breakpoints Detection and Signal\\nDenoising\\nRunsheng Rena,1, Jing Lia,1, Yanxiu Lia, Shixun Huanga,∗, Jun Shena, Wanqing Lia, John Lea,\\nSheng Wangb\\naSchool of Computing and Information Technology, University of Wollongong, Wollongong, Australia\\nbSchool of Computer Science, Wuhan University, Wuhan, China\\nAbstract\\nAccurately forecasting carbon prices is essential for informed energy market decision-making,\\nguiding sustainable energy planning, and supporting effective decarbonization strategies. How-\\never, it remains challenging due to structural breaks and high-frequency noise caused by frequent\\npolicy interventions and market shocks. Existing studies, including the most recent baseline ap-\\nproaches, have attempted to incorporate breakpoints but often treat denoising and modeling as\\nseparate processes and lack systematic evaluation across advanced deep learning architectures,\\nlimiting the robustness and the generalization capability. To address these gaps, this paper pro-\\nposes a comprehensive hybrid framework that integrates structural break detection (Bai–Perron,\\nICSS, and PELT algorithms), wavelet signal denoising, and three state-of-the-art deep learning\\nmodels (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot prices from\\n2007 to 2024 and exogenous features such as energy prices and policy indicators, the framework\\nconstructs univariate and multivariate datasets for comparative evaluation. Experimental results\\ndemonstrate that our proposed PELT-WT-TCN achieves the highest prediction accuracy, reduc-\\ning forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the state-of-the-art\\nbaseline model (Breakpoints with Wavelet and LSTM), and by 70.55% in RMSE and 74.42%\\nin MAE compared to the original LSTM without decomposition from the same baseline study.\\nThese findings underscore the value of integrating structural awareness and multiscale decom-\\nposition into deep learning architectures to enhance accuracy and interpretability in carbon price\\nforecasting and other nonstationary financial time series.\\nKeywords:\\nCarbon price forecasting, European Union Emissions Trading System, structural breakpoints\\ndetection, wavelet transform, deep learning\\n∗Corresponding author.\\nEmail address: shixunh@uow.edu.au (Shixun Huang)\\n1These authors contributed equally as co-first authors.\\narXiv:2511.04988v1  [cs.LG]  7 Nov 2025\\n\\n1. Introduction\\nBackground. Against the backdrop of increasingly severe global climate change, carbon mar-\\nkets have become a vital economic tool for controlling carbon emissions. Among them, the\\nEuropean Union Emissions Trading System (EU ETS) is the most mature and influential carbon\\ntrading framework in the world (European Commission, 2020). Within this system, European\\nUnion Allowances (EUA), the de facto tradable permits for greenhouse gas emissions, serve as\\na market indicator. Their prices (i.e., the \"EU carbon price\") (Koch et al., 2014) not only re-\\nflect supply and demand dynamics but also convey changes in macroeconomic conditions and\\npolicy signals. Therefore, accurately forecasting EUA carbon prices is crucial for policy for-\\nmulation, enabling hybrid data-driven and physical modeling approaches, supporting intelligent\\nenergy management, guiding decarbonization roadmaps, and addressing both energy system and\\nsocietal impacts.\\nHowever, the dynamic nature of the carbon market makes forecasting particularly complex.\\nUnlike traditional financial assets, carbon prices are heavily influenced by frequent policy shocks,\\nsuch as cap adjustments, energy price volatility, and geopolitical disruptions (Koch et al., 2014).\\nThese external factors often result in structural breaks and high-frequency noise (Lin and Zhang,\\n2022), making carbon prices highly nonlinear and nonstationary, which poses serious challenges\\nto existing modeling and forecasting techniques.\\nExisting Approaches and their Limitations. Although numerous studies have attempted to\\nforecast EUA carbon prices (Huang et al., 2021; Lin and Zhang, 2022), most models still rely\\non linear econometric approaches or fail to fully account for structural changes and noise in\\nthe carbon price series. For example, models such as ARIMA (Box et al., 1976), GARCH\\n(Bollerslev, 1986) have been widely applied. More recently, deep learning frameworks, includ-\\ning LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), and carbon price spe-\\ncific approaches (Zhang and Wen, 2022) have also emerged. While these models demonstrate\\nstrong predictive performance during stable periods, they often suffer from underfitting, lagged\\nresponses, and poor generalization when confronted with major institutional changes or macroe-\\nconomic disruptions, making them inadequate for real-world forecasting tasks.\\nWhile some recent works attempt to address these challenges, important limitations still re-\\nmain. For instance, Liu et al. (2012) applied wavelet analysis to carbon allowance price dynam-\\nics, showing that time and frequency methods can capture structural variations in carbon price\\nseries. Building on such approaches, later studies (Lin and Zhang, 2022) have developed hybrid\\nmodels that integrate structural breakpoint detection (e.g., Bai–Perron and ICSS algorithms) and\\nwavelet based denoising with deep learning architectures such as LSTM, which significantly im-\\nproved prediction accuracy. However, these studies typically employ only a single deep learning\\narchitecture (LSTM) without comparative evaluation of alternative models (e.g., GRU, TCN (Bai\\net al., 2018)), limiting the robustness assessment. On the other hand, breakpoint detection is\\nconducted solely on carbon price series, neglecting macroeconomic or policy driven external\\nfeatures, which limits interpretability. Moreover, most of these methods rely on Bai–Perron or\\nICSS, which are computationally intensive and less flexible for large-scale datasets. To over-\\ncome this, our study adopts the PELT algorithm, which offers linear computational complexity\\nand can detect multiple change points efficiently, making it particularly suitable for incorporating\\nmultivariate external factors alongside carbon price series. Further, despite employing wavelet\\ndenoising, they do not systematically explore multiscale decomposition, potentially missing im-\\nportant frequency domain dynamics.\\n2\\n\\nOur goals and Innovations. To address the aforementioned issues, our proposed framework\\nintegrates PELT breakpoints detection, wavelet signal decomposition, comparative evaluation\\nof multiple deep learning models, and the incorporation of multisource external features. This\\nintegrated design significantly enhances the robustness, accuracy, and interpretability of carbon\\nprice forecasting by:\\n• Integrating methods into a unified pipeline: aligning training data with stable market\\nregimes through breakpoint detection (Killick et al., 2012; Bai and Perron, 2003; Inclan\\nand Tiao, 1994), applying wavelet analysis to highlight key dynamics in carbon price\\ndata (Liu et al., 2012), and linking breaks to specific policy or economic events for clearer\\ninterpretation (Lin and Zhang, 2022).\\n• Comparative evaluation of multiple deep learning architectures: implementing LSTM\\n(Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), and TCN (Bai et al., 2018)\\nto systematically assess robustness and generalizability across different scenarios.\\n• Incorporating external features: expanding the feature space with commodity prices,\\nexchange rates, and policy signals to capture macroeconomic and policy-driven influences\\non carbon price dynamics.\\nThis study also builds upon a real EUA carbon market dataset, incorporating multiple external\\nvariables such as coal and natural gas prices (Lin and Zhang, 2022), policy events (Fan, 2025),\\nand broader financial indicators (e.g., exchange rates) that have been considered in recent deep\\nlearning approaches (Zhao and Xu, 2023). A new multifactor enhanced dataset is constructed to\\nimprove the feature richness and realism of the training data.\\nContributions. The main contributions of this study are summarized as follows:\\n• We propose a comprehensive forecasting framework that tightly couples structural break\\ndetection, wavelet denoising, and deep learning modeling in a unified pipeline. Unlike\\nprior modular approaches that treat these components independently, we leverage detected\\nbreakpoints to guide the segmentation and wavelet decomposition of the time series, en-\\nabling the learning models to focus on cleaner, regime consistent data. This integration\\nimproves robustness to structural changes and enhances forecasting accuracy under high\\nmarket volatility, directly addressing the limitations of baseline methods.\\n• We enrich the feature set by incorporating commodity prices, exchange rates, and policy\\nsignals, thereby expanding input diversity and enabling the models to better capture the\\neconomic and policy-driven dynamics underlying carbon price fluctuations.\\n• We implement and evaluate multiple deep learning models—PELT-WT-LSTM (both uni-\\nvariate and multivariate variants), PELT-WT-GRU, and PELT-WT-TCN—and conduct a\\nsystematic comparison against the baseline BP&ICSS-WT-LSTM. This allows us to assess\\nthe effectiveness of alternative architectures beyond traditional LSTM models.\\n• Our experiments demonstrate that incorporating breakpoint aware decomposition signifi-\\ncantly improves forecasting accuracy and robustness under volatile market conditions. Our\\nbest-performing method, PELT-WT-TCN, achieves a MAE of 1.1855 and an RMSE of\\n1.5866. This performance indicates an 18.63% reduction in MAE and a 22.35% reduction\\nin RMSE respectively, compared to the strongest baseline model, and a 70.55% reduction\\nin RMSE and a 74.42% reduction in MAE respectively, compared to the weakest baseline.\\n3\\n\\nStructure of the Remainder of this Paper. Section 2: Related work reviews previous studies\\nand defines our approach, including structural breakpoint detection, time series modeling, and\\nmodel research; Section 3: Methodology describes the research methods, including breakpoint\\ndetection, wavelet denoising, and model architecture design; Section 4: Experiments present\\nthe results along with their implications and limitations; Section 5: Conclusion summarizes this\\nwork and outlines future directions.\\n2. Related Work\\nSignal Decomposition and Deep Learning Models. Several recent studies (Zhang et al., 2024,\\n2023; Wang et al., 2023) have explored hybrid models that combine signal decomposition tech-\\nniques with machine learning or deep learning architectures. For instance, a deep learning based\\nframework including multisource features was proposed to improve prediction performance in\\ncarbon markets (Zhang and Wen, 2022). A multifrequency combined model was also introduced\\nto capture both long-term and short-term patterns in price dynamics (Duan et al., 2025). In ad-\\ndition, quantile regression with feature selection has been applied to improve accuracy under\\nuncertain market conditions (Pang et al., 2023). These methods generally aim to denoise input\\ndata and capture nonlinear structures, often using tools such as EMD (Lorentz, 1938), CEEM-\\nDAN (Torres et al., 2011), or wavelet transforms (Mallat, 1989). While such approaches have\\nshown promise in improving accuracy, most do not explicitly address the role of structural break-\\npoints, or only incorporate them in a limited fashion (Zhang and Wen, 2022; Duan et al., 2025).\\nFurthermore, few studies (Torres et al., 2011; Shahid et al., 2020; Zhang and Wen, 2022) conduct\\nsystematic comparisons of deep learning models under standardized conditions. It is therefore\\ndifficult to assess their generalizability, robustness and computational cost.\\nModel Comparisons and Advanced Neural Architectures. Although the application of deep\\nlearning in carbon price forecasting has rapidly expanded in recent years, systematic compar-\\nisons of different models under standardized conditions remain scarce. This lack of comparative\\nstudies limits a comprehensive evaluation of these methods in terms of generalization ability,\\nrobustness, and computational cost. To address this gap, several studies have explored the issue\\nfrom various perspectives. One study combined GARCH and LSTM models to handle the high\\nvolatility of the EU carbon market, demonstrating the potential of hybrid modeling in multi-\\nscale forecasting (Huang et al., 2021). Another study incorporated attention mechanisms into a\\nBiLSTM network to improve its ability to model degradation trends, thereby highlighting how\\nstructural modifications can impact model performance (Guo et al., 2023). Ensemble deep learn-\\ning methods based on high-frequency trading data were evaluated to emphasize the coupling\\nbetween data granularity and model compatibility (Zhao and Xu, 2023). Additionally, prediction\\naccuracy has been examined from the perspectives of model integration (Chen et al., 2021) and\\nregional heterogeneity (Zhao et al., 2023). An integrated CEEMDAN and TCN-LSTM frame-\\nwork has also been proposed to enhance adaptability and accuracy under nonlinear volatility (Cai\\net al., 2025).\\nModel Interpretability and Performance Optimization. In the area of model interpretabil-\\nity and decision support, SHAP and LIME techniques have been applied to improve the trans-\\nparency and practical applicability of AI models in carbon market forecasting (Olawumi and\\nOladapo, 2025; Lei et al., 2024). Comparative validation has been conducted within an SSA-\\nLSTM framework to provide empirical support for methodological evaluation (Wang et al.,\\n2022). Furthermore, LightGBM combined with Bayesian optimization has been used to as-\\nsess error convergence across multiparameter spaces, emphasizing the importance of structural\\n4\\n\\ntuning and performance optimization (Deng et al., 2024). Collectively, these studies underscore\\nthe critical need to establish standardized evaluation benchmarks and cross model performance\\ncomparison systems to support the sustainable development of intelligent forecasting in carbon\\nmarkets.\\nBreakpoint Detection and Carbon Price Forecasting. The baseline paper (Lin and Zhang,\\n2022) proposed a forecasting framework that integrates structural breakpoint detection with deep\\nlearning and highlights the value of capturing policy changes to improve the prediction of carbon\\nprices. While this provides a useful foundation for hybrid modeling approaches that account for\\nmarket dynamics, the structural breakpoint detection methods adopted are relatively dated and\\nmore suitable for identifying linear structural changes. This limits their ability to capture more\\ncomplex or nonlinear regime shifts that are often present in volatile markets such as carbon trad-\\ning. Building on this line of research, our study optimizes the breakpoint detection method to-\\ngether with wavelet based signal decomposition. In addition, we systematically evaluate several\\ndeep learning models using the preprocessed data (univariate, multivariate), including standard\\nLSTM without structural features, univariate LSTM(Hochreiter and Schmidhuber, 1997), mul-\\ntivariate LSTM (Karim et al., 2019), GRU(Shahid et al., 2020), and TCN(Chang et al., 2024).\\nAll models are trained and tested under consistent conditions to enable a fair comparison across\\ndifferent input configurations.\\n3. Our Methodology\\nFigure 1: The flowchart of the proposed framework. This framework outlines a carbon price forecasting process com-\\nbining structural break detection, wavelet denoising, and sequence models (LSTM, GRU, TCN) for univariate and mul-\\ntivariate analysis.\\nThis study formulates the task of carbon price forecasting as a multivariate, one-step regres-\\nsion problem, aiming to predict the carbon allowance price at the next time point based on histor-\\nical carbon price data. As illustrated in Figure 1, the overall prediction framework encompasses\\nkey stages such as data construction, preprocessing, feature extraction, and model forecasting,\\nforming a systematic and generalizable approach to carbon price prediction. First, the study con-\\nstructs the raw dataset based on actual carbon market data, and in the preprocessing stage, the\\ncarbon price series undergoes structural break detection and wavelet denoising to generate input\\nfeatures that are more stable and trend oriented. Specifically, Sections 3.1.1 and 3.1.2 present\\nthe theoretical foundations and implementation of the Bai-Perron+ICSS and PELT algorithms,\\nwhich identify potential breakpoints in the time series from different perspectives to enhance\\nthe robustness and interpretability of subsequent modeling. Section 3.2 explains the application\\n5\\n\\nof wavelet transform to separate different frequency components, remove high-frequency noise,\\nand retain the main trend signal, thereby improving the quality of extracted features. Based on\\nthis, the denoised carbon price series is processed with time lags to construct input variables\\nthat reflect temporal dependencies, which are then integrated with structural break information\\nto form the final modeling feature set. Subsequently, Section 3.3.1 introduces typical deep learn-\\ning models such as LSTM, GRU, and TCN, detailing their structural principles and suitability\\nfor nonlinear time series prediction, and demonstrating their respective advantages for carbon\\nprice forecasting tasks. Furthermore, Section 3.4 compares the detection performance of differ-\\nent structural break detection methods, discusses the rationale for model selection, andvisualizes\\nthe overall prediction process and the connections between each stage. Through the integration\\nof these steps, the proposed framework effectively achieves the transformation from raw data\\nto structured and denoised features, combines key breakpoints identification with deep learning\\nmodel training, and ultimately outputs more accurate and interpretable carbon price predictions.\\n3.1. Underlying Models\\n3.1.1. Bai Perron + ICSS\\nBai-Perron Structural Break Test. The Bai–Perron test (Bai and Perron, 2003) is a classical\\nmethod for detecting multiple structural breakpoints in time series data. It partitions the series\\nyt into m + 1 regimes by minimizing the residual sum of squares (RSS). Formally, the model is\\nexpressed as a piecewise linear regression:\\nyt = x⊤\\nt βj + ut,\\nt = τ j−1 + 1, . . . , τj, j = 1, . . . , m + 1\\n(1)\\nwhere\\n• yt: the dependent variable (e.g., carbon price) at time t.\\n• xt: a vector of explanatory variables (regressors) at time t.\\n• βj: the coefficient vector specific to regime j.\\n• ut: the error term at time t.\\n• τj: the j-th structural break point.\\n• m: the number of structural breaks in the series.\\nThe breakpoints {τ1, . . . , τm} are estimated by minimizing the overall residual sum of squares:\\nS T =\\nmin\\n{τ1,...,τm}\\nm+1\\nX\\nj=1\\nτ j\\nX\\nt=τ j−1+1\\n\\x10\\nyt −x⊤\\nt ˆβj\\n\\x112\\n(2)\\nTo evaluate significance, the S upF test statistic is constructed:\\nSupF =\\n⌊T−ηT⌋\\nmax\\nk=⌈1+ηT⌉\\nS 0 −S k\\nbσ2\\n(3)\\nwhere S 0 is the RSS under the null hypothesis (no break), S k is the RSS under the alternative\\n(break at k), and ˆσ2 is the variance estimate.\\n6\\n\\nICSS Algorithm. The Iterative Cumulative Sum of Squares (ICSS) algorithm (Inclan and Tiao,\\n1994) detects structural changes in the variance of the same time series yt. The observed series\\ncan be expressed as:\\nyt = µ + et,\\nt = 1, 2, . . . , T\\n(4)\\nwhere µ is the mean and et is the residual term.\\nTo test for variance shifts, the cumulative variance statistic is defined as:\\nCk =\\nk\\nX\\ni=1\\nz2\\ni ,\\nzt =\\nyt −¯y1:t\\nq\\n1\\nt + 1\\nT S y\\n,\\nt = 1, . . . , T\\n(5)\\nwhere ¯y1:t is the mean of the first t observations and S y is the sample standard deviation.\\nThe normalized deviation is:\\nDk = Ck\\nCT\\n−k\\nT ,\\nD0 = DT = 0,\\n(6)\\nand the test statistic is:\\nIT = sup\\n\\x12 q\\nT\\n2 |Dk|\\n\\x13\\n.\\n(7)\\nIf IT exceeds a critical threshold, a variance break is identified.\\nUnified Output. Both Bai–Perron (mean/trend changes) and ICSS (variance shifts) produce\\nbreakpoints {τ j}, which are encoded into regime labels rt. These regime labels serve as structured\\ninputs for subsequent forecasting models.\\n3.1.2. PELT\\nMultiple breakpoints Detection. The Pruned Exact Linear Time (PELT) algorithm (Killick\\net al., 2012) detects multiple breakpoints in yt by minimizing the cost function:\\nm+1\\nX\\ni=1\\nC(yτi−1+1:τi) + βm,\\n(8)\\nwhere C(·) is the segment cost (e.g., negative log-likelihood), β is the penalty, and {τ1, . . . , τm}\\nare breakpoints.\\nRecursive Formulation. Let F(t) denote the optimal segmentation cost up to time t:\\nF(t) = min\\ns<t\\nn\\nF(s) + C(ys+1:t) + β\\no\\n.\\n(9)\\nThis recursion prunes suboptimal candidates, achieving O(n) complexity in both time and space.\\nUnified Output. The resulting breakpoints {τ j} are also encoded into regime labels rt, ensuring\\nconsistency with Bai–Perron and ICSS. This unified representation allows fair comparison of\\nforecasting models under different breakpoint detection strategies.\\n7\\n\\n3.2. Wavelet Transform\\nWavelet transform (WT) is a time–frequency analysis tool that projects a signal f(t) onto a\\nfamily of dilated and translated basis functions, thereby revealing localised spectral information.\\nOwing to its multiresolution capability, WT has become a standard procedure for decomposing\\nnonstationary series into an approximation component (low frequency, denoised) and a detail\\ncomponent (high-frequency) (Mallat, 1989). Figure 2 illustrates the multilevel wavelet decom-\\nposition pipeline adopted in this study, where the original signal is first decomposed into an\\napproximation component (A1) and a detail component (D1). The approximation component is\\nthen recursively decomposed into further approximation (A2, A3, ...) and detail components\\n(D2, D3, ...) at subsequent levels.\\nFigure 2: Flowchart of wavelet-transform decomposition.\\nNested subspaces.\\nVj ⊂Vj+1,\\nj ∈Z\\n(10)\\nwhere\\n• Vj: approximation subspace at resolution 2 j;\\n• j: scale index.\\nLimit properties.\\n[\\nj∈Z\\nVj = L2(R),\\n\\\\\\nj∈Z\\nVj = {0}\\n(11)\\nwhere\\n• L2(R): space of square-integrable functions;\\n• Vj: approximation subspace.\\nScaling function and orthonormal basis.\\nφ j,n(x) = 2j/2 φ(2 jx −n),\\n{φ j,n(x)}n∈Z\\n(12)\\nwhere\\n• φ(x): scaling function (father wavelet);\\n• φj,n(x): scaling basis at scale j and translation n;\\n• j: scale index (dilation);\\n• n: translation index (shift).\\n8\\n\\nOrthogonal expansion.\\nAj f(x) =\\nX\\nn∈Z\\n⟨f, φj,n⟩φj,n(x)\\n(13)\\nwhere\\n• Aj f: projection of f onto subspace V j (approximation);\\n• ⟨f, φj,n⟩: approximation coefficient;\\n• φj,n(x): scaling basis function.\\nConvolution + downsampling (pyramid algorithm).\\n⟨f, φj,n⟩=\\nX\\nk∈Z\\nh(2n −k) ⟨f, φj−1,k⟩\\n(14)\\nwhere\\n• h(n): low-pass filter associated with the scaling function;\\n• f: original signal;\\n• φj−1,k: scaling basis at the previous level.\\nFrequency-domain definition of the wavelet.\\nˆψ(ω) = G\\n\\x10 ω\\n2\\n\\x11\\nˆφ\\n\\x10 ω\\n2\\n\\x11\\n,\\nG(ω) = e−iω H(ω + π)\\n(15)\\nwhere\\n• ψ(x): wavelet function (mother wavelet);\\n• ˆψ(ω): Fourier transform of the wavelet function;\\n• φ(x): scaling function;\\n• H(ω): low-pass filter in the Fourier domain;\\n• G(ω): high-pass filter in the Fourier domain.\\nDetail signal expansion.\\nDj f =\\n\\x10\\n⟨f, ψj,n⟩\\n\\x11\\nn∈Z\\n(16)\\nwhere\\n• Dj f: detail component at scale j;\\n• ψj,n(x): wavelet basis at scale j, translation n;\\n• ⟨f, ψj,n⟩: detail coefficient.\\n9\\n\\nComplete wavelet decomposition.\\nf = AJ f +\\n∞\\nX\\nj=J\\nDj f\\n(17)\\nwhere\\n• f: original signal;\\n• AJ f: approximation component at the coarsest scale J;\\n• Dj f: detail components at scales j ≥J.\\n3.3. Sequence Forecasting Models\\n3.3.1. Long Short-Term Memory (LSTM)\\nLong Short-Term Memory (LSTM). Long short-term memory (LSTM) network is a gated\\nvariant of the recurrent neural network (RNN) originally proposed by Hochreiter and Schmidhu-\\nber (1997). By introducing gating mechanisms, LSTM alleviates the vanishing-gradient problem\\nthat plagues conventional RNNs, thereby retaining long range dependencies in time series. It has\\nbecome the de facto backbone for sequential modelling across finance, speech and language\\ndomains. Figure 3 illustrates the architecture of the peephole-free LSTM cell employed in our\\nstudy, which takes the unified input zt = [˜yt, ut, et].\\nFigure 3: Architecture of a peephole-free LSTM cell with the unified input zt = [˜yt, ut, et].\\nFollowing Sections 3.1 and 3.2, we construct a unified input vector\\nzt =\\n\\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n˜yt\\nut\\net\\n\\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈Rd,\\nwhere ˜yt denotes the wavelet-denoised carbon price (from Sec. 3.2), ut ∈Rp represents exoge-\\nnous variables, and et ∈Rm+1 is the one-hot encoded regime label from breakpoint detection\\n(Sec. 3.1). At each time step t, the LSTM cell receives this input zt, together with the previous\\nhidden state ht−1 ∈Rh and the previous cell state Ct−1 ∈Rh. Three gates—forget, input, and\\noutput—jointly regulate the information flow:\\n10\\n\\nft = σ(Wf zt + U f ht−1 + b f ),\\n(18)\\nit = σ(Wizt + Uiht−1 + bi),\\n(19)\\n˜Ct = tanh(Wazt + Uaht−1 + ba),\\n(20)\\nwhere\\n• ft: the forget gate activation at time t, controlling how much of the previous memory to\\nretain.\\n• it: the input gate activation at time t, controlling how much new information to write to the\\ncell state.\\n• ˜Ct: the candidate cell state (candidate activation), computed using a tanh activation.\\n• zt: the unified input vector, consisting of the wavelet-denoised carbon price ˜yt, the exoge-\\nnous variables ut, and the one-hot regime label et.\\n• ht−1: the hidden state from the previous time step.\\n• W, U: weight matrices for input and hidden state, respectively, specific to each gate.\\n• b: the bias term corresponding to each gate.\\n• σ(·): the logistic sigmoid activation function, producing values in the range (0, 1).\\n• tanh(·): the hyperbolic tangent function, producing values in the range (−1, 1).\\nCell-state update. The forget gate ft⊙ct−1 determines how much historical information to retain,\\nwhile it ⊙˜Ct writes the current candidate content into the memory cell. The cell state is updated\\nas\\nCt = σ(ft ⊙Ct−1 + it ⊙˜Ct),\\n(21)\\nwhere\\n• Ct: the cell state at time t, representing the long-term memory of the LSTM unit.\\n• Ct−1: the previous cell state (from time t −1).\\n• ft: the forget gate value, controlling how much of ct−1 is retained.\\n• it: the input gate value, controlling how much new information is written into the cell.\\n• ˜Ct: the candidate cell state (or activation), proposed as new content.\\n• ⊙: element-wise (Hadamard) multiplication.\\n11\\n\\nHidden-state output. Finally, the output gate selects a portion of the updated cell state as the\\nhidden representation:\\not = σ(Wozt + Uoht−1 + bo),\\n(22)\\nht = ot ⊙tanh(Ct),\\n(23)\\nwhere\\n• ot: the output gate at time t, determining how much of the internal cell state is exposed to\\nthe output.\\n• Wo, Uo: weight matrices for the current input zt and previous hidden state ht−1 in the output\\ngate.\\n• bo: the bias vector for the output gate.\\n• ht: the hidden state (i.e., the actual output of the LSTM cell at time t).\\n• Ct: the updated internal cell state.\\n• ⊙: element-wise (Hadamard) multiplication.\\n• tanh(Ct): the nonlinear transformation of the cell state to produce the output.\\n3.3.2. Gated Recurrent Unit (GRU)\\nThe Gated Recurrent Unit (GRU) is a streamlined alternative to the LSTM, retaining only\\ntwo gates—update and reset—and discarding the explicit memory cell. Despite its compactness,\\nGRU still mitigates vanishing gradients and has shown competitive performance in sequence\\nmodelling tasks, such as pandemic trajectory forecasting (Shahid et al., 2020).\\nAt time step t, given the unified input vector zt ∈Rd (defined in Sec. 3.3.1) and the previous\\nhidden state ht−1 ∈Rh, the gates are computed as\\nz(g)\\nt\\n= σ(Wzzt + Uzht−1 + bz),\\n(24)\\nρt = σ(Wρzt + Uρht−1 + bρ),\\n(25)\\nwhere\\n• z(g)\\nt : the update gate at time t, controlling how much of the past hidden state is preserved.\\n• ρt: the reset gate at time t, determining how much past information to forget.\\n• zt: the unified input vector at time t, consisting of ˜yt, ut, and et.\\n• ht−1: the hidden state from the previous time step.\\n• Wz, Wρ: weight matrices for input-to-gate connections.\\n• Uz, Uρ: weight matrices for hidden-to-gate connections.\\n• bz, bρ: bias vectors for the two gates.\\n• σ(·): the logistic sigmoid activation function.\\n12\\n\\nCandidate activation. The reset gate modulates how much past information enters the candi-\\ndate activation:\\n˜ht = tanh(Wzt + U(ρt ⊙ht−1) + bh),\\n(26)\\nwhere\\n• ˜ht: the candidate activation at time t, representing the new content to be potentially added\\nto the hidden state.\\n• W, U: weight matrices for input-to-hidden and hidden-to-hidden connections.\\n• zt: the unified input vector at time t, consisting of ˜yt, ut, and et.\\n• ht−1: the previous hidden state.\\n• ρt: the reset gate at time t, controlling the contribution of past memory.\\n• bh: bias vector for the candidate activation.\\n• ⊙: element-wise (Hadamard) multiplication.\\n• tanh(·): hyperbolic tangent activation function.\\nHidden-state interpolation. Finally, the update gate linearly interpolates between the previ-\\nous state and the candidate activation:\\nht = (1 −z(g)\\nt ) ⊙ht−1 + z(g)\\nt\\n⊙˜ht,\\n(27)\\nwhere\\n• ht: the final hidden state (and output) of the GRU cell at time t.\\n• ht−1: the previous hidden state from time t −1.\\n• z(g)\\nt : the update gate, determining how much of the new candidate state ˜ht is used.\\n• ˜ht: the candidate activation computed in Eq. (26).\\n• (1 −z(g)\\nt ) ⊙ht−1: the retained part of the previous hidden state.\\n• z(g)\\nt\\n⊙˜ht: the newly added information.\\n• ⊙: element-wise (Hadamard) multiplication.\\nIntuitively, z(g)\\nt\\ncontrols the trade-off between retaining past information and overwriting it with\\nnew content, while ρt determines how much historical context contributes to the candidate ac-\\ntivation. Owing to this gated interpolation, GRU achieves comparable accuracy to LSTM with\\nfewer parameters, which explains its adoption in recent time series studies.\\n13\\n\\n3.3.3. Temporal Convolutional Network (TCN)\\nFigure 4: The structure of the TCN model.\\nTemporal Convolutional Network (TCN). The temporal convolutional network (TCN) adopted\\nin this work, originally introduced for grating nanomeasurement by Chang et al. (2024), com-\\nprises an input layer, N stacked residual blocks, a fully connected projection, and an output node.\\nAt time step t, the TCN receives a windowed input matrix\\nXt = [zt−T+1, . . . , zt] ∈RT×d,\\nwhere each zτ is the unified input vector defined in Sec. 3.3.1, consisting of the wavelet-denoised\\ncarbon price ˜yτ, the exogenous variables uτ, and the regime one-hot label eτ. This design en-\\nables the TCN to capture dependencies across a receptive field of length T, in contrast to re-\\ncurrent models which process one time step at a time. The overall dataflow is illustrated in\\nFigure 4 (Chang et al., 2024).\\nEach residual block performs:\\n1. Dilated causal convolution (twice): a dilation factor d enlarges the receptive field expo-\\nnentially while preserving temporal causality;\\n2. Weight normalisation (two layers): stabilises activation scale and accelerates conver-\\ngence;\\n3. Crop (shear): aligns sequence length after dilation and clips gradients to avert numerical\\noverflow;\\n4. ReLU activation: injects nonlinearity after each convolution;\\n14\\n\\n5. Dropout: randomly masks neurons to curb overfitting.\\nAn additional 1×1 convolution on the shortcut branch matches channel dimensions before\\nsummation. Finally, a dense layer maps the hidden representation to a scalar prediction delivered\\nby the output node. This cascade allows the TCN to model both short-range and long-range\\ndependencies efficiently and stably.\\n3.4. Comparison of related models\\nComparison of Structural Mutation Tests. The multiple structural mutation test method pro-\\nposed by Bai and Perron (2003) is used to analyze carbon market time series with significant\\ninstitutional changes. The core advantage of this method is that it can detect structural mutation\\npoints at multiple unknown locations simultaneously, and allows different regression models to\\nbe applied in each segment. This offers a powerful tool for revealing the dynamic relationship\\nbetween carbon prices and macroeconomic or energy policy regimes.\\nAlthough the Bai–Perron method performs well in detecting institutional shifts, it relies on\\nsegmented Ordinary Least Squares (OLS) regression, which imposes several limitations. First,\\nit assumes linearity within each segment, making it less suitable for capturing nonlinear or jump\\ntype dynamics. Second, the algorithm has high computational complexity, particularly when the\\nsample size is large or when many breakpoints exist.\\nIn contrast, the ICSS (Iterative Cumulative Sum of Squares) method (Inclan and Tiao, 1994)\\nis specifically designed to detect variance changes in time series. It does not require parametric\\nmodel specification within segments and is particularly useful for identifying abrupt shifts in\\nvolatility. However, ICSS assumes the mean of the series is constant and is more sensitive to\\noutliers or serial correlation in the residuals, which may affect its accuracy.\\nCompared with both BP and ICSS, the PELT (Pruned Exact Linear Time) method (Killick\\net al., 2012) is better suited for large-scale and high-frequency time series. It features a linear time\\ncomplexity O(n) and employs pruning strategies to significantly reduce redundant search paths,\\nthus maintaining high computational efficiency while preserving detection accuracy. PELT was\\nchosen because breakpoints were measured for each column of data.\\nComparison of Sequence Forecasting Models. Carbon market prices are influenced by mul-\\ntiple factors such as energy prices, policy interventions, supply and demand fluctuations, and\\nseasonality. This results in significant nonlinearity and long-term and short-term dependencies\\nin the price series. Effective forecasting requires models that can capture such complex tempo-\\nral dynamics. The Gated Recurrent Unit (GRU) is a recurrent neural network variant designed\\nto retain long-term dependencies via update and reset gates, which helps mitigate the vanishing\\ngradient problem (Shahid et al., 2020). In contrast, the Temporal Convolutional Network (TCN)\\nemploys dilated causal convolutions and residual connections to model long range dependencies\\nwith high efficiency and stability (Bai et al., 2018). While GRU is well-suited for sequential\\nprocessing, its inherent sequential nature limits training speed. TCN, by contrast, supports par-\\nallel computation and demonstrates faster convergence, making it advantageous for large scale\\nor multistep forecasting tasks (Bai et al., 2018) (Lim et al., 2021).\\nEmpirical studies have shown that TCN can outperform GRU in many time series forecasting\\nbenchmarks, particularly in energy and financial domains (Lim et al., 2021). As carbon prices\\nexhibit both market and policy driven volatility, a comparative evaluation of GRU and TCN\\nprovides valuable guidance in selecting robust modeling architectures for practical forecasting\\napplications.\\n15\\n\\n4. Experiment\\nThis section presents a comprehensive experimental analysis structured into four main parts.\\nSection 4.1 introduces the dataset, outlines its statistical characteristics, and identifies structural\\nbreakpoints that signal major shifts in carbon pricing patterns. It also explains the data pre-\\nprocessing technique using Wavelet Transform (WT) for denoising. Section 4.2 compares the\\nperformance of deep learning models, i.e., BP&ICSS-WT-LSTM, PELT-WT-LSTM, PELT-WT-\\nGRU, and PELT-WT-TCN, under various configurations, highlighting their predictive behaviors.\\nSection 4.3 provides a quantitative performance evaluation using metrics such as MAE, RMSE,\\nMAPE and R². Section 4.4 further analyzes the scalability of the models and the distribution of\\nresiduals and errors to evaluate models’ robustness and generalizability.\\n4.1. Dataset Analysis and Preprocessing\\nData Description. The daily spot trading price data of EU Allowances (EUA) from September\\n10, 2007, to June 4, 2024, totaling 6,113 samples, was used for the empirical analysis in this\\nstudy. This dataset was obtained from the financial market data and information service platform\\n(investing.com). The primary reason for selecting this dataset is its ability to capture the dynamic\\nevolution of the EU carbon trading market across multiple policy phases (European Commission,\\n2024). Moreover, the extended time span provides a richer set of training samples for machine\\nlearning models, which helps enhance their predictive performance and generalization capability.\\nKey Impact Feature Analysis. Figure 5 presents the Pearson correlation coefficients between\\ncarbon prices and selected explanatory features. The results indicate that policy indicators, coal\\nand natural gas prices, as well as stock market indices (e.g., Euro Stoxx 50) are strongly and\\npositively correlated with carbon prices, highlighting their role as major drivers. In contrast, fea-\\ntures such as the EUR/USD exchange rate and economic policy uncertainty indices show weaker\\nor even negative correlations. These findings suggest that energy market fundamentals and pol-\\nicy dynamics exert the strongest influence on carbon price movements, whereas macroeconomic\\nand financial uncertainty indicators play a more limited role. The correlation analysis provides a\\nbasis for feature selection in subsequent forecasting models.\\nFigure 5: Key drivers of carbon price\\n16\\n\\nFigure 6: Relative importance of input features for carbon price prediction\\nThis study aims to screen multiple external features to identify the key drivers most closely\\nassociated with carbon price fluctuations. Following the approach of Geurts et al. (2006), we\\nemployed the Extremely Randomized Trees (ET) method to evaluate the importance of input\\nfeatures. As shown in Figure 6, the results of the ET evaluation indicate that the policy features\\nhave the highest importance score, exceeding 0.5, which is significantly higher than the other\\ncharacteristics. Among the remaining features, energy-related features such as coal and natural\\ngas prices, as well as financial indicators like the Euro Stoxx 50 index and EUR/USD exchange\\nrate, also exhibit non-negligible contributions. In contrast, macroeconomic uncertainty indices\\nand geopolitical risk show very limited explanatory power, with importance scores close to zero.\\nThis analysis highlights that policy factors overwhelmingly dominate the drivers of carbon price\\ndynamics, while energy markets and selected financial indicators play secondary roles.\\nFigure 7: Carbon price time series with detected structural breakpoints by feature\\nBreakpoint Analysis. As shown in Figure 7, the carbon price time series can be divided into\\nthree distinct phases. The first phase, from September 2007 to the end of 2012, is characterized\\nby relatively small price fluctuations, with carbon prices being significantly influenced by policy\\nand coal price disturbances. The second phase, from 2013 to 2019, reflects a relatively stable\\nmarket, though structural shifts still occurred due to localized impacts from energy prices and\\npolicy signals. The third phase, from 2020 to 2024, is marked by a rapid increase and high volatil-\\nity in carbon prices, mainly driven by the combined influence of natural gas prices, electricity\\nmarket dynamics, and major policy announcements. In Figure 7, the solid black line represents\\nthe carbon price, while the dashed lines indicate structural breakpoints associated with various\\n17\\n\\nfeature features. Notably, breakpoints for Epex Spot Germany, Euro Stoxx 50, TTF natural gas,\\nand policy features appear densely after 2021, clearly highlighting the nonlinear, dynamic, and\\nhighly uncertain nature of carbon prices.\\nAround 2008, due to underdeveloped market mechanisms (EcoAct, 2022), carbon prices\\ndropped rapidly to near zero, indicating a clear structural break. The carbon price series exhibits\\npositive skewness and high kurtosis, further demonstrating its significant nonnormality. To en-\\nhance the model’s ability to learn from the carbon price series and improve prediction accuracy,\\nthe dataset is divided into training and testing sets. Common split strategies include “80%/20%”\\nor “90%/10%.” This study adopts the “80%/20%” split, with 80% of the data used for model\\ntraining and 20% for model testing, in order to evaluate the model’s predictive robustness.\\nBased on the above test results, we also attempt to interpret the structural breakpoints ob-\\nserved in carbon prices. In the first phase (September 2007 – end of 2012), the carbon market\\nwas still in its exploratory stage. Issues such as excessive allocation of allowances and frequent\\neconomic fluctuations led to generally low and stable carbon prices, with significant influence\\nfrom policy missteps and coal price disturbances. (1) In 2007, toward the end of Phase I of the\\nEU ETS (2005–2007), carbon prices plummeted from around €1.5/ton to nearly €0/ton due to\\nthe surplus of allowances and the inability to carry over unused allowances into the next phase.\\n(2) In January 2008, Phase II of the EU ETS officially began. Due to a reduction in the new round\\nof allowance allocations and the partial acceptance of international credits (such as CDM), car-\\nbon prices quickly rebounded from nearly €0/ton to around €20/ton. (3) In September 2008, the\\nglobal financial crisis severely impacted industrial production in Europe, causing a sharp drop in\\nthe demand for carbon allowances. As a result, carbon prices fell rapidly from around €20/ton to\\napproximately €8/ton. (4) In December 2009, the United Nations Climate Change Conference\\nin Copenhagen failed to reach a legally binding global emissions reduction agreement, under-\\nmining market confidence in future carbon prices and causing them to fall from around €15/ton\\nto €12/ton. (5) In the middle of 2012, the ongoing Eurozone debt crisis continued to suppress\\nindustrial activity. Combined with the persistent issue of allowance oversupply, carbon prices\\ndropped to approximately €6.76/ton in June 2012.\\nEntering the second phase (2013–2019), as reform measures. Such as the Market Stabil-\\nity Reserve (MSR) were gradually implemented, market confidence began to recover (European\\nCommission, 2009), and carbon prices rose significantly. However, localized disruptions caused\\nby energy price fluctuations and policy adjustments still persisted. (1) In April 2013, the Eu-\\nropean Parliament rejected the “backloading” proposal aimed at temporarily removing surplus\\nallowances from the market. This triggered disappointment among market participants, leading\\nto a carbon price drop from €7.10/ton to €2.75/ton. (2) In July 2015, the EU officially adopted\\nthe proposal to establish the Market Stability Reserve (MSR) to absorb surplus allowances and\\nimprove price flexibility. Although there was little short-term change in carbon prices, market\\nconfidence was strengthened, laying the groundwork for future price increases. (3) Throughout\\n2018, with the adoption of the reform after 2020 package—which included accelerating the an-\\nnual reduction of allowances (raising the Linear Reduction Factor to 2.2%) and tightening the\\nfree allocation mechanism—carbon prices surged from €7/ton at the beginning of the year to\\n€25/ton by end of the year, marking an increase of over 250%.\\nIn the third phase (2020–2024), carbon prices exhibited a combination of rapid increases\\nand high volatility. This period was driven by multiple factors, including the natural gas crisis,\\nthe Russia-Ukraine conflict, and new climate policies such as the “Fit for 55” package. The\\ncarbon market entered a new stage characterized by both policy driven momentum and structural\\nchanges in the energy sector (Fan, 2025). (1) In July 2021, the European Union proposed the “Fit\\n18\\n\\nfor 55” climate policy package, raising the 2030 emissions reduction target from 40% to 62%\\n(compared to 2005 levels). This led to a surge in carbon prices from €33/ton at the beginning of\\nthe year to €60/ton (European Parliament, 2022). (2) From February to August 2022, the Russia-\\nUkraine war caused a spike in natural gas prices, prompting a shift back to coal fired power\\ngeneration and increasing demand for carbon allowances. As a result, carbon prices reached a\\nhistorical peak of €97/ton in August 2022. (3) On February 21, 2023, the EU ETS carbon price\\nsurpassed €100/ton for the first time, reaching €101/ton. This was driven by a combination of\\npolicy expectations, energy shortages, and growing participation from financial institutions. (4)\\nFrom January to February 2024, due to milder winter temperatures, lower electricity demand,\\nand ample renewable energy supply, carbon prices fell from €84/ton to €52/ton, marking the\\nlowest level in 31 months.\\nWavelet Data Denoising for Carbon Price Series. In the original time series, there can exist\\nsome notable noises, which can interfere with the ability of machine learning models to learn\\nthe intrinsic features of the data. Therefore, it is necessary to remove noise from the original\\ncarbon price series before modeling. To address this, this study applies Wavelet Transform (WT)\\nto process the carbon price time series, decomposing it into approximation components (low\\nfrequency trends) and detail components (high-frequency noise). We retain only the approxima-\\ntion component as input to the model, thereby achieving the goal of denoising. A single level\\ndecomposition is chosen because excessive decomposition may lead to information loss, which\\ncould negatively affect the model’s predictive accuracy. As shown in Figure 8, the processed\\ncarbon price curve (in orange) is noticeably smoother compared to the original curve (in blue),\\neffectively eliminating abrupt changes and spikes caused by high-frequency fluctuations, while\\npreserving the main trend of price movements. One key motivation for using WT in this study is\\nto introduce structural breakpoint information on top of the denoised series, in order to explore\\nwhether such information can further improve the accuracy of carbon price prediction models\\nunder a cleaner signal background.\\nFigure 8: Denoised by Wavelet transformer\\n4.2. Comparison of Deep Learning Models\\nThis study evaluates several deep learning models for carbon price forecasting. The com-\\npared methods are as follows:\\n• BP&ICSS-WT-LSTM: Baseline approach that combines Bai–Perron and ICSS structural\\nbreak detection with wavelet-based LSTM modeling (Lin and Zhang, 2022).\\n• PELT-WT-LSTM (Univariate): LSTM model with PELT structural break detection and\\nwavelet transform applied to a single input feature (carbon price).\\n19\\n\\n• PELT-WT-LSTM (Multivariate): Extension of the above model incorporating multiple\\nexternal influencing factors.\\n• PELT-WT-GRU: GRU-based model with PELT structural break detection and wavelet\\ntransform.\\n• PELT-WT-TCN: Temporal Convolutional Network (TCN) model with PELT structural\\nbreak detection and wavelet transform.\\nTo ensure consistency and comparability in the training process of deep learning models for car-\\nbon price forecasting, this study adopts a unified training configuration for all models. The Adam\\noptimizer is employed during training due to its adaptive learning rate mechanism, which is well-\\nsuited for handling nonstationary time series data. The initial learning rate is set to 0.001, with\\nmomentum parameters 1 and 2 set to 0.9 and 0.999, respectively, to balance training speed and\\nstability. The models are trained with a batch size of 64 for a maximum of 50 epochs. An early\\nstopping mechanism is introduced to prevent overfitting, where training is halted if the validation\\nmean squared error (MSE) does not improve for 10 consecutive epochs. Additionally, 10% of the\\ntraining data is allocated as a validation set to continuously monitor model performance. A slid-\\ning window approach is used to construct the input data, with each input window consisting of\\n30 time steps and a stride of 1, ensuring sufficient extraction and modeling of temporal features.\\nBoth LSTM and GRU models were built with two hidden layers, each containing 128 units, and\\na dropout rate of 0.2 was applied between layers to mitigate overfitting. The TCN model was\\nconstructed with four residual blocks, each having 64 channels.\\nBP&ICSS-WT-LSTM Performance. Following the approach of Lin and Zhang (Lin and Zhang,\\n2022), Figure 9 presents the prediction results of the BP&ICSS-WT-LSTM model, reproduced\\nfrom the baseline study using our updated carbon price dataset. In this configuration, structural\\nbreakpoints were detected using a combination of the Bai & Perron and ICSS methods (Lin\\nand Zhang, 2022). As a result, the model remains exposed to high-frequency noise and abrupt\\nfluctuations in the time series.\\nFigure 9: Comparison of actual and predicted carbon prices using the BP&ICSS-WT-LSTM model\\nAlthough the inclusion of breakpoint information helps the model capture shifts in price\\nregimes, BP&ICSS-WT-LSTM is only able to approximate the overall trend of carbon prices.\\nThe model performs poorly around local peaks and troughs, exhibiting amplitude compression,\\na phenomenon indicating a degree of underfitting. This limitation is particularly evident during\\nperiods of high volatility (e.g., around time points 400 and 900), where prediction errors increase\\nsignificantly.\\n20\\n\\nPELT-WT-LSTM (Univariate vs. Multivariate). Figures 10 and Figures 11 reveal the effect\\nof multivariate inputs. The PELT-WT-LSTM(univariate) shows lagged response and error fluc-\\ntuations. The multivariate version better aligns with actual values in several segments (e.g., step\\n500 to 900), suggesting enhanced dynamic awareness and forecasting insights.\\nFigure 10: Performance of the PELT-WT-LSTM(uni) model in forecasting carbon prices\\nFigure 11: Comparison of actual and PELT-WT-LSTM(multi)\\nPELT-WT-GRU Performance. As shown in Figure 12, the GRU model offers structural sim-\\nplicity with solid fitting capabilities. Prediction trends closely match actual values, especially\\nduring price peaks and drops (e.g., step 600–1000). GRU demonstrates a balance between accu-\\nracy and training efficiency.\\nFigure 12: Comparison of actual and PELT-WT-GRU predicted carbon prices\\nPELT-WT-TCN Performance. Figure 13 indicates that TCN outperforms other models. Its\\n21\\n\\npredictions closely follow real values even during sharp fluctuations, thanks to its one dimen-\\nsional convolution architecture that excels in capturing local temporal features and modelling\\nnonlinearities.\\nFigure 13: PELT-WT-TCN model prediction performance for carbon price forecasting\\n4.3. Quantitative Performance Evaluation\\nThe experiment employed five key performance metrics to quantify model effectiveness, in-\\ncluding Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Mean Absolute Per-\\ncentage Error (MAPE), the coefficient of determination (R²), and training time. MAE measures\\nthe average deviation between predicted and actual values, with smaller values indicating higher\\nprediction accuracy. RMSE emphasizes large errors through squared differences and reflects the\\noverall dispersion of the prediction errors. MAPE expresses the prediction error as a percentage,\\nmaking it easier to compare across models or datasets. R² evaluates how well the model explains\\nthe variance of the target variable, with values closer to 1 indicating better fit. Training time\\nreflects the computational efficiency of each model.\\nTable 1, Figure 14 and Figure 15 present five key performance metrics—Mean Absolute\\nError (MAE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), R²\\n(coefficient of determination), and training time—for various deep learning models used in time\\nseries forecasting. The results reveal clear differences in accuracy and computational efficiency\\namong the models:\\nTable 1: Performance comparison for carbon price prediction\\nModel\\nMAE\\nRMSE\\nMAPE (%)\\nR2\\nBP&ICSS-WT-LSTM\\n4.6345\\n5.3878\\n5.8731\\n0.8712\\nPELT-WT-LSTM (uni)\\n2.3627\\n2.7488\\n3.0582\\n0.9664\\nPELT-WT-LSTM (multi)\\n1.8192\\n2.2967\\n2.3267\\n0.9765\\nPELT-WT-GRU\\n1.3308\\n1.6987\\n1.7401\\n0.9872\\nPELT-WT-TCN\\n1.1855\\n1.5866\\n1.6451\\n0.9888\\n22\\n\\nFigure 14: Performance comparison of multiple models in carbon price forecasting\\nFigure 15: Comparison of model performance metrics\\nPELT-WT-TCN. The TCN model delivers the best overall performance, achieving the lowest er-\\nror metrics (MAE = 1.1855, RMSE = 1.5866, MAPE = 1.6451%), indicating minimal deviation\\nbetween predicted and actual values. It also achieves the highest R² score of 0.9888, meaning it\\nexplains nearly all variance in the data. However, this comes at the cost of the longest training\\ntime (48.7 seconds), likely due to its deeper architecture or complex convolutional operations.\\nPELT-WT-GRU. The GRU model strikes a strong balance between performance and speed.\\nWith MAE = 1.3308, RMSE = 1.6987, MAPE = 1.7401%, it ranks closely behind TCN in\\naccuracy. Importantly, it has the fastest training time (19.6 seconds), making it highly suitable\\nfor time sensitive applications. Its R² of 0.9872 also confirms excellent predictive reliability.\\nPELT-WT-LSTM(Multivariate). The PELT-WT-LSTM(Multivariate) model, which incorpo-\\nrates multiple input features, performs well in terms of prediction accuracy (MAE = 1.8192,\\nRMSE = 2.2967, MAPE = 2.3267%) with a respectable R² of 0.9765. The training time is\\nmoderate (24.9 seconds), making it a viable option when balancing accuracy and computational\\ncost.\\nPELT-WT-LSTM(Univariate). This model uses only a single input feature and underperforms\\ncompared to its multivariate counterpart. With MAE = 2.3627, RMSE = 2.7488, MAPE =\\n3.0582%, and R² = 0.9664, it shows limited predictive power. The training time is around 24.7\\nseconds, similar to the multivariate version.\\n23\\n\\nBP&ICSS-WT-LSTM. The BP&ICSS-WT-LSTM configuration performs the worst overall,\\nwith high error rates (MAE = 4.6345, RMSE = 5.3878, MAPE = 5.8731%) and the lowest\\nR² score (0.8712), suggesting a poor fit to the data. Despite its moderate training time (24.5\\nseconds), its low accuracy renders it unsuitable for practical applications.\\nWhile PELT-WT-TCN achieves the highest accuracy, it requires longer training time. PELT-\\nWT-GRU provides a approximately optimal balance between speed and performance, making it\\nideal for use in real time. PELT-WT-LSTM(Multi) also performs well with multiple features.\\nIn contrast, PELT-WT-LSTM(Uni) and BP&ICSS-WT-LSTM models fall short, particularly the\\nlatter, which is not recommended for deployment due to its poor performance.\\n4.4. Scalability and Residual Distribution Analysis\\nFigure 16: Model training times\\nFigure 16 provides a detailed comparison of the training times required by different models\\nunder identical training conditions, highlighting significant differences in computational resource\\nconsumption. Overall, the PELT-WT-GRU model stands out with the fastest training time of\\njust 14.3 seconds, making it particularly advantageous in scenarios that are sensitive to time or\\nconstrained by resources. In contrast, the BP&ICSS-WT-LSTM, PELT-WT-LSTM(univariate),\\nand PELT-WT-LSTM(Multivariate) show very similar training durations, 26.1, 26.4, and 26.2\\nseconds respectively, indicating that the PELT-WT-LSTM architecture maintains relatively stable\\ntraining efficiency regardless of input dimensionality. Although their training times are moderate,\\nthese LSTM-based models are slightly less efficient than PELT-WT-GRU. On the other hand, the\\nPELT-WT-TCN model requires the longest training time, reaching 52.5 seconds, nearly 3.5 times\\nthat of PELT-WT-GRU. This extended duration is primarily due to the TCN’s deep convolutional\\nstructure and multiple stacked layers, which make the training process more computationally\\nintensive and complex.\\n24\\n\\nFigure 17: Residual distributions across time\\nFigure 17 illustrates the temporal distribution of residuals (actual minus predicted values)\\nfor five deep learning models. The PELT-WT-TCN model demonstrates relatively constrained\\nresidual variation, with values fluctuating closely around zero, which may reflect its strength\\nin capturing long range temporal dependencies through dilated causal convolutions (Bai et al.,\\n2018). At the other end of the spectrum, the BP&ICSS-WT-LSTM exhibits broader and more\\nirregular residual patterns, with multiple spikes exceeding ±10, potentially indicating heightened\\nsensitivity to input volatility or sequence length. PELT-WT-GRU and PELT-WT-LSTM (Mul-\\ntivariate) present moderate residual variability, while PELT-WT-LSTM (Univariate) maintains\\na more symmetric error pattern with smaller amplitude, although an increased concentration of\\nnegative deviations is observed in the latter part of the series. These findings highlight the extent\\nto which architectural design shapes residual dynamics and suggest that residual stability may\\nserve as a practical proxy for model generalization in time series forecasting.\\nFigure 18: Distribution of residuals density\\nFigure 18 illustrates the residual distributions of different models. Among all the methods,\\nthe PELT-WT-TCN model exhibits the most concentrated and symmetric density around zero,\\nindicating both low residual variance and minimal bias. The PELT-WT-GRU and PELT-WT-\\nLSTM(Multivariate) models demonstrate moderately peaked curves, with PELT-WT-GRU show-\\ning slight right skewness suggestive of occasional overestimation. In addition, the PELT-WT-\\nLSTM(Univariate) distribution is broader and shifted rightward, reflecting a greater frequency\\nof positive errors. The BP&ICSS-WT-LSTM displays the widest and most asymmetric distribu-\\ntion, characterized by a pronounced right tail, which implies a tendency toward persistent over\\n25\\n\\nprediction. The distributional characteristics of residuals provide an additional layer of model\\nevaluation beyond time domain analysis (Lawrance and Lewis, 2018), revealing that PELT-WT-\\nTCN and PELT-WT-GRU are more consistent and less prone to systematic error compared to the\\nBP&ICSS-WT-LSTM.\\n5. Conclusion\\nThis study proposed a hybrid forecasting framework that integrates structural breakpoint de-\\ntection, wavelet denoising, and advanced deep learning models to enhance the prediction of\\nEU carbon prices. Breakpoint detection using PELT enables the model to capture regime shifts\\ncaused by policy changes and market shocks, while wavelet transform reduces high-frequency\\nnoise and improves stability. Together, these preprocessing steps provide denoised and inputs\\nwith regime awareness for subsequent forecasting. Experimental results show that incorporat-\\ning structural and denoised information significantly improves predictive accuracy. Among the\\ntested models, PELT-WT-TCN achieves the lowest error metrics, while PELT-WT-GRU demon-\\nstrates a favorable balance between efficiency and performance, making it more suitable for real\\ntime applications. The comparison confirms that combining structural awareness with multiscale\\nlearning is essential for improving robustness and interpretability in carbon price forecasting.\\nOverall, the findings contribute to the growing literature on hybrid modeling for financial and\\nenergy markets, demonstrating the benefits of integrating advanced time series decomposition\\nwith deep learning. Future research may focus on improving model interpretability and extend-\\ning the dataset with more indicators related to policy and macroeconomic factors.\\nDeclaration of Competing Interest\\nThe authors declare that they have no known competing financial interests or personal rela-\\ntionships that could have appeared to influence the work reported in this paper.\\nReferences\\nBai, J., Perron, P., 2003. Computation and analysis of multiple structural change models. Journal\\nof Applied Econometrics 18, 1–22. doi:10.1002/jae.659.\\nBai, S., Kolter, J.Z., Koltun, V., 2018. An empirical evaluation of generic convolutional and\\nrecurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271 .\\nBollerslev, T., 1986.\\nGeneralized autoregressive conditional heteroskedasticity.\\nJournal of\\nEconometrics 31, 307–327. doi:10.1016/0304-4076(86)90063-1.\\nBox, G.E.P., Jenkins, G.M., Reinsel, G.C., Ljung, G.M., 1976. Time Series Analysis: Forecast-\\ning and Control. 5th ed., John Wiley & Sons.\\nCai, X., Yuan, B., Wu, C., 2025. An integrated ceemdan and tcn-lstm deep learning framework\\nfor forecasting. International Review of Financial Analysis 98, 103879. doi:10.1016/j.\\nirfa.2024.103879.\\nChang, L., Li, Z., Zhou, B., Xiu, G., Guo, Y., 2024. Research on grating nano-measurement\\nalgorithm based on tcn. Measurement Science and Technology 36, 015031. URL: https:\\n//doi.org/10.1088/1361-6501/ad889a, doi:10.1088/1361-6501/ad889a.\\n26\\n\\nChen, Y., Li, X., Wang, X., 2021. A hybrid model for carbon price forecasting. Applied Energy\\n285, 116485. doi:10.1016/j.apenergy.2021.116485.\\nCho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Ben-\\ngio, Y., 2014. Learning phrase representations using rnn encoder–decoder for statistical ma-\\nchine translation, in: Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP), Association for Computational Linguistics. pp. 1724–1734.\\ndoi:10.3115/v1/D14-1179.\\nDeng, S., Su, J., Zhu, Y., Yu, Y., Xiao, C., 2024. Forecasting carbon price trends based on an\\ninterpretable light gradient boosting machine and bayesian optimization. Expert Systems with\\nApplications 242, 122502. doi:10.1016/j.eswa.2023.122502.\\nDuan, Y., Fan, Y., Wang, X., Liu, K., Zhang, X., 2025. Dynamic prediction of carbon prices\\nbased on the multi-frequency combined model. PeerJ Computer Science 11, e2827. doi:10.\\n7717/peerj-cs.2827.\\nEcoAct, 2022. Key messages from 2022 state of the eu ets report. https://eco-act.com/\\nblog/state-of-the-eu-ets-2022/. Accessed 22 May 2025.\\nEuropean\\nCommission,\\n2009.\\nUn\\nclimate\\nchange\\nconference\\n(cop15).\\nhttps://climate.ec.europa.eu/news-your-voice/events/\\nun-climate-change-conference-cop15-2009-12-07_en. Accessed 22 May 2025.\\nEuropean Commission, 2020. The eu emissions trading system (eu ets). https://climate.\\nec.europa.eu/eu-action/eu-emissions-trading-system-eu-ets_en. Accessed 22\\nMay 2025.\\nEuropean Commission, 2024. 2024 Carbon Market Report: a stable and well-functioning mar-\\nket, driving emissions from power and industry installations to a historic reduction of 16.5%.\\nhttps://tinyurl.com/4m75hstt. Accessed 03 March 2025.\\nEuropean Parliament, 2022. Revision of the EU ETS market stability reserve. Technical Report.\\nEuropean Parliament.\\nURL: https://www.europarl.europa.eu/RegData/etudes/\\nBRIE/2022/698896/EPRS_BRI%282022%29698896_EN.pdf. accessed 22 May 2025.\\nFan, J., 2025. The external impact of eu climate policy: political responses to the eu’s car-\\nbon border adjustment mechanism. Climatic Change URL: https://doi.org/10.1007/\\ns10784-025-09667-z, doi:10.1007/s10784-025-09667-z. accessed 8 May 2025.\\nGeurts, P., Ernst, D., Wehenkel, L., 2006.\\nExtremely randomized trees.\\nMachine Learn-\\ning 63, 3–42.\\nURL: https://doi.org/10.1007/s10994-006-6226-1, doi:10.1007/\\ns10994-006-6226-1.\\nGuo, J., Liu, M., Luo, P., Chen, X., Yu, H., Wei, X., 2023. Attention-based bilstm for the\\ndegradation trend prediction of lithium battery. Energy Reports 9, 655–664. doi:10.1016/j.\\negyr.2023.03.056.\\nHochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Computation 9, 1735–\\n1780.\\n27\\n\\nHuang, Y., Dai, X., Wang, Q., Zhou, D., 2021. A hybrid model for carbon price forecasting using\\ngarch and long short-term memory network. Applied Energy 298, 116485. doi:10.1016/j.\\napenergy.2021.116485.\\nInclan, C., Tiao, G.C., 1994. Use of cumulative sums of squares for retrospective detection of\\nchanges of variance. Journal of the American Statistical Association 89, 913–923. doi:10.\\n2307/2290916.\\nKarim, F., Majumdar, S., Darabi, H., Harford, S., 2019. Multivariate lstm-fcns for time series\\nclassification. Neural networks 116, 237–245.\\nKillick, R., Fearnhead, P., Eckley, I., 2012. Optimal detection of changepoints with a linear\\ncomputational cost. Journal of the Royal Statistical Society: Series B 74, 493–508.\\nKoch, N., Fuss, S., Grosjean, G., Edenhofer, O., 2014.\\nCauses of the eu ets price drop:\\nRecession, cdm, renewable policies or a bit of everything?—new evidence.\\nEnergy Pol-\\nicy 73, 676–685.\\nURL: https://www.sciencedirect.com/science/article/pii/\\nS0301421514003966, doi:https://doi.org/10.1016/j.enpol.2014.06.024.\\nLawrance, A.J., Lewis, P.A.W., 2018. Modelling and residual analysis of nonlinear autoregres-\\nsive time series in exponential variables. Journal of the Royal Statistical Society: Series B\\n(Methodological) 47, 165–183. doi:10.1111/j.2517-6161.1985.tb01344.x.\\nLei, H., Xue, M., Liu, H., Ye, J., 2024. Unveiling the driving patterns of carbon prices through an\\nexplainable machine learning framework: Evidence from chinese emission trading schemes.\\nJournal of Cleaner Production 438, 140697. doi:10.1016/j.jclepro.2024.140697.\\nLim, B., Arik, S.O., Loeff, N., Pfister, T., 2021. Temporal fusion transformers for interpretable\\nmulti-horizon time series forecasting. International Journal of Forecasting 37, 1748–1764.\\nLin, B., Zhang, C., 2022. Forecasting carbon price in the european carbon market: The role of\\nstructural changes. Process Safety and Environmental Protection 166, 341–354. URL: https:\\n//doi.org/10.1016/j.psep.2022.08.011, doi:10.1016/j.psep.2022.08.011.\\nLiu, X., Chen, C., Zhao, L., et al., 2012. Wavelet analysis of carbon-allowance price dynamics.\\nEnergy Economics 34, 1028–1034. doi:10.1016/j.eneco.2011.09.002.\\nLorentz, H.A., 1938.\\nThe limitations of the lorentz model of the electron.\\nProceedings of\\nthe Royal Society of London. Series A, Mathematical, Physical, and Engineering Sciences\\n167, 148–169. URL: https://doi.org/10.1098/rspa.1938.0124, doi:10.1098/rspa.\\n1938.0124.\\nMallat, S.G., 1989.\\nA theory for multiresolution signal decomposition: The wavelet repre-\\nsentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 11, 674–693.\\ndoi:10.1109/34.192463.\\nOlawumi, M.A., Oladapo, B.I., 2025. Ai-driven predictive models for sustainability. Journal of\\nEnvironmental Management 373, 123472. doi:10.1016/j.jenvman.2024.123472.\\nPang, T., Tan, K., Fan, C., 2023. Carbon price forecasting with quantile regression and feature\\nselection. URL: https://arxiv.org/abs/2305.03224, arXiv:2305.03224.\\n28\\n\\nShahid, F., Zameer, A., Muneeb, M., 2020. Predictions for covid-19 with deep learning models\\nof lstm, gru and bi-lstm. Chaos, Solitons & Fractals 140, 110212. doi:10.1016/j.chaos.\\n2020.110212.\\nTorres, M.E., Colominas, M.A., Schlotthauer, G., Flandrin, P., 2011. A complete ensemble\\nempirical mode decomposition with adaptive noise, in: 2011 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP), IEEE. pp. 4144–4147. URL: https:\\n//doi.org/10.1109/ICASSP.2011.5947265, doi:10.1109/ICASSP.2011.5947265.\\nWang, F., Jiang, J., Shu, J., 2022. Carbon trading price forecasting: based on improved deep\\nlearning method. Procedia Computer Science 214, 845–850.\\nWang, Y., Wang, Z., Wang, X., Kang, X., 2023. Multi-step-ahead and interval carbon price fore-\\ncasting using transformer-based hybrid model. Environmental Science and Pollution Research\\n30, 95692–95719. doi:10.1007/s11356-023-28135-3.\\nZhang, F., Wen, N., 2022. Carbon price forecasting: a novel deep learning approach. Envi-\\nronmental Science and Pollution Research 29, 54782–54795. URL: https://doi.org/10.\\n1007/s11356-022-19713-x, doi:10.1007/s11356-022-19713-x.\\nZhang, K., Yang, X., Wang, T., Thé, J., Tan, Z., Yu, H., 2023. Multi-step carbon price fore-\\ncasting using a hybrid model based on multivariate decomposition strategy and deep learning\\nalgorithms. Journal of Cleaner Production 405, 136959. doi:10.1016/j.jclepro.2023.\\n136959.\\nZhang, Z., Liu, X., Zhang, X., Yang, Z., Yao, J., 2024. Carbon price forecasting using optimized\\nsliding window empirical wavelet transform and gated recurrent unit network to mitigate data\\nleakage. Energies 17, 4358. doi:10.3390/en17174358.\\nZhao, Y., Liu, L., Wang, A., Liu, M., 2023. A novel deep learning based forecasting model\\nfor carbon emissions trading: A comparative analysis of regional markets. Solar Energy 262,\\n111863. doi:10.1016/j.solener.2023.111863.\\nZhao, Y., Xu, J., 2023.\\nHybrid deep learning approaches for high-frequency carbon price\\nforecasting. https://doi.org/10.2139/ssrn.5020501. doi:10.2139/ssrn.5020501.\\nsSRN preprint.\\n29\\n',\n",
       " 'Early Alzheimer’s Disease Detection from Retinal\\nOCT Images: A UK Biobank Study\\n1st Yasemin Turkan\\nDepartment of Computer Engineering\\nIsik University\\nIstanbul, Turkey\\n2nd F. Boray Tek\\nDepartment of Artificial Intelligence and Data Engineering\\nIstanbul Technical University\\nIstanbul, Turkey\\n3rd M. Serdar Nazlı\\nDepartment of Artificial Intelligence and Data Engineering\\nIstanbul Technical University\\nIstanbul, Turkey\\n4th ¨Oyk¨u Eren\\nDepartment of Artificial Intelligence and Data Engineering\\nIstanbul Technical University\\nIstanbul, Turkey\\nAbstract—Alterations in retinal layer thickness, measurable\\nusing Optical Coherence Tomography (OCT), have been associ-\\nated with neurodegenerative diseases such as Alzheimer’s disease\\n(AD). While previous studies have mainly focused on segmented\\nlayer thickness measurements, this study explored the direct\\nclassification of OCT B-scan images for the early detection of AD.\\nTo our knowledge, this is the first application of deep learning\\nto raw OCT B-scans for AD prediction in the literature. Unlike\\nconventional medical image classification tasks, early detection\\nis more challenging than diagnosis because imaging precedes\\nclinical diagnosis by several years. We fine-tuned and evaluated\\nmultiple pretrained models, including ImageNet-based networks\\nand the OCT-specific RETFound transformer, using subject-\\nlevel cross-validation datasets matched for age, sex, and imaging\\ninstances from the UK Biobank cohort. To reduce overfitting\\nin this small, high-dimensional dataset, both standard and OCT-\\nspecific augmentation techniques were applied, along with a year-\\nweighted loss function that prioritized cases diagnosed within\\nfour years of imaging. ResNet-34 produced the most stable\\nresults, achieving an AUC of 0.62 in the 4-year cohort. Although\\nbelow the threshold for clinical application, our explainability\\nanalyses confirmed localized structural differences in the central\\nmacular subfield between the AD and control groups. These find-\\nings provide a baseline for OCT-based AD prediction, highlight\\nthe challenges of detecting subtle retinal biomarkers years before\\nAD diagnosis, and point to the need for larger datasets and\\nmultimodal approaches.\\nIndex Terms—Alzheimer’s Disease, ImageNet, Transfer Learn-\\ning, UK Biobank, Deep Learning, Augmentation\\nI. INTRODUCTION\\nAlzheimer’s disease (AD) is an irreversible and progressive\\nbrain disorder characterized by a decline in cognitive function\\nand is the most prevalent type of dementia. Currently, there is\\nno known cure, but it is marked by a significant reduction in\\nbrain size (neurodegeneration) caused by the accumulation of\\nproteins (amyloid-beta and tau) in neurons [1]. As the retina\\nand brain originate from the same neural tube, the eyes are of-\\nten regarded as extensions of the brain [2]. Postmortem studies\\nThis study was supported by Scientific and Technological Research Council\\nof Turkey (TUBITAK) under the Grant Number 122E509. The authors thank\\nto TUBITAK for their supports.\\nhave shown that amyloid-beta and tau proteins accumulate in\\nthe retinas of individuals with AD [3].\\nHigh-resolution visual imaging technologies, such as optical\\ncoherence tomography (OCT), have recently been proposed\\nto examine the structural and vascular changes in the retinas\\nof patients with AD. In contrast to current gold-standard\\ndiagnostic tools, such as positron emission tomography (PET),\\ncerebrospinal fluid (CSF) analysis, and genetic screening, OCT\\noffers a non-invasive, rapid, and cost-efficient imaging modal-\\nity that is already widely available in clinical ophthalmology\\nsettings. Incorporating such retinal imaging biomarkers into\\nclinical workflows could help identify at-risk individuals for\\nfurther neurological testing, thus enabling earlier intervention\\nstrategies and improving patient outcomes in the future.\\nExisting studies have shown statistical associations between\\nretinal layer thickness and AD. However, to our knowledge,\\nthis is the first study to apply deep learning to raw OCT B-scan\\nimages from the UK Biobank for early AD prediction. This\\nrepresents a significant methodological advance over prior\\nstudies, which primarily relied on retinal fundus images [4],\\n[5] or quantitative summary measurements relying on a layer\\nsegmentation procedure [6] rather than on raw, high-resolution\\nstructural scans.\\nOur study introduces an end-to-end deep learning frame-\\nwork that includes anatomically guided pre-processing, mul-\\ntichannel input design, and rigorous cross-validation tailored\\nfor OCT data. We evaluate multiple state-of-the-art models, in-\\ncluding CNNs and vision transformers, to assess the feasibility\\nof learning subtle retinal biomarkers from small, temporally\\ndistant datasets.\\nThe dataset, model, and methodology are explained in\\nSection II. Section III describes the experiments and presents\\nthe results of this study. Our discussion and conclusions are\\npresented in Sections IV and V, respectively.\\nII. METHODS\\nWe adopted a standardized deep learning pipeline in align-\\nment with the Deep Learning Flow framework tailored for\\narXiv:2511.05106v1  [cs.CV]  7 Nov 2025\\n\\nTABLE I: Summary of Cohorts with Dementia and AD Counts\\nand Percentages\\nDataset\\nCohort\\nAD\\nN\\nN\\n%\\nUK Biobank cohort\\n502,386\\n3955\\n0.79%\\nOCT-Scanned cohorts\\n85,704\\n539\\n0.65%\\nHigh Quality Cohorts with OCT scans\\n43,934\\n223\\n0.51%\\n4-Year dataset cohorts (Dataset 1)\\n49\\n19\\n39%\\n4-Year dataset scans (Dataset 1)\\n58\\n28\\n48%\\n4-Year dataset cohorts (Dataset 2)\\n45\\n19\\n42%\\n4-Year dataset scans (Dataset 2)\\n55\\n28\\n51%\\nAD/MCI diagnosis using OCT-OCTA [7]. This framework\\nemphasizes the best practices for the curation of retinal OCT\\ndatasets, model training, and evaluation protocols. This modu-\\nlar structure emphasizes the reproducibility and fairness of the\\nframework in developing deep learning models for ophthalmic\\nimaging.\\nA. Dataset Curation\\nInclusion and Exclusion: The UK Biobank database in-\\ncludes 502,386 participants, of whom 85,704 underwent OCT\\ntomography scans at instances 0 and 1 using the same OCT\\ndevice, Topcon 3D OCT 1000 Mk 2 [8]. Until July 2023,\\n3,955 patients were diagnosed with Alzheimer’s disease (AD).\\nHowever, only 539 patients with AD underwent corresponding\\nOCT scans. Low-quality scans were excluded based on the cri-\\nteria used by Patel et al. [6], [9]. After applying the exclusion\\ncriteria, 43,934 participants remained in the dataset, with only\\n223 of them diagnosed with Alzheimer’s disease (AD). The\\ndataset included patients who were diagnosed over a 12-year\\nperiod. Due to the slow progression of AD, more significant\\nchanges are typically observed in patients at least three years\\nprior to diagnosis. Since only 9 patients with AD had reached\\nthe three-year mark in our dataset, a four-year threshold was\\nselected to ensure a larger and more representative sample\\nsize (19 patients with AD). Table I summarizes the number of\\ncohorts in the full dataset versus AD at each data-exclusion\\niteration.\\nBoth eyes of participants in the AD cohort were included\\nin the study if they were eligible for the study. However, to\\nprevent data leakage during model development, both eyes\\nfrom a single patient were assigned exclusively to the training\\nor validation set. Age, sex, and instance matched individuals\\nfrom the non-AD group were randomly selected to form a\\nbalanced control group.\\nImage Pre-process and Feature Extraction: In the UK\\nBiobank, FDS bulk files contain the entire, 16-bit raw OCT\\nvolume for both the left and right eyes, providing the complete\\ndynamic range and unprocessed scan data necessary for cus-\\ntom image processing or resegmentation. In contrast, the FDA\\nbulk files are an 8-bit downsampled representation derived\\ndirectly from FDS volumes; they include four segmentation\\nlayers but do not permit recovery of the original 16-bit voxel\\nintensities. The differences between the FDA and FDS image\\nformats are shown in Fig. 1 (a) and (b). In this study, we used\\nFig. 1: Overview of the preprocessing pipeline and retinal\\nlayer annotations in OCT B-scans. (a) Original grayscale\\nOCT scan with inner and outer retinal boundaries. (b) Retinal\\nlayer segmentation with 11 color-coded contours representing\\nthe anatomical boundaries. (c) Alternate view of the same\\nsegmented B-scan for visualization consistency. (d) Pixel-wise\\nretinal layer mask used as input for the second channel of the\\nmodel. The legend on the right maps each color to a specific\\nretinal layer, from the inner limiting membrane (ILM) to the\\nouter boundary of the retinal pigment epithelium (OB RPE).\\nThis multichannel representation encodes both intensity and\\nanatomical structure, providing richer input for deep learning\\nmodels. Images used with permission from the UK Biobank\\nunder Application Number 82266.\\nthe OCTExplorer software, which utilizes the Iowa Reference\\nAlgorithms (Retinal Image Analysis Lab, Iowa Institute for\\nBiomedical Imaging, Iowa City, IA) [10], [11] for the seg-\\nmentation of the fds files (Fig. 1 (c)).\\nThis study focused on ImageNet-pretrained 2D models.\\nTherefore, the middle slice (B-scans) of the raw image vol-\\numes (128 × 512 × 650) was selected as the input. The OCT\\nimage was rectified using the bottom contour, as shown in Fig.\\n1 (c). The areas above the top contour and below the bottom\\ncontour were removed to eliminate noise at the top and bottom\\nof the scans. An additional layer-masked image was generated\\n(Fig. 1 (d) ). Finally, all pre-processed images of size 512 ×\\n650 were cropped to 512 × 512.\\nTo adapt the single-channel OCT images to the three-\\nchannel input requirements of the pretrained models, we\\nconstructed a composite 3-channel representation for each\\nsample, as shown in Fig. 2. Specifically, the original grayscale\\nOCT B-scan was assigned to the first channel of the network.\\nThe second channel contained a layer-masked version of the\\nimage, where each retinal layer was selectively enhanced to\\nhighlight the structural regions of interest [12]. The third\\nchannel consisted of a binary image encoding the retinal layer\\ncontours, providing explicit anatomical boundaries as auxiliary\\ninput. This multichannel representation was designed to enrich\\nthe input with both intensity-based and structural information.\\nThis facilitates improved feature extraction in downstream\\nconvolutional-based architectures.\\n\\nFig. 2: Composite RGB representation of a single OCT B-\\nscan used as model input. (a) First channel: original grayscale\\nOCT image. (b) The second channel: the layer-masked version,\\nwhere the retinal layers are selectively enhanced to highlight\\nthe structural features. (c) Third channel: binary retinal layer\\ncontours providing anatomical boundary information. Images\\nused with permission from the UK Biobank under Application\\nNumber 82266.\\nB. Training\\nAugmentation: Deep learning models are prone to over-\\nfitting, particularly when they are trained on small datasets.\\nTo mitigate this issue and improve generalization, we applied\\nextensive augmentation strategies to artificially expand the\\ntraining set and introduce greater variability, thereby enhanc-\\ning the robustness of the model and reducing the risk of\\noverfitting. During training, the augmentation techniques were\\nrandomly selected and applied one at a time. Custom image\\naugmentations, such as flipping and affine transformations\\n(excluding shear and rotation in rectified images because of\\nthe nature of these images), were applied to all channels.\\nIn addition, OCT-specific augmentations such as occlusion,\\ncontrast adjustment, artificial vascular patterns, and noise\\naddition were employed only in the first channel of the model.\\nTraining Models and Parameters: To investigate the effec-\\ntiveness of deep learning for OCT-based classification, we\\nemployed a pretrained convolutional neural network (CNN)\\nand transformer architecture. Specifically, we used ResNet\\n(depths of 18,34,50, and 101) and VGG (depths of 8 and 11).\\nIn addition to these standard architectures, we also tested\\na domain-specific RETFound [13], a vision transformer pre-\\ntrained on a large dataset (over a million) of OCT and fundus\\nimages. RETFound has demonstrated strong performance in\\nretinal imaging tasks [14] and was therefore included for com-\\nparative evaluation in our Alzheimer’s disease classification\\nframework. We implemented two distinct training strategies:\\nRETFound-S, trained using three identical mid-scan OCT\\nimages as input, and RETFound-C, trained end-to-end using\\nthe composite 3-channel representation described in Fig. 2.\\nWe also developed and trained a custom convolutional\\nmodel to serve as a baseline and explored architectural varia-\\ntions tailored to the specific characteristics of OCT data.\\nAll models were trained using a batch size of 4, which was\\nselected to maintain stable gradient updates while accommo-\\ndating the constraints of small dataset size. We experimented\\nwith a range of learning rates (0.001, 0.0001, and 0.000027)\\nto explore the optimal convergence behavior across different\\nmodel architectures. The AdamW optimizer was used for\\noptimization in all experiments. We employed extensive image\\naugmentation techniques (as detailed in the previous section)\\nto address the risk of overfitting.\\nTo adapt the pretrained backbones for binary classification\\nunder low-data conditions, we appended a lightweight classi-\\nfication head consisting of a fully connected layer (from the\\nfeature dimension to 64 units), Layer Normalization, followed\\nby a ReLU activation, dropout (p = 0.4), and a final linear\\nlayer projecting to two output classes. This minimal regular-\\nization structure was selected to balance the expressiveness\\nand overfitting control.\\nWe employed a year-weighted loss function [14], assigning\\nhigher importance to samples temporally closer to the clinical\\ndiagnosis of Alzheimer’s disease to recognize disease progres-\\nsion dynamics.\\nDue to the extensive data augmentation and regularization\\ntechniques applied to mitigate overfitting, the training and val-\\nidation accuracy exhibited considerable fluctuations, making\\nit difficult to identify an appropriate stopping epoch for the\\nmodel. To stabilize the training and improve convergence, we\\ntrained the models for 100 epochs and then applied Stochastic\\nWeight Averaging (SWA) [15] starting from epoch 80.\\nThe source code supporting the findings of this study\\nwill be released upon publication in the GitHub repository:\\nhttps://github.com/OCTALZ-Project/UKBiobankAD\\nC. Validation\\nComparing Results: We employed a nested cross-validation\\nstrategy [16] for a robust and unbiased performance estima-\\ntion. The data were split into five outer folds, each of which\\nserved as a held-out test set. Within the remaining training\\ndata, three-fold inner cross-validation was used to tune the\\nhyperparameters. The selected model was evaluated using the\\ncorresponding test fold. The predictions from the five outer\\nfolds were pooled to compute a single AUC per run. This\\nprocedure was repeated five times with different random splits,\\nyielding five AUC values per model (5 runs × 5 outer folds ×\\n3 inner folds).\\nWe applied this procedure separately to ResNet (18, 34,\\n50, and 101), VGG (8 and 11), RetFound (S and C), and the\\nCNN baseline models. For each architecture, we calculated the\\nmean AUC across five runs and selected the top-performing\\nvariant within each family. These best variants were then\\ncompared using a calibrated paired t-test [17], with adjusted\\ndegrees of freedom to account for dependence on repeated\\ncross-validation.\\nExplainability: We chose the Grad-CAM method to explain\\nour best-performing model. Instead of using the standard\\nimage overlay format, we used 10 OCT-specific layers and\\nthe central subfield of the macula. We applied a 0.8 threshold\\nto the Grad-CAM results to generate a highly focused explain-\\nability image. The contours of the OCT scan were overlaid on\\nthis image. To evaluate the class-level performance, we used\\nthe Intersection Over Union (IoU), Dice Score, and Filling\\nRatio on each layer, as detailed in the study by Nazli et al. [14].\\n\\nFinally, we added the center subfield region of the OCT b-\\nscans, where the overlaid GradCAM results were highlighted.\\nRunning with another sample set: We generated another\\ntraining dataset with different age, sex, and instance-matching\\nCNs while keeping the AD population the same. For the best-\\nperforming model, the same experiments were performed to\\ndetermine whether the results were consistent with different\\ndatasets.\\nAblation Studies: The model trained with only one of the\\nchannels was replicated for all three channels to determine\\nthe impact of the segmentation and contour information on\\nthe training results.\\nIII. RESULTS\\nWe evaluated multiple deep learning models using nested\\ncross-validation, 5 outer folds, and 3 inner folds to assess the\\nclassification performance in predicting Alzheimer’s disease\\n(AD) using retinal OCT scans acquired 4 years before diag-\\nnosis. Table II lists the top AUC results obtained for each\\nmodel.\\nThe highest performing model was ResNet-34, which\\nachieved a mean AUC of 0.624 ± 0.060 across five nested\\ncross-validation runs. This model was used as a reference\\npoint for pairwise comparisons with alternative architectures.\\nVGG-11 (mAUC = 0.581±0.017) and RETFound-C (mAUC\\n= 0.540 ± 0.037) showed reduced performance relative to\\nResNet-34, but the differences were not statistically significant\\n(corrected paired t-tests p = 0.1458 and p = 0.0845). The\\ncustom CNN model (mAUC = 0.519±0.026) also performed\\nworse than ResNet-34, with significance under the standard\\nt-test (p = 0.0266), but not after correction (p = 0.0364).\\nIn contrast, RETFound-S (mAUC = 0.459 ± 0.068) was\\nsignificantly worse than ResNet-34 under both tests (standard\\np = 0.0043; corrected p = 0.0202).\\nWe further validated the ResNet architecture on an inde-\\npendent sample set (Dataset 2), which achieved a very similar\\nperformance (mAUC = 0.652 ± 0.058), This supports the\\nrobustness of the model.\\nFinally, ablation experiments were conducted by rerunning\\nResNet-34 with reduced feature inputs. When trained with\\nmasked images only (ResNet1, mAUC = 0.452 ± 0.042),\\nOCT images replicated to three channels (ResNet2, mAUC\\n= 0.561 ± 0.061), or layer contour inputs (ResNet3, mAUC\\n= 0.529 ± 0.071), The performance decreased significantly\\ncompared to that of the full multichannel representation. These\\nresults confirm that combining raw OCT, masked, and contour\\ninformation provides the strongest feature representation for\\nearly AD prediction.\\nTo investigate the classification decision of the model and\\nensure that it learns clinically relevant features, we generated\\nand analyzed saliency maps using GradCAM. The results\\nare shown in Fig. 3. First, we aggregated the top 5% most\\nsalient pixels from all test images for each class to identify\\nthe most consistently important regions for classification (Fig.\\n3(a)). Furthermore, we analyzed individual cases to connect\\n(a) Aggregated Top 5% Saliency Regions\\n(b) Saliency Maps for Individual Test Samples\\nFig. 3: Model interpretability analysis. (a) The aggregated\\ntop 5% most salient pixels for the AD (left, red/yellow) and\\nCN (right, blue/green) classes highlight the model’s consistent\\nfocus on distinct anatomical regions. (b) Individual examples\\nshow model attention for True Positives (TP), True Negatives\\n(TN), False Negatives (FN), and False Positives (FP). Images\\nused with permission from the UK Biobank under Application\\nNumber 82266.\\nthe model’s attention to its performance on specific examples\\n(Fig. 3(b)).\\nThe saliency analysis in Table III shows that the model\\ngave minimal attention to the RNFL (Filling Ratio < 6%\\nfor AD) and instead focused on the central macular region\\n(34.9% Filling Ratio for AD), especially the BMEIS and\\nIS/OSJ layers.\\nIV. DISCUSSION\\nThis study presents the first end-to-end deep learning\\nframework for predicting Alzheimer’s disease (AD) from UK\\nBiobank Optical Coherence Tomography (OCT) B-scans up\\nto four years before diagnosis. Our best-performing model,\\nResNet-34, achieved a mean AUC (mAUC) of 0.624 ± 0.060\\non a rigorously selected age, sex, and instance-matched AD\\nand control sample.\\nEarlier studies using data from the UK Biobank predicted\\nearly AD from fundus images. Tian et al [4]. achieved an accu-\\nracy of 0.824 using fundus images with vessel segmentation.\\nWisely et al. [18] obtained an AUC of 0.625 using only OCTA\\nimages and 0.681 with GC-IPL projection maps. However,\\nwhen they used quantitative data, including age and sex, their\\n\\nTABLE II: Model accuracies on 4-Year dataset\\nModel\\nmAUC\\nf1-score\\nPrecision\\nSensitivity\\nSpecificity\\nt-test\\ncorrected t-test\\np Value\\np Value\\nResNet\\n0.624 ± 0.060\\n0.552 ± 0.135\\n0.583 ± 0.086\\n0.680 ± 0.018\\n0.486 ± 0.159\\nVGG\\n0.581 ± 0.017\\n0.527 ± 0.064\\n0.568 ± 0.055\\n0.633 ± 0.062\\n0.500 ± 0.076\\n0.1354\\n0.1458\\nRETFound-C\\n0.540 ± 0.037\\n0.524 ± 0.061\\n0.557 ± 0.039\\n0.607 ± 0.037\\n0.507 ± 0.081\\n0.0728\\n0.0845\\nCustom CNN model\\n0.519 ± 0.026\\n0.490 ± 0.039\\n0.553 ± 0.028\\n0.600 ± 0.041\\n0.464 ± 0.051\\n*0.0266\\n*0.0364\\nRETFound-S\\n0.459 ± 0.068\\n0.468 ± 0.072\\n0.461 ± 0.046\\n0.446 ± 0.122\\n0.479 ± 0.117\\n*0.0043\\n*0.0202\\nResNet1\\n0.452 ± 0.042\\n0.424 ± 0.018\\n0.463 ± 0.021\\n0.520 ± 0.038\\n0.407 ± 0.020\\n*0.0133\\n*0.0208\\nResNet2\\n0.561 ± 0.061\\n0.523 ± 0.095\\n0.531 ± 0.090\\n0.527 ± 0.086\\n0.536 ± 0.107\\n*0.0464\\n0.0577\\nResNet3\\n0.529 ± 0.071\\n0.498 ± 0.091\\n0.517 ± 0.081\\n0.533 ± 0.071\\n0.500 ± 0.104\\n0.0643\\n0.0759\\nResNet4\\n0.652 ± 0.058\\n0.613 ± 0.037\\n0.604 ± 0.051\\n0.593 ± 0.094\\n0.614 ± 0.030\\n0.6815\\n0.6773\\nNote: ResNet1: the ablation test run with masked images replicated to 3 channels. ResNet2: ablation test run with OCT images replicated\\ninto three channels. ResNet3: The ablation test was performed with layer contours replicated into three channels. ResNet4: the ResNet model\\ntrained with the dataset 2. Corrected p-values were computed using a calibrated paired t-test to account for dependence induced by repeated\\ncross-validation [17].\\nTABLE III: Quantitative Overlap of Saliency Maps with\\nRetinal Layers\\nLayer\\nIoU\\nDice\\nFill (%)\\nCN\\nAD\\nCN\\nAD\\nCN\\nAD\\nRNFL-GCL\\n.020\\n.013\\n.039\\n.025\\n10.6\\n5.6\\nGCL-IPL\\n.038\\n.017\\n.070\\n.033\\n15.8\\n7.4\\nIPL-INL\\n.048\\n.028\\n.089\\n.054\\n15.7\\n8.6\\nINL-OPL\\n.052\\n.021\\n.097\\n.041\\n22.7\\n10.7\\nOPL-HFL\\n.067\\n.034\\n.124\\n.065\\n23.7\\n12.6\\nBMEIS\\n.164\\n.103\\n.273\\n.177\\n26.3\\n18.6\\nIS/OSJ\\n.022\\n.015\\n.042\\n.029\\n16.8\\n13.4\\nIB OPR\\n.022\\n.017\\n.042\\n.033\\n15.7\\n14.9\\nIB RPE\\n.019\\n.015\\n.036\\n.029\\n11.6\\n10.9\\nOB RPE\\n.016\\n.015\\n.030\\n.028\\n9.5\\n9.9\\nMacula\\n.249\\n.192\\n.379\\n.304\\n41.3\\n34.9\\nNote: ”Fill (%)” is the Filling Ratio. The filling Ratio is defined\\nas |Saliency ∩Layer|/|Layer|. The bold values indicate the highest\\noverlap for each class within the specific layers.\\nmodel achieved an AUC of 0.96. When all the information\\n(image and quantitative) was combined, the AUC was 0.809.\\nChua et al [19]. obtained an AUC of 0.82 when using a\\nGC-IPL projection map dataset; however, their performance\\nwas reduced to 0.76 when trained with age-matched subjects.\\nThese results show that age and sex can be strong confounding\\nfactors in AD prediction models. There is no OCTA image\\ndataset in the UK Biobank; therefore, these studies [18], [19]\\nused private OCTA image datasets. Our goal was to study only\\nthe structural retinal changes related to AD. As a result, our\\nmodel performance was moderate, but it likely reflects AD-\\nrelated structural features in the retina more directly, as it was\\nnot influenced by demographic confounders.\\nWhen comparing the architectures, ResNet-34 outperformed\\nboth VGG-11 and OCT-pretrained transformer models. VGG-\\n11 achieved a mAUC of 0.581 ± 0.017, which was not\\nsignificantly different from that of ResNet-34 after correction.\\nRETFound-C reached an mAUC of 0.540 ± 0.037, which was\\nnot significantly different. In contrast, RETFound-S performed\\nconsiderably worse, with an mAUC of 0.459 ± 0.068, and\\nthe difference relative to ResNet-34 remained statistically\\nsignificant even after correction for multiple comparisons.\\nThese findings suggest that in low-sample settings, convolu-\\ntional networks may provide more stable representations than\\ntransformer-based models, despite being pre-trained on OCT\\ndata. To further test robustness, we repeated the analysis using\\nthe same AD cohort and a randomly matched control group.\\nPerformance was preserved (AUC = 0.652 ± 0.058), indicating\\nthat the results were stable against variations in the control\\nselection.\\nAblation experiments further emphasized the importance of\\nthe three-channel input strategy. When ResNet was trained\\nusing only masked images, only replicated grayscale OCT, or\\nonly retinal layer contours, performance dropped substantially\\ncompared to the full multichannel input. This demonstrates\\nthat the combination of raw OCT intensity, layer-specific\\nmasking, and anatomical boundary (contour) information pro-\\nvides the most informative feature representations.\\nExplainability analysis showed that the model allocated very\\nlittle attention to the RNFL (Filling Ratio <6% for AD),\\ndespite its frequent use as a biomarker in previous studies.\\nInstead, attention was concentrated on the central macular\\nregion (34.9% for AD), with the highest overlap observed in\\nthe BMEIS and IS/OSJ layers. This indicates that the network\\nrelies on features within the photoreceptor and RPE complex,\\nhighlighting regions that may act as early biomarkers of AD.\\nThese results further support the potential of retinal OCT\\ncombined with AI for noninvasive and scalable early AD risk\\nprediction.\\nDespite these promising results, this study has several lim-\\nitations. The dataset size, particularly in the 4-year diagnostic\\ngroup, constrained the statistical analyses and may have lim-\\nited the generalizability of the findings to broader populations.\\nOne of the main contributing factors to the modest prediction\\naccuracy was the study’s dependency on a single dataset,\\nwhich limited the model’s ability to generalize and increased\\nthe risk of overfitting. Although our nested cross-validation\\nframework was designed to mitigate overfitting and provide\\na robust internal estimate of model performance, external\\nvalidation remains a critical next step. However, to the best\\nof our knowledge, no publicly available OCT datasets exist\\nthat include both retinal imaging and longitudinal Alzheimer’s\\n\\ndisease outcomes, making such validation infeasible. Future\\ncollaborations with clinical centers or research initiatives will\\nbe essential to acquire independent cohorts for further vali-\\ndation. Establishing shared datasets and benchmarks in this\\ndomain would greatly enhance reproducibility and compara-\\nbility across studies and facilitate the translation of OCT-based\\nbiomarkers into clinical use. Additionally, while nested cross-\\nvalidation mitigates overfitting, further improvements could\\nbe achieved through ensembling, multimodal integration (e.g.,\\nOCT angiography, cognitive testing), or temporal modeling\\nof longitudinal scans. Finally, clinical validation against gold-\\nstandard biomarkers (e.g., amyloid PET and CSF tau) is\\nnecessary to establish OCT’s role in preclinical AD screening.\\nV. CONCLUSION\\nThis study presents the first deep learning framework ap-\\nplied to UK Biobank retinal OCT data for early prediction\\nof Alzheimer’s disease up to four years before diagnosis. We\\nproposed an anatomically guided preprocessing pipeline with\\nmultichannel OCT input, hybrid augmentation, and nested\\ncross-validation. ResNet-34 achieved the best performance,\\nwith a mean AUC of 0.624, and robustness was confirmed with\\na re-matched control dataset. Ablation experiments showed\\nthat combining raw, masked, and contour inputs improved\\nperformance, while saliency maps highlighted the macular\\nBMEIS and IS/OSJ layers. Our findings provide a repro-\\nducible baseline for OCT-based AD prediction, highlight the\\nchallenges of detecting subtle retinal biomarkers years before\\nAD diagnosis, and point to the need for larger datasets and\\nmultimodal approaches.\\nACKNOWLEDGEMENTS\\nThis study was conducted using the UK Biobank Resource\\nunder Application Number 82266. Computational resources\\nwere provided by the Turkish National High-Performance\\nComputing Center (UHeM, Project Number 1017802024).\\nThis study was also supported by the Scientific and Techno-\\nlogical Research Council of Turkey (T ¨UB˙ITAK) under Grant\\nNumber 122E509.\\nREFERENCES\\n[1] WHO, “World Health Organization: Dementia Fact Sheet,” pp. 1–5,\\n2022. [Online]. Available: https://www.who.int/news-room/fact-sheets/\\ndetail/dementia\\n[2] A. Grzybowski and P. Barboni, The Eye as a Window to the Brain OCT\\nand Imaging in Central Nervous System Diseases.\\nSpringer, 2020.\\n[3] A. London, I. Benhar, and M. Schwartz, “The retina as a window to the\\nbrain - From eye research to CNS disorders,” Nature Reviews Neurology,\\nvol. 9, no. 1, pp. 44–53, jan 2013.\\n[4] J.\\nTian,\\nG.\\nSmith,\\nH.\\nGuo,\\nB.\\nLiu,\\nZ.\\nPan,\\nZ.\\nWang,\\nS.\\nXiong,\\nand\\nR.\\nFang,\\n“Modular\\nmachine\\nlearning\\nfor\\nAlzheimer’s disease classification from retinal vasculature,” Scientific\\nReports\\n—,\\nvol.\\n11,\\np.\\n238,\\n2021.\\n[Online].\\nAvailable:\\nhttps://doi.org/10.1038/s41598-020-80312-2\\n[5] N. Yousefzadeh, C. Tran, A. Ramirez-Zamora, J. Chen, R. Fang,\\nand M. T. Thai, “Neuron-level explainable ai for alzheimer’s disease\\nassessment from fundus images,” Scientific Reports, vol. 14, 12 2024.\\n[6] F. C. van der Heide, A. Khawaja, T. T. Berendschot, T. J. Littlejohns,\\nE. Ku´zma, R. Luben, P. J. Patel, P. J. Foster, G. Bertelsen, T. von Hanno,\\nB. Johnsen, H. Schirmer, S. C. Rebouc¸as, L. Grasset, C. Delcourt,\\nC. Helmer, and C. D. Stehouwer, “Associations of inner retinal layers\\nwith risk of incident dementia: An individual participant data analysis\\nof four prospective cohort studies,” Alzheimer’s and Dementia, vol. 20,\\npp. 211–220, 1 2024.\\n[7] Y. Turkan, F. B. Tek, F. Arpaci, O. Arslan, D. Toslak, M. Bulut, and\\nA. Yaman, “Automated diagnosis of alzheimer’s disease using oct and\\nocta: A systematic review,” IEEE Access, vol. 12, pp. 104 031–104 051,\\n2024.\\n[8] S. Y. L. Chua, D. Thomas et al., “Cohort profile: Design and methods\\nin the eye and vision consortium of UK Biobank,” BMJ Open, vol. 9,\\nno. 2, pp. 1–13, 2019.\\n[9] P. J. Patel, P. J. Foster, C. M. Grossi, P. A. Keane, F. Ko,\\nA. Lotery, T. Peto, C. A. Reisman, N. G. Strouthidis, and Q. Yang,\\n“Spectral-domain optical coherence tomography imaging in 67 321\\nadults: Associations with macular thickness in the uk biobank study,”\\nOphthalmology, vol. 123, pp. 829–840, 2016. [Online]. Available:\\nhttp://dx.doi.org/10.1016/j.ophtha.2015.11.009\\n[10] M. K. Garvin, M. D. Abr`amoff, X. Wu, S. R. Russell, T. L. Burns, and\\nM. Sonka, “Automated 3-d intraretinal layer segmentation of macular\\nspectral-domain optical coherence tomography images,” IEEE Transac-\\ntions on Medical Imaging, vol. 28, pp. 1436–1447, 9 2009.\\n[11] M. D. Abramoff, M. K. Garvin, and M. Sonka, “Retinal imaging and\\nimage analysis,” pp. 169–208, 2010.\\n[12]\\n¨Oyk¨u Eren, F. B. Tek, and Y. Turkan, “Segmentation based classification\\nof retinal diseases in oct images.” Institute of Electrical and Electronics\\nEngineers Inc., 2024, pp. 890–895.\\n[13] Y. Zhou, M. A. Chia, and other, “A foundation model for generalizable\\ndisease detection from retinal images,” Nature, vol. 622, pp. 156–163,\\n10 2023.\\n[14] M. S. Nazlı, Y. Turkan, F. B. Tek, D. Toslak, M. Bulut, F. Arpacı, and\\nM. C. ¨Ocal, “Retinal disease diagnosis in oct scans using a foundational\\nmodel,” vol. 15618 LNCS.\\nSpringer Science and Business Media\\nDeutschland GmbH, 2025, pp. 208–220.\\n[15] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson,\\n“Averaging weights leads to wider optima and better generalization,”\\narxiv, 2 2019. [Online]. Available: http://arxiv.org/abs/1803.05407\\n[16] Y. Zhong, P. Chalise, and J. He, “Nested cross-validation with ensemble\\nfeature selection and classification model for high-dimensional biolog-\\nical data,” Communications in Statistics: Simulation and Computation,\\nvol. 52, pp. 110–125, 2023.\\n[17] R. R. Bouckaert, “Choosing between two learning algorithms based\\non calibrated tests,” in Proceedings of the Twentieth International\\nConference on Machine Learning (ICML), 2003.\\n[18] C. E. Wisely, D. Wang, R. Henao, D. S. Grewal, A. C. Thompson,\\nC. B. Robbins, S. P. Yoon, S. Soundararajan, B. W. Polascik, J. R.\\nBurke, A. Liu, L. Carin, and S. Fekrat, “Convolutional neural network\\nto identify symptomatic Alzheimer’s disease using multimodal retinal\\nimaging,” British Journal of Ophthalmology, vol. 106, no. 3, pp. 388–\\n395, 2022.\\n[19] J. Chua, C. Li, F. Antochi, E. Toma, D. Wong, B. Tan, G. Garh¨ofer,\\nS. Hilal, A. Popa-Cherecheanu, C. L. H. Chen, and L. Schmetterer,\\n“Utilizing deep learning to predict alzheimer’s disease and mild cogni-\\ntive impairment with optical coherence tomography,” Alzheimer’s and\\nDementia: Diagnosis, Assessment and Disease Monitoring, vol. 17, 1\\n2025.\\n',\n",
       " 'DL101 Neural Network Outputs and Loss Functions\\nFernando Berzal\\nberzal@acm.org\\nDepartment of Computer Science and Artificial Intelligence, University of Granada, Spain\\nI. Activation Functions\\n1\\nII. Loss Functions\\n2\\nIII. Statistical Justification\\n3\\nA. Maximum Likelihood Estimation\\n3\\n1. Regression and Mean Squared Error (MSE)\\n4\\n2. Regression and Mean Absolute Error (MAE)\\n5\\n3. Binary Classification and Binary Cross-Entropy\\n6\\n4. Multi-Class Classification and Categorical\\nCross-Entropy\\n6\\n5. Summary\\n7\\nB. Generalized Linear Models\\n7\\n1. Regression: Gaussian GLM\\n9\\n2. Regression: Laplace GLM\\n9\\n3. Binary Classification: Bernoulli GLM\\n9\\n4. Multi-Class Classification: Multinomial GLM\\n10\\n5. Summary\\n10\\nIV. Additional Situations\\n10\\nA. Binary Classification with Bipolar Encoding\\n10\\nB. Regression of Positive Values\\n11\\nC. Fat Tails\\n13\\nV. Conclusion\\n15\\nA. Activation functions\\n15\\n1. Logistic function\\n15\\n2. Hyperbolic tangent\\n15\\n3. Softmax function\\n16\\n4. Softplus function\\n17\\n5. Swish function\\n17\\n6. ReLU function\\n18\\nB. Loss Functions\\n19\\nReferences\\n22\\nI keep saying the sexy job in the next ten\\nyears will be statisticians. People think I’m\\njoking, but who would’ve guessed that com-\\nputer engineers would’ve been the sexy job of\\nthe 1990s?\\nThe ability to take data -to be\\nable to understand it, to process it, to extract\\nvalue from it, to visualize it, to communicate\\nit- that’s going to be a hugely important skill in\\nthe next decades... Because now we really do\\nhave essentially free and ubiquitous data. So\\nthe complimentary scarce factor is the ability to\\nunderstand that data and extract value from it.\\nHal R. Varian\\nThe McKinsey Quarterly, January 2009\\nChoosing the right loss function is absolutely central\\nto building effective machine learning models.\\nIn the case of deep learning models (1, 3, 4, 6, 17), the\\noutput layer of the neural network can be viewed as a\\ngeneralized linear model. Given its inputs (x1..xn), the\\noutput of the final layer of the neural network is typically\\ncomputed as:\\ny = f\\n n\\nX\\ni=1\\nwixi\\n!\\n= f(⃗w · ⃗x) = f(w⊤x)\\nwhere f is the activation function and (w1..wn) are the\\noutput neuron parameters, a.k.a. weights.\\nIt is also common that a neuron incorporates a bias\\nb, so that its output can be computed as f(⃗w · ⃗x + b).\\nAssuming an extra input x0 = 1 with its corresponding\\nweight w0 = b allows us to absorb the bias into the above\\nexpression, thus avoiding clutter.\\nI. ACTIVATION FUNCTIONS\\nThe output of an artificial neuron is often just a linear\\ncombination of its inputs and its parameters or weights.\\nFor such neurons, it is suitable to define its net input\\n(a.k.a. pre-activation) as z = ⃗w · ⃗x. Such scalar input is\\ntransformed by the neuron activation function f to ob-\\ntain the neuron output y = f(z). Table I summarizes\\nsome common activation functions, which are often non-\\nlinear so that a multilayer neural network can act as an\\nuniversal approximator (11, 15, 21, 28, 34):\\n• The linear or identity function is used mainly in\\nthe output layer for regression tasks where the goal\\nis to predict a continuous numerical value.\\nIt is\\nalso used within specialized network architectures,\\nsuch as convolutional networks, but in hidden lay-\\ners it should also be combined with non-linear ac-\\ntivation functions (since you could always replace\\nmultiple linear layers with a single linear layer, thus\\nloosing the universal approximation properties of\\nmulti-layered networks with non-linear activation\\nfunctions).\\n• The logistic function (σ) is a sigmoid function (i.e.\\nS-shaped) that squashes it input into the [0, 1] in-\\nterval, making it suitable for output layers in bi-\\nnary classification problems, when the output can\\nbe interpreted as a probability.\\narXiv:2511.05131v1  [cs.LG]  7 Nov 2025\\n\\n2\\nActivation function\\nRange\\nDerivative\\nLinear (identity)\\nf(z) = z\\n(−∞, ∞)\\nf ′(z) = 1\\nLogistic (standard sigmoid)\\nf(z) = σ(z) =\\n1\\n1 + e−z\\n(0, 1)\\nf ′(z) = f(z)(1 −f(z))\\nHyperbolic tangent (tanh)\\nf(z) = tanh(z) = ez −e−z\\nez + e−z\\n(−1, 1)\\nf ′(z) = 1 −f 2(z)\\nSoftmax\\nfi(⃗z) =\\nezi\\nPK\\nj=1 ezj\\n[0,1]\\n∂yi\\n∂xj = ∂fi(⃗z)\\n∂xj\\n= yi(δij −yj)\\nSoftplus\\nf(z) = ln(1 + ez)\\n[0, ∞)\\nf ′(z) =\\nez\\n1 + ez =\\n1\\n1 + e−z = σ(z)\\nReLU\\nf(z) = max{0, z}\\n[0, ∞)\\nf ′(z) = H(z) =\\n(\\n1\\nif z > 0\\n0\\nif z ≤0\\nLeaky ReLU\\nf(z) =\\n(\\nz\\nif z > 0\\nαz\\nif z ≤0\\n(−∞, ∞)\\nf ′(z) =\\n(\\n1\\nif z > 0\\nα\\nif z ≤0\\nHeaviside step (LTU)\\nf(z) = H(z) =\\n(\\n1\\nif z > 0\\n0\\nif z ≤0\\n{0, 1}\\nf ′(z) = δ(z) =\\n(\\n∞\\nif z = 0\\n0\\nif z ̸= 0\\nTable I Common activation functions in neural networks: The activation function determines the output of a neuron, trans-\\nforming the weighted sum of its inputs into an output signal. For additional activation functions and their variants, check\\nAppendix A.\\n• The hyperbolic tangent (tanh) function is similar to\\nthe sigmoid but maps the input to the [−1, 1]. It\\nis zero-centered, which often aids in faster conver-\\ngence during training when compared to the logistic\\nfunction. It is often used in conjunction with bipo-\\nlar encoding −1, 1.\\nThe hyperbolic tangent and\\nthe logistic function are related sigmoid squashing\\nfunctions: tanh(z) = 2σ(2z) −1.\\n• The softmax function is typically used in the output\\nlayer for multi-class classification problems with K\\nclasses. It converts a vector of numbers into a prob-\\nability distribution, where the sum of the probabil-\\nities is equal to 1.1\\n• The softplus activation function avoids the main\\ndrawback sigmoid functions (logistic and tanh),\\nwhose gradients go to zero when their net inputs\\nhave large positive or large negative values.\\nFor\\nlarge positive inputs z ≫0, f(z) ≈ln(ez) = z,\\nso that its gradient is not zero. For large negative\\ninputs z ≪0, ez ≈0 and f(z) ≈ln(1) = 0, i.e.\\napproaches 0 but is always positive.\\n1 The derivative of the softmax function can be expressed in terms\\nof the Kronecker delta (δij) and it results in a Jacobian matrix\\nbecause softmax is a vector-valued function. The Kronecker delta\\nis δij = 1 when i = j and δij = 0 when i ̸= j. The derivative\\nof the i-th component of the softmax output, yi, with respect to\\nthe j-th component of the input, xj, is yi(δij −yj).\\n• Rectified linear units (ReLU) are a popular choice\\nfor hidden layers in deep learning models.\\nThe\\nReLU function introduces a single non-linearity\\nand is computationally efficient. The softplus func-\\ntion is a smooth, differentiable approximation of\\nthe ReLU function.\\nAt z = 0, the ReLU func-\\ntion has a sharp corner (non-differentiable point),\\nwhile the softplus function transitions smoothly\\nfrom near-zero to linear.\\n• The Leaky ReLU is a variant of the ReLU function\\nthat attempts to solve the “dying ReLU” problem\\nby allowing a small, non-zero gradient for negative\\ninputs (typically α = 0.01).\\n• The Heaviside step function was used as the acti-\\nvation function in the linear threshold units used\\nby McCulloch and Pitts (32). However, it is not\\nused in modern artificial neural networks because\\nits derivative is the Dirac delta function, zero ev-\\nerywhere except at z = 0, where it is infinitely\\nlarge.\\nTherefore, it is not suitable for gradient-\\nbased learning algorithms.\\nII. LOSS FUNCTIONS\\nWe can now return to our original problem, that of\\nchoosing the right loss function for our particular prob-\\nlem. Let us consider different tasks and justify which loss\\nfunctions seem to be the most natural fit:\\n\\n3\\n• In regression problems, we predict continuous\\nvalues, so the loss must measure the distance be-\\ntween predicted and true values.\\nMean Squared\\nError (MSE) or Mean Absolute Error (MAE) di-\\nrectly quantify this distance, making them natural\\nchoices:\\nMSE = 1\\nn\\nn\\nX\\ni=1\\n(ti −yi)2\\nMAE = 1\\nn\\nn\\nX\\ni=1\\n|ti −yi|\\nwhere n is the number of data points, ti is the ac-\\ntual (target) value, and yi is the predicted value.\\nMSE calculates the average of the squared differ-\\nences between the predicted values and the actual\\nvalues. MSE penalizes large errors more heavily be-\\ncause of the square term, which can be useful when\\nyou want your model to be especially sensitive to\\noutliers or large deviations.\\nMAE treats all errors linearly, so it’s more robust\\n/ less sensitive to outliers. MAE would be a better\\nchoice when you care about median-like behavior\\nrather than mean-like behavior. MAE is also more\\nintuitive to understand as it represents the average\\nerror in the same units as the target variable.\\n• Binary classification is about predicting the\\nprobabilities of two possible outcomes (i.e.\\ntwo\\nclasses). Binary cross-entropy is the most princi-\\npled way to compare predicted probabilities with\\nthe true labels:\\nH = −1\\nn\\nn\\nX\\ni=1\\n(ti · log(yi) + (1 −ti) · log(1 −yi))\\nwhere n is the number of observations, ti is the ac-\\ntual class label (0 or 1), and yi is the predicted\\nprobability of the i-th observation belonging to\\nclass 1.\\nThis loss function is applied when the model out-\\nputs a probability between 0 and 1. Cross-entropy\\nmeasures the “distance” between the predicted\\nprobability distribution and the true distribution\\n(0 or 1). It quantifies how close the predicted prob-\\nability is to the actual class label (0 or 1) for each\\nexample.\\nA lower binary cross-entropy value in-\\ndicates a more accurate model.\\nThe logarithmic\\ncomponent of the formula heavily penalizes confi-\\ndent but incorrect predictionss, which encourages\\nthe model to output well-calibrated probabilities.\\n• In multi-class classification problems, with\\nK different classes (K > 2), the goal is to pre-\\ndict a probability distribution across K classes (of-\\nten via the softmax function). Categorical cross-\\nentropy measures how well the predicted distribu-\\ntion matches the true one-hot encoded label:\\nH = −1\\nn\\nn\\nX\\ni=0\\nK\\nX\\nj=0\\ntij · log(yij)\\nwhere K is the number of classes, n is the number of\\nobservations, tij is a binary indicator (0 or 1) if class\\nlabel j is the correct classification for observation i,\\nand yij is the predicted probability of observation\\ni belonging to class j.\\nThe categorical cross-entropy ensures the model\\nnot only predicts the correct class but also as-\\nsigns high probability to it relative to other classes.\\nSimilar to its binary counterpart, a lower categori-\\ncal cross-entropy value signifies a better-performing\\nmodel.\\nAs we will see, the above choices of loss functions are\\nfar from arbitrary. They are not only natural, but they\\nare also the statistically justified choice for training clas-\\nsification models because minimizing those loss functions\\nis equivalent to maximizing the likelihood of the model\\npredictions.\\nIII. STATISTICAL JUSTIFICATION\\nThe formal justification for why mean squared error\\n(MSE) or mean absolute error (MAE), binary cross-\\nentropy (BCE), and categorical cross-entropy (CCE) are\\nthe natural loss/error functions for their respective prob-\\nlems comes from the principle of maximum likelihood\\nestimation (MLE), a method for estimating the param-\\neters of an assumed probability distribution, given some\\nobserved data. In short, choosing a loss function is equiv-\\nalent to assuming a specific probability distribution for\\nyour data.\\nIt turns out that these common loss func-\\ntions are precisely the negative log-likelihoods for stan-\\ndard probability distributions.\\nA. Maximum Likelihood Estimation\\nThe MLE principle states that we should choose model\\nparameters that maximize the probability (or likelihood)\\nof observing the training data, i.e. the most likely model\\nthat could have produced the observed data. Minimizing\\nthe negative log-likelihood (NLL) of the data is the same\\nas maximizing their likelihood.\\nThe likelihood function measures how ”likely” the ob-\\nserved training data are for different values of the model\\nparameters. Assuming independent and identically dis-\\ntributed data (i.i.d.), the likelihood function is the prod-\\nuct of the probabilities of each individual observation,\\ngiven the model parameters:\\nL(θ|x1, x2, ..., xn) = P(x1|θ) · P(x2|θ) · ... · P(xn|θ)\\n\\n4\\nL(θ|x1, x2, ..., xn) =\\nn\\nY\\ni=1\\nP(xi|θ)\\nwhere L(θ|x1, ..., xn) is the likelihood of the parameter θ\\ngiven the data x1, ..., xn.\\nMultiplying many small probabilities can lead to nu-\\nmerical underflow problems (i.e. a rounding error when\\nfloating-point numbers near zero are rounded to zero).\\nIt is then common to work with the natural logarithm of\\nthe likelihood function, called the log-likelihood function.\\nTaking the log turns the product into a sum:\\nlog L(θ) =\\nn\\nX\\ni=1\\nlog P(xi|θ)\\nSince the logarithm is a monotonically increasing func-\\ntion, maximizing the log-likelihood is the same as max-\\nimizing the likelihood.\\nTherefore, to find the value\\nof the distribution parameters that maximizes the log-\\nlikelihood function of the observed data, you can take\\nthe derivative of the log-likelihood function with respect\\nto the model parameters, set it to zero, and solve for the\\nparameters. When closed analytical solutions cannot be\\nfound, numerical optimization algorithms are used.\\nMLE is widely used because its estimates have sev-\\neral desirable properties: consistency (as the sample size\\ngrows, the MLE gets closer to the true value of the pa-\\nrameters), asymptotic normality (with a large enough\\nsample size, the distribution of the MLE is approximately\\nnormal), efficiency (for large samples, the MLE has the\\nsmallest possible variance among all unbiased estima-\\ntors), and invariance (if you apply a function to a param-\\neter, the MLE of the transformed parameter is simply the\\nfunction applied to the original MLE).\\nThe choice of different underlying probability distribu-\\ntions for the observed data leads naturally to different\\nloss functions. Obviously, MLE relies on the assumption\\nthat the chosen probability distribution should be a good\\nfit for the data. If that is not the case, your MLE esti-\\nmates will be inaccurate. Another key assumption is that\\nthe data must be independent and identically distributed\\n(i.i.d.), i.e. each data point is drawn from the same un-\\nderlying distribution and is independent of the others, a\\nrequirement that might not always hold in practice.\\n1. Regression and Mean Squared Error (MSE)\\nFor regression, we want to predict a continuous scalar\\nvalue y from the input vector ⃗x. We can assume that the\\ntarget variable y is generated from our model prediction\\nf(x) plus some Gaussian (normal) noise, ϵ.\\ny = f(x) + ϵ\\nwhere\\nϵ ∼N(0, σ2)\\nThe noise ϵ is assumed to have a mean of 0 and a constant\\nvariance σ2.\\nThis is equivalent to saying the conditional probability\\nof y given x is a Gaussian distribution centered at our\\nmodel prediction:\\np(y|x) = N(y|f(x), σ2) =\\n1\\nσ\\n√\\n2π e−1\\n2(\\ny−f(x)\\nσ\\n)\\n2\\nTo find the best model parameters, we maximize the\\nlikelihood of observing our entire dataset of n i.i.d. points\\nor, equivalently, its log-likelihood:\\nL(f) =\\nn\\nY\\ni=1\\np(yi|xi) =\\nn\\nY\\ni=1\\nN(yi|f(xi), σ2)\\nlog L(f) =\\nn\\nX\\ni=1\\nlog p(yi|xi) =\\nn\\nX\\ni=1\\nlog N(yi|f(xi), σ2)\\nGiven our assumption of Gaussian noise:\\nlog L(f) =\\nn\\nX\\ni=1\\nlog\\n\\x12\\n1\\nσ\\n√\\n2π\\n\\x13\\n−1\\n2\\n\\x12yi −f(xi)\\nσ\\n\\x132\\nlog L(f) =\\nn\\nX\\ni=1\\nlog\\n\\x12\\n1\\nσ\\n√\\n2π\\n\\x13\\n−\\nn\\nX\\ni=1\\n1\\n2σ2 (yi −f(xi))2\\nlog L(f) = n log\\n\\x12\\n1\\nσ\\n√\\n2π\\n\\x13\\n−\\n1\\n2σ2\\nn\\nX\\ni=1\\n(yi −f(xi))2\\nlog L(f) = −n\\n2 log\\n\\x002πσ2\\x01\\n−\\n1\\n2σ2\\nn\\nX\\ni=1\\n(yi −f(xi))2\\nThe first term is independent of the model predictions\\nf(xi), so to maximize the log-likelihood, we must mini-\\nmize the second term, whose leading factor 1/2σ2 is also\\nindependent of the model predictions. So we just have\\nto minimize\\nn\\nX\\ni=1\\n(yi −f(xi))2\\nwhich is precisely the sum of squared errors (SSE). There-\\nfore, minimizing the mean squared error MSE (just SSE\\ndivided by the constant n) is equivalent to performing\\nmaximum likelihood estimation under the assumption of\\nGaussian noise.\\nIn other words, MSE is the negative\\nlog-likelihood under Gaussian noise.\\nBayes optimality refers to a decision-making princi-\\nple that minimizes the expected loss, representing the\\nbest possible performance for a given task. A model that\\nachieves this lowest possible error rate is called the Bayes\\noptimal classifier (for classification) or Bayes optimal pre-\\ndictor (for regression). It’s a theoretical benchmark that\\nsets the performance ceiling; no other model can do bet-\\nter on average.\\n\\n5\\nIn a regression setting, the Bayes optimal predic-\\ntor, f ∗(x), is the function that minimizes this expected\\nsquared error. It achieves this by always predicting the\\nconditional expectation of the output given the input:\\nf ∗(x) = E[y|x]\\nWhy is the conditional expectation the best possible\\nprediction?\\nImagine you have a fixed input x.\\nThere\\nmight be a range of possible y values associated with it,\\neach with a certain probability. If you pick any prediction\\nˆy, the expected squared error at that point is E[(y −\\nˆy)2|x]. This expression is minimized when ˆy is exactly the\\naverage of all possible y values, which is the conditional\\nmean E[y|x].\\nFrom a more formal point of view, we can prove that\\nthe conditional mean minimizes the squared loss. Let Y\\nbe a random variable with conditional distribution given\\nX = x. For any predictor ˆy ∈R, consider the conditional\\nrisk, i.e. the expected loss for a specific decision, given a\\nparticular observation:\\nR2(ˆy) = E\\n\\x02\\n(Y −ˆy)2|X = x\\n\\x03\\nR2(ˆy) = E[Y 2|x] −2ˆy E[Y |x] + ˆy2\\nIn Bayesian decision theory, the conditional risk is used\\nto evaluate how good a decision-making rule is for each\\npossible data point you might see. The rule that mini-\\nmizes the conditional risk for every possible observation\\nis called the Bayes estimator or Bayes decision rule. So\\nwe differentiate with respect to the prediction ˆy:\\nd\\ndˆy R2(ˆy) = −2 E[Y |x] + 2ˆy.\\nSetting the derivative to zero, we obtain that the condi-\\ntional expectation the best possible prediction:\\nˆy∗= E[Y |x].\\nGiven that the second derivative is 2 > 0, so ˆy∗is the\\nunique minimizer of the MSE. MSE risk is minimized\\nby the conditional mean and learning with the MSE loss\\nfunction estimates E[Y |X].\\nTherefore, minimizing the MSE is equivalent to finding\\nthe Bayes optimal predictor. The remaining error, known\\nas the Bayes error, is caused by the inherent noise or\\nrandomness in the data itself. Under the assumption of\\nGaussian noise, minimizing the negative log-likelihood is\\nequivalent to minimizing the mean squared error. Since\\nminimizing MSE gives us the Bayes optimal predictor, it\\nfollows that minimizing the NLL under Gaussian noise\\nalso leads to the Bayes optimal solution.\\n2. Regression and Mean Absolute Error (MAE)\\nIf instead of starting from a regression model with ad-\\nditive Gaussian noise, we assume additive Laplace noise:\\ny = f(x) + ε,\\nε ∼Laplace(0, b),\\nwith a Laplace probability density\\np(ε) = Laplace(0, b) = 1\\n2be−|ε|\\nb\\nThe conditional probability of y given x is a Laplace\\ndistribution centered at our model prediction:\\np(y|x) = 1\\n2be−|y−f(x)|\\nb\\n,\\nThe log-likelihood of observing our entire dataset of\\ni.i.d. examples is now\\nlog L(f) = −n log(2b) −1\\nb\\nn\\nX\\ni=1\\n|yi −f(xi)|\\nMaximizing L(f) is equivalent to minimizing\\nn\\nX\\ni=1\\n|yi −ˆyi|,\\ni.e., the empirical sum of absolute errors.\\nTherefore,\\nMAE is the negative log-likelihood under Laplace noise.\\nUsing Bayesian decision theory, we can check that the\\nconditional median minimizes the absolute loss. For any\\nˆy ∈R, the conditional risk is now\\nR1(ˆy) = E[|Y −ˆy| | X = x] =\\nZ\\n|y −ˆy| dFY |x(y).\\nThe conditional risk function is R1(ˆy), the mean abso-\\nlute error (MAE), and our goal is to find the prediction\\nˆy that minimizes it. The problem is that the absolute\\nvalue function, |z|, is not differentiable at z = 0. Since\\nwe cannot just take the standard derivative of R1(ˆy) with\\nrespect to ˆy and set it to zero, we find the subgradient2\\nof the risk function R1(ˆy). We split the risk integral into\\ntwo parts, based on where y −ˆy is positive or negative:\\nR1(ˆy) =\\nZ ˆy\\n−∞\\n(ˆy −y) dFY |x(y) +\\nZ ∞\\nˆy\\n(y −ˆy) dFY |x(y)\\nNow, we can differentiate with respect to ˆy using the\\nLeibniz rule for differentiating under the integral sign:\\n∂R1(ˆy) =\\nZ ˆy\\n−∞\\n1 dFY |x(y) −\\nZ ∞\\nˆy\\n1 dFY |x(y)\\nThe integrals of the probability distribution function\\nare, by definition, probabilities:\\nZ ˆy\\n−∞\\ndFY |x(y) = P(Y ≤ˆy | X = x) = FY |x(ˆy)\\n2 A subgradient is a way to generalize the concept of a derivative\\n(or gradient) to functions that aren’t smooth or differentiable\\neverywhere. For a convex function, instead of a single tangent\\nline at a point, you might have a whole set of lines that lie below\\nthe function. Any of these lines’ slopes is a subgradient. A sub-\\ngradient allows us to optimize a function even when the original\\nfunction is not perfectly differentiable.\\n\\n6\\nZ ∞\\nˆy\\ndFY |x(y) = P(Y > ˆy | X = x) = 1 −FY |x(ˆy)\\nSubstituting these back we obtain a subgradient with\\nrespect to ˆy is\\n∂R1(ˆy) = FY |x(ˆy) −(1 −FY |x(ˆy)) = 2FY |x(ˆy) −1,\\nwhere FY |x is the conditional cdf.\\nAny ˆy with 0 ∈∂R1(a) satisfies FY |x(a) = 1\\n2, i.e., ˆy is a\\nconditional median, the point where the cumulative dis-\\ntribution function (CDF) equals 0.5. Thus, minimizing\\nthe expected absolute loss yields the conditional median.\\n3. Binary Classification and Binary Cross-Entropy\\nFor binary classification problems, the target y is ei-\\nther 0 or 1, y ∈{0, 1}. We can assume that the outcome\\nfollows a Bernoulli distribution. The model, with a sig-\\nmoid output ˆy, predicts the probability of the outcome\\nbeing 1. The classifier outputs ˆy ∈(0, 1):\\nP(y = 1|x) = ˆy\\nP(y = 0|x) = 1 −ˆy\\nBoth expressions can be combined into a more succinct\\nsingle expression, the Bernoulli likelihood for a single ob-\\nservation:\\nP(y|x) = ˆyy(1 −ˆy)1−y\\nAssuming i.i.d. examples, as always, the likelihood for\\nthe whole training dataset is:\\nL =\\nn\\nY\\ni=1\\nP(yi|xi) =\\nn\\nY\\ni=1\\nˆyyi\\ni (1 −ˆyi)1−yi\\nTaking the logarithm gives the log-Likelihood:\\nlog L =\\nn\\nX\\ni=1\\n[yi log(ˆyi) + (1 −yi) log(1 −ˆyi)]\\nSince our goal is maximizing this log-likelihood, we can\\nalso minimize the negative of this quantity. The averaged\\n(per example) negative log-likelihood is precisely the bi-\\nnary cross-entropy:\\nBCE = −1\\nn\\nn\\nX\\ni=1\\n[yi log(ˆyi) + (1 −yi) log(1 −ˆyi)]\\nOver the training examples, minimizing the average\\nBCE is equivalent to the maximum likelihood estimate\\nfor a Bernoulli model:\\nminimizing the binary cross-\\nentropy is identical to maximizing the log-likelihood of\\nthe data under the assumption of a Bernoulli distribu-\\ntion.\\nTrue probabilities minimize the expected log loss (i.e.\\nthe binary cross-entropy).\\nGiven x, for any candidate\\nˆy ∈(0, 1), the conditional risk is\\nRBCE(ˆy) = E[−Y log ˆy −(1 −Y ) log(1 −ˆy) | X = x]\\nRBCE(ˆy) = −y log ˆy −(1 −y) log(1 −ˆy),\\nWe minimize such conditional risk by differentiation:\\nd\\ndˆy RBCE(ˆy) = −y\\nˆy + 1 −y\\n1 −ˆy\\nSetting the derivative to zero:\\n−y\\nˆy + 1 −y\\n1 −ˆy = 0\\n⇒\\n(1 −y)ˆy = y(1 −ˆy)\\n⇒\\nˆy = y\\nThe second derivative is\\nd2\\ndˆy2 RBCE(ˆy) = y\\nˆy2 +\\n1 −y\\n(1 −ˆy)2 > 0\\nshows that the conditional risk is strictly convex, so ˆy =\\ny is its unique minimum.\\nTherefore, the log loss is a\\nstrictly proper scoring rule: it is minimized by the true\\nconditional probability.\\n4. Multi-Class Classification and Categorical Cross-Entropy\\nFor multi-class classification, the target y belongs to\\none of K classes. We can now assume that the outcome\\nfollows a multinomial distribution, also known as cate-\\ngorical, multinoulli, or generalized Bernoulli distribution.\\nThe true label yi is represented as by one-hot encoded\\nvector and the model softmax output ˆyi is a probability\\ndistribution, i.e. a vector of probabilities for each of the\\nK classes:\\nˆyi = [pi1, pi2, ..., piK]\\nThe probability of observing the one-hot vector yi is:\\nP(yi|xi) =\\nK\\nY\\nk=1\\npyik\\nik\\nSince yik is 1 only for the true class and 0 otherwise,\\nthis product simply picks the predicted probability for\\nthe correct class.\\nThe likelihood for the whole training dataset is:\\nL =\\nn\\nY\\ni=1\\nK\\nY\\nk=1\\npyik\\nik\\nTaking the logarithm gives the log-likelihood:\\nlog L =\\nn\\nX\\ni=1\\nK\\nX\\nk=1\\nyik log(pik)\\n\\n7\\nMaximixing this log-likelihood is equivalent to mini-\\nmizing its negative, the negative log-likelihood. In this\\ncase, the negative log-likelihood is just the categorical\\ncross-Entropy loss:\\nCCE = −\\nn\\nX\\ni=1\\nK\\nX\\nk=1\\nyik log(pik)\\nTherefore, minimizing the categorical cross-entropy is\\nformally equivalent to maximizing the log-likelihood of\\nthe data under the assumption of a multinomial distri-\\nbution: the categorical cross-entropy is the multinomial\\nnegative log-likelihood.\\nLet us now check how the true distribution minimizes\\nthe expected cross-entropy. Given x, let y = (y1, . . . , yK)\\nbe the true class distribution and ˆy any candidate model\\noutput. The conditional risk is now\\nRCCE(ˆy) = E\\n\"\\n−\\nK\\nX\\nk=1\\nYk log ˆyk | X = x\\n#\\n= −\\nK\\nX\\nk=1\\nyk log ˆyk.\\nThis is the cross-entropy H(y, ˆy), since\\n−\\nX\\nk\\nyk log ˆyk = −\\nX\\nk\\nyk log yk +\\nX\\nk\\nyk log yk\\nˆyk\\nH(y, ˆy) = H(y) + KL(y∥ˆy)\\nGiven that H(y) is constant with respect to ˆy and\\nKL(y∥ˆy) ≥0 with equality if and only if ˆy = y, the\\nunique minimizer of the conditional risk is ˆy = y. There-\\nfore, categorical cross-entropy is a strictly proper scoring\\nrule: its expected value is uniquely minimized when the\\npredicted probability distribution exactly matches the\\ntrue (or empirical) probability distribution.\\n5. Summary\\nWe have seen that choosing loss functions is not ar-\\nbitrary.\\nCommon loss functions arise as the negative\\nlog-likelihood of natural probabilistic models and yield\\nBayes-optimal estimators under common noise assump-\\ntions (see Table II).\\n• For regression problems, MSE is justified by Gaus-\\nsian MLE and Bayes-optimality for the mean;\\nwhereas MAE is the result of Laplace MLE and\\nBayes-optimality for the median.\\n• For binary and multi-class classification, cross-\\nentropy is the negative log-likelihood of Bernoulli\\nand multinomial models, respectively, and is a\\nstrictly proper scoring rule, uniquely minimized by\\nthe true conditional probabilities. Properness en-\\nsures calibrated probability estimation.\\nWith respect to their robustness:\\n• MSE is optimal under homoscedastic Gaussian\\nnoise and targets the mean. It is sensitive to out-\\nliers because Gaussian tails penalize large devia-\\ntions quadratically.\\n• MAE is optimal under Laplace noise and targets\\nthe median. Heavier tails yield linear penalties and\\nrobustness to outliers.\\n• Cross-entropy arises from discrete likelihoods. Its\\nstrict convexity in predicted probabilities penal-\\nizes overconfident misclassifications and aligns with\\nmaximum likelihood estimation for categorical out-\\ncomes.\\nThe loss function used in training deep learning models\\nis the negative log-likelihood of the chosen distribution.\\nThat is the reason why the “right” loss function depends\\non the prediction task.\\nB. Generalized Linear Models\\nThe output layer of many deep neural networks can\\nbe seen as a generalized linear model (GLM), where the\\nactivation function of the output neurons corresponds to\\nthe inverse of a canonical link function and the error (or\\nloss) function is derived from the probability distribution\\nassumed by the GLM. This connection provides a statis-\\ntical foundation for the design of neural network output\\nlayers for various machine learning tasks, since the loss\\nfunctions discussed (MSE, MAE, cross-entropy) are the\\nnegative log-likelihoods of GLMs under different assump-\\ntions about the distribution of the response variable.\\nGeneralized linear models (GLMs) extend the princi-\\nples of linear regression to response variables that are not\\nnormally distributed. A GLM is composed of three key\\nelements:\\n• A random component: The probability distribution\\nof the response variable Y , taken from the expo-\\nnential family, a broad class of distributions that\\nincludes the Gaussian, Bernoulli, binomial, multi-\\nnomial, exponential, gamma, beta, chi-squared, ge-\\nometric, Dirichlet, Whishart, and Poisson distribu-\\ntions.\\n• A systematic component: A linear predictor, η =\\nXβ, which is a linear combination of the input fea-\\ntures. The model parameters β would be the out-\\nput layer weights w in a deep learning context.\\n• A link function: A function g(·) that connects the\\nexpected value of the random component (e.g. the\\nmean of Y ) to the systematic component, i.e. the\\nlinear predictor (e.g. g(µ) = η).\\nA distribution is in the exponential family if its proba-\\nbility mass/density function can be written in the form:\\np(y|η) = h(y)eηT (y)−A(η)\\n\\n8\\nProblem type\\nOutput activation Loss function Distribution\\nMLE estimate\\nRegression\\nLinear\\nMSE\\nGaussian\\nMean\\nRegression\\nLinear\\nMAE\\nLaplace\\nMedian\\nRegression\\nLinear\\nlog\\nPareto\\nMode\\nClassification: Binary\\nLogistic sigmoid\\nBCE\\nBernoulli\\nConditional probability\\nClassification: Binary (bipolar) Hyperbolic tangent\\nBCE\\nBernoulli\\nConditional probability\\nClassification: Multi-class\\nSoftmax\\nCCE\\nMultinomial Class probability distribution\\nTable II Common loss functions and their MLE justification.\\nwhere y is the outcome variable, η (eta) is the natural pa-\\nrameter of the distribution, T(y) is the sufficient statistic\\n(often, just y), h(y) > 0 is the base measure, and A(η)\\nis the log-partition of the particular probability distribu-\\ntion.\\nCanonical link functions are a specific type of link func-\\ntion that have desirable mathematical properties, often\\nleading to more stable and efficient model fitting. Each\\nprobability distribution in the exponential family has a\\nunique canonical link function. In a generalized linear\\nmodel (GLM), the canonical link function, g(.), is the\\nspecific function that maps the expected value of the out-\\ncome, µ = E[Y ], to the natural/canonical parameter, η,\\ni.e.\\nη = g(µ).\\nWhen η(θ) = θ, the exponential fam-\\nily is said to be in canonical form. In the special case\\nthat η(θ) = θ and T(y) = y, the distribution is in the\\nnatural exponential family (NEF), a special case of the\\nexponential family.\\nThis choice is “canonical” or natural because it directly\\nlinks the model linear predictor to the distribution nat-\\nural parameter.\\nGiven the canonical link function, an\\nalternative, equivalent form for members of the exponen-\\ntial family of probability distributions is\\np(y|η, s) = 1\\nsh\\n\\x10y\\ns\\n\\x11\\ng(η)e\\nηy\\ns\\ntaking into account that, when f(x) is a normalized den-\\nsity function, 1\\nsf\\n\\x00 y\\ns\\n\\x01\\nis also a normalized density func-\\ntion.\\nTherefore, the function g(η) can be interpreted as the\\ncoefficient that ensures that the distribution is normal-\\nized:\\ng(η) s.t.\\nZ\\np(y|η, s)dy = 1\\nwhich can be used to compute the conditional expecta-\\ntion of y, E[y]\\nE[y|η, s] =\\nZ\\np(y|η, s)ydy\\nby computing the gradient of the above equality with\\nrespect to the natural parameter η:\\n∇η\\nZ\\np(y|η, s)dy = ∇η1 = 0\\n∇ηg(η)\\nZ 1\\nsh\\n\\x10y\\ns\\n\\x11\\ng(η)e\\nηy\\ns dy = 0\\n∇ηg(η)\\nZ 1\\nsh\\n\\x10y\\ns\\n\\x11\\ne\\nηy\\ns dy\\n+ g(η)\\nZ 1\\nsh\\n\\x10y\\ns\\n\\x11\\ne\\nηy\\ns\\n\\x10y\\ns\\n\\x11\\ndy = 0\\n∇ηg(η)\\ng(η)\\nZ 1\\nsh\\n\\x10y\\ns\\n\\x11\\ng(η)e\\nηy\\ns dy\\n+ 1\\ns\\nZ 1\\nsh\\n\\x10y\\ns\\n\\x11\\ng(η)e\\nηy\\ns ydy = 0\\n∇ηg(η)\\ng(η)\\nZ\\np(y|η, s)dy + 1\\ns\\nZ\\np(y|η, s)ydy = 0\\n∇ηg(η)\\ng(η)\\n· 1 + 1\\nsE[y|η, s] = 0\\nso\\nE[y|η, s] = −s∇ηg(η)\\ng(η)\\n= −s d\\ndη log g(η)\\nThe log likelihood for this model, in terms of η, is\\nlog p(y|η) =\\nn\\nX\\ni=1\\nlog\\n\\x141\\nsh\\n\\x10yi\\ns\\n\\x11\\ng(ηi)e\\nηiyi\\ns\\n\\x15\\nlog p(y|η) =\\nn\\nX\\ni=1\\nh\\n−log s + log h\\n\\x10yi\\ns\\n\\x11\\n+ log g(ηi) + ηiyi\\ns\\ni\\nAssuming that all examples share a common scale pa-\\nrameter, s, and h(yi/s) are independent of i and the\\nlog h(yi/s) term can be considered constant. The deriva-\\ntive of the log likelihood with respect to the model pa-\\nrameters is then\\n∇w log p(y|η) =\\nn\\nX\\ni=1\\n\\x14 d\\ndηi\\nlog g(ηi) + yi\\ns\\n\\x15 dηi\\ndyi\\ndyi\\ndzi\\n∇wzi\\n\\n9\\nwhere zi = wixi.\\nGiven our expression for the conditional mean of y\\nd\\ndηi\\nlog g(ηi) = −1\\nsE[yi|η] = −ˆyi\\ns\\nTherefore\\n∇w log p(y|η) =\\nn\\nX\\ni=1\\n1\\ns [yi −ˆyi] ψ′( ˆyi)f ′(zi)xi\\nwhere ˆyi = f(zi), f ′(zi) = dyi\\ndzi , ψ′( ˆyi) = dηi\\ndyi , and xi =\\n∇wzi = ∇wwixi.\\nThat expression can be simplified if we choose a par-\\nticular link function f−1(ˆy) = ψ(ˆy). In other words, the\\nactivation function used in the output layer of a neu-\\nral network is the inverse of the canonical link function,\\ng−1(η).\\nGiven that z = f −1(ˆy), z = ψ(ˆy). Since f ′(ψ)ψ′(ˆy) =\\n1, f ′(z)ψ′(ˆy) = 1.\\nTherefore, the gradient of the log\\nlikelihood reduces to\\n∇w log p(y|η) = 1\\ns\\nn\\nX\\ni=1\\n[yi −ˆyi] xi\\nIt comes as no surprise that the derivative of the log\\nlikelihood with respect to the parameter vector, used by\\ngradient-descent algorithms to train neural networks, al-\\nways results in contributions to the error function of the\\nform [yi −ˆyi] xi when the natural pairs of activation and\\nloss functions are chosen (linear/MSE, sigmoid/BCE,\\nsoftmax/CCE). The formal connection between the loss\\nfunctions derived from maximum likelihood estimation\\nand canonical link functions lies in the exponential fam-\\nily of distributions.\\n1. Regression: Gaussian GLM\\nIn a regression problem, the goal is to predict a con-\\ntinuous value. Linear regression is a GLM with Gaussian\\nerrors and the identity link. MSE is its natural loss. From\\na GLM perspective:\\n• Random component: The response variable is as-\\nsumed to follow a Gaussian (normal) distribution,\\nY ∼N(µ, σ2).\\n• Canonical link function: For a Gaussian distribu-\\ntion, the expected value is its mean, so the natural\\nparameter is η = E[Y ] = µ.\\nThe canonical link\\nfunction connects the expected value to the nat-\\nural parameter, so, for the Gaussian distribution,\\nthe canonical link function is the function g(µ) = η.\\nSince both η and µ are the same, the canonical link\\nfunction is the identity function g(µ) = µ.\\nThis\\nmeans the expected value of the response is directly\\nmodeled by the linear predictor: µ = Xβ.\\nAs shown previously, assuming a Gaussian distribu-\\ntion for the data leads directly to the MSE loss func-\\ntion via maximum likelihood estimation: minimizing the\\nMSE is mathematically equivalent to maximizing the log-\\nlikelihood of the data under a Gaussian distribution.\\nThe canonical link for the Gaussian distribution is the\\nidentity function. The inverse link, g−1(η), which dic-\\ntates the neural network output layer activation, is also\\nthe identity function (i.e., a linear layer). This formally\\nconnects the linear output layer and MSE loss function to\\nthe canonical link of the underlying Gaussian probability\\nassumption.\\nIn summary, a neural network with a linear output\\nlayer trained with MSE is performing a form of non-linear\\nregression, where the hidden layers learn a complex trans-\\nformation of the input features, and the output layer acts\\nas a simple linear regression on these learned features.\\n2. Regression: Laplace GLM\\nLinear regression in terms of MAE is a GLM with\\nLaplace errors and the identity link. The GLM goal is\\nminimizing the average absolute difference between pre-\\ndicted and actual values rather than the more common\\nmean squared error:\\n• Random component:\\nThe response variable is\\nassumed to follow a Laplace distribution, Y\\n∼\\nLaplace(µ, b), with mean µ and scale parameter b.\\n• Canonical link function: For a Laplace distribu-\\ntion, the expected value is its mean, so its natural\\nparameter is η = E[Y ] = µ. For the Laplace dis-\\ntribution, the canonical link function is the func-\\ntion g(µ) = η. Since both η and µ are the same,\\nthe canonical link function is the identity function\\ng(µ) = µ. This means the expected value of the re-\\nsponse is directly modeled by the linear predictor:\\nµ = Xβ.\\nAssuming a Laplace distribution for the data leads\\ndirectly to the MAE loss function via maximum likeli-\\nhood estimation: minimizing the MAE is mathematically\\nequivalent to maximizing the log-likelihood of the data\\nunder a Laplace distribution.\\nAs in the Gaussian case, since the canonical link is the\\nidentity function, the inverse link, g−1(η), which dictates\\nthe neural network output layer activation, is also the\\nidentity function (i.e., a linear layer). This formally con-\\nnects the linear output layer and MAE loss function to\\nthe canonical link of the underlying Laplace probability\\nassumption.\\n3. Binary Classification: Bernoulli GLM\\nFor binary classification, the objective is to predict one\\nof two possible classes.\\nLogistic regression is a GLM\\n\\n10\\nwith Bernoulli distribution and logit link. Binary cross-\\nentropy is its natural loss. From a GLM perspective:\\n• Random component: The response variable is as-\\nsumed to follow a Bernoulli distribution, Y\\n∼\\nBernoulli(p).\\nThe expected value of a Bernoulli\\nvariable is its probability of success, so µ = E[Y ] =\\np.\\n• Canonical link function: The canonical link func-\\ntion for the Bernoulli distribution is the logit func-\\ntion: η = logit(p) = log\\n\\x10\\np\\n1−p\\n\\x11\\n, where p is the prob-\\nability of success (i.e., the probability of y = 1).\\nThe logit transforms probabilities into log odds,\\ngiven that an event odds is just the ratio of the\\nprobability of the event occurring to the probabil-\\nity of not occurring, p/(1 −p).\\nAs shown in the previous Section, assuming a Bernoulli\\ndistribution leads directly to the binary cross-entropy\\nloss function (also known as log loss): Minimizing the\\nbinary cross-entropy is equivalent to maximizing the log-\\nlikelihood of the data under a Bernoulli distribution.\\nThe inverse link, g−1(η), gives us the required activa-\\ntion function:\\np = logit−1(η) =\\n1\\n1 + e−η\\nThe inverse of the logit function is the logistic (or stan-\\ndard sigmoid) function, which maps the linear predic-\\ntor to a probability between 0 and 1.\\nThis formally\\nconnects the sigmoid activation function and the binary\\ncross-entropy loss to the canonical link of the Bernoulli\\ndistribution.\\nA neural network with a single sigmoid output unit\\ntrained with binary cross-entropy loss is essentially learn-\\ning a logistic regression model on a transformed feature\\nspace created by its hidden layers.\\n4. Multi-Class Classification: Multinomial GLM\\nIn multi-class classification, the task is to predict one of\\nseveral mutually exclusive classes. Multinomial logistic\\nregression, also known as softmax regression, is a GLM\\nwith multinomial distribution and generalized logit link.\\nCategorical cross-entropy is its natural loss.\\n• Random component: The response variable is as-\\nsumed to follow a multinomial/categorical distri-\\nbution, Y ∼Multinomial(1, p). Its expected value\\nis the vector of class probabilities, µ = E[Y ] =\\n[p1, . . . , pK].\\n• Canonical link function: The canonical link con-\\nnects the vector of probabilities to the natural pa-\\nrameters. For K classes, there are K −1 natural\\nparameters, ηk = log\\n\\x10\\npk\\npK\\n\\x11\\n. This is the generalized\\nlogit function.\\nAs shown before, assuming a categorical/multinomial\\ndistribution leads directly to the categorical cross-\\nentropy loss function: Minimizing the categorical cross-\\nentropy\\nloss\\nis\\nequivalent\\nto\\nmaximizing\\nthe\\nlog-\\nlikelihood of the data under a categorical/multinomial\\ndistribution.\\nThe inverse of the generalized logit link function,\\ng−1(η), which maps the linear outputs back to proba-\\nbilities, is the softmax function, which takes a vector of\\narbitrary real numbers and transforms them into a prob-\\nability distribution over the problem classes:\\npk =\\neηk\\nPK\\nj=1 eηj\\nA neural network with a softmax output layer trained\\nwith categorical cross-entropy loss is effectively a multi-\\nnomial logistic regression classifier (also known as soft-\\nmax classifier) that operates on the high-level features\\nextracted by the preceding layers of the network.\\n5. Summary\\nThe choice of activation function (linear, sigmoid, soft-\\nmax) is the inverse of the canonical link function for the\\nassumed data distribution, and the choice of loss func-\\ntion (MSE, cross-entropy) is the negative log-likelihood\\nof that same distribution.\\nThis theoretical foundation\\nprovides a cohesive, statistically-grounded framework for\\ndesigning neural network output layers.\\nIV. ADDITIONAL SITUATIONS\\nA. Binary Classification with Bipolar Encoding\\nThe loss function for binary classification problems was\\nobtained for logistic activation functions, where the logis-\\ntic output estimates the probability of the example be-\\nlonging to the positive class, p(y = 1 | x). Training data\\nwas then coded so that y = 1 indicated that the example\\nbelonged to the positive class, whereas y = 0 indicated\\nthat that example belonged to the negative class.\\nHowever, a bipolar encoding of output classes is also\\ncommon in practice: y = +1 for the positive class, y =\\n−1 for the negative class. In that case, we are just scaling\\nand shifting our binary output:\\nybipolar = 2y −1\\nso that\\ny = ybipolar + 1\\n2\\nThe bipolar loss function can be derived from the bi-\\nnary cross-entropy log likelihood:\\nlog L =\\nn\\nX\\ni=1\\n[yi log(ˆyi) + (1 −yi) log(1 −ˆyi)]\\n\\n11\\nProblem type\\nGLM\\nGLM distribution Output activation\\nLink function\\nLoss function\\nRegression\\nLinear regression\\nGaussian\\nLinear\\nIdentity\\nMSE\\nRegression\\nLinear regression\\nLaplace\\nLinear\\nIdentity\\nMAE\\nBinary classification\\nLogistic regression\\nBernoulli\\nLogistic sigmoid\\nLogit\\nBCE\\nBinary classification (bipolar) Logistic regression\\nBernoulli\\nHyperbolic tangent\\nLogit\\nBCE\\nMulti-class classification\\nSoftmax regression\\nMultinomial\\nSoftmax\\nGeneralized logit\\nCCE\\nTable III Common loss functions for different problem types and their GLM justification.\\njust by applying the above transformation both to the\\ntarget variable yi and the model estimation ˆyi.\\nlog L =\\nn\\nX\\ni=1\\nyi + 1\\n2\\nlog\\n\\x12 ˆyi + 1\\n2\\n\\x13\\n+\\n\\x12\\n1 −yi + 1\\n2\\n\\x13\\nlog\\n\\x12\\n1 −ˆyi + 1\\n2\\n\\x13\\nlog L = 1\\n2\\nn\\nX\\ni=1\\n(yi + 1) log (ˆyi + 1)\\n+ (1 −yi) log (1 −ˆy) −n log 2\\nThe last term n log 2 does not affect the log likelihood\\nmaximization, since it is independent of the model pa-\\nrameters, so it can be dropped.\\nThe activation function that matches the above BCE-\\nlike minimization problem can be obtained by applying\\nthe linear transformation to the logistic function, which is\\nthe activation function of choice for binary classification\\nproblems:\\ny(z) = 2σ(z) −1\\n=\\n2\\n1 + e−z −1\\n= 1 −e−z\\n1 + e−z\\n= ez/2\\nez/2\\n1 −e−z\\n1 + e−z\\n= ez/2 −e−z/2\\nez/2 + e−z/2\\n= tanh(z/2)\\nTherefore, the hyperbolic tangent activation function\\nis the suitable activation function to be used for solv-\\ning binary classification problems using bipolar encoding\\nwhen we want to minimize the binary cross-entropy (i.e.\\nthe negative log-likelihood of the data under the assump-\\ntion of a Bernoulli distribution).\\nB. Regression of Positive Values\\nIn many real-world applications, regression models\\nshould always return a positive value for predicting\\nprices, counts, durations, or intensities (e.g.\\nwaiting\\ntimes, insurance claims, rainfall amounts, energy con-\\nsumption...). The choice of activation/loss pair depends\\non the distribution of the target variable and the con-\\nstraints you want to enforce. Let us consider several al-\\nternatives:\\n• The quick and dirty solution is resorting to con-\\nventional regression loss functions. Since they do\\nnot enforce positivity, you can constraint the out-\\nput using an activation function that ensures that\\nthe network output is in the correct domain. ReLU\\nand softplus ensure non-negative values.\\nReLU can be seen as a piecewise linear approxima-\\ntion to the log link role (the canonical link for a\\nPoisson likelihood). ReLU enforces non-negativity,\\nbut grows linearly instead of exponentially, which\\nmight prevent numerical problems. Even though\\nReLU is not the exact MLE link, it is a computa-\\ntionally efficient surrogate that enforces the posi-\\ntivity constraint.\\nThe softplus activation function also ensures pos-\\nitivity.\\nSoftplus is a smooth approximation to\\nReLU. Softplus behaves like ez for large negative\\nz (small positive outputs) and like z for large pos-\\nitive z (linear growth, like ReLU). It also has a\\nstronger tie to likelihood theory. For distributions\\nrequiring strictly positive parameters (e.g. variance\\nin Gaussian, scale in Gamma, rate in Poisson), the\\ncanonical MLE link is the log link (θ = ez). Soft-\\nplus can be loosely interpreted as a numerically sta-\\nble approximation to the exponential link used in\\nMLE, avoiding overflow problems while still enforc-\\ning positivity.\\n• A popular strategy is training the network to pre-\\ndict log ˆy instead of ˆy. You can use the MSE loss\\nfunction on log y vs. log ˆy. The log transformation\\nnaturally enforces positivity: the final output is eˆν,\\nwhere ν is the model output.\\nIt also makes the\\nmodel target multiplicative errors, since y = f(x)·ϵ\\nbecomes log y = log f(x) + ϵ.\\nIn practice, therefore, a model is trained to predict\\nˆν = log ˆy and the final prediction is ˆy = eˆν.\\nOptionally, you can perform bias correction: If er-\\nrors on log scale are approximately Gaussian with\\n\\n12\\nvariance σ2, an unbiased mean estimate is ˆµ =\\neˆν+σ2/2, where the variance σ2 itself can be esti-\\nmated from residuals.\\nSince the output of the model is transformed us-\\ning an exponential function, it does not need to be\\nconstrained and a simple linear output layer can be\\nused with the logarithmic transformation.\\n• The Gamma distribution is a distribution of the\\nexponential family set of probability distributions\\nthat generalizes the exponential, Erlang, and chi-\\nsquared distributions.\\np(x) =\\nλα\\nΓ(α)xα−1e−λx\\nwhere α is a shape parameter and λ = 1/θ is a rate\\nparameter (an equivalent parameterization uses the\\nα shape and θ scale parameters). Its mean is µ =\\nα/λ = αθ and its variance is σ2 = α/λ2 = αθ2.\\nThe log likelihood of the Gamma distribution (for\\na single example) is\\nlog p(x) = α log λ −log Γ(α) + (α −1) log x −λx\\nMinimizing the negative log likelihood of the\\nGamma distribution, a model can learn to output\\nthe two positive parameters of the Gamma distribu-\\ntion using softplus activation functions (to ensure\\npositivity). For stability, a small constant ε ≈10−3\\nis added to the softplus output (to avoid zero pa-\\nrameters). The model can then be used to predict\\nvalues (using the mean of the distribution µ = α/λ)\\nor sample from the distribution to provide uncer-\\ntainty estimates.\\nWe could directly predict the mean ˆy = ˆµ by repa-\\nrameterizing α = λµ:\\np(x) = xλµ−1λλµe−λx\\nΓ(λµ)\\nand maximizing the following log likelihood:\\nlog L =\\nn\\nX\\ni=1\\n(λ ˆyi log λ + (λ ˆyi −1) log yi −λyi −log Γ(λ ˆyi))\\n=\\nn\\nX\\ni=1\\n(λ ˆyi(log λ + log yi) −log yi −λyi −log Γ(λ ˆyi))\\nA simpler approach for mean-only Gamma regres-\\nsion resorts to the deviance3 of the Gamma distri-\\n3 Deviance is a measure of how well a proposed statistical model\\nfits the data compared to a ”perfect” model. This perfect model,\\nknown as the saturated model, has one parameter for every data\\npoint, meaning it fits the data perfectly but has no predictive\\npower. Deviance is derived from the likelihood ratio test, since\\nratio of the likelihoods of the two models gives us an idea of\\nhow much worse our proposed model is compared to the perfect\\none: Λ = Lp/Ls. Deviance is formally defined as -2 times the\\nlog-likelihood ratio: D = −2(Lp −Ls).\\nbution as loss function:\\nLΓ = 2\\n\\x12y −ˆy\\nˆy\\n−log y\\nˆy\\n\\x13\\nUnlike MSE and MAE, which penalizes absolute\\ndifferences, Gamma deviance focuses on relative er-\\nrors. It is invariant to scaling of the target vari-\\nable.\\nIts first term is the percent error relative\\nto the prediction and the logarithmic term makes\\nthe loss function asymmetric (penalizes underpre-\\ndictions much more heavily than overpredictions).\\nAs a result, the loss grows very quickly as your pre-\\ndiction approaches zero for a positive true value.\\nFor the output layer, we can enforce a positive out-\\nput using a positive activation function, such as\\nsoftplus or exp (ez), adding a small positive value\\nε ≈10−6 to avoid zero outputs.\\nThe Gamma GLM canonical link is the inverse link\\n(η = 1/µ). However, divisions by zero are prob-\\nlematic in computers, so a log link is more stable.\\nIn practice, you predict ν = log µ) and then output\\nµ = eν.\\n• If your prediction target is a count (0, 1, 2...), i.e.\\nthe number of events, you can resort to a Poisson\\ndistribution:\\np(k) = λke−λ\\nk!\\nwhere the positive real number λ ≥0 is equal to\\nthe expected value of the distribution (and also its\\nvariance).\\nlog p(k) = k log λ −λ −log k!\\nIn Poisson regression, the dependent variable y is\\nan observed count that follows the Poisson distribu-\\ntion whose rate λ is determined as λ = ew⊤x = eˆy\\nusing a linear activation function.\\nTherefore, the log likelihood of our model is\\nlog L =\\nn\\nX\\ni=1\\n\\x00yi ˆyi −e ˆ\\nyi −log yi!\\n\\x01\\nAn alternative loss function can be derived from\\nthe Poisson distribution calculating the deviance\\nbetween the true count (y) and the predicted rate\\n(ˆy):\\nLP oisson = 2 (ˆy −y log ˆy)\\nSince the Poisson loss function includes a log ˆy\\nterm, undefined for non-positive numbers, the\\nmodel output must always be positive. An expo-\\nnential is the canonical choice (the inverse link in\\nthe underlying GLM model). The softplus smooth\\napproximation of the ReLU function might also be\\nused. It should also be noted that the target vari-\\nable should not be scaled nor normalized, since it\\nmust represent raw counts.\\n\\n13\\n• Tweedie distributions (23) unify Gaussian (p = 0),\\nPoisson (p = 1), Gamma (p = 2), compound Pois-\\nson–Gamma, and inverse Gaussian (p = 3) distri-\\nbutions.\\nFor positive continuous data with mass\\nnear zero, use p ∈(1, 2], where p = 2 corresponds\\nto pure Gamma-like behavior. The loss per exam-\\nple is also derived from the distribution deviance:\\nLT weedie =\\n2\\n1 −p\\n\\x12\\nyˆy1−p −y2−p\\n2 −p\\n\\x13\\nwhere the parameter p can be tuned to match\\nvariance-mean relationship.\\nGamma deviance\\naligns with variance scaling like σ2 ∝µ2, log-MSE\\naligns with constant variance on the log scale, and\\nTweedie covers intermediate power laws (σ2 ∝µp).\\nAs activation function for the output layer, you\\ncould use either softplus (softplus(z)+ε), exp (ez),\\nor even a square (z2 + ε). Softplus is stable and\\nsmooth, exp strongly enforces positivity but can\\nexplode, and a square is less common because its\\ngradients vanish near zero. However, a log link is\\nusually recommended: just predict ν = log µ) using\\na linear output layer and then output µ = eν.\\nMSE or MAE can be used in conjunction with ReLU or\\nsoftplus activation functions when your data is roughly\\nsymmetric around the mean after a log transform. The\\nlogarithmic transformation handles skewed distributions\\n(common in positive-only data) and can be used with\\nlog-normal-like heteroscedasticity, multiplicative noise,\\nor many orders of magnitude in the output. The Gamma\\nor Tweedie loss functions are statistically sound and suit-\\nable for skewed positive values.\\nThe Poisson loss can\\nbe used for predicting counts.\\nThe log link matches\\nthe multiplicative noise and skew of positive-value out-\\ncomes and should be the default choice for implementing\\nGamma/Poisson/Tweedie regression.\\nC. Fat Tails\\nThe tails of the exponential family of probability distri-\\nbutions, which includes the Gaussian, Laplace, and Pois-\\nson distributions, are characterized by their tendency to\\ndecay exponentially, i.e. faster than polynomially, mak-\\ning them light-tailed distributions.\\nDistributions that\\nare not in the exponential family often exhibit heavy\\ntails, where the decay is slower than exponential, such\\nas power-law decay.\\nThese distributions, like the Stu-\\ndent’s t-distribution, Cauchy, and Pareto distributions,\\nassign a much higher probability to extreme values.\\nThe Gaussian assumption, which justifies the use of\\nMSE, penalizes large errors more heavily because of the\\nsquare error term and leads to models model that are\\nespecially sensitive to outliers or large deviations. The\\nLaplace assumption lead to the use of MAE and is more\\nrobust to outliers, a better choice when you care about\\nmedian-like behavior rather than mean-like behavior.\\nThe use of fat-tailed distributions can make models more\\nrobust to the presence of outliers.\\nThe exponential family works well when data is well-\\nbehaved (light-tailed, symmetric), outliers are rare and\\nnot influential, and our main concern the average-case er-\\nror in classification/regression. Fat-tailed distributions,\\nwith power-law instead of exponentially-decaying tails,\\nare the natural choice when residuals/errors are heavy-\\ntailed, extreme deviations matter and should not be\\nwashed out by excessive penalties, and robustness to out-\\nliers is sought without discarding tail information.\\nFor example, let us assume a double symmetric Pareto\\ndistribution, with a sharp probability peak at x = 0 and\\nsymmetric long probability tails derived from the Pareto\\ndistribution:\\np(x) = α\\n2\\n1\\n(1 + |x|)α+1\\nwhere α is a shape parameter, known as the tail in-\\ndex, that controls the heaviness of the distribution tails.\\nThe double Pareto distribution exhibits power-law tails,\\na defining feature of scale-invariant properties (observed\\nin many natural and economic phenomena).\\nIn a regression problem, the target variable y is gen-\\nerated from our model prediction ˆy = f(x) plus some\\nfat-tailed (double Pareto) noise, ϵ:\\ny = f(x) + ϵ\\nwhere\\nϵ ∼DoublePareto(α)\\nThe noise ϵ is assumed to have a mean of 0 (the center\\nof our symmetric double Pareto distribution).\\nThis is equivalent to saying the likelihood of observing\\ny given ˆy is:\\np(y|ˆy) = α\\n2\\n1\\n(1 + |y −ˆy|)α+1\\nAssuming a flat prior over ˆy, the Bayesian-optimal es-\\ntimator is the maximum a posteriori (MAP) estimate:4\\nˆyMAP = arg max\\nˆy\\np(y|ˆy)\\nTaking logarithms:\\nlog p(y|ˆy) = −(α + 1) log(1 + |y −ˆy|) + log (α/2)\\nAssuming that α can be considered constant, the MAP\\nestimate minimizes the logarithmic loss\\nlog L = log(1 + |y −ˆy|)\\n4 MAP (maximum a posteriori) estimation and MLE (maximum\\nlikelihood estimation) are both statistical methods to find the\\nmost likely parameter values, but MAP includes prior knowl-\\nedge, while MLE considers only the observed data. The MAP\\nestimate is the mode of the posterior distribution. MAP finds\\nthe maximum of the posterior probability by combining the like-\\nlihood function with a prior distribution, while MLE finds the\\nmaximum of the likelihood function alone. MAP is a Bayesian\\napproach, while MLE is a frequentist approach. MLE is a special\\ncase of MAP when the prior is uniform.\\n\\n14\\nThe double Pareto loss behaves like MSE near zero, like\\nMAE for moderate errors, and becomes even more robust\\nin the tails due to its logarithmic growth, which resembles\\ncross-entropy. Comparing its gradient with respect to the\\nmodel output (against MSE and MAE) shows how double\\nPareto naturally down-weights outliers:\\n• MSE: Linear gradient (large for big errors).\\nd\\ndˆy LMSE = 2(ˆy −y)\\n• MAE: Constant gradient (more robust).\\nd\\ndˆy LMAE = sign(ˆy −y)\\n• Log: Decaying gradient (robust and smooth).\\nd\\ndˆy Llog = sign(ˆy −y)\\n1 + |ˆy −y|\\nThe double Pareto likelihood is symmetric and uni-\\nmodal. Under a flat prior, the MAP estimate coincides\\nwith the mode of the likelihood. This mirrors the Gaus-\\nsian case, where the conditional mean is optimal under\\nMSE, and the Laplace case, where the conditional me-\\ndian is optimal under MAE, but here, the conditional\\nmode is optimal under double Pareto noise.5\\nLet Y be a random variable with conditional distribu-\\ntion given X = x. For any predictor ˆy ∈R, consider\\nthe conditional risk, i.e. the expected loss for a specific\\ndecision, given a particular observation:\\nRlog(ˆy) = E[log(1 + |Y −ˆy|)|X = x]\\nThe Bayes estimator, or Bayes decision rule, minimizes\\nthe conditional risk for every possible observation:\\nd\\ndˆy Rlog(ˆy) = sign(E[Y |x] −ˆy)\\n1 + |E[Y |x] −ˆy|\\nSetting the derivative to zero, we obtain that the condi-\\ntional expectation the best possible prediction:\\nsign(E[Y |x] −ˆy)\\n1 + |E[Y |x] −ˆy| = 0\\nsign(E[Y |x] −ˆy) = 0\\n5 In a symmetric unimodal double Pareto distribution, the mode\\nis the point of maximum density. The median, for a symmet-\\nric distribution centered at 0, is also 0. In asymmetric double\\nPareto distribution, with tails decaying at different rates, the\\nmode, median, and mean are all distinct. The mean exists only\\nwhen the tail index α > 1 but is undefined when α ≤1 because\\nthe integral diverges due to heavy tails. When extreme values\\ndominate, the mean becomes unstable, and the MAP (mode) or\\nmedian are preferred estimators.\\nE[Y |x] −ˆy\\n|E[Y |x] −ˆy| = 0\\nˆy∗= E[Y |x].\\nLogarithmic risk is minimized by the expectation and\\nlearning with the logarithmic loss function estimates\\nE[Y |X].\\nThe Gaussian distribution underestimates tail risk and\\nthe mean is sensitive to outliers. The Laplace distribu-\\ntion captures moderate tails and the median is somewhat\\nmore robust to outliers. The double Pareto distribution\\nexplicitly models fat tails and is extremely robust to out-\\nliers, since the mode ignores tail mass.\\nLet us now model ˆy = f(x) to find the optimal form\\nof f. If we use a linear output layer f(x) = w⊤x = ⃗w · ⃗x,\\nis this form optimal under the double Pareto likelihood?\\nThe double Pareto loss is convex in w⊤x. The true con-\\nditional expectation E[y|x] is linear in x (E[y|x] = ˆy =\\nw⊤x) and, therefore, the Bayesian-optimal estimator un-\\nder symmetric noise is also linear. A linear output layer is\\nthe optimal solution because the symmetry and convexity\\nof the double Pareto likelihood preserve the optimality of\\nlinear predictors.\\nSince we are exploring loss–activation pairs, we can\\nsketch how a double Pareto-inspired loss could also be\\npaired with a non-saturating activation function, given\\nthat premature saturation would hide tail information.\\nPotential candidates, for classification settings, would in-\\nclude:\\n• Softsign (similar to a sigmoid, but with polynomial\\ntails consistent with Pareto-like decay):\\nf(z) =\\n1\\n1 + |z|\\n• Arctan (bounded, but with slower saturation than\\na sigmoid):\\nf(z) = arctan(z)\\nSuch pairs would be robust alternatives for heavy-\\ntailed distributions: the loss functions discounts extreme\\ndeviations without ignoring them, whereas the activa-\\ntion allows gradients to flow even for large latent values\\n(avoiding sigmoid vanishing gradient in the tails).\\nFat tails beat exponential ones in situations with\\nheavy-tailed leptokurtic distributions, which include fi-\\nnancial and risk modeling, robust regression with out-\\nliers (e.g.\\nspikes due to sensor faults), extreme event\\npredictions (e.g. insurance claims, natural disasters, or\\nnetwork traffic spikes), and privacy-preserving or adver-\\nsarial robust learning (i.e. when adversaries inject large\\nperturbations).\\n\\n15\\nV. CONCLUSION\\nError (or loss) functions used for training deep learn-\\ning models measure the discrepancy between the model’s\\npredictions and the true values.\\nThe choice of an ap-\\npropriate error function is crucial for training a neural\\nnetwork effectively.\\nFrom a statistical point of view, the “right” loss func-\\ntion is simply the negative log-likelihood of the GLM cor-\\nresponding to the assumed distribution of the response.\\nThat is the reason why MSE, MAE, BCE, and CCE are\\nnot just convenient, they are statistically principled. The\\nsame idea of negative log-likelihood minimization can be\\nused to obtain a logarithmic loss function for regression\\nunder fat-tailed distributions, which should be the loss\\nof choice for extreme value regression.\\nIn classification problems, cross-entropy losses are the\\nnegative log-likelihoods of exponential family distribu-\\ntions (Bernoulli, multinomial) and are strictly proper\\nscoring rules, uniquely minimized by the true probabili-\\nties.\\nThe activation function in the network output layer\\nensures that the network outputs lie in the correct do-\\nmain (e.g. real values, probabilities, positive rates). The\\nmost suitable activation function can be justified as the\\ninverse canonical link function of a GLM-like likelihood\\nmodel (for exponential distributions) or the Bayesian-\\noptimal estimator under the assumed noise distribution\\n(for fat-tailed distributions).\\nAppendix A: Activation functions\\n1. Logistic function\\nThe logistic or standard sigmoid function is defined as\\nσ(x) =\\n1\\n1 + e−x\\nThe derivative of the logistic function has a remarkably\\nsimple and useful form:\\nσ′(x) = d\\ndxσ(x) = σ(x) · (1 −σ(x))\\nThis form is highly efficient for computation, especially\\nin neural networks where the output σ(x) is often already\\ncalculated during the forward pass. The derivative is a\\nbell-shaped curve with a maximum value of 0.25 at x = 0,\\nwhere σ(0) = 0.5 (σ(x)(1 −σ(x)) is maximized when\\nσ(x) = 1 −σ(x), or σ(x) = 0.5). As the input |x| gets\\nvery large (positive or negative), σ(x) approaches 1 or 0,\\nmaking the derivative σ(x)(1 −σ(x)) approach 0, which\\ncontributes to the vanishing gradient problem.\\nStep-by-step derivation:\\n• Start with the logistic function:\\nσ(x) =\\n1\\n1 + e−x = (1 + e−x)−1\\n• Apply the chain rule: u = 1 + e−x, so σ(x) = u−1.\\nd\\ndxσ(x) = d\\ndu(u−1) · d\\ndx(1 + e−x)\\n• Calculate the derivatives:\\nd\\ndu(u−1) = −1 · u−2 = −u−2\\nd\\ndx(1 + e−x) = 0 + d\\ndx(e−x) = e−x · (−1) = −e−x\\n• Substitute and simplify:\\nd\\ndxσ(x) = (−u−2) · (−e−x) = e−x\\nu2\\nSubstitute u = 1 + e−x back into the equation:\\nσ′(x) =\\ne−x\\n(1 + e−x)2\\n• Express in terms of σ(x):\\nσ′(x) =\\ne−x\\n1 + e−x ·\\n1\\n1 + e−x\\ne−x\\n1 + e−x = 1 + e−x −1\\n1 + e−x\\n= 1 + e−x\\n1 + e−x −\\n1\\n1 + e−x = 1 −σ(x)\\nσ′(x) = (1 −σ(x)) · σ(x)\\n2. Hyperbolic tangent\\nThe hyperbolic tangent function is defined as\\ntanh(x) = ex −e−x\\nex + e−x\\nWe can relate the hyperbolic tangent to the logistic\\nfunction as follows:\\ntanh(x) = ex −e−x\\nex + e−x\\n= ex\\nex\\n1 −e−2x\\n1 + e−2x\\n= 2 −1 −e−2x\\n1 + e−2x\\n=\\n2\\n1 + e−2x −1 + e−2x\\n1 + e−2x\\n=\\n2\\n1 + e−2x −1\\n= 2σ(2x) −1\\n\\n16\\nFrom the above relationship, we could also express the\\nlogistic function in terms of the hyperbolic tangent:\\nσ(x) = 1\\n2\\n\\x10\\ntanh\\n\\x10x\\n2\\n\\x11\\n+ 1\\n\\x11\\nThe derivative of the hyperbolic tangent function,\\ntanh(x),\\nis the hyperbolic secant squared function,\\nsech2(x):\\nd\\ndx tanh(x) = sech2(x)\\nThe derivative can be found by first expressing tanh(x)\\nin terms of sinh(x) and cosh(x), and then applying the\\nquotient rule and the fundamental hyperbolic identity:\\n• Definition of the hyperbolic tangent and the deriva-\\ntives of the hyperbolic sine and cosine functions:\\ntanh(x) = sinh(x)\\ncosh(x)\\nd\\ndx sinh(x) = cosh(x)\\nd\\ndx cosh(x) = sinh(x)\\n• Applying the quotient rule,\\nd\\ndx\\n\\x00 u\\nv\\n\\x01\\n=\\nu′v−uv′\\nv2\\n,\\nwhere u = sinh(x) and v = cosh(x):\\nd\\ndx tanh(x) = d\\ndx\\n\\x12 sinh(x)\\ncosh(x)\\n\\x13\\nd\\ndx tanh(x) = (cosh(x))(cosh(x)) −(sinh(x))(sinh(x))\\n(cosh(x))2\\nd\\ndx tanh(x) = cosh2(x) −sinh2(x)\\ncosh2(x)\\n• Given\\nthe\\nfundamental\\nhyperbolic\\nidentity,\\ncosh2(x) −sinh2(x) = 1:\\nd\\ndx tanh(x) =\\n1\\ncosh2(x) = sech2(x)\\nsince the hyperbolic secant function is defined as\\nsech(x) =\\n1\\ncosh(x).\\nThe derivative of tanh(x) can be also be expressed\\nin terms of tanh(x) itself using the fundamental hyper-\\nbolic identity, the hyperbolic Pythagorean identity relat-\\ning cosh(x) and sinh(x):\\ncosh2(x) −sinh2(x) = 1\\nDividing every term in this identity by cosh2(x), you\\nget the desired relationship:\\ncosh2(x)\\ncosh2(x) −sinh2(x)\\ncosh2(x) =\\n1\\ncosh2(x)\\n1 −\\n\\x12 sinh(x)\\ncosh(x)\\n\\x132\\n=\\n\\x12\\n1\\ncosh(x)\\n\\x132\\n1 −tanh2(x) = sech2(x)\\nSince\\nd\\ndx(tanh(x)) = sech2(x), we can substitute the\\nexpression above:\\nd\\ndx tanh(x) = 1 −tanh2(x)\\n3. Softmax function\\nThe softmax function, often used as an activation func-\\ntion in the output layer of a neural network for multi-class\\nclassification problems, is defined as:\\nyi = softmaxi(⃗x) =\\nexi\\nPK\\nk=1 exk\\nwhere yi is the i-th element of the output vector ⃗y, xi is\\nthe i-th element of the weighted input vector ⃗x, and the\\ndenominator is the sum over all K classes.\\nThe derivative of the softmax function is most easily\\nexpressed in terms of the Kronecker delta (δij), and it\\nresults in a matrix of values (a Jacobian matrix) because\\nsoftmax is a vector-valued function. The derivative of the\\ni-th component of the softmax output, yi, with respect\\nto the j-th component of the input, xj, is\\n∂yi\\n∂xj\\n= yi(δij −yj)\\nwhere yi and yj are the outputs of the softmax function,\\nwhereas δij is the Kronecker delta (1 if i = j and 0 if\\ni ̸= j).\\nIn the full derivation of the softmax derivative, we must\\nconsider two separate cases: when i = j and when i ̸= j.\\n• The derivative ∂yi\\n∂xi with respect to the same index\\n(i = j):\\n∂\\n∂xi\\n(exi) = exi\\n∂\\n∂xi\\n N\\nX\\nk=1\\nexk\\n!\\n= exi\\n(only the k = i term in the sum depends on xi)\\n\\n17\\n∂yi\\n∂xi\\n= (exi)(PN\\nk=1 exk) −(exi)(exi)\\n(PN\\nk=1 exk)2\\n∂yi\\n∂xi\\n=\\nexi\\nPN\\nk=1 exk\\n PN\\nk=1 exk −exi\\nPN\\nk=1 exk\\n!\\n∂yi\\n∂xi\\n= yi\\n \\n1 −\\nexi\\nPN\\nk=1 exk\\n!\\n= yi(1 −yi)\\n• The derivative ∂yi\\n∂xj with respect to a different index\\n(i ̸= j):\\n∂\\n∂xj\\n(exi) = 0\\n∂\\n∂xj\\n N\\nX\\nk=1\\nexk\\n!\\n= exj\\n∂yi\\n∂xj\\n= (0)(PN\\nk=1 exk) −(exi)(exj)\\n(PN\\nk=1 exk)2\\n∂yi\\n∂xj\\n= −\\nexiexj\\n(PN\\nk=1 exk)(PN\\nk=1 exk)\\n∂yi\\n∂xj\\n= −\\n \\nexi\\nPN\\nk=1 exk\\n!  \\nexj\\nPN\\nk=1 exk\\n!\\n∂yi\\n∂xj\\n= −yiyj\\nCombining both cases:\\n∂yi\\n∂xj\\n=\\n(\\nyi(1 −yi)\\nif i = j\\n−yiyj\\nif i ̸= j\\nwhich can be written compactly using the Kronecker\\ndelta (δij) as\\n∂yi\\n∂xj\\n= yi(δij −yj)\\n4. Softplus function\\nThe softplus function (12), or SmoothReLU function,\\nis defined as\\nf(x) = ln (1 + ex)\\nFor large negative x, it is roughly ln 1, just above 0.\\nFor large positive x, it is roughly ln(ex) ,just above x.\\nThe derivative of the softplus function is the logistic\\nfunction (the standard sigmoid function).\\nGiven the softplus function, f(x) = ln(1 + ex), the\\nsoftplus derivative can be derived using the chain rule\\nand simplified into the standard sigmoid function:\\n• Apply the chain rule\\nd\\ndu[ln(u)] = 1\\nu · du\\ndx, where u =\\n1 + ex:\\nf ′(x) =\\n1\\n1 + ex · d\\ndx(1 + ex)\\nf ′(x) =\\n1\\n1 + ex · (0 + ex)\\nf ′(x) =\\nex\\n1 + ex\\n• The form\\nex\\n1+ex is a common representation of the\\nsigmoid function, σ(x). To show the most common\\nform of the sigmoid,\\n1\\n1+e−x , just multiply both nu-\\nmerator and denominator by e−x:\\nf ′(x) =\\nex\\n1 + ex · e−x\\ne−x =\\nex · e−x\\ne−x(1 + ex) =\\n1\\ne−x + 1\\nTherefore, the derivative of softplus is:\\nd\\ndx(softplus(x)) = σ(x) =\\n1\\n1 + e−x\\n5. Swish function\\nThe swish family function (13, 20, 35) is defined as\\nswishβ(x) = xσ(βx) =\\nx\\n1 + e−βx\\nwhere β can be constant or trained with the network\\nparameters.\\nThe swish family was designed to smoothly interpolate\\nbetween a linear function and the ReLU function:\\n• When β = 0, the function is linear:\\nswish0(x) = x/2\\n• When β = 1, the function is the standard sigmoid\\nor logistic:\\nswish1(x) = σ(x)\\n\\n18\\n• When β →∞, the function converges to the ReLU:\\nswish∞(x) = lim\\nβ→∞\\nx\\n1 + e−βx = ReLU(x)\\nsince for x\\n>\\n0, e−βx\\n→\\n0 and, therefore,\\nswish∞(x) →x, whereas, for x < 0, e−βx →∞\\nand swish∞(x) →0.\\nThe derivative of the swish function is given by\\nswish′\\nβ(x) = σ(βx) + βx · σ′(βx)\\n= σ(βx) + βx · σ(βx)(1 −σ(βx))\\n= βx · σ(βx) + σ(βx)(1 −βx · σ(βx))\\n= β · swishβ(x) + σ(βx)(1 −β · swishβ(x))\\n6. ReLU function\\nThe rectifier or ReLU (rectified linear unit) activation\\nfunction is defined as the non-negative part of its argu-\\nment, i.e., the ramp function:\\nReLU(x) = x+ = max{0, x} =\\n(\\nx\\nif x > 0\\n0\\nif x ≤0\\nThe ReLU is analogous to half-wave rectification in\\nelectrical engineering and one of the most common acti-\\nvation functions for artificial neural networks.\\nMany variants of the ReLU function have been pro-\\nposed in the literature:\\n• Leaky ReLU (31) is a piecewise-linear variant that\\nallows a small, positive gradient when the unit is\\ninactive (α typically between 0.01 and 0.3):\\nLReLU(x) =\\n(\\nx\\nif x > 0\\nαx\\nif x ≤0\\n• Parametric ReLU (18) makes the leaky ReLU α a\\nlearnable parameter. For α ≤1:\\nPReLU(x) = max{x, αx}\\n• Concatenated ReLU (37) preserves both positive\\nand negative inputs by returning two values:\\nCReLU(x) = [ReLU(x), ReLU(−x)]\\nApart from the softplus function, or SmoothReLU, ad-\\nditional smooth approximations to the rectifier function\\ninclude:\\n• Exponential linear units, ELUs, which smoothly al-\\nlow negative values (8):\\nELU(x) =\\n(\\nx\\nif x > 0\\nα(ex −1)\\nif x ≤0\\nELU′(x) =\\n(\\n1\\nif x > 0\\nαex\\nif x ≤0\\nELU can be viewed as a smoothed version of a\\nshifted ReLU (SReLU), which has the form f(x) =\\nmax{−α, x}.\\n• Gaussian-error linear units, GELUs (20):\\nGELU(x) = xΦ(x)\\nGELU ′(x) = xΦ′(x) + Φ(x)\\nwhere Φ(x) is the cumulative distribution function\\nof the standard normal distribution N(0, 1). The\\nswish function, or SiLU (sigmoid linear unit), is\\nsimilar, also with a bump with negative derivative\\nto the left of x = 0, and computationally cheaper.\\n• The mish function (33) was obtained by experi-\\nmenting with functions similar to swish and ex-\\nhibits a self-regularizing behavior attributed to a\\n∆(x) term in its first derivative:\\nmish(x) = x tanh(softplus(x))\\nmish’(x) = ∆(x)swish1(x) + mish(x)\\nx\\nwhere ∆(x) = sech2(softplus(x)).\\n• Squareplus (2) is an algebraic softplus-like function\\nsquareplus(x) = x +\\n√\\nx2 + b\\n2\\nsquareplus′(x) = 1\\n2\\n\\x12\\nx\\n√\\nx2 + b\\n+ 1\\n\\x13\\nwhere the b ≥0 hyperparameter determines the\\nextent of the curved region near x = 0.\\nSimilar\\nto softplus, squareplus can be computed using only\\nalgebraic functions, making it suitable for compu-\\ntational efficiency and numerical stability.\\n• Extended exponential linear units, DELUs, are\\nsmoother within the neighborhood of zero and\\nsharper for larger values (7):\\nDELU(x) =\\n(\\nx\\nif x > xc\\n(eαx −1)/β\\nif x ≤xc\\nDELU′(x) =\\n(\\n1\\nif x > xc\\n(α/β)eαx\\nif x ≤xc\\nwhose hyperparameters are typically set as α =\\n1, β = 2, xc = 1.25643.\\nThe value of xc ≥0\\nresults from imposing the continuity constraint for\\nthe chosen values of α and β:\\nxc = (eαxc −1)/β\\n\\n19\\nAppendix B: Loss Functions\\nCommon loss functions are described in this Appendix,\\ngrouped by category. Well-known loss functions for clas-\\nsification problems (and matching probability distribu-\\ntions) include the following:\\n• Cross-entropy loss (a.k.a. log loss): Standard for\\nprobabilistic classification, with special cases for bi-\\nnary (binary cross-entropy, BCE) and multi-class\\n(categorical cross-entropy) classification.\\nLCE = H(y∥ˆy) = −\\nX\\nk\\nyk log ˆyk\\n• Focal loss (29, 30): Down-weights easy examples,\\nfocuses on hard ones (common in imbalanced clas-\\nsification). Common for image segmentation.\\nLF L = −\\nX\\nk\\nyk(1 −ˆyk)γ log ˆyk\\nFocal loss adds a modulating factor (1−ˆyk)γ to the\\ncross entropy loss, with a tunable focusing param-\\neter γ ≥0.\\n• Hinge loss (9): Used in support vector machines\\n(SVMs), with a geometrical justification. Margin-\\nbased, with a linear penalty when the margin is\\nviolated. For a binary classification problem using\\nbipolar encoding for the target y ({−1, +1}) and a\\nclassifier score ˆy:\\nLHL = max{0, 1 −yˆy}\\nThe squared hinge loss is a variant of the hinge loss\\nwith squared penalty (quadratic penalty when the\\nmargin is violated):\\nLSHL = (max{0, 1 −yˆy})2\\nFor\\nmulti-class\\nclassification\\nproblems,\\nthe\\nCrammer-Singer (10) and Weston-Watkins (41)\\nloss functions can be used:\\nLCS = max{0, 1 + max\\nk̸=y {ˆyy −ˆyk}}\\nLW W =\\nX\\nk̸=y\\nmax{0, 1 + ˆyyˆyk}\\n• Kullback–Leibler divergence, or KL loss (26), also\\nknown as the relative entropy of y with respect to\\nˆy:\\nLKL = DKL(y∥ˆy) =\\nX\\nk\\nyk log yk\\nˆyk\\nThe KL divergence is an f-divergence measure be-\\ntween true and predicted distributions.\\nSince y is usually represented using one-hot encod-\\ning, it simplifies to the cross-entropy loss, given\\nthat DKL(y∥ˆy) = H(y∥ˆy) −H(y).\\n• Hellinger divergence, a.k.a. Hellinger distance (19):\\nAnother f-divergence that quantifies the similarity\\nbetween two probability distributions:\\nLHD = DH(y∥ˆy) =\\n1\\n√\\n2\\nsX\\nk\\n\\x10√yk −\\np\\nˆyk\\n\\x112\\nwhich is directly related to the Euclidean norm of\\nthe difference of the square root vectors.\\nSome-\\ntimes the factor 1/\\n√\\n2 is omitted, in which case the\\nHellinger distance ranges from zero to the square\\nroot of two.\\n• Total variation distance, a.k.a. statistical distance,\\nstatistical difference,or variational distance:\\nAn-\\nother f-divergence that provides a statistical dis-\\ntance between probability distributions.\\nLT V D = DT V D(y∥ˆy) =\\nX\\nk\\n|yk −ˆyk|\\ni.e. half of the L1 distance between the probability\\nfunctions on discrete domains.\\nThe three f-divergences (36) above are related:\\nDT V D(y∥ˆy) ≤\\nr\\n1\\n2DKL(y∥ˆy)\\nD2\\nH(y∥ˆy) ≤DT V D(y∥ˆy) ≤\\n√\\n2DH(y∥ˆy)\\ni.e. inequalities that follow immediately from the\\nrelationship between the 1-norm and the 2-norm.\\n• Jensen–Shannon divergence, also known as infor-\\nmation radius (IRad) or total divergence to the av-\\nerage: Yet another f-divergence, a symmetrized and\\nsmoothed version Kullback–Leibler divergence that\\nalways has a finite value.\\nLJS = DJS(y∥ˆy) = DKL(y∥m) + DKL(ˆy∥m)\\n2\\nwhere m = (y + ˆy)/2 is a mixture of y and ˆy (i.e.\\nthe pointwise mean of y and ˆy probabilities). The\\nsquare root of the Jensen–Shannon divergence is\\na metric often referred to as Jensen–Shannon dis-\\ntance,\\np\\nDJS(y∥ˆy).\\n• Wasserstein Distance (24, 40), a.k.a.\\nthe Earth\\nmover’s distance or the optimal transport distance:\\nAnother similarity metric between two probability\\ndistributions.\\nIt can be interpreted as the mini-\\nmum energy cost of moving and transforming a pile\\nof dirt in the shape of one probability distribution\\nto the shape of the other distribution. The cost is\\ncalculated as the product of the amount of proba-\\nbility mass being moved and the distance it is being\\nmoved.\\nδ0 = 0\\n\\n20\\nδk+1 = δk + yk −ˆyk\\nLW1 = W1(y, ˆy) =\\nX\\nk\\n|δk|\\nIn general, for any Minkowski distance:\\nLWp = Wp(y, ˆy) =\\n X\\nk\\n||δk||p\\n!1/p\\n• Bhattacharyya distance (5): Not a metric, actually,\\nsince it does not satisfy the triange inequality.\\nLB = DB(y, ˆy) = −log BC(y, ˆy)\\nwhere\\nBC(y, ˆy) =\\nX\\nk\\np\\nyk ˆyk\\nis the Bhattacharyya coefficient for discrete proba-\\nbility distributions.\\n• Renyi divergence, or α-divergence (36): A spec-\\ntrum of divergence measures generalize the KL di-\\nvergence.\\nIts order α controls how sensitive you\\nwant to be to rare differences.\\nLα = Dα(y∥ˆy) =\\n1\\nα −1 log\\nX\\nk\\nyα\\nk\\nˆyα−1\\nk\\nFor special values, we can define the R´enyi diver-\\ngence by taking a limit:\\n– As α →0, D0 measures how much ˆy supports\\ny, checking whether ˆy assigns probability to\\nall points where y is nonzero.\\nD0(y∥ˆy) = −log\\nX\\nk\\nˆy · 1yk>0\\nwhere 1yk>0 is an indicator function that is 1\\nif yk > 0, and 0 otherwise.\\n– As α →1, the R´enyi divergence converges to\\nthe Kullback–Leibler (KL) divergence:\\nD1(y∥ˆy) = DKL(y∥ˆy) =\\nX\\nk\\nyk log yk\\nˆyk\\n– As α →∞, D∞becomes the max-divergence,\\nfocusing only on the largest discrepancy be-\\ntween the two distributions:\\nD∞(y∥ˆy) = log max\\nk\\nyk\\nˆyk\\n– When α = 1/2, D1/2 is twice the Bhat-\\ntacharyya divergence:\\nD2(y∥ˆy) = 2DB(y∥ˆy) = −2 log\\np\\nyk ˆyk\\n– When α = 2, D2 is the logarithm of the ex-\\npected ratio of probabilities:\\nD2(y∥ˆy) = log\\n\\x12\\nyk\\nyk\\nˆyk\\n\\x13\\n= log\\n\\x1cyk\\nˆyk\\n\\x1d\\nThe R´enyi divergence is indeed a divergence, i.e.\\nit is greater than or equal to zero, and zero only\\nwhen both distributions are the same.\\nFor any\\ngiven pair of distributions, the R´enyi divergence is\\nnondecreasing as a function of its order α.\\nFor regression problems, a wide variety of loss func-\\ntions have been proposed:\\n• Mean Squared Error, MSE (16): Highly sensitive\\nto outliers, but smooth and differentiable. Equiv-\\nalent to MLE under Gaussian noise with constant\\nvariance.\\nLMSE = 1\\nn\\nX\\ni\\n(yi −ˆyi)2\\n∂LMSE\\n∂ˆyi\\n= −2(yi −ˆyi)\\n• Mean Absolute Error, MAE (27), a.k.a. L1 Loss:\\nMore robust to outliers, but non-differentiable at\\n0. MLE under Laplace (double exponential) noise,\\nleads to median regression.\\nLMAE = 1\\nn\\nX\\ni\\n|yi −ˆyi|\\n∂LMAE\\n∂ˆyi\\n= −sign(yi −ˆyi)\\n• Huber loss (22): Quadratic near 0, linear for large\\nresiduals. Balances MSE for small errors and MAE\\nfor large errors. Robust, differentiable, widely used\\nin robust regression.\\nLHuber =\\n(\\n1\\n2(y −ˆy)2\\n|y −ˆy| ≤δ\\nδ(|y −ˆy| −1\\n2δ)\\n|y −ˆy| > δ\\n∂LHuber\\n∂ˆyi\\n=\\n(\\n−(yi −ˆyi)\\n|yi −ˆyi| ≤δ\\n−δ sign(yi −ˆyi)\\n|yi −ˆyi| > δ\\n• Log-cosh loss: Smooth approximation to MAE, less\\nharsh than MSE, with bounded gradient, shares\\nsimilarities with Huber loss but is infinitely differ-\\nentiable.\\nLLCL =\\nX\\ni\\nlog cosh(yi −ˆyi)\\n∂LLCL\\n∂ˆyi\\n= −tanh(yi −ˆyi)\\n\\n21\\nLoss function\\nSmall errors Large errors Derivative ∂L/∂ˆyi\\nMSE\\nϵ2\\nϵ2\\nϵ\\nMAE\\nϵ\\nϵ\\n1\\nHuber\\nϵ2\\nϵ\\n1\\nLog-cosh\\nϵ2\\nϵ\\n1\\nHinge\\n0\\nϵ\\n1\\nLogarithmic\\nlog ϵ\\nlog ϵ\\n1/ϵ\\nCauchy\\nlog ϵ2\\nlog ϵ2\\n1/ϵ\\nStudent\\nlog ϵ2\\nlog ϵ2\\n1/ϵ\\nFair\\nϵ\\nϵ\\n1\\nTukey\\nϵ2\\n1\\n0\\nPinball/quantile\\nϵ\\nϵ\\n1\\nTable IV Regression loss functions and their asymptotic behavior with small and large errors.\\n• Hinge loss for regression, a.k.a. ϵ-insensitive loss\\n(39): Robust to small noise, focuses on large devi-\\nations. Support Vector Regression (SVR) general-\\nizes hinge loss to continuous targets. Unlike MSE\\nand MAE, SVR ignores small noise: errors smaller\\nthan the ϵ margin of tolerance are ignored (SVR\\nhas a dead zone of zero penalty, which makes it\\nless sensitive to tiny fluctuations).\\nErrors larger\\nthan ϵ are penalized linearly, as in MAE.\\nLSV R = Lϵ = max(0, |y −ˆy| −ϵ)\\n∂LSV R\\n∂ˆy\\n=\\n(\\n0\\n|y −ˆy| ≤ϵ\\n−sign(y −ˆy)\\n|y −ˆy| > ϵ\\nThe hinge loss can be justified geometrically, con-\\nnecting regression to margin-based learning: SVR\\nfinds a function such that most data points lie\\nwithin an ϵ-tube, while keeping the function “flat”.\\nHowever, the hinge loss is not a proper scoring rule\\n(the forecasted distribution does not match the dis-\\ntribution of the observation, i.e. the true distribu-\\ntion). Variations of the hinge loss include:\\n-\\nϵ-insensitive\\nsquared\\nloss\\n(MSE-like,\\nwith\\nquadratic penalty outside the tube):\\nL(2)\\nϵ\\n=\\n\\x00max{0, |y −ˆy| −ϵ}\\n\\x012\\n- Huberized ϵ-insensitive loss (Huber-like, more nu-\\nmerically stable than sharp ϵ-insensitive): Flat in-\\nside the tube, quadratic near the boundary, linear\\nfar out.\\nLϵ,δ =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n0\\n|y −ˆy| ≤ϵ\\n1\\n2δ(|y −ˆy| −ϵ)2\\nϵ < |y −ˆy| ≤ϵ + δ\\n|y −ˆy| −ϵ −δ\\n2\\n|y −ˆy| > ϵ + δ\\n- Pinball (quantile) loss in SVR: Useful for predict-\\ning intervals. If you set ϵ = 0 and choose asymmet-\\nric penalties, SVR becomes quantile regression.\\nLτ =\\n(\\nτ(y −ˆy)\\ny ≥ˆy\\n(1 −τ)(ˆy −y)\\ny < ˆy\\n• Logarithmic loss for regression (i.e. Pareto loss):\\nHeavy-tailed, robust to outliers, decaying gradient.\\nMLE under double Pareto noise. In probabilistic\\nregression, the log score is a proper scoring rule,\\nensuring calibrated predictive distributions.\\nLlog =\\nX\\ni\\nlog (1 + |yi −ˆyi|)\\n∂Llog\\n∂ˆyi\\n= sign(ˆyi −yi)\\n1 + |ˆyi −yi|\\n• Cauchy loss: Heavy-tailed, robust to outliers. MLE\\nunder Cauchy noise.\\nLCauchy =\\nX\\ni\\nlog\\n\\x12\\n1 + (yi −ˆyi)2\\nc2\\n\\x13\\n∂LCauchy\\n∂ˆyi\\n= −\\n2(yi −ˆyi)\\nc2 + (yi −ˆyi)2\\n• Student-t loss:\\nHeavy-tailed, robust to outliers.\\nMLE under Student-t noise.\\nLStudent =\\nX\\ni\\nν + 1\\n2\\nlog\\n\\x12\\n1 + (yi −ˆyi)2\\nνσ2\\n\\x13\\n∂LStudent\\n∂ˆyi\\n= −(ν + 1)(yi −ˆyi)\\n(yi −ˆyi)2 + σ2ν\\n\\n22\\n• Fair loss (14):\\nSmooth,\\nlarge residuals with\\nbounded influence, less aggressive than Cauchy.\\nLF air = c2\\n\\x12|y −ˆy|\\nc\\n−log\\n\\x12\\n1 + |y −ˆy|\\nc\\n\\x13\\x13\\n∂Lfair\\n∂ˆy\\n= −\\ny −ˆy\\n1 + |y −ˆy|/c\\n• Tukey’s biweight loss (38):\\nBounded influence,\\ncompletely ignores very large residuals beyond\\ntheshold c.\\nExtremely robust, but non-convex.\\nUsed for outlier rejection.\\nLT ukey =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nc2\\n6\\n\"\\n1 −\\n\\x12\\n1 −\\n\\x10\\ny−ˆy\\nc\\n\\x112\\x133#\\n|y −ˆy| ≤c\\nc2\\n6\\n|y −ˆy| > c\\nFor small error values, Tukey’s biweight loss func-\\ntion can be closely approximated by a simple\\nquadratic polynomial, behaving much like the stan-\\ndard squared error loss: LT ukey ≈ϵ2/2 −ϵ4/2c2\\nfrom the Taylor series expansion of the loss func-\\ntion around ϵ = 0.\\n• Pinball loss, a.k.a. quantile loss (25): For quan-\\ntile regression, asymmetric penalty depending on\\nquantile τ.\\nMLE under asymmetric Laplace dis-\\ntribution computes conditional quantiles instead of\\nmean. Useful for heteroskedastic or skewed data.\\nLpinball =\\nX\\ni\\nmax{τ(yi −ˆyi), (1 −τ)(yi −ˆyi)}\\nThe function gets its name from its characteristic V\\nshape, which is asymmetrical when τ is not 0.5: for\\nunderprediction (yi ≥ˆyi), the loss is weighted by τ;\\nfor overprediction (yi < ˆyi), the loss is weighted by\\n1−τ. This weighting encourages the model to make\\npredictions that are more likely to be above (τ >\\n0.5) or below (τ < 0.5) the actual value, depending\\non the chosen quantile.\\nReferences\\n[1] Aggarwal, C. C. Neural Networks and Deep Learning:\\nA Textbook, 2nd edition. Springer, 2023.\\n[2] Barron, J. T. Squareplus: A Softplus-Like Algebraic\\nRectifier, 2021.\\n[3] Berzal, F. Redes Neuronales & Deep Learning - Vol-\\numen I: Entrenamiento de redes neuronales artificiales\\n[Neural Networks and Deep Learning - Volume I: Train-\\ning Artificial Neural Networks, in Spanish].\\nAmazon\\nKDP, Granada, Spain, 2019.\\n[4] Berzal, F. Redes Neuronales & Deep Learning - Vol-\\numen II: Regularizaci´on, optimizaci´on & arquitecturas\\nespecializadas [Neural Networks and Deep Learning -\\nVolume II: Regularization, Optimization, and Special-\\nized Architectures, in Spanish]. Amazon KDP, Granada,\\nSpain, 2019.\\n[5] Bhattacharyya, A. K. On a Measure of Divergence\\nbetween Two Multinomial Populations.\\nSankhy¯a 7, 4\\n(1946), 401–406.\\n[6] Bishop, C. M., and Bishop, H. Deep Learning: Foun-\\ndations and Concepts. Springer, 2024.\\n[7] C¸atalbas¸, B., and Morg¨ul, ¨O. Deep learning with\\nextendeD exponential linear unit (DELU). Neural Com-\\nputing and Applications 35, 30 (2023), 22705–22724.\\n[8] Clevert, D.-A., Unterthiner, T., and Hochreiter,\\nS. Fast and Accurate Deep Network Learning by Expo-\\nnential Linear Units (ELUs), 2016.\\n[9] Cortes, C., and Vapnik, V. Support-vector networks.\\nMachine learning 20, 3 (1995), 273–297.\\n[10] Crammer, K., and Singer, Y.\\nOn the Algorithmic\\nImplementation of Multiclass Kernel-based Vector Ma-\\nchines. Journal of Machine Learning Research 2 (Mar.\\n2002), 265–292.\\n[11] Cybenko, G. Approximation by superpositions of a sig-\\nmoidal function.\\nMathematics of Control, Signals and\\nSystems 2, 4 (1989), 303–314.\\n[12] Dugas, C., Bengio, Y., B´elisle, F., Nadeau, C.,\\nand Garcia, R. Incorporating second-order functional\\nknowledge for better option pricing.\\nIn Advances in\\nNeural Information Processing Systems (2000), T. Leen,\\nT. Dietterich, and V. Tresp, Eds., vol. 13, MIT Press.\\n[13] Elfwing, S., Uchibe, E., and Doya, K.\\nSigmoid-\\nweighted linear units for neural network function approx-\\nimation in reinforcement learning, 2017.\\n[14] Fair, R. C. On the robust estimation of econometric\\nmodels. Annals of Economic and Social Measurement 3,\\n4 (1974), 667–677.\\n[15] Funahashi, K.-I. On the approximate realization of con-\\ntinuous mappings by neural networks. Neural Networks\\n2, 3 (1989), 183–192.\\n[16] Gauss, C. F.\\nTheoria motus corporum coelestium in\\nsectionibus conicis solem ambientium [Theory of the Mo-\\ntion of Heavenly Bodies Moving about the Sun in Conic\\nSections, in Latin]. Friedrich Perthes & Johann Heinrich\\nBesser, 1809.\\n[17] Goodfellow, I., Bengio, Y., and Courville, A.\\nDeep Learning. MIT Press, 2016.\\n[18] He, K., Zhang, X., Ren, S., and Sun, J. Delving Deep\\ninto Rectifiers: Surpassing Human-Level Performance on\\nImageNet Classification, 2015.\\n[19] Hellinger, E. Neue Begr¨undung der Theorie quadratis-\\ncher Formen von unendlichvielen Ver¨anderlichen [New\\nfoundation of the theory of quadratic forms of infinitely\\nmany variables, in German]. Journal f¨ur die reine und\\nangewandte Mathematik [Journal of Pure and Applied\\nMathematics] 136 (1909), 210–271.\\n[20] Hendrycks, D., and Gimpel, K. Gaussian Error Lin-\\near Units (GELUs), 2016.\\n[21] Hornik, K., Stinchcombe, M., and White, H. Multi-\\nlayer feedforward networks are universal approximators.\\nNeural Networks 2, 5 (1989), 359–366.\\n[22] Huber, P. J. Robust Estimation of a Location Parame-\\nter. The Annals of Mathematical Statistics 35, 1 (1964),\\n73 – 101.\\n\\n23\\n[23] Jørgensen, B. Exponential dispersion models. Journal\\nof the Royal Statistical Society: Series B (Methodological)\\n49, 2 (1987), 127–145.\\n[24] Kantorovich, L. V. On the translocation of masses.\\nIn Doklady Akademii Nauk USSR (NS) (1942), vol. 37,\\npp. 199–201.\\n[25] Koenker, R., and Bassett Jr, G. Regression quan-\\ntiles. Econometrica: Journal of the Econometric Society\\n46, 1 (1978), 33–50.\\n[26] Kullback, S., and Leibler, R. A.\\nOn information\\nand sufficiency. Annals of Mathematical Statistics 22, 1\\n(1951), 79–86.\\n[27] Laplace, P.-S. M´emoire sur la Probabilit´e des Causes\\npar les ´Ev´enements [Memoir on the Probability of Causes\\nfrom Events, in French]. M´emoires de l’Acad´emie Royale\\ndes Sciences de Paris 6 (1774), 621–656.\\n[28] Leshno, M., Lin, V. Y., Pinkus, A., and Schocken,\\nS. Multilayer feedforward networks with a nonpolyno-\\nmial activation function can approximate any function.\\nNeural Networks 6, 6 (1993), 861–867.\\n[29] Lin, T.-Y., Goyal, P., Girshick, R., He, K., and\\nDoll´ar, P.\\nFocal Loss for Dense Object Detection.\\nIn Proceedings of the IEEE International Conference on\\nComputer Vision (2017), pp. 2980–2988.\\n[30] Lin, T.-Y., Goyal, P., Girshick, R., He, K., and\\nDoll´ar, P.\\nFocal Loss for Dense Object Detection.\\nIEEE Transactions on Pattern Analysis and Machine In-\\ntelligence 42, 2 (2020), 318–327.\\n[31] Maas, A. L., Hannun, A. Y., and Ng, A. Y. Rectifier\\nnonlinearities improve neural network acoustic models.\\nIn ICML Workshop on Deep Learning for Audio, Speech\\nand Language Processing (2013), Atlanta, GA.\\n[32] McCulloch, W. S., and Pitts, W. A logical calculus\\nof the ideas immanent in nervous activity. The Bulletin\\nof Mathematical Biophysics 5, 4 (1943), 115–133.\\n[33] Misra, D. Mish: A Self Regularized Non-Monotonic Ac-\\ntivation Function. In 31st British Machine Vision Virtual\\nConference (2020), British Machine Vision Association.\\n[34] Mont´ufar, G., Pascanu, R., Cho, K., and Bengio,\\nY. On the number of linear regions of deep neural net-\\nworks. In Proceedings of the 28th International Confer-\\nence on Neural Information Processing Systems - Volume\\n2 (Cambridge, MA, USA, 2014), NIPS’14, MIT Press,\\np. 2924–2932.\\n[35] Ramachandran, P., Zoph, B., and Le, Q. V. Search-\\ning for activation functions, 2017.\\n[36] R´enyi, A. On measures of entropy and information. In\\nProceedings of the 4th Berkeley Symposium on Mathe-\\nmatical Statistics and Probability (1961), vol. 4, Univer-\\nsity of California Press, pp. 547–562.\\n[37] Shang, W., Sohn, K., Almeida, D., and Lee, H.\\nUnderstanding and improving convolutional neural net-\\nworks via concatenated rectified linear units.\\nIn Pro-\\nceedings of The 33rd International Conference on Ma-\\nchine Learning (New York, New York, USA, 20–22 Jun\\n2016), M. F. Balcan and K. Q. Weinberger, Eds., vol. 48\\nof Proceedings of Machine Learning Research, PMLR,\\npp. 2217–2225.\\n[38] Tukey, J. W.\\nExploratory Data Analysis.\\nAddison-\\nWesley, 1977.\\n[39] Vapnik, V. N. The Nature of Statistical Learning The-\\nory. Springer, 1995.\\n[40] Vaserstein, L. N. Markov processes over denumerable\\nproducts of spaces describing large systems of automata\\n[in Russian]. Problemy Peredaci Informacii 5, 3 (1969),\\n64 – 72.\\n[41] Weston, J., Watkins, C., et al.\\nSupport Vec-\\ntor Machines for Multi-Class Pattern Recognition.\\nIn\\nESANN’1999 European Symposium on Artificial Neural\\nNetworks (1999), pp. 219–224.\\n',\n",
       " '1\\nDeepForgeSeal: Latent Space-Driven Semi-Fragile\\nWatermarking for Deepfake Detection Using\\nMulti-Agent Adversarial Reinforcement Learning\\nTharindu Fernando, Member, IEEE, Clinton Fookes, Senior Member, IEEE, and Sridha Sridharan, Life Senior\\nMember, IEEE.\\nAbstract—Rapid advances in generative AI have led to in-\\ncreasingly realistic deepfakes, posing growing challenges for law\\nenforcement and public trust. Existing passive deepfake detectors\\nstruggle to keep pace, largely due to their dependence on specific\\nforgery artifacts, which limits their ability to generalize to new\\ndeepfake types. Proactive deepfake detection using watermarks\\nhas emerged to address the challenge of identifying high-\\nquality synthetic media. However, these methods often struggle\\nto balance robustness against benign distortions with sensitivity\\nto malicious tampering. This paper introduces a novel deep\\nlearning framework that harnesses high-dimensional latent space\\nrepresentations and the Multi-Agent Adversarial Reinforcement\\nLearning (MAARL) paradigm to develop a robust and adaptive\\nwatermarking approach. Specifically, we develop a learnable\\nwatermark embedder that operates in the latent space, capturing\\nhigh-level image semantics, while offering precise control over\\nmessage encoding and extraction. The MAARL paradigm em-\\npowers the learnable watermarking agent to pursue an optimal\\nbalance between robustness and fragility by interacting with a dy-\\nnamic curriculum of benign and malicious image manipulations\\nsimulated by an adversarial attacker agent. Comprehensive eval-\\nuations on the CelebA and CelebA-HQ benchmarks reveal that\\nour method consistently outperforms state-of-the-art approaches,\\nachieving improvements of over 4.5% on CelebA and more than\\n5.3% on CelebA-HQ under challenging manipulation scenarios.\\nIndex Terms—Deepfake Detection, Semi-Fragile Watermarks,\\nMulti-Agent Adversarial Reinforcement Learning, Digital Foren-\\nsics\\nI. INTRODUCTION\\nGenerative AI has rapidly transformed the way we create\\nand edit multimedia, making it faster and easier than ever to\\nproduce high-quality images, audio, and video. While these\\ninnovations unlock exciting possibilities, they also introduce\\nserious risks. Among the most concerning threats are image\\nand video deepfakes, highly realistic, AI-generated images and\\nvideos that mimic real persons, posing significant challenges,\\nnot only for their role in spreading misinformation and disin-\\nformation at an alarming pace but also for their implications\\non privacy, consent, security, and societal trust [1], [2], [3],\\n[4], [5].\\nTo combat the rapid spread of deepfakes, watermarking\\nhas emerged as a technique that allows users to validate\\nthe authenticity and integrity of the multimedia they con-\\nsume. A robust watermarking technique should be resilient to\\nT. Fernando, C. Fookes, and S. Sridharan are with The Signal Processing,\\nArtificial Intelligence and Vision Technologies (SAIVT), Queensland Univer-\\nsity of Technology, Australia.\\nAdaptive Watermark Attacks\\nLearned Image \\nCompression Attacks\\nHair Colour and Expression \\nChanged\\nDiﬀusion Attacks\\nHair Colour Changed\\nFace Swap\\nFig. 1.\\nWatermarking and watermark retrieval performance of proposed\\nDeepForgeSeal framework. The visualisations include low-resolution and\\nhigh-resolution bonafide images (row 1), numerous learned attacks against\\nwatermarking (row 2), and watermarked images that have undergone semantic-\\naltering manipulations (row 3).\\ndistortions such as JPEG compression, cropping, and colour\\nchanges to keep the embedded watermark intact. While being\\nresilient to such incidental distortions, a good watermark-\\ning technique should demonstrate fragility towards content-\\naltering modifications, such as face-swapping-type malicious\\nedits that alter the semantic-level meaning of the image.\\nRecently, deep learning techniques [6], [7], [8], [9], [10] have\\nmade significant progress towards achieving this paradoxical\\ncombination of resilience and fragility. For instance, SepMark\\n[8] introduced an encoder–decoder architecture featuring a\\nunified encoder for watermark embedding and two distinct\\ndecoders designed to recover the watermark under varying\\nrobustness constraints, thereby enabling selective resilience\\nagainst different levels of adversarial distortion. However, most\\nexisting deep watermarking techniques for deepfake detection\\nrely on a fixed, pre-defined watermark location for extraction,\\narXiv:2511.04949v1  [cs.CV]  7 Nov 2025\\n\\n2\\nwhich makes them vulnerable, as perpetrators can easily detect\\nand tamper the watermark once its position is known. In\\naddition, these methods often lack robustness against benign\\nimage manipulations such as brightness adjustments or heavy\\ncompression, leading to a high rate of false positives in deep-\\nfake detection. Furthermore, current supervised and adversarial\\nlearning approaches provide only limited fidelity in addressing\\nthe resilience–fragility paradox.\\nMost recently, OmniGuard [9] introduced a more flexible\\napproach for image manipulation detection and localisation by\\nincorporating a degradation-aware tamper extraction network\\ndesigned for blind, robust copyright protection and tamper\\nlocalization. However, the framework embeds watermarks\\ndirectly in the pixel space, which makes them more suscep-\\ntible to attacks. Even minor changes in image orientation\\nor scale can significantly alter pixel values, making pixel-\\nbased systems highly sensitive to trivial modifications despite\\nthe semantic content remaining unchanged. Consequently, the\\nOmniGuard framework is less effective in detecting deepfakes,\\nwhere the primary objective is to identify only malicious edits\\nthat alter the semantic-level meaning of the image.\\nIn contrast to existing methods, we embed watermarks\\nin the latent space rather than the pixel space, leveraging\\nsemantic-level manipulations of the image’s representation.\\nSince the latent space encodes high-level meaning, the wa-\\ntermark becomes tightly coupled with the image’s semantics.\\nAny malicious attempt to alter the image’s meaning disrupts\\nthis coupling, effectively breaking the watermark and revealing\\ntampering. Most importantly, through a learnable watermark\\nembedder network, we identify less perceptually salient direc-\\ntions to embed information without significantly altering the\\nhuman-perceived semantics of the input image. We propose\\nto leverage a spherical latent space for the embedding of the\\nproposed watermark, which offers normalised operations for\\nthe embedding of our watermark, preventing the watermark\\nembedder from drifting arbitrarily far from the original input\\nrepresentation, and making significant changes.\\nEmbedding the watermark in this space offers two key\\nadvantages: (1) inherent resilience to benign transformations,\\nsuch as resizing, compression, or brightness adjustments, that\\ndo not significantly alter semantic features, and (2) heightened\\nsensitivity to malicious manipulations that change the semantic\\nmeaning of the image, such as identity swaps or expression\\nalterations in deepfakes. By disentangling the watermark from\\nthe pixel domain and leveraging semantic representations,\\nour approach aligns watermarking with the very nature of\\ngenerative models, making it both resilient and semantically\\naware.\\nWe propose a novel Multi-agent Adversarial Reinforcement\\nLearning (MAARL) paradigm for learning watermarks that are\\nboth resilient to benign manipulations and fragile to semantic\\nalterations, enabling robust deepfake detection. The framework\\nis structured as a multi-objective game between two agents: the\\nwatermarking agent (i.e., the watermark embedder, extractor),\\nand the attacker. The watermarking agents aim to embed latent\\nspace watermarks that survive traditional image transforma-\\ntions but fail under semantic changes. The attacker, in turn,\\nseeks to break the watermark by maximizing extractor loss. It\\ncan dynamically adjust the strength and type of attacks, rang-\\ning from conventional (e.g., compression, resizing) to semantic\\n(e.g., facial attribute editing via StyleGAN), and combine them\\ninto complex, evolving curricula. This adversarial setup forces\\nthe watermarking agent to learn a sophisticated strategy that\\nbalances robustness and fragility, avoiding both overly weak\\nand excessively strong watermarking schemes, allowing us to\\narrive at the right combination for this paradoxical objective.\\nThe result is an architecture that achieves unprecedented levels\\nof resilience across bonafide transformations and greater levels\\nof fragility towards malicious manipulations (See Fig. 1).\\nThe main technical contributions of this paper, through which\\nwe introduce the proposed DeepForgeSeal framework, can be\\nsummarised as follows:\\n1) We introduce a novel deep watermarking framework for\\ndeepfake detection that operates in the high-dimensional\\nsemantic space of input images.\\n2) We design a learnable watermarking agent with explicit\\ncontrol over message encoding and extraction, achieving\\nsemantic stealth.\\n3) We propose a new MAARL (Multi Agent Adversar-\\nial Reinforcement Learning) paradigm that enables the\\nagent to learn a strategy balancing watermark resilience\\nand fragility.\\n4) We develop a reward function that guides the watermark\\nattacker to discover failure regions in the latent space\\nand image manipulations that induce significant latent\\nshifts, allowing it to devise a structured curriculum of\\nattacks.\\nII. RELATED WORK\\nA. Watermarking Approaches for Deepfake Detection\\nPassive deepfake detection refers to techniques that analyse\\nthe media without requiring any prior knowledge of the\\ngeneration process or embedding of security features such as\\nwatermarks. These approaches have demonstrated remarkable\\nperformance over the past decade, primarily by identifying\\nartefacts introduced during the deepfake generation process,\\nsuch as unsynchronised lip movements [11], irregular facial\\nlandmarks [12], or physiological inconsistencies like abnormal\\nblood oxygen concentration [13]. However, their effectiveness\\nis inherently tied to the presence of such artefacts, which vary\\ndepending on the generation technique used. Consequently,\\nthe generalisability of these detectors to novel or unseen\\ndeepfake generation methods remains limited. Moreover, the\\nrapid evolution of deepfake technologies poses a significant\\nchallenge to maintaining detection robustness, raising concerns\\nabout the long-term reliability of passive detection strategies.\\nSuch limitations have led to the development of proactive\\ndeepfake detection approaches, with watermarking being a\\nprominent example. This technique embeds invisible signals\\ninto benign images before any manipulation occurs. These\\nsignals act as verification markers, enabling systems to actively\\ndetermine whether the content has been tampered with based\\non the presence and integrity of the embedded watermark.\\nOne of the earliest works in watermarking based deepfake\\ndetection can be attributed to [14], which embeds a robust,\\n\\n3\\ninvisible watermark into facial images before any manipulation\\noccurs. These watermarks are resilient to various deepfake\\ntransformations and image processing operations. The embed-\\nded watermark acts as a unique identifier, allowing the origin\\nof a manipulated image to be traced. The works [15], [16]\\nhave also leveraged the concept of robust watermarking as\\na line of defense against deepfakes. Specifically, in contrast\\nto [14], which embeds identity-linked tags using a learned\\nnetwork which is resilient to deepfake transformations, [15],\\n[16] introduce training-free watermarking mechanisms based\\non facial landmarks. For example, instead of relying on learned\\nembeddings, the authors of [16] propose to transform structural\\nfacial representations into binary perceptual watermarks, en-\\nabling robust and imperceptible embedding without extensive\\nmodel training. Despite these advances, robust watermarking-\\nbased deepfake detection approaches require validating the\\nembedded watermark against a known source or identity,\\nwhich can be logistically complex in real-world applications,\\nsuch as in social media platforms. Each image or video must\\nbe checked to ensure the watermark matches a trusted source\\nidentity, which assumes access to and maintenance of such a\\ndatabase.\\nSemi-fragile watermarks, which leverage both the fragility\\nand robustness of watermarking, simplify the detection process\\nby serving as self-contained integrity checks. Specifically, if\\nthe watermark is broken or unreadable, it directly signals\\ntampering, without needing to compare against a source,\\nmaking them more efficient and scalable for deployment\\nin environments where rapid and autonomous verification is\\ncritical. One of the early works in semi-fragile watermarks\\nfor deepfake detection can be attributed to FaceGuard [17],\\nwhich encodes a semi-fragile identity-specific watermark.\\nAlong similar lines, Zhao et al. [6] proposed an identity-\\nbased semi-fragile watermarking approach that is sensitive to\\nface swap manipulations while remaining resilient to benign\\nimage transformations. Despite its advances, the reliance on\\nidentity-specific watermarks for each individual makes these\\ntechniques less appealing for real-world applications in which\\nthe true identity of the individuals is not available or cannot\\nbe readily extracted. WaterLo [18] addresses these limitations\\nby introducing a localised, fixed semi-fragile watermark that\\ndisappear in manipulated regions, allowing not only detection\\nbut also precise localisation of tampered areas. Following this\\napproach, the authors of FaceSigns [19] have introduced a\\nsemi-fragile watermarking framework that embeds a 128-bit\\nsecret message directly into the image pixels.\\nMost recently, the EditGuard [10] introduces a novel ap-\\nproach for decoupling the training process from specific tam-\\npering types. Traditional watermarking systems often require\\nretraining or fine-tuning for each manipulation scenario. In\\ncontrast, EditGuard’s image-to-image steganography frame-\\nwork generalises across diverse tampering methods, including\\nface swaps, background replacements, and AI-generated edits,\\nwithout needing retraining. OmniGuard [9] further enhances\\nthis approach by introducing a hybrid forensic framework that\\ncombines proactive embedding of the semi-fragile watermark\\nwith passive blind extraction. In a different line of work,\\nthe authors of [7] leverage the mathematical properties of\\nfractals to generate watermark patterns through a parameter-\\ndriven pipeline. Moreover, watermarks are embedded using an\\nentry-to-patch strategy, where each watermark matrix entry is\\nmapped to a specific image patch, enabling precise localisation\\nof manipulated regions. Though significant strides have been\\nmade, the majority of existing semi-fragile watermarking\\napproaches still rely on the pixel space for embedding, rather\\nthan leveraging the latent space, which could offer greater\\nflexibility and robustness. Furthermore, current adversarial\\ntraining pipelines used when training the watermarking frame-\\nworks are often simplistic and lack the sophistication needed to\\nsimulate complex attack curricula. As a result, the resilience of\\nwatermarking techniques under realistic and increasingly so-\\nphisticated adversarial conditions remains limited. In contrast,\\nby leveraging a spherical latent space, our framework ensures\\nmore structured and disentangled representations, which en-\\nhances watermark embedding consistency, robustness, and ma-\\nnipulation sensitivity. Additionally, the integration of MAARL\\nenables the system to simulate and defend against complex,\\ncurriculum-based adversarial attacks, significantly improving\\nthe robustness and generalisability of the watermarking strat-\\negy across diverse manipulation scenarios.\\nB. Multi Agent Adversarial Reinforcement Learning\\nIn multi-agent learning setting the relationships among\\nagents can be categorised into corporative, competitive, and\\nboth [20], while most of the prior works [21], [22], [23],\\n[24], [25] in Multi Agent Reinforcement Learning (MARL)\\nhave focused on cooperative tasks, in which the cumulative\\nreward is maximized as a group. Among the limited number\\nof work extending traditional MARL into adversarial settings,\\nresearchers have introduced adversarial elements, either as\\ncompeting agents or perturbations, to enhance the robustness\\nand generalisation capabilities of the overall algorithm. For\\nexample, in [26] the authors have shown the potential of learn-\\ning a robust generalise policy using Multi Agent Adversarial\\nReinforcement Learning (MAARL), where as in the authors\\nof [27] have used MAARL to avoid overfitting. In a similar\\nline of work, Bukharin et. al [28] address the lack robustness\\nand sensitivity to environment changes in MARL through\\nadversarial training. Specifically, the authors of [28] introduce\\nadversarial regularization to enforce Lipschitz continuity in\\npolicies, improving robustness against noisy observations.\\nMost recently, Yuan et. al [29] have proposed an approach\\nbased on evolutionary learning to enhance robustness in\\nmessage-passing for improving the communication efficiency\\nof agents.\\nWhile adversarial training has shown promise in both\\nMARL and watermarking independently, to the best of our\\nknowledge, none of the prior works have investigated the\\nintegration of MAARL into watermarking. Through empirical\\nevaluations we demonstrate that MAARL offers a compelling\\nframework to train watermarking agents that can dynami-\\ncally adapt to image manipulations, potentially leading to\\nwatermarking systems that are both robust and semi-fragile,\\nproviding resistant to benign transformations yet sensitive to\\nmalicious tampering.\\n\\n4\\nIII. METHODS\\nIn this section, we discuss our proposed approach. We first\\nprovide an overview of the main components that constitute\\nour DeepForgeSeal framework in Sec. III-A. Our watermark-\\ning agent, watermark attacker, and watermark extraction and\\ndeepfake detection processes are discussed in Secs. III-B,\\nIII-C, and III-D, respectively. Finally, implementation details\\nof the framework are discussed in Sec. III-E.\\nA. Overview\\nGiven an input image x, we define a watermarking agent\\nθ that learns the distribution of the semantic features f(x),\\nprojects these features into a spherical latent space S, inserts\\na watermark wx in the latent space ensuring that it does not\\nsignificantly alter the semantic meaning of x, and decodes\\nthe new image xw with the watermark from the spherical\\nlatent space back to the image space. The objective of the\\nwatermarking agent is to identify the region in S where it\\ncould embed the watermark without significantly altering the\\nhuman-perceived meaning of x. Similarly, it should learn to\\nmaintain an optimal balance between the resilience of the\\nembedded watermark towards benign operations, such as JPEG\\ncompression, cropping, and color adjustments, and its fragility\\nto malicious edits, such as face swapping and facial attribute\\nediting, that alter the semantic content of the image.\\nThe objective of the watermark attacker agent, η, is to gen-\\nerate an adversarial image xa from the watermarked image xw,\\ni.e., η(xw) →xa, by applying transformations that degrade or\\nremove the embedded watermark. To achieve this, the attacker\\ncan choose from a range of image manipulations, including\\nbenign operations and malicious edits. These manipulations\\nmay also be combined, and for each type, the agent can control\\nthe associated parameters to vary the strength of the attack.\\nThe watermark extractor, δ, is responsible for retrieving\\nthe embedded watermark from a given image x′. Since the\\nwatermarking agent θ dynamically adjusts both the location\\nand content of the watermark, the extractor must co-adapt\\nand learn in tandem with the watermarking agent to ensure\\nreliable extraction. In our framework, the success or failure\\nof watermark extraction serves as a heuristic for deepfake\\ndetection. Specifically, if the extractor δ fails to recover a\\nvalid watermark from the image x′, the image is flagged as\\na potential deepfake. Our approach leverages the learned con-\\nsistency between the watermarking and extraction processes\\nas an indirect signal of authenticity, enabling the detection\\nof semantic-level manipulations without relying on a fixed\\nwatermark pattern. These modules, and the flow of information\\nbetween them, are illustrated in Fig. 2.\\nB. Watermarking Agent\\nThe watermarking agent is responsible for encoding and\\nembedding the watermark into the image. Its architecture is\\ndesigned to operate within the semantic latent space of a pre-\\ntrained CLIP model, ensuring that the embedding process is\\nguided by high-level image representations. The procedure\\ncomprises two key stages: (i) directional embedding in the\\nlatent sphere, and (ii) perturbative watermark embedding.\\nThese stages are discussed in detail in the following sub-\\nsections.\\n1) Directional Embedding in Latent Sphere: To extract\\nsemantically meaningful information from the input image,\\nx, our watermarking agent, θ, operates in the feature space\\nof the CLIP image encoder [30], which is denoted as ECLIP.\\nSpecifically, the feature vector f(x) = ECLIP(x) ∈Rζ is\\nL2-normalized, such that ∥f(x)∥2 = 1, constraining it to the\\nsurface of a ζ-dimensional hypersphere. The watermarking\\nagent has explicit control over the orthonormal embedding\\ndirections that it leverages to embed the message (i.e., wa-\\ntermark), M. However, allowing the watermarking agent to\\nfreely modify both the embedded message and the latent space\\ndirections makes it infeasible for the watermark extractor to\\nrecover the message without access to either the original image\\nx or the watermark M. To address the challenge of extracting\\ndynamically embedded messages, we propose deriving the la-\\ntent space directions through a learnable direction generator,\\nµ, while relying on a shared secret key K. Specifically, the\\nembedding directions are deterministically derived from K,\\nwhich may be a password-like string. The key is first projected\\ninto the CLIP text embedding space [30] to obtain a feature\\nvector f(K). The direction generator network µ then uses this\\nfeature to produce a canonical set of L orthonormal direction\\nvectors D = {d1, d2, . . . , dL}. Formally,\\nD = µ(f(K)),\\nwhere di ∈Rd,\\n∥di∥2 = 1\\n(1)\\nSince D depends only on K, it remains consistent across\\nall images. However, the directions are still learnable, as the\\nweights ω of µ are optimised during training. For each bit\\nmi ∈{0, 1} of a dynamically generated message M, the\\nobjective is to modify the image x to produce x′ such that the\\nprojection of its new feature vector f(x′) onto each direction\\ndi matches a target value,\\n⟨f(x′), di⟩≈ptarget\\ni\\n=\\n(\\nξ1\\nif mi = 1\\nξ0\\nif mi = 0\\n(2)\\nwhere ⟨·, ·⟩denotes the dot product, and ξ1, ξ0 are prede-\\nfined projection magnitudes that encode binary values.\\n2) Perturbative Watermark Embedding: Once the mi ∈\\n{0, 1} of the message M have been identified, the water-\\nmarking agent employs a MLP layer, ϕ, that receives semantic\\nfeatures, f(x), of x and the watermark, M, and synthesizes\\na perturbation map p over ζ-dimensional feature vector f(x)\\nsuch that:\\nq = ϕ(x, M).\\n(3)\\nA decoder π then reconstructs the watermarked image x′ from\\nthe perturbed features:\\nx′ = π(q).\\n(4)\\nThe watermarking agent’s objective is to minimize a com-\\nposite loss function LW that balances imperceptibility (Lclip),\\nembedding accuracy (Ldir), and the conditional fragility ob-\\njective (Lext). Formally, this can be written as:\\nLW = αLclip + βLdir + γLext,\\n(5)\\n\\n5\\nWatermarker\\nAttacker\\nCLIP Image \\nFeature Extraction\\nWatermark \\nExtractor (𝞭)\\nDirection \\ngenerator \\nCLIP Text Feature \\nExtraction\\n“Secret Key”\\nWatermarked Image (𝒙w)\\nOriginal Image (𝒙)\\nBenign manipulations\\n•\\nCrop\\n•\\nColor Jitter\\n•\\nAffine Transform\\n•\\nCompression\\nMalicious manipulations\\n•\\nChange Hair Color\\n•\\nChange Expression\\n•\\nChange Age\\nAttack Curriculum\\nMLP\\nAttack Parameters\\nWatermarking Agent (𝛳)\\nAttacker Agent (η) \\nD={d1,…,dL}.\\nD={d1,…,dL}\\nD={d1,…,dL}\\nExtracted Directional \\nVectors\\nLearned Directional \\nVectors\\nBit Error Rate\\nBenign / Tampered\\nDeepfake Detector\\nAttacked Image (𝒙a)\\nf(𝒙)\\nFig. 2.\\nMethod Overview: Given an input image x, the watermarking agent θ embeds a watermark wx into the spherical latent space S derived from semantic\\n(CLIP Image) features f(x), producing a watermarked image xw. An attacker η generates an adversarial image xa to disrupt watermark integrity. η uses both\\nbenign edits (e.g., JPEG compression, cropping) and semantic-altering manipulations (e.g., face swaps) when generating its attack curriculum. The extractor\\nδ attempts to recover wx from any image x′; failure to extract a valid watermark flags x′ as a potential tampered image, leveraging watermark consistency\\nas a proxy for semantic authenticity.\\nwhere,\\nLclip = ∥f(x) −f(x′)∥2\\n2\\n(6)\\nLdir = CrossEntropy(M, M ′),\\n(7)\\nLext = −BER(M, M ′),\\n(8)\\nwhere M ′ is the recovered message obtained by decoding\\nthe projection vector P = {⟨f(x′), di⟩}L\\ni=1 and BER(., .) is\\nBit Error Rate. See Sec. III-D for details.\\nC. Watermark Attacker\\nThe objective of the watermark attacker, η, is to learn an\\noptimal policy to destroy the embedded watermark. One of our\\nkey innovations is its ability to compose complex attacks using\\na predefined set of benign/traditional image transformations\\nand malicious/generative transformations, effectively learning\\na dynamic attack curriculum which includes both type and\\nintensity of each attack. The following subsections illustrate:\\n(i) the process of composing a combinatorial attack, (ii) how\\nthe watermark attacker agent is trained, and (iii) how it is\\nincentivized to identify attack curricula that lead to substantial\\nsemantic drifts and are closer to historically known failure\\nregions in the latent space, where the watermark extractor fails\\nto recover the watermark.\\n1) Composing a Combinatorial Attack: Let the set of\\navailable attacks, including the benign and malicious trans-\\nformation, be denoted as A = {a1, . . . , al}. The watermark\\nattacker agent, η, receives the CLIP features, f(x′), of the\\nwatermarked image, x′, as input and outputs two vectors:\\nlogits for attack selection and normalized strength parameters\\nτ as:\\n(logits, τ) = η(f(x′)).\\n(9)\\nThese logits are passed through a sigmoid function (denoted\\nby σ to convert them into probabilities:\\nB = σ(logits) ∈[0, 1]L.\\n(10)\\nThen, for each probability, bl , we sample a binary action, ak,\\nfrom a Bernoulli distribution as:\\nal ∼Bernoulli(Pl)\\n∀l ∈{1, ..., L}\\n(11)\\nThe result is a binary action vector a ∈{0, 1}L in which\\neach element is independently sampled, creating a combina-\\ntorial attack strategy.\\nSimultaneously, the strength vector τ ∈[0, 1]L determines\\nthe intensity for each attack. Each normalized value τl is\\nmapped to a meaningful parameter, paraml, in the selected\\nattack type, al, and within a predefined range [minl, maxl] as:\\nparaml = minl + (maxl −minl) · τl.\\n(12)\\n2) Learning an Attack Curriculum: We train our watermark\\nattacker agent using reinforcement learning to learn an\\noptimal policy ψ that maximizes the likelihood of watermark\\nextraction failure, thereby ensuring the success of the attack.\\n\\n6\\nSpecifically, the primary reward R for the watermark attacker\\nagent can be formulated as the failure of the Watermark\\nExtractor, measured by the Binary Cross-Entropy (BCE) loss\\nbetween the extracted message ˜\\nM and the original message\\nM as:\\nRfailure = BCE( ˜\\nM, M).\\n(13)\\nA strong and adaptive attack policy would drive the water-\\nmarker to learn more resilient and sophisticated watermarking\\nstrategies. In contrast, the simplistic objective in Eq. 13,\\nwhich focused solely on extractor failure, lacks the distinction\\nrequired to drive meaningful improvements. As such, an\\nadvanced reward signal formulation is required to incentivize\\nthe attacker to pursue diverse and exploratory policies that\\nprovide richer adversarial signals.\\n3) Promoting Semantic Drift and Targeted Shifts Toward\\nKnown Failure Regions: We propose to augment the re-\\nward with bonuses to encourage exploration and exploitation.\\nSpecifically, we define a curiosity reward which is propor-\\ntional to the squared Euclidean distance between the CLIP\\nfeatures of the image before and after the attack. This could\\nbe written as:\\nRcuriosity = ∆∥f(x′) −f(a(x′))∥2,\\n(14)\\nwhere ∆is a scaling factor. This reward incentivizes the\\nattacker to discover attacks that cause the most significant\\nsemantic disruption, which are often the most challenging for\\na watermark to survive.\\nIn addition, we introduce a proximity reward that encour-\\nages the attacker to perturb the image toward regions of the\\nlatent space known to challenge the watermark extractor. In\\nparticular, we introduce a memory buffer, J, that stores the la-\\ntent representations, f(a(x′)), of images that have historically\\ncaused failures in watermark extraction. Then, the potential, ρ,\\nof a new attacked image a(x′′) can be defined as its proximity\\nto its closest feature vector stored in the failure memory. Now\\nthe attacker’s proximity reward can be defined as inversely\\nproportional to the distance of its attacked image from the\\nclosest failure memory embedding. Formally, this could be\\nwritten as:\\nρ(a(x′′)) = min\\nfj∈J ∥f(a(x′′)) −fj∥2,\\nRproximity =\\nν\\nρ(a(x′′)) + ϵ,\\n(15)\\nwhere ν is a scaling factor and ϵ is a small constant to ensure\\nnumerical stability.\\nNow, the final loss function of the watermark attacker agent\\ncan be written as:\\nLη = −Ea∼ϕ\\nh\\nRfailure + Rproximity + Rcuriosity + o\\nL\\nX\\nl=1\\nal\\ni\\n−rH(P)\\n(16)\\nwhere PL\\nl=1 al is a small regularization term penalizing the\\nnumber of active actions, H(P) is the entropy of the action\\ndistribution P, and promotes exploration in the policy ϕ,\\nand o and r are hyperparameters. It should be noted that\\nRcuriosity encourages goal-directed exploration while H(P) is\\nused to encourage policy-level exploration such that the agent\\nmaintains diversity in action selection, serving complementary\\npurposes in the learning process.\\nD. Watermark Extraction and Deepfake Detection\\nThe watermark extractor, δ, is responsible for recovering the\\nmessage M from a potentially corrupted image x′′. Specifi-\\ncally, given the input x′′, it first extracts CLIP features fx′′,\\nand independently generates the direction vectors, D, from\\nthe secret key, K, using the direction generator, µ. Then it\\nproduces the projection of fx′′ onto each of the direction\\nvector di ∈D as:\\np′′\\ni = ⟨fx′′, di⟩.\\n(17)\\nThe resulting vector of projections P ′′ = {p′′\\n1, ..., p′′\\nL} is\\npassed through an MLP, which outputs the logits for the\\npredicted message, M ′.\\nIn the proposed framework, the fragility of the watermark\\nserves as the deepfake detection mechanism. Even when the\\ndirection generator, µ, the integrity of an input image can be\\nverified at inference time by computing the Bit Error Rate\\n(BER) between the extracted message M ′ and the known orig-\\ninal message M. Since the same trained direction generator µ\\nand secret key K are used during both watermark embedding\\nand extraction, the resulting direction vectors should remain\\nconsistent. A high BER indicates potential generative manip-\\nulation, enabling us to leverage the fragility of the watermark\\nas a deepfake detection signal. Formally, this could be written\\nas:\\nfDEEPFAKE(x′′) =\\n(\\nTrue,\\nif BER(M ′, M) > λ\\nFalse,\\notherwise\\n(18)\\nwhere λ is a predefined classification threshold.\\nE. Implementation Details\\nImplementation of this framework is completed using Py-\\nTorch. All the images are resized to a resolution of 224 × 224.\\nFor both watermarking agent and attacker agent, Adam [31]\\noptimiser with a learning rate of 1e−4 is used for optimisation.\\nThe model is trained for 50 epochs with a batch size of 16\\non an NVIDIA A100 GPU. For consistency in comparison,\\nsimilar to baseline methods, the length of the watermark bits\\nis set to 512 bits. The classification threshold, λ is evaluated\\nexperimentally and set to 0.8. The architecture details of the\\nwatermarking agent, watermark attacker, direction generator,\\nand watermark extractor are provided below.\\nThe Direction Generator (µ) receives the 512-D CLIP-Text\\nfeature of the secret message. This is processed by a two-\\nlayer MLP: a linear layer of 256 dimensions followed by\\nReLU, and a second linear layer (R256 →R512). The output\\nis L2-normalized along the feature dimension.\\nThe watermarker operates in the CLIP feature space. It\\ncombines the original CLIP image feature (512 dimensions)\\nand the directional vectors (512 dimensions) into a 1024-D\\nvector. This is processed by a linear layer with 512 hidden\\n\\n7\\nunits and ReLU activation, and outputs a 512-D feature-space\\nperturbation using Tanh activation, which is then passed to\\nthe integrated Image Decoder. The decoder, π, first maps\\nthis vector to a higher-dimensional space using a single FC\\nlayer (R512 →R4096), reshaping the output to 256 × 4 × 4.\\nThis tensor is sequentially upsampled through five transposed\\nconvolutional layers, using ReLU (except the final layer’s\\nTanh) and a stride of 2 (except for the final layer’s stride of\\n4) to reach the final 3 × 224 × 224 image resolution.\\nThe watermark extractor (δ) recovers the message by first\\nprojecting the 512-D CLIP features onto a 128-D latent space.\\nThis vector is then passed through a two-layer MLP: a linear\\nlayer (R128 →R256) with ReLU, followed by a final linear\\nlayer (R256 →R512) to output the message logits.\\nThe watermark attacker (η) uses a shared two-layer MLP\\nbackbone, first layer with 256 hidden units and ReLU acti-\\nvation, and the next layer with 128 hidden units and ReLU\\nactivation. This backbone outputs two vectors: a vector of\\nlogits for attack selection, and a vector of normalized strength\\nparameters. The logits are passed through a Sigmoid function\\nto yield a probability vector, B. For each probability bl, a\\nbinary action al is sampled from a Bernoulli(Pl) distribution,\\nresulting in a binary action vector a ∈{0, 1}L that defines the\\ncombinatorial attack strategy. The strength vector τ determines\\nthe intensity for each attack.\\nIV. EXPERIMENTS\\nIn this section, we present the results of our experiments\\ndesigned to evaluate and compare the effectiveness of the\\nproposed DeepForgeSeal framework against existing state-\\nof-the-art methods. We begin by detailing the datasets used\\nin our evaluations (Sec. IV-A), followed by a description\\nof the evaluation metrics employed to assess model perfor-\\nmance (Sec. IV-B). The main experimental results, where\\nwe benchmark our method against current state-of-the-art\\napproaches, are provided in Sec. IV-C. Additionally, ablation\\nstudies conducted to validate the contributions of the learnable\\ndirectional embedding strategy, adversarial learning pipeline,\\nand novel reward function are discussed in Sec. IV-D. Finally,\\nwe analyse the time complexity of the DeepForgeSeal model\\nin Sec. IV-E.\\nA. Datasets\\nFollowing prior works [6], we use Flickr-Faces-HQ (FFHQ)\\n[32] dataset for training our model, and CelebA [33] and\\nCelebA-HQ [34] datasets for evaluating the model and demon-\\nstrating its generalisability. The FFHQ dataset contains 70,000\\nimages at a resolution of 1024 × 1024 pixels. In accordance\\nwith the dataset authors’ guidelines, the first 60,000 images\\nare designated for training, while the remaining 10,000 are\\nreserved for validation. We follow this protocol and use the\\ntraining subset of FFHQ to train our model. CelebA comprises\\n10,000 distinct identities, with each identity represented by 20\\nimages at a 128x128 resolution. CelebA-HQ is a high-quality\\nversion of the CelebA dataset, consisting of 30,000 images at\\na resolution of 1024 × 1024 pixels. Specifically, we followed\\nthe official test split provided by the CelebA and CelebA-HQ\\ndataset authors to enable direct comparisons.\\nB. Evaluation Metics\\nWe evaluate our method from two key perspectives. First,\\nwe assess the visual fidelity of the watermarked images com-\\npared to the original images using two widely adopted metrics:\\nPeak Signal-to-Noise Ratio (PSNR) and Structural Similarity\\nIndex Measure (SSIM). PSNR provides a quantitative measure\\nof the pixel-level differences, where higher values indicate less\\ndistortion and better preservation of image quality. SSIM, on\\nthe other hand, evaluates perceptual similarity by considering\\nfactors such as luminance, contrast, and structural information,\\noffering a more human-aligned assessment of image quality.\\nTogether, these metrics help us determine how effectively our\\nmethod maintains the integrity of the original image while\\nembedding the watermark.\\nTo evaluate the performance of deepfake detection, fol-\\nlowing prior works [6], we compute image-level Accuracy\\n(ACC) and F1-Score. Accuracy reflects the overall proportion\\nof correctly classified images, encompassing both correctly\\nidentified real and fake samples. The F1-Score, which balances\\nprecision and recall, is particularly informative in capturing\\nthe trade-off between false alarms (incorrectly classifying real\\nimages as fake) and miss detections (failing to identify fake\\nimages). Together, these metrics provide a reliable measure\\nof the system’s ability to detect deepfakes while minimizing\\nclassification errors.\\nC. Comparisons with Existing State-of-the-art Methods\\nIn this section, we report results for the proposed model and\\ncompare with the existing state-of-the-art methods considering\\nthe visual quality of the watermarking (See Sec. IV-C1), deep-\\nfake detection performance (See Sec. IV-C2), and watermark’s\\nresilience and fragility (See Sec. IV-C3).\\n1) Visual Quality of Watermarking Approaches: As base-\\nlines we use state-of-the-art methods, including FaceSigns\\n[19], EditGard [10], and Zhao et. al [6]. When choosing\\nour baselines, we ensured that a variety of different deep\\nlearning approaches, including adversarial trained models [19],\\ndual-purpose watermarking methods for tamper detection and\\ncopyright protection [10], and identity-entangled watermark-\\ning methods [6], are compared, enabling a comprehensive\\ncomparison.\\nTABLE I\\nVISUAL FIDELITY OF DIFFERENT WATERMARKING TECHNIQUES. THE\\nBEST RESULTS ARE HIGHLIGHTED IN BOLD.\\nModel\\nQuality Metrics\\nPSNR (↑)\\nSSIM (↑)\\nFaceSigns [19]\\n32.03\\n0.92\\nZhao et. al [6]\\n38.32\\n0.94\\nEditGard [10]\\n45.07\\n0.96\\nDeepForgeSeal (ours)\\n48.39\\n0.97\\nQuantitative comparisons for the CelebA-HQ dataset are\\npresented in Tab. I. When comparing the proposed DeepForge-\\nSeal with the previous state-of-the-art methods, we observe\\nthat our method has the highest PSNR and SSIM, demonstrat-\\ning our framework’s ability to embed watermarks stealthily,\\npreserving the image’s visual quality. We attribute this to\\n\\n8\\noperating within the high-dimensional semantic space of the\\ninput images, which enables subtle yet effective modifications.\\nIn addition to the above quantitative comparisons, in Fig. 3\\nwe present a qualitative comparison between FaceSigns [19],\\nEditGard [10], and the proposed DeepForgeSeal method for\\nembedding watermarks in two sample images from the CelebA\\ndataset. Figure 3 supports the findings from our quantitative\\ncomparisons, showing that our watermarked images maintain\\nhigh visual realism by accurately preserving the original\\ntextures, hue, and lighting. In contrast, both FaceSigns and\\nEditGuard noticeably alter the image hue, with FaceSigns\\nadditionally introducing visible artefacts in the facial region.\\nThese results highlight the effectiveness of our proposed\\nframework, which embeds watermarks in the latent space\\nrather than directly in the pixel space, enabling more natural\\nand less intrusive modifications.\\n2) Deepfake Detection Performance: We compare the pro-\\nposed DeepForgeSeal method with both passive and proac-\\ntive deepfake detection methods. Following [6], we employ\\nCelebA and CelebA-HQ datasets to represent low and high-\\nresolution real image cases and generate deepfakes using a\\nvariety of deepfake generation methods, including AttGAN,\\nStarGAN, InfoSwap and SimSwap, CIAGAN, and DeepPri-\\nvacy. These methods allow us to evaluate the performance of\\nthe proposed deepfake detection framework against a wide\\nrange of malicious manipulations, including identity swap-\\nping and face anonymization techniques. In our experimental\\npipeline, for passive deepfake detection methods, we use\\nnon-watermarked images from the CelebA and CelebA-HQ\\ndatasets as real images. The above deepfake generation tech-\\nniques are then applied to create their fake counterparts. For\\nproactive deepfake detection methods, including the proposed\\nDeepForgeSeal approach, we first watermark the images from\\nthe CelebA and CelebA-HQ datasets, and then generate the\\ncorresponding fake images using the same deepfake generation\\nmethods.\\nFollowing [6], as baselines for passive deepfake detection\\nwe use the state-of-the-art BTS [35], CD [36], Bonettini et.\\nal [37], PF [38], RFM [39], SBI [40], and CBO-DD [41].\\nIn addition, the state-of-the-art FaceSigns [19], EditGard [10],\\nand Zhao et al. [6], which are proactive deepfake detection\\nmethods, have also been used for comprehensiveness.\\nTab. II summarises the comparison results for deepfake\\ndetection in CelebA and CelebA-HQ datasets. It is evident\\nthat passive deepfake detection methods generally struggle\\nto generalise across different deepfake generation techniques.\\nAmong the approaches evaluated, only methods such as\\nSBI [40] and CBO-DD [41] consistently demonstrate strong\\nperformance across all considered generation methods. In\\ncontrast, most other passive detection techniques perform well\\nagainst a limited subset of generation methods but show\\nsignificant performance degradation when applied to others.\\nThis limitation stems from their training approach, which\\nfocuses on identifying artefacts specific to the generation\\ntechniques used during training. Consequently, these methods\\ntend to perform poorly when tested on deepfakes produced by\\nunfamiliar or diverse generation techniques.\\nWhen comparing the results of the proactive detection\\nmethods FaceSigns [19], Zhao et. al [6], and EditGard [10]\\nwith the proposed DeepForgeSeal method, we see that our\\nmethod has been able to achieve significant detection accuracy\\nacross all considered generation methods. Although state-of-\\nthe-art methods such as Zhao et. al [6], and EditGard [10]\\nhave been able to achieve impressive performance against\\ndeepfake generation methods such as AttGAN and DeepPri-\\nvacy, their performance degrade when tested against Deep-\\nPrivacy and StarGAN2 generation methods. In contrast, our\\nproposed DeepForgeSeal method consistently demonstrates\\nsuperior detection performance across all evaluated deepfake\\ngeneration techniques. We attribute this to the learnable wa-\\ntermarking strategy embedded within the framework, which\\nenables explicit control over message encoding and extraction.\\nAdditionally, the adversarial reinforcement learning pipeline\\nallows the watermarking agent to actively observe and respond\\nto malicious attacks, dynamically adapting its watermarking\\nstrategy. Together, these components contribute to DeepForge-\\nSeal’s strong generalisation capability across a wide range of\\ndeepfake generation methods.\\nTo further demonstrate the strong generalisation capabilities\\nof the proposed method, we conducted an additional evaluation\\ninvolving a series of facial attribute manipulations and face\\nreenactments using watermarked images. For this experiment,\\nwe employed several state-of-the-art generative AI (GenAI)\\nvideo synthesis tools, including OpenAI SORA, Gemini Veo\\n3, StyleMask, and Hyper Reenact, and tested the deepfake\\ndetection performance of our DeepForgeSeal approach us-\\ning these manipulated images. It is important to note that\\nthe DeepForgeSeal model was not exposed to these types\\nof manipulations during training. Fig 4 presents qualitative\\nvisualisations, highlighting the faces our method successfully\\nidentified as fake. For video-level decisions, we computed\\nthe average of frame-level detection scores. As anticipated,\\nour model effectively detected manipulations generated by\\nthe latest GenAI video synthesis tools, demonstrating its\\nrobustness and ability to provide comprehensive protection\\nagainst malicious deepfakes. For additional visual examples,\\nplease refer to the supplementary material.\\n3) Watermark Resilience and Fragility Against Attacks:\\nIn this section, we compare the resilience of our Deep-\\nForgeSeal watermarking methods with the current state-of-\\nthe-art methods under different attacks against watermarks.\\nFor comprehensiveness, we consider both traditional benign\\nattacks, such as cropping, compression, and color changes,\\nas well as the learnable benign attacks proposed by Zhao et.\\nal [42] and Lukas et. al [43]. In addition, to quantitatively\\nassess the fragility against malicious image manipulations,\\nwe generate attacks using state-of-the-art GAN-based face\\nswapping and reenactment techniques, including SimSwap\\n[44], UniFace [45], FaceDancer [46], and HyperReenact [47].\\nThe evaluation results are presented in Tab. III. As baselines,\\nwe use the state-of-the-art FaceSigns [19] and EditGard [10]\\nsemi-fragile watermarking methods. Following prior works\\n[19], [10], we measure the Bit Recovery Accuracy (BRA)\\nas the evaluation metric, where we expect a higher BRA\\nagainst benign and lower BRA against malicious transforms\\nto achieve the goal of semi-fragile watermarking. As reported\\n\\n9\\nOriginal\\nWatermarked\\nAttacked\\nDeepForgeSeal(Ours)\\nFace Signs\\nEdit Gard\\nPSNR: 46.21 dB\\nPSNR: 32.89 dB\\nPSNR: 35.07 dB\\nBit Acc: 100%\\nBit Acc: 62.50%\\nBit Acc: 60.94%\\nJPEG compression, \\nNoise, Crop, Jitter, \\nAffine transform\\nOriginal\\nWatermarked\\nAttacked\\nPSNR: 48.58 dB\\nPSNR: 33.01 dB\\nPSNR: 36.63 dB\\nBit Acc: 100%\\nBit Acc: 31.25%\\nBit Acc: 48.44%\\nJPEG compression, \\nNoise, Jitter, Affine \\ntransform\\nFig. 3.\\nQualitative Results of Visual Quality: A comparison between the existing state-of-the-art models, FaceSigns [19], EditGard [10], and the proposed\\nDeepForgeSeal method for watermarking two sample images from the CelebA dataset [33].\\nTABLE II\\nACCURACY (↑) AND F1 SCORES (↑) OF DIFFERENT METHODS’ DEEPFAKE DETECTION RESULTS. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.\\nDetection methods\\nLow Resolutions (CelebA)\\nHigh Resolutions (CelebA-HQ)\\nAttGAN\\nCIAGAN\\nDeepPrivacy\\nInfoSwap\\nSimSwap\\nStarGAN2\\nAttGAN\\nCIAGAN\\nDeepPrivacy\\nInfoSwap\\nSimSwap\\nStarGAN2\\nBTS [35]\\n0.86/0.87\\n0.51/0.66\\n0.50/0.66\\n0.49/0.66\\n0.51/0.67\\n0.53/0.67\\n0.86/0.87\\n0.50/0.66\\n0.50/0.66\\n0.49/0.65\\n0.50/0.66\\n0.55/0.68\\nCD [36]\\n0.88/0.86\\n0.51/0.03\\n0.51/0.01\\n0.54/0.17\\n0.51/0.01\\n0.78/0.71\\n0.81/0.77\\n0.51/0.04\\n0.52/0.07\\n0.52/0.07\\n0.52/0.06\\n0.84/0.81\\nBonettini et. al [37]\\n0.59/0.69\\n0.62/0.62\\n0.58/0.68\\n0.57/0.64\\n0.60/0.71\\n0.63/0.63\\n0.53/0.68\\n0.65/0.62\\n0.56/0.69\\n0.51/0.66\\n0.55/0.69\\n0.49/0.65\\nPF [38]\\n0.76/0.79\\n0.51/0.66\\n0.52/0.65\\n0.57/0.68\\n0.54/0.67\\n0.99/0.98\\n0.75/0.79\\n0.51/0.66\\n0.55/0.68\\n0.56/0.69\\n0.54/0.68\\n0.98/0.97\\nRFM [39]\\n0.50/0.67\\n0.51/0.67\\n0.51/0.67\\n0.50/0.67\\n0.51/0.67\\n0.50/0.67\\n0.50/0.67\\n0.50/0.67\\n0.51/0.67\\n0.50/0.67\\n0.50/0.67\\n0.50/0.67\\nSBI [40]\\n0.79/0.82\\n0.77/0.8\\n0.78/0.82\\n0.77/0.81\\n0.78/0.81\\n0.72/0.75\\n0.80/0.80\\n0.72/0.78\\n0.78/0.80\\n0.76/0.77\\n0.83/0.84\\n0.69/0.70\\nCBO-DD [41]\\n0.84/0.84\\n0.91/0.93\\n0.86/0.87\\n0.79/0.85\\n0.84/0.88\\n0.85/0.85\\n0.83/0.86\\n0.92/0.93\\n0.86/0.87\\n0.80/0.85\\n0.89/0.91\\n0.82/0.84\\nFaceSigns [19]\\n0.91/0.93\\n0.83/0.84\\n0.91/0.95\\n0.93/0.95\\n0.95/0.95\\n0.96/0.98\\n0.87/0.88\\n0.92/0.93\\n0.80/0.83\\n0.91/0.91\\n0.82/0.82\\n0.84/0.87\\nZhao et. al [6]\\n0.94/0.94\\n0.87/0.86\\n0.98/0.98\\n0.98/0.98\\n0.97/0.98\\n0.82/0.84\\n0.94/0.94\\n0.85/0.82\\n0.99/0.98\\n0.99/0.99\\n0.98/0.98\\n0.85/0.87\\nEditGard [10]\\n0.96/0.97\\n0.88/0.88\\n0.96/0.98\\n0.94/0.97\\n0.97/0.98\\n0.91/0.93\\n0.95/0.96\\n0.87/0.91\\n0.97/0.99\\n0.99/0.99\\n0.98/0.99\\n0.93/0.95\\nDeepForgeSeal (Ours)\\n0.96/0.98\\n0.92/0.95\\n0.99/0.99\\n0.98/0.99\\n0.98/0.98\\n0.99/0.99\\n0.96/0.98\\n0.95/0.96\\n0.99/0.99\\n0.99/0.99\\n0.99/0.99\\n0.98/0.97\\nTABLE III\\nBIT RECOVERY ACCURACY (BRA) OF BASELINE FACESIGNS [19] AND EDITGARD [10] WATERMARKING METHODS AND PROPOSED DEEPFORGESEAL\\nMETHOD UNDER BENIGN AND MALICIOUS TRANSFORMATIONS. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.\\nMethod\\nBRA (%) - Benign Transforms (↑)\\nBRA (%) - Malicious Transforms (↓)\\nJPG\\nNoise\\nCrop\\nJitter\\nAffine\\nZhao et. al [42]\\nLukas et. al [43]\\nSimSwap\\nUniFace\\nFaceDancer\\nHyperReenact\\nFaceSigns [19]\\n0.87\\n0.73\\n0.72\\n0.91\\n0.96\\n0.87\\n0.88\\n0.52\\n0.49\\n0.62\\n0.51\\nEditGard [10]\\n0.95\\n0.78\\n0.96\\n0.96\\n0.99\\n0.92\\n0.95\\n0.48\\n0.49\\n0.44\\n0.50\\nDeepForgeSeal (ours)\\n1.00\\n0.98\\n0.98\\n0.99\\n1.00\\n0.98\\n0.99\\n0.24\\n0.11\\n0.10\\n0.06\\n\\n10\\nOpenAI - SORA\\nStyle Mask\\nFaceDancer\\nGoogle Veo 3\\nHyper Reenact\\nOpenAI - SORA\\nFig. 4. Qualitative results of our DeepForgeSeal model when tested on videos generated by completely unseen image and video deepfake generation techniques\\n(OpenAI SORA, Gemini Veo 3, StyleMask, and Hyper Reenact). We visualise the sample images showing the detected face, along with the original (bonafide)\\nwatermarked image before the manipulation (provided in bottom left). For additional visualisations, please refer to the supplementary material.\\nin Tab. III, we observe that both baseline models perform\\npoorly against benign image manipulations, demonstrating that\\nthese watermarking techniques are too fragile to withstand\\nthe benign image transformations that users can apply in\\nthe real world. At the same time, both baseline techniques\\ndemonstrate less fragility towards the malicious, content-\\naltering manipulations that the considered face swapping and\\nreenactment techniques have generated. These evaluations\\nclearly demonstrate the limitations of the existing state-of-\\nthe-art watermarking techniques and the superiority of the\\nDeepForgeSeal paradigm that enables us to learn a strategy\\nthat optimally balances watermark resilience and fragility.\\nD. Ablation Evaluations\\nWe conducted a series of ablation studies to systematically\\nanalyse the impact of the individual innovations that our\\nDeepForgeSeal framework proposes. Several design choices\\ncontribute to the robustness of our model: i) the proposed\\nlearnable directional embedding strategy that uses a latent\\nsphere; ii) the proposed adversarial learning pipeline with\\ncomposed attacks with a learnable attack curriculum; and\\niii) the novel reward function that promotes semantic drift\\nand targeted shifts toward known failure regions. All ablation\\nexperiments were conducted on the CelebA-HQ dataset, and\\nfor testing the ablation models, we used the validation set of\\nCelebA-HQ. As evaluation metrics, we use PSNR and average\\nBRA against benign and malicious manipulations.\\n1) Effects of Learnable Directional Embedding in Latent\\nSphere: To study the effect of the proposed learnable di-\\nrectional embedding strategy that uses a latent sphere, we\\ngenerated two ablation variants of the proposed DeepForge-\\nSeal model: i) DeepForgeSeal - w/o [D] - a model with\\nnaive latent embedding in which we embed the message\\ndirectly in the latent space without directional encoding, and\\nii) DeepForgeSeal - FD - a model with fixed directional\\nembeddings.\\nResults of this comparison are shown in Tab. IV. We observe\\n\\n11\\nTABLE IV\\nEFFECT OF LEARNABLE DIRECTIONAL EMBEDDING IN LATENT SPHERE.\\nTHE BEST RESULTS ARE HIGHLIGHTED IN BOLD.\\nModel\\nPSNR (↑)\\nBRA - Benign (↑)\\nBRA - Malicious (↓)\\nDeepForgeSeal - w/o [D]\\n48.43\\n0.78\\n0.42\\nDeepForgeSeal - FD\\n46.58\\n0.86\\n0.19\\nDeepForgeSeal\\n48.12\\n0.99\\n0.12\\nthat the learnable directional embedding strategy is an integral\\npart of the proposed framework. Specifically, we observe that\\nthe naive latent embedding approach, where the message is\\ndirectly embedded into the latent space, leads to a reduction\\nin PSNR, distorts the natural image characteristics, and dimin-\\nishes the watermark’s resilience against benign image transfor-\\nmations. The introduction of directional embedding enhances\\nthe robustness of the watermark (see the row corresponding\\nto DeepForgeSeal - FD). Notably, the experimental results\\ndemonstrate that the learnable directional embedding strategy,\\nwhich encodes the message within a latent sphere, achieves\\nthe most favorable balance between resilience and fragility,\\nwhile preserving image fidelity. These experiments confirm\\nthe utility of our learnable directional embedding procedure.\\n2) Effects of Composing a Combinatorial Attack and Learn-\\ning an Attack Curriculum: The effectiveness of the proposed\\nadversarial learning pipeline with composed attacks with a\\nlearnable attack curriculum is evaluated in this experiment.\\nWe generated eight ablation variants of the proposed Deep-\\nForgeSeal model: i) DeepForgeSeal - w/o η - a model without\\na watermark attacker agent, ii) DeepForgeSeal - w/o A - a\\nmodel in which one fixed attack type (no composition or\\nlearning of an attack curriculum) is applied, iii) DeepForgeSeal\\n- |A| - a model in which multiple attacks are applied in a\\nfixed sequence, iv) DeepForgeSeal - ˜\\nA - a model in which\\nmultiple random combinations of attacks applied, v) Deep-\\nForgeSeal - a1, τ ↑- a model in which a single attack type with\\ngradually increasing strength is applied, vi) DeepForgeSeal -\\nA→- a model which progressively introduces more complex\\ncombinations of attacks, vii) DeepForgeSeal - a1, τ - a model\\nwith single attack type with learnable strength parameter, viii)\\nDeepForgeSeal - [A] - a model with a predefined attack\\ncurriculum without adaptation to agent performance.\\nTABLE V\\nEFFECTS OF COMPOSING A COMBINATORIAL ATTACK AND LEARNING AN\\nATTACK CURRICULUM. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.\\nModel\\nPSNR (↑)\\nBRA - Benign (↑)\\nBRA - Malicious (↓)\\nDeepForgeSeal - w/o η\\n32.11\\n0.61\\n0.59\\nDeepForgeSeal - w/o A\\n41.58\\n0.72\\n0.55\\nDeepForgeSeal - |A|\\n46.20\\n0.82\\n0.37\\nDeepForgeSeal - ˜\\nA\\n47.15\\n0.87\\n0.31\\nDeepForgeSeal - a1, τ ↑\\n47.12\\n0.76\\n0.42\\nDeepForgeSeal - A→\\n48.09\\n0.92\\n0.22\\nDeepForgeSeal - a1, τ\\n48.05\\n0.79\\n0.38\\nDeepForgeSeal - [A]\\n47.11\\n0.89\\n0.28\\nDeepForgeSeal\\n48.12\\n0.99\\n0.12\\nFrom the results in Tab. V, we can confirm that there are\\nbenefits to incorporating both combinatorial attacks with a\\nlearnable attack curriculum as an adversary in the proposed\\nframework, which is evidenced by the rows in Tab. V corre-\\nsponding to models DeepForgeSeal - A→, and DeepForgeSeal\\n- a1, τ. Most importantly, when the attacker agent has the\\nknowledge of the current vulnerabilities of the victim model\\n(i.e. watermarking agent), it helps the attacker dynamically\\nadapt its strategy and generate a more complex curriculum\\nof attacks, in turn helping the victim to understand the vul-\\nnerabilities of its watermarking strategy and rectifying those.\\nWe believe this is the reason for the poor performance of\\nthe DeepForgeSeal - [A] model, in which a predefined attack\\ncurriculum was used without adaptation to the victim’s perfor-\\nmance. Furthermore, rows corresponding to DeepForgeSeal -\\n|A|, and DeepForgeSeal - ˜\\nA confirm the importance of using\\na combinatorial attack in which a learnable agent determines\\nthe attack sequence. Therefore, using this experiment, we\\ncan confirm the necessity of both combinatorial attacks and\\nlearning an attack curriculum to enhance the robustness of the\\nwatermark, while simultaneously preserving image fidelity.\\n3) Effects of Promoting Semantic Drift and Targeted Shifts\\nToward Known Failure Regions: To better establish the con-\\ntributions of the proposed reward function that promotes\\nsemantic drift and targeted shifts toward known failure regions,\\nwe conducted a further ablation experiment. Specifically, three\\nablation variants of the proposed model were generated: i)\\nDeepForgeSeal - w/o [Rproximity, Rcuriosity] - proposed model\\nwithout both curiosity reward and proximity reward, ii) Deep-\\nForgeSeal - w/o Rproximity - proposed model with curiosity\\nreward but without proximity reward, iii) DeepForgeSeal - w/o\\nRcuriosity - proposed model without curiosity reward but with\\nproximity reward.\\nTABLE VI\\nEFFECTS OF THE PROMOTING SEMANTIC DRIFT AND TARGETED SHIFTS\\nTOWARD KNOWN FAILURE REGIONS. THE BEST RESULTS ARE\\nHIGHLIGHTED IN BOLD.\\nModel\\nPSNR (↑)\\nBRA - Benign (↑)\\nBRA - Malicious (↓)\\nDeepForgeSeal - w/o [Rproximity, Rcuriosity]\\n48.10\\n0.82\\n0.35\\nDeepForgeSeal - w/o Rproximity\\n48.12\\n0.91\\n0.24\\nDeepForgeSeal - w/o Rcuriosity\\n48.08\\n0.94\\n0.21\\nDeepForgeSeal\\n48.12\\n0.99\\n0.12\\nThe results presented in Tab. VI confirm the need for the\\nproposed innovative reward function. In particular, the ablation\\nmodel without both the multi-head retrieval mechanism and\\nthe dynamic masking procedure (see the row corresponding to\\nDeepForgeSeal - w/o [Rproximity, Rcuriosity] in Tab. VI) struggles\\nto achieve a good watermark robustness. This is because the\\nattacker agent has learned a sub-optimal attacking policy,\\nmarking the watermarking agent unprepared for real-world\\nattacks against watermarks. Comparing rows corresponding\\nto DeepForgeSeal - w/o Rproximity and DeepForgeSeal - w/o\\nRcuriosity models in Tab. VI, we can confirm that proximity\\nand curiosity rewards complement each other. By leveraging\\nunderexplored regions of the latent space alongside known\\nfailure zones encountered during watermark retrieval, the\\nattacker agent can formulate a sophisticated attack policy. This,\\nin turn, drives the watermarker agent to refine and strengthen\\nits watermarking strategy. These observations strongly validate\\nthe importance of the proposed reward function that promotes\\nsemantic drift and targeted shifts toward known failure regions\\nE. Time Complexity\\nWe perform a detailed time complexity analysis using two\\nstate-of-the-art watermarking techniques: FaceSigns [19] and\\n\\n12\\nEditGard [10]. These methods were selected due to the public\\navailability of their implementations. Using a single NVIDIA\\nA100 GPU, we measured the average time required to generate\\n100 image-level predictions, including preprocessing, feature\\nextraction, watermark extraction, and deepfake classifications.\\nThe results of this analysis are summarized in Table VII.\\nModel\\nTotal Params (M)\\nRuntime (in Sec)\\nFaceSigns [19]\\n7.58\\n7.1205\\nEditGard [10]\\n9.32\\n8.6201\\nDeepForgeSeal (Ours)\\n8.27\\n7.8535\\nTABLE VII\\nTIME COMPLEXITY ANALYSIS: THE PARAMETER COUNT IN MILLIONS\\nAND THE TIME TAKEN TO GENERATE 100 IMAGE-LEVEL PREDICTIONS\\nUSING A SINGLE NVIDIA A100 GPU\\nV. CURRENT LIMITATIONS AND FUTURE DIRECTIONS\\nWe observe two limitations of DeepForgeSeal: (i) The pro-\\nposed system has been developed for watermarking image data\\nand has not yet been tested for multimodal data or other modal-\\nities such as audio data. Given the growing prevalence of mul-\\ntimodal deepfakes, it is crucial to assess the applicability of the\\nproposed learnable watermarking strategy, particularly its use\\nof high-dimensional semantic spaces, multi-agent adversarial\\nreinforcement learning, and innovative reward functions, for\\nother data types. Future research could also focus on extending\\nthis framework to support multimodal watermarking. One\\npromising direction would be to design a more sophisticated\\nmulti-agent architecture that facilitates collaboration among\\nwatermarking agents. Such a system could more effectively\\nnavigate and exploit the multimodal latent space, enhancing\\nsemantic stealth and robustness across diverse modalities. (ii)\\nWhile our evaluations (Tab. VII) indicate that the proposed\\nmodel exhibits computational complexity comparable to ex-\\nisting state-of-the-art watermarking systems, it has not been\\ntested on resource-constrained environments such as edge\\ndevices (e.g., smartphones). The reliance on high-dimensional\\nsemantic embeddings and multi-agent interactions increases\\nthe computational demands, potentially limiting scalability and\\nreal-time performance. To address these limitations, future\\nresearch could explore model compression techniques such as\\npruning, quantization, or knowledge distillation. These strate-\\ngies may help reduce inference time and memory footprint\\nwhile preserving detection accuracy, thereby improving the\\nfeasibility of deploying the DeepForgeSeal model in edge\\nscenarios.\\nVI. CONCLUSION\\nThis paper presented DeepForgeSeal, a Latent Space-Driven\\nSemi-Fragile Watermarking using Multi-Agent Adversarial\\nReinforcement Learning (MAARL) for accurate detection of\\nface deepfakes. We demonstrate that the proposed learnable\\nwatermarking agent achieves enhanced robustness and seman-\\ntic stealth through a directional embedding strategy in the\\nlatent space. The MAARL paradigm enabled the agent to dy-\\nnamically balance resilience and fragility by interacting with a\\ncurriculum of benign and adversarial manipulations introduced\\nby an attacker agent. To intensify this adversarial interaction,\\nwe designed reward functions that encouraged semantic drift\\nand targeted perturbations toward known failure regions. This\\nincentivised the attacker agent to develop sophisticated attack\\nstrategies, which in turn drove the watermarking agent to\\nrefine and strengthen its embedding approach, resulting in\\na more resilient and adaptive watermarking system. Exten-\\nsive experiments were conducted on two public benchmarks:\\nCelebA and CelebA-HQ, which demonstrated the ability of the\\nproposed framework to outperform the current state-of-the-art\\nalgorithms by significant margins.\\nACKNOWLEDGMENT\\nThe research was supported by the Australian Government\\nthrough the Office of National Intelligence Postdoctoral Grant\\nawarded to the primary author under Project NIPG-2024-022.\\nREFERENCES\\n[1] T.\\nHunter,\\n“Princess\\ncatherine\\ncancer\\nvideo\\nspawns\\nfresh\\nround\\nof\\nai\\nconspiracies,”\\n2024,\\naccessed\\non\\n31\\n09,\\n2025.\\n[Online]. Available: https://www.washingtonpost.com/technology/2024/\\n03/27/kate-middleton-video-cancer-ai/\\n[2] W.\\nGalston,\\n“Is\\nseeing\\nstill\\nbelieving?\\nthe\\ndeepfake\\nchallenge\\nto\\ntruth\\nin\\npolitics,”\\n2020,\\naccessed\\non\\n09\\n23,\\n2025.\\n[Online].\\nAvailable:\\nhttps://www.brookings.edu/articles/\\nis-seeing-still-believing-the-deepfake-challenge-to-truth-in-politics/\\n[3] N. S. Agenc, “Nsa, u.s. federal agencies advise on deepfake threats,”\\n2023, accessed on 08 23, 2025. [Online]. Available: https://www.\\nnsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/\\nArticle/3523329/nsa-us-federal-agencies-advise-on-deepfake-threats/\\n[4] T. Fernando, D. Priyasad, S. Sridharan, A. Ross, and C. Fookes, “Face\\ndeepfakes–a comprehensive review,” arXiv preprint arXiv:2502.09812,\\n2025.\\n[5] M. Mustak, J. Salminen, M. M¨antym¨aki, A. Rahman, and Y. K. Dwivedi,\\n“Deepfakes: Deceptions, mitigations, and opportunities,” Journal of\\nBusiness Research, vol. 154, p. 113368, 2023.\\n[6] Y. Zhao, B. Liu, M. Ding, B. Liu, T. Zhu, and X. Yu, “Proactive deepfake\\ndefence via identity watermarking,” in Proceedings of the IEEE/CVF\\nwinter conference on applications of computer vision, 2023, pp. 4602–\\n4611.\\n[7] T. Wang, H. Cheng, M.-H. Liu, and M. Kankanhalli, “Fractalforensics:\\nProactive deepfake detection and localization via fractal watermarks,”\\narXiv preprint arXiv:2504.09451, 2025.\\n[8] X. Wu, X. Liao, and B. Ou, “Sepmark: Deep separable watermarking for\\nunified source tracing and deepfake detection,” in Proceedings of the 31st\\nACM International Conference on Multimedia, 2023, pp. 1190–1201.\\n[9] X. Zhang, Z. Tang, Z. Xu, R. Li, Y. Xu, B. Chen, F. Gao, and J. Zhang,\\n“Omniguard: Hybrid manipulation localization via augmented versatile\\ndeep image watermarking,” in Proceedings of the Computer Vision and\\nPattern Recognition Conference, 2025, pp. 3008–3018.\\n[10] X. Zhang, R. Li, J. Yu, Y. Xu, W. Li, and J. Zhang, “Editguard: Versatile\\nimage watermarking for tamper localization and copyright protection,”\\nin Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, 2024, pp. 11 964–11 974.\\n[11] A. Haliassos, K. Vougioukas, S. Petridis, and M. Pantic, “Lips don’t\\nlie: A generalisable and robust approach to face forgery detection,”\\nin Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, 2021, pp. 5039–5049.\\n[12] S. Agarwal, H. Farid, Y. Gu, M. He, K. Nagano, and H. Li, “Protecting\\nworld leaders against deep fakes.” in CVPR workshops, vol. 1, no. 38,\\n2019.\\n[13] S. Fernandes, S. Raj, E. Ortiz, I. Vintila, M. Salter, G. Urosevic, and\\nS. Jha, “Predicting heart rate variations of deepfake videos using neural\\node,” in Proceedings of the IEEE/CVF international conference on\\ncomputer vision workshops, 2019, pp. 0–0.\\n[14] R. Wang, F. Juefei-Xu, M. Luo, Y. Liu, and L. Wang, “Faketagger: Ro-\\nbust safeguards against deepfake dissemination via provenance tracking,”\\nin Proceedings of the 29th ACM international conference on multimedia,\\n2021, pp. 3546–3555.\\n\\n13\\n[15] T. Wang, M. Huang, H. Cheng, B. Ma, and Y. Wang, “Robust identity\\nperceptual watermark against deepfake face swapping,” arXiv preprint\\narXiv:2311.01357, 2023.\\n[16] T. Wang, M. Huang, H. Cheng, X. Zhang, and Z. Shen, “Lampmark:\\nProactive deepfake detection via training-free landmark perceptual wa-\\ntermarks,” in Proceedings of the 32nd ACM International Conference\\non Multimedia, 2024, pp. 10 515–10 524.\\n[17] Y. Yang, C. Liang, H. He, X. Cao, and N. Z. Gong, “Faceguard:\\nProactive deepfake detection,” arXiv preprint arXiv:2109.05673, 2021.\\n[18] N. Beuve, W. Hamidouche, and O. D´eforges, “Waterlo: Protect images\\nfrom deepfakes using localized semi-fragile watermark,” in Proceedings\\nof the IEEE/CVF international conference on computer vision, 2023,\\npp. 393–402.\\n[19] P. Neekhara, S. Hussain, X. Zhang, K. Huang, J. McAuley, and\\nF. Koushanfar, “Facesigns: Semi-fragile watermarks for media authenti-\\ncation,” ACM Transactions on Multimedia Computing, Communications\\nand Applications, vol. 20, no. 11, pp. 1–21, 2024.\\n[20] A. Wachi, “Failure-scenario maker for rule-based agent using multi-\\nagent adversarial reinforcement learning and its application to au-\\ntonomous driving,” Twenty-Eighth International Joint Conference on\\nArtificial Intelligence, 2019.\\n[21] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson,\\n“Counterfactual multi-agent policy gradients,” in Proceedings of the\\nAAAI conference on artificial intelligence, vol. 32, no. 1, 2018.\\n[22] J. Wang, W. Xu, Y. Gu, W. Song, and T. C. Green, “Multi-agent\\nreinforcement learning for active voltage control on power distribution\\nnetworks,” Advances in neural information processing systems, vol. 34,\\npp. 3271–3284, 2021.\\n[23] Z. Peng, Q. Li, K. M. Hui, C. Liu, and B. Zhou, “Learning to simu-\\nlate self-driven particles system with coordinated policy optimization,”\\nAdvances in neural information processing systems, vol. 34, pp. 10 784–\\n10 797, 2021.\\n[24] M. Kouzehgar, M. Meghjani, and R. Bouffanais, “Multi-agent reinforce-\\nment learning for dynamic ocean monitoring by a swarm of buoys,” in\\nGlobal Oceans 2020: Singapore–US Gulf Coast.\\nIEEE, 2020, pp. 1–8.\\n[25] J. Wang, Z. Ren, B. Han, J. Ye, and C. Zhang, “Towards understanding\\ncooperative multi-agent q-learning with value factorization,” Advances\\nin Neural Information Processing Systems, vol. 34, pp. 29 142–29 155,\\n2021.\\n[26] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, and S. Russell, “Robust\\nmulti-agent reinforcement learning via minimax deep deterministic\\npolicy gradient,” in Proceedings of the AAAI conference on artificial\\nintelligence, vol. 33, no. 01, 2019, pp. 4213–4220.\\n[27] T. van der Heiden, C. Salge, E. Gavves, and H. van Hoof, “Robust multi-\\nagent reinforcement learning with social empowerment for coordination\\nand communication,” arXiv preprint arXiv:2012.08255, 2020.\\n[28] A. Bukharin, Y. Li, Y. Yu, Q. Zhang, Z. Chen, S. Zuo, C. Zhang,\\nS. Zhang, and T. Zhao, “Robust multi-agent reinforcement learning via\\nadversarial regularization: Theoretical foundation and stable algorithms,”\\nAdvances in neural information processing systems, vol. 36, pp. 68 121–\\n68 133, 2023.\\n[29] L. Yuan, F. Chen, Z. Zhang, and Y. Yu, “Communication-robust multi-\\nagent learning by adaptable auxiliary multi-agent adversary generation,”\\nFrontiers of Computer Science, vol. 18, no. 6, p. 186331, 2024.\\n[30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\\nG. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable\\nvisual models from natural language supervision,” in International\\nconference on machine learning.\\nPmLR, 2021, pp. 8748–8763.\\n[31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\\narXiv preprint arXiv:1412.6980, 2014.\\n[32] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture\\nfor generative adversarial networks,” in Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition, 2019, pp. 4401–\\n4410.\\n[33] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes\\nin the wild,” in Proceedings of the IEEE international conference on\\ncomputer vision, 2015, pp. 3730–3738.\\n[34] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing\\nof gans for improved quality, stability, and variation,” arXiv preprint\\narXiv:1710.10196, 2017.\\n[35] Y. He, N. Yu, M. Keuper, and M. Fritz, “Beyond the spectrum: Detecting\\ndeepfakes via re-synthesis,” arXiv preprint arXiv:2105.14376, 2021.\\n[36] S.-Y. Wang, O. Wang, R. Zhang, A. Owens, and A. A. Efros, “Cnn-\\ngenerated images are surprisingly easy to spot... for now,” in Proceedings\\nof the IEEE/CVF conference on computer vision and pattern recognition,\\n2020, pp. 8695–8704.\\n[37] N. Bonettini, E. D. Cannas, S. Mandelli, L. Bondi, P. Bestagini, and\\nS. Tubaro, “Video face manipulation detection through ensemble of\\ncnns,” in 2020 25th international conference on pattern recognition\\n(ICPR).\\nIEEE, 2021, pp. 5012–5019.\\n[38] L. Chai, D. Bau, S.-N. Lim, and P. Isola, “What makes fake images\\ndetectable? understanding properties that generalize,” in European con-\\nference on computer vision.\\nSpringer, 2020, pp. 103–120.\\n[39] C. Wang and W. Deng, “Representative forgery mining for fake face\\ndetection,” in Proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition, 2021, pp. 14 923–14 932.\\n[40] K. Shiohara and T. Yamasaki, “Detecting deepfakes with self-blended\\nimages,” in Proceedings of the IEEE/CVF conference on computer vision\\nand pattern recognition, 2022, pp. 18 720–18 729.\\n[41] T. Fernando, C. Fookes, S. Sridharan, and S. Denman, “Cross-branch\\northogonality for improved generalization in face deepfake detection,”\\narXiv preprint arXiv:2505.04888, 2025.\\n[42] X. Zhao, K. Zhang, Z. Su, S. Vasan, I. Grishchenko, C. Kruegel, G. Vi-\\ngna, Y.-X. Wang, and L. Li, “Invisible image watermarks are provably\\nremovable using generative ai,” in Advances in Neural Information\\nProcessing Systems, 2024.\\n[43] N. Lukas, A. Diaa, L. Fenaux, and F. Kerschbaum, “Leveraging op-\\ntimization for adaptive attacks on image watermarks,” The Twelfth\\nInternational Conference on Learning Representations (ICLR 2024),\\n2024.\\n[44] R. Chen, X. Chen, B. Ni, and Y. Ge, “Simswap: An efficient framework\\nfor high fidelity face swapping,” in MM ’20: The 28th ACM International\\nConference on Multimedia, 2020.\\n[45] C. Xu, J. Zhang, Y. Han, G. Tian, X. Zeng, Y. Tai, Y. Wang, C. Wang,\\nand Y. Liu, “Designing one unified framework for high-fidelity face\\nreenactment and swapping,” in European conference on computer vision.\\nSpringer, 2022, pp. 54–71.\\n[46] F. Rosberg, E. E. Aksoy, F. Alonso-Fernandez, and C. Englund,\\n“Facedancer: Pose-and occlusion-aware high fidelity face swapping,”\\nin Proceedings of the IEEE/CVF winter conference on applications of\\ncomputer vision, 2023, pp. 3454–3463.\\n[47] S. Bounareli, C. Tzelepis, V. Argyriou, I. Patras, and G. Tzimiropoulos,\\n“Hyperreenact: one-shot reenactment via jointly learning to refine and\\nretarget faces,” in Proceedings of the IEEE/CVF international conference\\non computer vision, 2023, pp. 7149–7159.\\nTharindu Fernando\\nreceived his BSc (special\\ndegree in computer science) from the University of\\nPeradeniya, Sri Lanka, and his PhD from Queens-\\nland University of Technology (QUT), Australia.\\nHe is currently a Postdoctoral Research Fellow in\\nthe Signal Processing, Artificial Intelligence, and\\nVision Technologies (SAIVT) research program at\\nthe School of Electrical Engineering and Robotics\\nat Queensland University of Technology (QUT). He\\nis a recipient of the 2019 QUT University Award for\\nOutstanding Doctoral Thesis, the QUT Early Career\\nResearcher Award in 2022, the QUT Faculty of Engineering Early Career\\nAchievement Award in 2024, and the 2024 National Intelligence Post-Doctoral\\nGrant. His research interests include Artificial Intelligence, Computer Vision,\\nDeep Learning, Bio Signal Processing, and Video Analytics.\\n\\n14\\nClinton Fookes\\n(Senior Member, IEEE) received\\nthe B.Eng. in Aerospace/Avionics, the MBA degree,\\nand the Ph.D. degree in computer vision. He is\\ncurrently the Associate Dean Research, a Professor\\nof Vision and Signal Processing, and Co-Director of\\nthe SAIVT Lab (Signal Processing, Artificial Intelli-\\ngence and Vision Technologies) with the Faculty of\\nEngineering at the Queensland University of Tech-\\nnology, Brisbane, Australia. His research interests\\ninclude computer vision, machine learning, signal\\nprocessing, and artificial intelligence. He serves on\\nthe editorial boards for IEEE TRANSACTIONS ON IMAGE PROCESSING\\nand Pattern Recognition. He has previously served on the Editorial Board\\nfor IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECU-\\nRITY. He is a Fellow of the International Association of Pattern Recognition, a\\nFellow of the Australian Academy of Technological Sciences and Engineering,\\nand a Fellow of the Asia-Pacific Artificial Intelligence Association. He is a\\nSenior Member of the IEEE and a multi-award winning researcher including\\nan Australian Institute of Policy and Science Young Tall Poppy, an Australian\\nMuseum Eureka Prize winner, Engineers Australia Engineering Excellence\\nAward, Australian Defence Scientist of the Year, and a Senior Fulbright\\nScholar.\\nSridha Sridharan has obtained an MSc (Commu-\\nnication Engineering) degree from the University\\nof Manchester, UK, and a PhD degree from the\\nUniversity of New South Wales, Australia. He is\\ncurrently with the Queensland University of Tech-\\nnology (QUT) where he is a Professor in the School\\nof Electrical Engineering and Robotics. He has pub-\\nlished over 600 papers consisting of publications in\\njournals and in refereed international conferences in\\nthe areas of Image and Speech technologies during\\nthe period 1990-2023. During this period he has also\\ngraduated 85 PhD students in the areas of Image and Speech technologies.\\nProf Sridharan has also received a number of research grants from various\\nfunding bodies including the Commonwealth competitive funding schemes\\nsuch as the Australian Research Council (ARC) and the National Security\\nScience and Technology (NSST) unit. Several of his research outcomes have\\nbeen commercialised.\\n',\n",
       " ' \\n1 \\n \\nAn End-to-End Deep Reinforcement Learning Approach for Solving \\nthe Traveling Salesman Problem with Drones \\n \\nTaihelong Zeng 1, Yun Lin 1 *, Yuhe Shi 2, Yan Li 1, 3, Zhiqing Wei 1, Xuanru Ji 4 \\n \\n1. School of Management Science and Real Estate, Chongqing University, Chongqing, 400045, \\nPR China \\n2. School of Management, Guizhou University, Guiyang, 550025, PR China \\n3. Chongqing Changan Minsheng APLL Logistics Co., Ltd., Chongqing, 401122, PR China \\n4. State Grid Chongqing Electric Power Company Material Branch, Chongqing, 401120, PR \\nChina \\n \\n \\n• \\nCorresponding author: Dr. Yun Lin, Email: linyun@cqu.edu.cn \\nHomepage: https://faculty.cqu.edu.cn/YunLin/zh_CN/index.htm \\nhttp://www.msre.cqu.edu.cn/szdw/jslb/ly2.htm \\n \\n \\n \\n \\n \\n\\n \\n2 \\n \\nAbstract \\nThe emergence of truck-drone collaborative systems in last-mile logistics has positioned the \\nTraveling Salesman Problem with Drones (TSP-D) as a pivotal extension of classical routing \\noptimization, where synchronized vehicle coordination promises substantial operational efficiency \\nand reduced environmental impact, yet introduces NP-hard combinatorial complexity beyond the \\nreach of conventional optimization paradigms. Deep reinforcement learning offers a theoretically \\ngrounded framework to address TSP-D\\'s inherent challenges through self-supervised policy \\nlearning and adaptive decision-making. This study proposes a hierarchical Actor-Critic deep \\nreinforcement learning framework for solving the TSP-D problem. The architecture consists of two \\nprimary components: a Transformer-inspired encoder and an efficient Minimal Gated Unit decoder. \\nThe encoder incorporates a novel, optimized k-nearest neighbors sparse attention mechanism \\nspecifically for focusing on relevant spatial relationships, further enhanced by the integration of \\nglobal node features. The Minimal Gated Unit decoder processes these encoded representations to \\nefficiently generate solution sequences. The entire framework operates within an asynchronous \\nadvantage actor-critic paradigm. Experimental results show that, on benchmark TSP-D instances of \\nvarious scales (N=10 to 100), the proposed model can obtain competitive or even superior solutions \\nin shorter average computation times compared to high-performance heuristic algorithms and \\nexisting reinforcement learning methods. Moreover, compared to advanced reinforcement learning \\nalgorithm benchmarks, the proposed framework significantly reduces the total training time \\nrequired while achieving superior final performance, highlighting its notable advantage in training \\nefficiency. \\n \\nKeywords \\nTraveling salesman problem \\nDrones \\nDeep Reinforcement learning \\nAttention mechanism \\nMinimal gated unit\\n\\n \\n3 \\n \\n1. Introduction \\nContemporary logistics systems, especially in last-mile delivery operations, increasingly confront \\nchallenges related to costs, operational efficiency, and environmental sustainability \\n(Pourmohammadreza et al., 2025). In this context, Unmanned Aerial Vehicles (UAVs) or drones \\nhave emerged as promising solutions due to their operational flexibility and ability to navigate \\naround ground-based obstacles (Li et al., 2022). Collaborative delivery models combining drones \\nwith traditional trucks have consequently emerged, among which the Traveling Salesman Problem \\nwith Drones (TSP-D) has garnered significant attention as a representative paradigm (Murray and \\nChu, 2015; Zhou et al., 2025). This paradigm leverages the large payload capacity and long range \\nof trucks combined with the efficient short-haul delivery capabilities of drones, aiming to \\nsignificantly reduce delivery times, lower operational costs, and improve delivery coverage and \\nresponsiveness in complex environments (such as urban or remote areas), demonstrating significant \\ntheoretical research value and practical application prospects (Lu et al., 2025). \\nThe TSP-D problem can be described as a hybrid delivery system where one or more trucks, \\ncarrying several drones, depart from a distribution center to serve a set of customer locations (Wang \\net al., 2023). During the delivery process, drones can be launched from truck stopping points to \\nindependently serve nearby customers before rendezvousing with the truck at predetermined \\nlocations (Murray and Chu, 2015). TSP-D is an extension of the classic Traveling Salesman Problem \\n(TSP) and Vehicle Routing Problem (VRP). However, unlike traditional VRP, TSP-D introduces \\ncollaborative decision-making between trucks and drones, task allocation, path planning, and \\nprecise time synchronization requirements, which significantly increases the problem’s complexity \\n(Duan et al., 2025). The solution space expands dramatically with the number of customers, drones, \\nand associated constraints, and the problem has been proven to be NP-hard, posing severe \\nchallenges to the efficiency and quality of solution algorithms (Yu et al., 2023). \\nFor combinatorial optimization problems like TSP-D, traditional solution approaches are \\nprimarily categorized into two classes: exact algorithms and heuristic algorithms. Exact algorithms \\nincluding branch-and-bound, dynamic programming, and integer linear programming guarantee \\nglobal optimal solutions but suffer from exponential computational complexity growth in large-\\nscale instances, making them impractical for real-world applications (Laporte, 1992). Heuristic \\nalgorithms like simulated annealing, genetic algorithms, and ant colony optimization achieve \\ncomputational efficiency at the expense of solution quality, gaining widespread adoption in practical \\nimplementations. However, these methods often require manual design for specific problem \\ninstances, exhibit poor adaptability, and risk converging to local optima (Karimi-Mamaghan et al., \\n2022). Recently, Deep Reinforcement Learning (DRL) has emerged as a cutting-edge AI technology \\n\\n \\n4 \\n \\ndemonstrating significant potential in solving combinatorial optimization problems through its \\npowerful representation learning and decision optimization capabilities (Bengio et al., 2021). \\nCompared with traditional methods, DRL approaches offer advantages including end-to-end \\nlearning, enhanced generalization capabilities, and elimination of manual heuristic design. Growing \\nevidence demonstrates their superior performance over conventional heuristics in solving large-\\nscale TSP problems (Liu et al., 2022; Zhao et al., 2021), establishing a new paradigm for path \\nplanning solutions. \\nWhile deep reinforcement learning has made remarkable progress in solving conventional TSP \\nand VRP problems, research specifically addressing TSP-D remains relatively scarce, with existing \\nstudies predominantly limited to single-agent perspectives. The challenges of TSP-D primarily \\nmanifest in three aspects: (1) Dual-agent coordination requiring simultaneous optimization of truck \\nand drone routes; (2) The speed difference between trucks and drones complicates the calculation \\nof travel time; (3) Spatiotemporal synchronization requiring precise coordination of truck-drone \\nrendezvous at specified locations and timestamps (Agatz et al., 2018). \\nTherefore, we propose an Actor-Critic-based deep reinforcement learning framework \\nspecifically designed for TSP-D resolution. The design of this solution involves several crucial \\ncomponents: \\n(1) We propose a Transformer-inspired encoder architecture. By integrating enhanced k-nearest \\nneighbors sparse attention mechanisms with dynamic graph masking and global node features, this \\nencoder effectively captures the crucial spatial structural patterns inherent in TSP-D problems while \\nsignificantly reducing computational complexity. \\n(2) We replace conventional Gated Recurrent Units (GRU) / Long Short-Term Memory (LSTM) \\nstructures with an efficient Minimal Gated Unit (MGU) to enhancing the encoder\\'s efficiency. This \\nsubstitution maintains the model’s representational capacity for sequential decision-making while \\nsubstantially reducing the parameter count and accelerating inference speed, contributing to a leaner \\noverall model. \\n(3) For effectively training process, we employ an asynchronous advantage actor-critic \\nframework. This learning process is further enhanced through prioritized experience replay and \\nadaptive learning rate scheduling, strategically improving sample efficiency and overall training \\nstability. \\n(4) Building upon the adaptive training mechanisms, we introduce a reward plateau-aware \\nadaptive learning rate strategy. This specifically targets and resolves convergence challenges often \\nencountered in large-scale TSP-D instances. \\nThe remainder of this paper is organized as follows. Section 2 briefly reviews current research \\n\\n \\n5 \\n \\non TSP-D path optimization, introducing both traditional methods and deep reinforcement learning \\napproaches. Section 3 formally defines the TSP-D problem and formulates it through Markov \\nDecision Processes. Section 4 presents the proposed Actor-Critic framework and training strategies. \\nSection V details computational experiments and analytical results. Finally, Section 5 concludes the \\npaper and outlines future research directions. \\n \\n \\n\\n \\n6 \\n \\n2 Literature Review \\n2.1. Traveling Salesman Problem with Drones  \\nSince (Murray and Chu, 2015) first proposed the vehicle-drone collaborative delivery problem, \\nthe TSP-D has gained significant attention in recent years as an optimization method for delivery \\nsystems. Current research identifies two primary collaborative delivery models integrating vehicles \\nand drones. The first model employs independent delivery operations, where vehicles and drones \\ndepart from distribution centers with cargo, complete their respective delivery tasks, and \\nsubsequently return to the centers. (Ham, 2018) addressed this model by proposing a multi-vehicle, \\nmulti-drone, multi-depot pickup-delivery problem, where drones can collect goods at subsequent \\nnodes after deliveries before returning to depots, optimizing independent vehicle-drone routes for \\nglobal efficiency. (Zhang et al., 2023) established a mixed-integer programming model for electric \\nvehicle-drone collaborative delivery systems with time windows and endurance constraints, and \\ndeveloped an extended adaptive large neighborhood search algorithm for efficient solution \\ngeneration. (Nguyen et al., 2022) restricted drone usage by defining maximum operation time \\nconstraints, extending the problem with parallel drone scheduling. The second category features \\ncollaborative delivery operations, where drones are carried by vehicles departing from depots, \\ndeployed at specific nodes, and later rendezvous with vehicles after completing deliveries. (Wang \\net al., 2017) first established mixed-integer linear programming formulations for dual-drone \\ndelivery problems. (Poikonen et al., 2017) extended (Wang et al., 2017) \\'s work by analyzing worst-\\ncase scenarios under battery constraints, incorporating crow-flight distance metrics for drones \\nversus truck routing, and considering transportation costs beyond temporal optimization. (Schermer \\net al., 2018) comparatively evaluated two novel heuristic approaches: Two-Phase Heuristic (TPH) \\nand Single-Phase Heuristic (SPH). Their work first produced numerical results for large-scale \\ninstances, demonstrating TPH\\'s superior performance over SPH in most scenarios. (Sacramento et \\nal., 2019) developed an adaptive large neighborhood search metaheuristic incorporating time-\\nwindow constraints. (Kitjacharoenchai et al., 2019) addressed operational constraints related to \\ndrone launch windows and delivery time coordination. (Mahmoudinazlou and Kwon, 2024) \\nhybridized genetic algorithms with dynamic programming to create the escape strategy-based HGA-\\nTAC+ algorithm for TSP-D resolution, (Wang et al., 2022) formulated a time-dependent truck-drone \\nhybrid routing problem accounting for traffic conditions, and proposed an iterated local search \\nalgorithm. A key limitation of the existing drone-assisted truck delivery solutions lies in their \\nreliance on the simplified assumption of single-customer service per drone sortie. To address this \\nconstraint and enhance practicality, (Gu et al., 2022) developed a novel approach enabling multi-\\ncustomer service per drone mission. (Nguyen et al., 2022) incorporated constraints on total truck \\n\\n \\n7 \\n \\npayload capacity and combined operational durations for both trucks and drones. (Montaña et al., \\n2021) quantified carbon emissions from last-mile drone deliveries, emphasizing the sustainability \\nimplications. (Rave et al., 2023) proposed a novel micro-depot concept. In the problem they defined, \\ndrones can be launched from trucks or from microdepots on trucks. (Sitek et al., 2022) defined this \\nproblem as the Extended Vehicle Routing Problem with Drones, which differs from the traditional \\nVehicle Routing Problem with Drones in that they consider what are called mobile hubs (similar to \\nmicrodepots) (Rave et al., 2023) as mobile points from which drones can be launched. At the same \\ntime, a genetic algorithm was developed to minimize operational costs and select mobile hubs. (Wu \\net al., 2022) investigated the effects of payload and flight duration on drone energy consumption, \\ncreating a variable neighborhood descent-based heuristic algorithm. \\n2.2. Deep reinforcement learning solves path planning \\nWith the rapid advancement of artificial intelligence, Deep Reinforcement Learning, \\nintegrating deep learning\\'s representational power with reinforcement learning\\'s decision-making \\ncapabilities, has demonstrated significant potential in solving combinatorial optimization problems, \\nparticularly in path planning. Ranging from the initial exploration of (Hopfield and Tank, 1985) \\nneural networks to the widespread application of deep learning in recent years, DRL-based path \\nplanning research has achieved remarkable progress. Early research primarily focused on employing \\nneural networks to directly solve small-scale TSP and VRP. The Hopfield neural network represents \\none of the earliest attempts, utilizing energy function concepts to derive approximate solutions. The \\nsubsequent development of Pointer Networks (PN) marked a new phase in applying deep learning \\ntechniques to path planning challenges. (Sutskever et al., 2014) enhanced the Seq2Seq framework \\nto develop the PN, which employs LSTM-based encoding and attention mechanism decoding to \\neffectively extract node coordinate features. This work laid the foundation for applying deep \\nreinforcement learning to path planning challenges, However, this supervised training paradigm \\nrequired extensive TSP datasets with optimal tours, limiting its practicality. This methodology \\nproved challenging to implement, as the supervised training process inherently constrained solution \\nquality to the performance level of the training data. To address these limitations, (Bello et al., 2016) \\nimplemented an Actor-Critic DRL training algorithm to optimize PN without requiring pre-\\ncomputed optimal tours. This demonstrates the feasibility of DRL in learning high-quality strategies \\nwithout supervised labels, and establishes an important foundational framework for numerous \\nsubsequent DRL-based path planning studies, including our current work. (Nazari et al., 2018) \\nobserved that the RNN structure in the original PN might introduce unnecessary complexity. They \\nsimplified the architecture, directly used embedded inputs, and incorporated dynamic element inputs, \\nproposing the NM model. For multi-objective planning problems, (Li et al., 2021) adopted an \\n\\n \\n8 \\n \\nimproved PN similar to (Nazari et al., 2018)  for modeling. Using an RNN model based on GRU \\n(Cho et al., 2014) , this model has similar performance to the LSTM used in the original PN in \\n(Nazari et al., 2018), but with fewer parameters. (Kong and Jiang, 2024) designed a composite PN \\nframework integrating graph neural network representations and attention mechanisms to address \\nDrone-Assisted truck transportation. (Shi, 2022) integrated PN with heuristic algorithms like \\nVariable Neighborhood Search to enhance initial solution quality in Traveling Salesman Problem \\napplications. Experimental results demonstrated improved convergence rates and optimization \\noutcomes. The proposed method exhibited superior generalization capability and faster training \\nspeeds compared to PN networks on TSP and Capacitated Vehicle Routing Problem (CVRP) \\nbenchmarks. Despite the relative success of PN and their variants, they still show limitations in \\ncapturing complex dependencies between nodes, especially in large-scale problems, which has \\nprompted researchers to explore more powerful architectures. \\nFollowing Google\\'s introduction of the Transformer architecture combining attention \\nmechanisms and multilayer perceptron (Vaswani et al., 2017), attention-based approaches have \\nproliferated in this problem domain. (Kool et al., 2018) first adapted Transformer architectures for \\nroute optimization, developing an Attention model (AM) framework that significantly improved \\nboth training efficiency and solution optimality for various path planning tasks, The AM model \\nbecame an important benchmark for subsequent research, but it completely removed positional \\nencoding in the encoder, which may result in the loss of important spatial information. Additionally, \\nthe self-attention mechanism of standard Transformers faces a bottleneck of quadratic \\ncomputational complexity when processing large-scale graphs. Building on (Kool et al., 2018) \\'s \\nexperimental framework, (Joshi et al., 2019) combined supervised learning and reinforcement \\nlearning (RL) to train models on 100-node TSP instances, achieving improved prediction accuracy. \\n(Peng et al., 2019) enhanced the AM framework with dynamic decoding mechanisms, achieving \\nsuperior solution accuracy and generalization capabilities for CVRP problems, (Zhang et al., 2020) \\ndeveloped a Multi-Agent Attention Model (MAAM) encoder-decoder framework using \\nTransformer architectures, featuring attention layers that iteratively generate multi-vehicle routes. \\n(Xu et al., 2022) advanced AM research by incorporating dynamic network architectures through \\nattention aggregation modules across multiple relational structures, enabling context-aware \\nembeddings that enhance model performance and generalization. These works demonstrate the \\npowerful potential of the Transformer architecture in path planning, but also highlight the need to \\nfurther enhance its representational capacity and efficiency, especially in scenarios requiring the \\ncapture of complex graph structures and long-distance dependencies. \\nResearchers have also explored combining Transformer with other technologies or modifying \\n\\n \\n9 \\n \\nit. (Deudon et al., 2018) replaced the recurrent encoder in the PN framework with Transformer-\\nbased attention encoding for TSP heuristic learning. Integration with 2-opt heuristics further \\nenhanced solution quality. (Ma et al., 2019) proposed a Dual-Aspect Collaborative Transformer \\n(DACT) with cyclic positional encoding (CPE) to separately learn node and positional embeddings, \\napplying it to TSP and CVRP. (Gebreyesus et al., 2023) implemented a Transformer-based encoder-\\ndecoder architecture with Edge-Enhanced Multi-Head Attention (EEMHA) layers in the encoder. \\nIn terms of decoders, those based on recurrent networks (especially LSTM) have been widely \\napplied due to their ability to maintain state information during the sequential decision-making \\nprocess (Nazari et al., 2018; Peng et al., 2019). For the TSP-D problem, the relative positions and \\ncoordination between vehicles in a heterogeneous fleet are crucial, and the hidden state memory \\ncapability of LSTM may be more advantageous than stateless attention decoders. As (Bogyrbayeva \\net al., 2023) innovatively integrated AM and NM, adopted an attention-based encoder and LSTM \\ndecoder in their model for TSP-D. However, LSTM has a relatively large number of parameters. \\nThis inspires us to consider whether there exists a more lightweight recurrent unit that can both \\ncapture the temporal dynamics needed for TSP-D coordination and maintain high efficiency. \\nThrough a review and analysis of existing literature, we found that: \\n(1) Actor-Critic framework has proven to be an effective DRL paradigm for solving path \\nplanning problems. \\n(2) Attention models show greater potential in encoding graph structure information compared \\nto earlier PN and its variants, but standard attention mechanisms have limitations in position \\ninformation encoding and computational efficiency for large-scale graphs. \\n(3) The choice of decoder is important for sequential decision tasks. For heterogeneous agent \\nsystems requiring coordination (such as TSP-D), the state memory capability of recurrent networks \\nmay be superior to stateless attention, though efficiency issues need to be considered. \\n(4) Despite the progress of DRL in problems such as TSP and VRP, research on heterogeneous \\nagent systems, especially the drone-assisted TSP-D problem, remains relatively scarce, with \\nexisting methods mostly limited to single-agent scenarios. \\nThe trade-off rule between solution quality and runtime in existing algorithms for TSP-D \\nsolutions is unavoidable. To address this issue as much as possible, based on the opportunities and \\nshortcomings identified in the above literature analysis, we designed an end-to-end deep \\nreinforcement learning architecture. \\n \\n \\n\\n \\n10 \\n \\n3 Problem description \\n3.1. Problem Definition \\nThis study focuses on the TSP-D, where a truck and a drone collaboratively execute delivery \\nservices for multiple customer nodes. The problem scenario is defined within a network containing \\nn nodes, where \\n nodes represent customers and the remaining node serves as the depot. Each \\ncustomer node possesses unit demand, while the depot requires no service. Both vehicles depart \\nfrom the depot, with the objective of minimizing total service time through optimized routing \\ncoordination while satisfying all customer demands. \\n \\nFig. 1. TSP-D problem diagram \\nAs shown in Fig. 1, the truck executes main route delivery tasks along the path \\n1→3→4→6→7→1 in this distribution scheme. This route forms the backbone of the distribution \\nnetwork, providing multiple launch/recovery points for the drone. The drone services customers \\ndeviating from the main route through two distinct flight missions, requiring return to the truck after \\neach delivery. With the truck\\'s dynamic repositioning, the drone may:  \\n(1) rendezvous at the truck\\'s next delivery node (1→2→3). \\n(2) meet at a subsequent multi-node service location (4→5→7). \\n(3) remain idle during certain segments (3→4). \\n3.2. MDP-based Formalization  \\nWe formulate the TSP-D as a Markov Decision Process (MDP). The MDP provides a \\nmathematical framework for sequential decision-making, where the agent (coordinated truck-drone \\nsystem) learns optimal policies through environmental interactions. \\n(1) State Space \\nThe state space \\n encapsulates complete information about truck/drone statuses and \\ncustomer demands at each decision epoch. A state \\n can be represented as a tuple:  \\n1\\nn -\\n  \\nS\\ns\\n\\n \\n11 \\n \\n \\n(1) \\nWe define the system state \\n as follows: the environment contains \\n nodes comprising,\\n customers and one depot. Spatial positions are defined through Cartesian coordinates \\nfor each node 𝑖. The coordinate set is denoted as \\n,demand vector\\ntracks customer statuses: \\n indicates unmet demand for customer \\n, \\ndenotes fulfilled \\ndemand. The truck\\'s position \\n and drone\\'s location \\n are continuously tracked.\\n indicates drone operation status: \\n (engaged in delivery), \\n (available).\\n records drone return status: \\n(not returned), \\n(returned). Truck and drone may \\nhave pending actions. Let \\n represent the truck\\'s pending action, where \\n is the \\ntarget node and \\n the remaining time to reach \\n. If no pending action exists, \\n. \\nSimilarly, define \\n for drone operations, where \\n is the drone\\'s target node and,\\n the remaining flight time. If the drone has no pending action, \\n. \\nThe state space is constrained by: In the initial state, all customer demands are unserved \\n, \\n; The truck and drone initially start at the depot node \\n; \\nThe drone can only launch when docked with/returned to the truck (\\n ⟹ \\n); When \\nservicing customers, the drone cannot be at the depot (\\n ⟹ \\n). \\n(2) Action Space \\nThe action space \\n encompasses all feasible actions available to both truck and drone at \\neach decision epoch. Given their collaborative operation, joint actions are represented as \\n, where \\n denotes the truck\\'s next target node and \\n specifies the drone\\'s \\nsubsequent destination. \\nThe action space is constrained by: Trucks and drones cannot visit customer nodes with \\nsatisfied demands \\n. When the drone is servicing a customer \\n, it must \\nreturn to the truck\\'s current node \\n. If the drone has returned (\\n), it cannot relaunch until \\nthe truck visits another customer. Vehicles with pending actions (\\n≠0 or \\n≠0) must continue \\nexecuting them until completion. \\n(3) Transition Function \\nThe state transition function \\n specifies the probability of transitioning to state \\n given current state \\n and action \\n. In our problem formulation, state transitions are \\ndeterministic: given the current state and action, the subsequent state is uniquely determined. The \\nstate transition function can be expressed as \\n, where \\n represents a deterministic \\nmapping. \\nThe state transition process comprises: First, calculate the time step \\n required to reach the \\nnext state based on vehicle actions and distances to target nodes. Specifically, truck arrival time \\n and drone arrival time \\n, where \\n is the distance between nodes \\n  \\n(\\n)\\n{ }\\n, , , ,\\n,\\n,\\n,\\ni N\\nt\\nd\\nd\\nd\\ns\\nl l s r\\nÎ\\n=\\ni\\nt\\nd\\np\\nc\\na a\\n \\ns ÎS\\n n\\n \\n1\\nn -\\n( ,\\n)\\ni\\ni\\nx y\\n{\\n}i N\\nÎ\\nip\\n(\\n)\\n1\\n1\\n2\\n1\\n,\\n,\\n,\\n{0,1}n\\nn\\nc c\\nc\\n-\\n-\\n=\\n…\\nÎ\\nc\\n1\\nic =\\ni\\n0\\nic =\\ntl\\nN\\nÎ\\ndl\\nN\\nÎ\\n{\\n}\\n0,1\\nds Î\\n1\\nds =\\n0\\nds =\\n{\\n}\\n0,1\\ndr Î\\n1\\ndr =\\n0\\ndr =\\n(\\n)\\n,\\nt\\nj t\\n=\\nta\\n j\\nN\\nÎ\\nt\\nR\\nt\\n+\\nÎ\\nj\\n(\\n)\\n,0\\ntl\\n=\\nta\\n(\\n)\\n,\\nd\\nk t\\n=\\nda\\n \\nk\\nN\\nÎ\\nd\\nR\\nt\\n+\\nÎ\\n(\\n)\\n,0\\ndl\\n=\\nda\\n1, \\nic\\ni\\nC\\n=\\n\" Î\\n{\\n}\\n  1,2,\\n,\\n1\\nC\\nn\\n=\\n…\\n-\\nt\\nd\\nl\\nl\\nn\\n=\\n=\\n0\\nds =\\nd\\nt\\nl\\nl\\n=\\n1\\nds =\\ndl\\nn\\n¹\\nA\\n(\\n)\\n,\\nt\\nd\\na a\\n=\\na\\nta\\nN\\nÎ\\nda\\nN\\nÎ\\n,\\n{\\n|\\n0}\\nt\\nd\\ni\\na a\\ni\\nC c\\nÏ\\nÎ\\n=\\n1\\nds =\\nd\\nt\\na\\nl\\n=\\n0\\ndr =\\nta\\nd\\na\\n:\\n´\\n®\\nP\\nS\\nA\\nS\\ns\\ns¢\\na\\n(\\n)\\n,\\ns\\nf s a\\n¢ =\\nf\\nΔt\\n,\\nt\\nt\\nt\\nl a\\nt\\nt\\nd\\nv\\n=\\n,\\nd\\nd\\nd\\nl\\na\\nd\\nt\\nd\\nv\\n=\\n,i j\\nd\\ni\\n\\n \\n12 \\n \\nand \\n,\\n and \\n denote respective speeds. Incorporating pending action residuals, \\n=\\n(excluding terms where \\n or \\n = 0). Update vehicle positions using \\n and selected actions: if \\n, truck arrives: \\n; if \\n, drone arrives:\\n.  \\nElse positions remain unchanged. Update customer states: if \\n or \\n, set \\n or \\n. Update drone status: if \\n, set service state \\n. If drone returns to truck \\n, \\nset \\n and \\n. If \\n transitions from \\n, set \\n, Finally, update pending actions: \\n for truck, \\nfor drone. \\n(4) Reward Function \\nThe reward function \\n defines the immediate reward for executing action \\n in \\nstate\\n. To minimize total service duration, we formulate the reward as negative temporal \\nprogression: \\n \\n(2) \\nWhere \\n represents the time step required to transition from \\n to \\n through action \\n. \\nThis design incentivizes the agent to prioritize time-efficient action sequences. \\n \\n \\nj\\ntv\\nd\\nv\\nΔt\\n(\\n)\\nmin\\n,\\n, ,\\nt\\nt\\nd\\nd\\nt\\nd\\nt\\nt\\nt\\nt t t\\n-\\n-\\ntt\\nd\\nt\\nΔt\\nΔ\\nt\\nt\\nt\\nt\\nt\\n=\\n-\\n\\'\\nt\\nt\\nl\\na\\n=\\nΔ\\nd\\nd\\nt\\nt\\nt\\n=\\n-\\n\\'\\nd\\nd\\nl\\na\\n=\\n\\'\\ntl\\nC\\nÎ\\n\\'\\ndl\\nC\\nÎ\\n\\'\\n\\'\\n0\\ntlc =\\n\\'\\n\\'\\n0\\ndlc\\n=\\n\\'\\ndl\\nC\\nÎ\\n\\'\\n1\\nds =\\n(\\n)\\n\\'\\n\\'\\nd\\nt\\nl\\nl\\n=\\n\\'\\n0\\nds =\\n\\'\\n0\\ndr =\\nds\\n0\\n1\\n®\\n1\\ndr =\\n(\\n)\\n(\\n)\\n\\'\\n,max 0,\\nΔ\\nt\\nt\\na\\na\\nt\\nt\\n=\\n-\\nt\\n(\\n)\\n(\\n)\\n\\'\\n,max 0,\\nΔ\\nd\\nd\\na\\na\\nt\\nt\\n=\\n-\\nd\\n:\\nR\\n´\\n®\\nR\\nS\\nA\\na\\ns\\n(\\n)\\n,\\nΔ\\nr s\\nt\\n= -\\na\\nΔt\\ns\\ns¢\\na\\n\\n \\n13 \\n \\n4 Solution Model Architecture \\nTo address TSP-D, we propose an end-to-end deep reinforcement learning architecture. \\nBuilding upon the demonstrated efficacy of Actor-Critic frameworks in solving complex routing \\nproblems (Bello et al., 2016; Kool et al., 2018; Nazari et al., 2018), we adopt this paradigm as the \\nfoundation of our approach. At the core of our contribution, we develop an Adaptive Expansion \\nGraph Attention for graph representation learning, Our encoder substantially enhances attention \\nmechanisms, employing relative and sparse positional encoding instead of removing them as in AM \\n(Kool et al., 2018) and hybrid model (HM) that uses multi-head attention for encoding \\n(Bogyrbayeva et al., 2023), To overcome computational bottlenecks in traditional Transformers for \\nlarge-scale graphs and capture long-range dependencies, we adapt Google\\'s Exphormer sparse \\nattention framework (Shirzad et al., 2023), introducing an innovative adaptive graph expansion \\nmechanism. This enables dynamic construction of sparse interaction graphs. Additionally, our \\nencoder incorporates global node mechanisms to enhance contextual awareness. LSTM-based \\ndecoders have proven effective for routing problems (Nazari et al., 2018; Peng et al., 2019), For \\ndrone-assisted planning where relative vehicle positions in heterogeneous fleets are crucial, LSTM\\'s \\nhidden state memory outperforms stateless attention decoders (Bogyrbayeva et al., 2023). Diverging \\nfrom HM\\'s LSTM, we investigate MGU decoders as lightweight alternatives, our findings show \\nthat MGU achieves comparable generalization to LSTM through simplified gating, sufficiently \\ncapturing temporal dynamics essential for TSP-D coordination. As shown in Fig. 2, our model \\narchitecture diagram is clearly presented. \\n \\nFig. 2. The encoder-decoder framework in our model \\n\\n \\n14 \\n \\n4.1. Encoder \\nGiven a graph composed of node set \\n where each node \\n possesses input features\\n. To embed this graph structure, we initialize node embeddings \\nthrough linear \\ntransformation: \\n \\n(3) \\nWhere \\n and \\n are learnable parameters, with \\n denoting hidden \\nlayer dimension. These initial node embeddings \\n are processed through \\n stacked \\nExpander Graph Attention (EGA) encoder layers. \\nEach EGA encoder layer dynamically expands the graph structure \\n, where \\ndenotes edge connections. To enhance graph connectivity and incorporate global context, we \\nemploy: \\n(1) k-Nearest Neighbor Connection: Selects the top 𝑘= ⌈log! |𝑁|⌉ neighbors based on latent \\nspace distance: \\n \\n(4) \\n(2) Hierarchical Connection: Divides nodes into \\n layers and establishes connections \\nwithin each layer. \\n \\n(5) \\nWhere stride \\n. \\n(3) Depot connections: guarantee direct links between all nodes and a predefined depot node \\nto enable rapid information aggregation and distribution. \\nRecognizing that expanded graphs may compromise long-range dependency modeling \\ncompared to fully-connected graphs, we augment the architecture with a special global node \\n, \\nextending the adjacency matrix to \\n, this composite matrix integrates original \\nedges, expanded connections, and global node linkages. The relevant schematic diagram is shown \\nin Fig. 3. \\nTo precisely regulate node interactions in attention mechanisms while maintaining structural \\nalignment with\\n, we implement attention mask \\n, each mask element\\ncorresponds respectively with \\nentries. During attention score calculation. \\n is additively \\ncombined with raw attention scores. This configuration assigns \\n attention scores to \\nunconnected node pairs, which become zero after Softmax normalization.  \\nN\\n n\\nN\\nÎ\\nin\\nD\\nnx\\nR\\nÎ\\n( )\\n0\\nnh\\n( )\\n0\\n   \\nn\\ninput\\nn\\ninput\\nh\\nW\\nx\\nb\\nn\\nN\\n=\\n+\\n\" Î\\nin\\nh\\nD\\nD\\ninput\\nW\\nR\\n´\\nÎ\\nh\\nD\\ninput\\nb\\nR\\nÎ\\nh\\nD\\n( )\\n{\\n}\\n0 :\\nnh\\nn\\nN\\nÎ\\nL\\n(\\n)\\n,\\nN\\n=\\nG\\nE\\nE\\n(\\n)\\n(\\n)\\n1\\n1\\n2\\nl\\nl\\nij\\ni\\nj\\nd\\nh\\nh\\n-\\n-\\n=\\n-\\n!\\n!\\n  \\n8\\nL\\nN\\n=\\n{\\n}\\nlayer\\n1\\n( ,\\n) |\\n,|\\n| 2\\nL\\ni\\nj\\nl\\nE\\nv v\\ni s\\nj s\\ni\\nj\\n=\\n=\\n=\\n-\\n£\\nê\\nú\\nê\\nú\\në\\nû\\në\\nû\\n!\\n  \\ns\\nN L\\n=\\nglobal\\nn\\n(\\n) (\\n)\\n1\\n1\\n{0,1}\\nN\\nN\\nA\\n+ ´\\n+\\nÎ\\n!\\nA!\\n(\\n) (\\n)\\n1\\n1\\n{0,\\n}\\nN\\nN\\nM\\n¥\\n+ ´\\n+\\nÎ\\n-\\n,i j\\nM\\nA!\\nM\\n¥\\n-\\n\\n \\n15 \\n \\n \\nFig. 3. Schematic Diagram of expander graph Masking \\nFor node representations \\n  input to this sub-layer, we first compute query vectors \\n, key vectors \\n, and value vectors \\n: \\n \\n(6) \\n \\n(7) \\n \\n(8) \\n are learnable weight matrices with dimensions \\n, and\\nrespectively. To compute attention scores \\n between nodes \\n and \\n, we employ \\nmasked dot-product attention combined with relative positional encoding: \\n \\n(9) \\nwhere \\n represents the relative positional encoding vector between nodes \\n and \\n. In our \\nimplementation, relative positional encodings are retrieved from a sparse positional embedding \\nmatrix \\n, where \\n denotes the number of attention heads and \\n the \\ndimensionality of sparse positional embeddings. Attention weights \\n are obtained by Softmax \\nnormalization of attention scores: \\n \\n(10) \\nThe updated representation \\n  for node \\n is computed as the weighted sum of all nodes\\' \\n(\\n)1\\nl\\nnh\\n-\\nk\\nD\\nnq\\nR\\nÎ\\nk\\nD\\nnk\\nR\\nÎ\\nv\\nD\\nnv\\nR\\nÎ\\n(\\n)1\\nl\\nn\\nQ\\nn\\nq\\nW h\\n-\\n=\\n(\\n)1\\nl\\nn\\nK\\nn\\nk\\nW h\\n-\\n=\\n(\\n)1 , \\nl\\nn\\nV\\nn\\nv\\nW h\\nn\\nN\\n-\\n=\\n\" Î\\n,\\n,\\nQ\\nK\\nV\\nW\\nW\\nW\\n(\\n) (\\n)\\n,\\nh\\nk\\nh\\nk\\nD\\nD\\nD\\nD\\n´\\n´\\n(\\n)\\nh\\nv\\nD\\nD\\n´\\n,i j\\ns\\ni\\nj\\n,\\nT\\nT\\ni j\\ni\\nj\\nk\\ni\\nij\\ns\\nq k\\nD\\nq r\\n=\\n+\\nijr\\ni\\nj\\nsparse\\nk\\nH D\\nD\\nR\\nR\\n´\\n´\\nÎ\\nH\\nsparse\\nD\\n,i j\\na\\n(\\n)\\n(\\n)\\n,\\n,\\n,\\nexp\\nexp\\ni j\\ni j\\ni j\\nj\\na\\ns\\ns\\n¢\\n¢\\n=\\nå\\nn\\nh¢\\nn\\n\\n \\n16 \\n \\nvalue vectors: \\n \\n(11) \\nTo enhance the model\\'s expressive capability, we employ a multi-head attention mechanism \\nutilizing \\n independent attention heads. The messages computed from each head are \\nconcatenated and linearly transformed through an output projection matrix \\n to \\nproduce the final EGA sublayer output: \\n \\n(12) \\nTo stabilize the training procedure, we apply Instance Normalization after the EGA sublayer \\nand incorporate residual connections: \\n \\n(13) \\nThe normalized output \\n is subsequently fed into the Feed-Forward Network (FFN) \\nsublayer. The FFN sublayer consists of two linear layers with a ReLU activation function. \\n \\n(14) \\nMirroring the attention sublayer, we apply Instance Normalization and residual connections \\nafter the FFN sublayer: \\n \\n(15) \\nAt the inception of each EGA layer, we compute mean-pooled features \\n from original \\nnode inputs (excluding the global node): \\n \\n(16) \\nThis averaged feature is then added to the global node\\'s representation \\n  to enable layer-\\nwise global information updating: \\n \\n(17) \\nAfter processing through \\n EGA layers, we obtain final node representations\\n. \\nTo integrate global information across EGA layers, we compute multi-scale mean-pooled features \\n: \\n \\n(18) \\nFinally, we add \\n to the terminal layer\\'s node features \\n to accomplish ultimate \\ninformation fusion: \\n \\n(19) \\n,\\nn\\nn j\\nj\\nj\\nh\\na\\nv\\n¢ = å\\nH\\n(\\n)\\nv\\nh\\nH D\\nD\\nO\\nW\\nR\\n×\\n´\\nÎ\\n(\\n)\\n(\\n)\\n(\\n)\\n( )\\n(\\n)\\n(\\n)\\n1\\n1\\n1\\n1\\n1\\nEGA\\n,\\n,\\nConcat\\n,\\n,\\nl\\nl\\nH\\nN\\nO\\nn\\nn\\nh\\nh\\nW\\nh\\nh\\n-\\n-\\n+\\nû\\n¢\\né\\nù\\n…\\n=\\n…\\në\\n¢\\n( )\\n(\\n)\\n(\\n)\\n(\\n)\\n(\\n)\\n(\\n)\\n1\\n1\\n1\\n1\\n1\\nˆ\\nInstanceNorm\\nEGA\\n,\\n,\\nl\\nl\\nl\\nl\\nn\\nn\\nN\\nh\\nh\\nh\\nh\\n-\\n-\\n-\\n+\\n=\\n+\\n…\\n( )\\nˆ l\\nnh\\n( )\\n( )\\n(\\n)\\n( )\\n(\\n)\\n,2\\n,1\\n,1\\n,2\\nF\\nˆ\\nF\\nReL\\nˆ\\nU\\nl\\nl\\nl\\nn\\nff\\nff\\nn\\nff\\nff\\nh\\nW\\nW\\nh\\nb\\nb\\n=\\n×\\n+\\n+\\n( )\\n( )\\n( )\\n( )\\n(\\n)\\n(\\n)\\nInstanceNorm ˆ\\nˆ\\nFF\\nl\\nl\\nl\\nl\\nn\\nn\\nn\\nh\\nh\\nh\\n=\\n+\\n( )l\\np\\n( )\\n(\\n)\\n(\\n)\\n1\\n1:\\nMean\\n,dim\\n1\\nl\\nl\\nN\\np\\nh\\n-\\n=\\n=\\n(\\n)1\\n1\\nl\\nN\\nh\\n-\\n+\\n(\\n)\\n(\\n)\\n( )\\n1\\n1\\n1\\n1\\nl\\nl\\nl\\nN\\nN\\nh\\nh\\np\\n-\\n-\\n+\\n+\\n=\\n+\\nL\\n( )\\n{\\n}\\n:\\n1\\nL\\nnh\\nn\\nN\\nÎ\\n+\\nmulti scale\\np\\n-\\n( )\\n( )\\n(\\n)\\n1\\nMean\\n,\\n,\\n,dim\\n1\\nL\\nmulti scale\\np\\np\\np\\n-\\né\\nù\\n=\\n…\\n=\\në\\nû\\nmulti scale\\np\\n-\\n( )\\nL\\nnh\\n( )\\n( )\\nL\\nL\\nn\\nn\\nmulti scale\\nh\\nh\\np\\n-\\n=\\n+\\n\\n \\n17 \\n \\nTo prepare encoder outputs for decoding, we process the final node features through specific \\ntransformations. Specifically, we extract original node features (excluding the global node), apply \\ndimension unification via \\n operation, and designate them as static decoder inputs\\n. \\n4.2. Decoder \\nThese encoded node features will be used to autoregressively generate action sequences for \\nsolving combinatorial optimization problems. The decoder employs a unidirectional MGU recurrent \\nneural network integrated with attention mechanisms to select appropriate actions at each decoding \\nstep. \\nThe decoder\\'s objective is to progressively construct solution sequences using graph node \\nrepresentations from the encoder and evolving dynamic state information. To achieve this, we \\nimplement an L-layer unidirectional MGU recurrent neural network as the decoder\\'s core \\ncomponent. At each decoding timestep \\n, the network receives: static features \\n, \\ndynamic features \\n, from the encoder, decoder input \\n, and previous \\nhidden state \\n. The decoding process proceeds autoregressively until complete \\nsolution sequences are generated. The decoder core consists of L stacked MGU recurrent layers. At \\ntimestep \\n, the MGU processes decoder input \\n and previous hidden state \\n to update its \\nstate, generating current output \\n and updated hidden state \\n,  \\nThe MGU update procedure can be formally expressed as: \\n \\n(20) \\nHere, MGU(⋅) represents the forward propagation of the multi-layer MGU network. We \\nemploy an optimized MGU cell architecture designed to enhance computational efficiency and \\nmodel performance. This MGU cell selectively updates/resets hidden states via gating mechanisms \\nto better capture long-term dependencies in sequential data. \\nTo enable decoder focus on critical graph nodes at each step, we incorporate an attention \\nmechanism. Specifically, a content-based attention mechanism processes: static features\\n, \\ndynamic features \\n and MGU output \\n. The mechanism computes node relevance scores \\nper decoding step to generate selection logits. \\nFirst, we transform dynamic features \\n, static features \\n and MGU output \\n \\nthrough linear projection layers, obtaining projected features \\n, \\n and \\n. We then compute attention scores \\n, measuring the compatibility between decoder \\nstates and node: \\n \\n \\n(21) \\nWhere \\nis a learnable parameter vector, \\n and \\n denote the \\n column \\npermute\\nstatic\\nE\\nt\\nh\\nB D\\nN\\nR\\n´\\n´\\nÎ\\nstatic\\nE\\nh\\nB D\\nN\\ndynamic\\nE\\nR\\n´\\n´\\nÎ\\n( )\\nh\\nt\\nB D\\ndec\\nx\\nR ´\\nÎ\\n(\\n)1\\nh\\nt\\nL B D\\nh\\nR\\n-\\n´ ´\\nÎ\\nt\\n( )t\\ndec\\nx\\n(\\n)1\\nt\\nh\\n-\\n( )\\nh\\nt\\nB D\\nr\\nR ´\\nÎ\\n( )\\nh\\nt\\nL B D\\nh\\nR ´ ´\\nÎ\\n( )\\n( )\\n( )\\n(\\n)\\n(\\n)\\n1\\n,\\nMGU\\n,\\nt\\nt\\nt\\nt\\ndec\\nr\\nh\\nx\\nh\\n-\\n=\\nstatic\\nE\\ndynamic\\nE\\n( )t\\nr\\ndynamic\\nE\\nstatic\\nE\\n( )t\\nr\\n( )\\nh\\nt\\nB D\\nN\\nd\\nR ´\\n´\\nÎ\\n( )\\nh\\nt\\nB D\\nN\\ne\\nR ´\\n´\\nÎ\\n( )\\nh\\nt\\nB D\\nq\\nR ´\\nÎ\\n( )t\\niu\\n( )\\n( )\\n( )\\n( )\\n(\\n)\\ntanh\\nt\\nt\\nt\\nt\\nT\\ni\\ni\\ni\\nu\\nv\\ne\\nq\\nd\\n=\\n+\\n+\\nh\\nD\\nv\\nR\\nÎ\\n( )t\\nie\\n( )t\\nid\\ni\\nth\\n-\\n\\n \\n18 \\n \\nvectors of \\n and \\n respectively, with \\n broadcasted to match dimensions. The attention \\nweights \\n are obtained by Softmax normalization of scores, indicating relative selection \\nprobabilities: \\n \\n(22) \\nThese weights generate context vectors aggregating node information weighted by decoder \\nstates. \\nTo regulate the scale of logits and introduce nonlinearity, we apply activation to the attention \\nscores and multiply by a learnable scaling factor, obtaining final node selection logits \\n: \\n \\n \\n(23) \\nWhere \\n. \\nThe decoder utilizes the attention-generated logits \\n to compute node selection probability \\ndistribution through the Softmax function: \\n \\n \\n(24) \\nDuring the training phase, we employ a sampling-based action selection strategy. To ensure \\nthe validity of selected actions, we implement an action masking mechanism prior to generating \\nprobability distributions. The action mask dynamically filters out invalid action options (e.g., visited \\nnodes or constraint-violating nodes) based on real-time environmental states. During model \\nevaluation, we adopt a greedy action selection strategy. \\nTo capture state evolution during decoding, the dynamic features \\n are updated at each \\ndecoding timestep. \\n4.3. Training strategy \\nAs show in Algorithm 1, This approach employs an Asynchronous Advantage Actor-Critic \\nframework enhanced with prioritized experience sampling and adaptive learning rate scheduling, \\nspecifically tailored for combinatorial optimization in vehicle routing problems. The core objective \\nminimizes the expected completion time of coordinated truck-drone delivery routes, mathematically \\nformulated as: \\n where \\n denotes policy parameters, \\n represents \\nproblem instances sampled from distribution. and \\n is the action sequence generated by the policy \\nnetwork. \\nTo address computational challenges in large-scale routing optimization, we designed a hybrid \\ntraining architecture decoupling environment interaction from parameter updates. As show in \\nAlgorithm 2 and Algorithm 3, The policy network (actor) and value network (critic) operate through \\nparallel threads: the main thread generates trajectories via environment interaction, while auxiliary \\nthreads asynchronously update network parameters using gradients queued in a prioritized buffer. \\nThis design minimizes computational resource idling and accelerates convergence. \\n( )t\\ne\\n( )t\\nd\\n( )t\\nq\\n( )\\nα\\nt\\nB N\\nR ´\\nÎ\\n( )\\n( )\\n(\\n)\\n( )\\n(\\n)\\nα\\nexp\\nexp\\nt\\nt\\nt\\ni\\ni\\nj\\nj N\\nu\\nu\\nÎ\\n=\\nå\\n( )t\\nB N\\nl\\nR ´\\nÎ\\n( )\\n( )\\n(\\n)\\ntanh\\nt\\nt\\nl\\nC\\nu\\n=\\n×\\n( )\\n( )\\n( )\\n( )\\n1\\n2\\n,\\n,\\n,\\nT\\nt\\nt\\nt\\nt\\nN\\nu\\nu\\nu\\nu\\né\\nù\\n=\\n…\\në\\nû\\n( )tl\\n( )\\n( )\\n(\\n)\\nSoftmax\\nt\\nt\\np\\nl\\n=\\ndynamic\\nE\\n( )\\n( ) (\\n)\\n|\\ns\\np\\ns\\nJ\\nE\\nE\\nC\\ns\\nq\\np\\nq\\np\\n~\\n~\\n×\\né\\nù\\n=\\në\\nû\\nS\\nq\\ns\\np\\n\\n \\n19 \\n \\nThe policy gradient is derived using the REINFORCE algorithm with a critic-based baseline \\nfor variance reduction. For a batch of B problem instances\\n,  the gradient estimate is \\nexpressed as: \\n \\n(25) \\nwhere, \\n denotes the critic network\\'s predicted expected completion time, and\\n represents trajectories sampled from the policy. The critic network is trained by \\nminimizing the mean squared error (MSE): \\n \\n(26) \\nA dynamic prioritization mechanism filters high-impact experiences to accelerate the learning \\nprocess. Let the advantage function be defined as: \\n \\n(27) \\nWhen the trajectory\\'s average absolute advantage exceeds threshold\\n,  \\n prioritized immediate gradient updates are triggered. This prioritization \\nfocuses training on episodes where policy performance significantly deviates from critic estimates, \\nefficiently allocating computational resources to challenging instances. \\nNetwork optimization employs the AdaBelief optimizer (Zhuang et al., 2020), which \\nadaptively adjusts step sizes based on gradient magnitude and directional consistency. \\n \\n(28) \\n \\n(29) \\n \\n(30) \\nWhere \\n is the stochastic gradient, \\n is the initial learning rate, and the hyperparameters \\nare set to \\n, \\n, \\n. Weight decay (0.01) and decoupled weight updates \\nregularize the network. \\nThe learning rate follows a cosine annealing schedule, completing five full cycles throughout \\nthe entire training period: \\n \\n(31) \\nWhere \\n, \\n, \\n is the total number of training cycles. This \\ncyclical schedule promotes exploration by periodically resetting the learning rate, preventing \\npremature convergence to suboptimal policies. \\n1\\n({ }\\n)\\nB\\ni\\ni\\ns\\n=\\n( )\\n(\\n)\\n( )\\n(\\n)\\n(\\n)\\n1\\n1\\nlog\\nB\\ni\\ni\\ni\\ni\\ni\\ni\\nJ\\nC\\ns\\nb\\ns\\np\\ns\\nB\\nq\\nf\\nq\\nq\\nq\\np\\np\\n=\\nÑ\\n»\\n-\\nÑ\\nå\\n( )\\n(\\n)\\ni\\nb\\ns\\nf\\n(\\n)\\n(\\n)\\n|\\ni\\ni\\np\\ns\\nq\\np ~\\n×\\n( )\\n( )\\n(\\n)\\n2\\n2\\n1\\n1\\n||\\nπ |\\n||\\nB\\ni\\ni\\ni\\ni\\nb\\ns\\nC\\ns\\nB\\nf\\nf\\n=\\n=\\n-\\nå\\nL\\n(\\n)\\n(\\n)\\n( )\\n(\\n)\\n,π\\nπ |\\ni\\ni\\ni\\ni\\ni\\nA s\\nC\\ns\\nb\\ns\\nf\\n=\\n-\\n(\\n)\\nτ\\n0.5\\n=\\n(\\n)\\n1\\n1\\n,\\nB\\ni\\ni\\ni\\nA s\\nB\\np\\nt\\n=\\n>\\nå\\n(\\n)\\n1\\n1\\n1\\nβ\\n1 β\\n,\\nt\\nt\\nt\\nm\\nm\\ng\\n-\\n=\\n+\\n-\\n(\\n)(\\n)\\n2\\n2\\n1\\n2\\nβ\\n1 β\\nt\\nt\\nt\\nt\\nv\\nv\\ng\\nm\\n-\\n=\\n+\\n-\\n-\\n+ ，\\n1\\nθ\\nθ\\nη\\nt\\nt\\nt\\nt\\nm\\nv\\n+ =\\n-\\n+\\ntg\\nh\\n1\\n0.9\\nb =\\n2\\n0.999\\nb =\\n16\\n10-\\n=\\n(\\n)\\n(\\n)\\n(\\n)\\nmin\\nmax\\nmin\\nmax\\nη\\nη\\n1 2 η\\nη\\n1 cos π\\nt\\nt\\nT\\n=\\n+\\n-\\n+\\nmax\\ntrain 5\\nT\\nN\\n=\\nmin\\nmax\\nη\\n0.01η\\n=\\ntrain\\nN\\n\\n \\n20 \\n \\nAlgorithm 1: Training Process \\nInput: \\n    \\n: Initial actor network parameters \\n    \\n: Initial critic network parameters \\n    \\n: Maximum training epochs \\n    \\n: Priority threshold \\n    \\n: Maximum steps per episode (decode_len) \\n1  Initialize \\n and critic \\n with  \\n2  Start asynchronous \\n and \\n  \\n3 \\n \\n4      \\n  \\n5      // Environment Interaction \\n6      \\n   \\n7      \\n        \\n8      \\n          \\n9      \\n \\n10         \\n \\n11         \\n \\n12         \\n   \\n13         \\n \\n14         \\n \\n15         \\n \\n16     \\n \\n17     \\n \\n18     \\n \\n19         \\n \\n20         \\n \\n21         \\n \\n22         \\n \\n23     \\n \\n24     \\n \\n0\\nq\\n0\\nj\\nK\\nt\\nT\\n(\\n)\\n \\nactor\\nq\\np\\nVj\\n(\\n)\\n0\\nActorUpdateThread q\\n(\\n)\\n0\\nCriticUpdateThread j\\n \\n \\n1   \\nforepoch\\nto K do\\n¬\\n(\\n)\\n  \\nnodes\\nG\\nGenerateTrainingData n\\n¬\\n(\\n)\\n( )\\n,\\n.\\nS avail\\nEnv Reset G\\n¬\\n  \\n()\\nh\\nInitializeMGU\\n¬\\n 0\\ntime\\ncurrent\\n¬\\n  \\n 0 \\n \\n1 \\nfort\\nto T\\ndo\\n¬\\n-\\n(\\n)\\n, ,\\n,\\n,\\ntr\\ntr\\ntr\\ntr\\nS\\nh avail\\na logp\\nh\\nActorq\\n¬\\n \\n \\n \\ndr\\ntr\\nUpdateavail based ona\\n(\\n)\\n, ,\\n,\\n,\\ndr\\ndr\\ndr\\ndr\\nS\\nh avail\\na\\nlogp\\nh\\nActorq\\n¬\\n(\\n)\\nmin\\n,\\n,\\nstep\\ntr\\ndr\\ntime\\ntime\\nt\\nt\\nremaining\\n¬\\n \\ntime\\nstep\\ncurrent\\ntime\\n+ =\\n,\\n,\\nS avail\\nS avail\\n¬\\n¢\\n¢\\n \\nend for\\n(\\n)\\n,\\n   – \\nstatic w\\nA\\nR Vj\\n¬\\n(\\n)\\n \\n \\n \\nif mean A\\nthen\\nt\\n>\\n(\\n)\\n(\\n)\\n(\\n)\\n1\\n0\\nmean\\nlog π\\nlogπ\\nT\\nt\\nt\\nloss\\ntr\\ndr\\nt\\nactor\\nA\\na\\na\\n-\\n=\\né\\nù\\n¬ -\\n×\\n+\\në\\nû\\nå\\n(\\n)\\n2\\n \\nloss\\ncritic\\nmean A\\n¬\\n \\n \\nloss\\nSend actor to ActorQueue\\n \\n \\nloss\\nSend critic\\ntoCriticQueue\\n \\nend if\\n \\n \\n \\n 0 \\ninterval\\nif epochmod S\\nthen\\n=\\n\\n \\n21 \\n \\n25         \\n   \\n26         \\n \\n27             \\n \\n28             \\n \\n29             \\n \\n30             \\n \\n31         \\n \\n32     \\n \\n33 \\n \\n \\nAlgorithm 2: ActorUpdateThread \\n1  while training not terminated do \\n2      \\n \\n3          \\n \\n4          \\n   \\n5          \\n        \\n6      \\n \\n7  \\n \\n \\nAlgorithm 3: CriticUpdateThread \\n1  while training not terminated do \\n2      \\n \\n3          \\n \\n4          \\n  \\n5          \\n       \\n6      \\n \\n7  \\n \\n(\\n)\\n \\n,\\nval\\nR\\nValidate q j\\n¬\\n \\n \\nval\\nbest\\nif R\\nR\\nthen\\n<\\n \\nbest\\nval\\nR\\nR\\n¬\\n*\\nq\\nq\\n¬\\n*\\nj\\nj\\n¬\\n(\\n)\\n*, *\\nSaveModel q j\\n \\nend if\\n \\nend if\\n \\nend for\\n \\n \\n \\n \\nif ActorQueuenotemptythen\\n( )\\n \\n.\\nloss\\nactor\\nActorQueue get\\n¬\\n(\\n)\\n  \\n,\\nloss\\nAdaBeliefUpdate\\nactor\\nq\\nq\\n¬\\n( )\\nUpdateActorLearningRate q\\n \\nend if\\n \\nend while\\n \\n \\n \\n \\nif CriticQueuenotemptythen\\n( )\\n \\n.\\nloss\\ncritic\\nCriticQueue get\\n¬\\n(\\n)\\n  \\n,\\nloss\\nAdaBeliefUpdate\\ncritic\\nj\\nj\\n¬\\n( )\\nUpdateCriticLearningRate j\\n \\nend if\\n \\nend while\\n\\n \\n22 \\n \\n5 Computational Experiments \\nTo comprehensively evaluate the performance of this deep reinforcement learning method, this \\nchapter compares our algorithm with traditional heuristic algorithms and high-performance \\nreinforcement learning algorithms in the field. In all experiments, we assume the drone is twice as \\nfast as the truck (α = 2) and has unlimited flight range. \\nFew standard datasets exist for TSP-D. (Bogyrbayeva et al., 2023) developed a new dataset \\nbased on (Haider et al., 2019) \\'s study , which has subsequently been adopted in TSP-D research  \\n(Mahmoudinazlou and Kwon, 2024), This collection contains two TSP-D instance subsets \\n(\"Random\" and \"Amsterdam\") with unlimited drone range, differing in instance generation \\nmethodologies. In the first instance subset, uniform distributions over [0, 1] × [0, 1] and [0, 100] \\n× [0, 100] are employed to sample x and y coordinates of depots and customer nodes, respectively. \\nThis configuration ensures depot positioning at the lower-left corner. The generation method \\nresembles that proposed by (Agatz et al., 2018) We present 100 instance samples for node sizes of \\n20, 50, and 100. The second instance subset comprises 100 samples with node sizes of 10, 20 and \\n50. In this subset, depots are randomly selected from the nodes. Additionally, we evaluated another \\nuniform dataset for TSP-D maintained in the TSBlib project on GitHub. \\nWe employ the following formula to report relative gaps between solutions: \\n \\n(32) \\nHere, \\n denotes the cost of a specific solution for instance 𝑖, and \\n represents the \\nsolution method widely recognized as high-quality for instance 𝑖 among existing comparative \\napproaches. We then report \\n as the average of all \\n values under identical configurations. \\n5.1. Model Inference Performance \\nFor reinforcement learning algorithms, since this study builds upon AM (Kool et al., 2018)  \\nand HM (Bogyrbayeva et al., 2023), natural comparisons should be made with them. Notably, AM \\nhas demonstrated inferior performance to HM in both solution accuracy and speed across varying \\nnode scales (Bogyrbayeva et al., 2023), thus excluded from our comparison. Inference was \\nconducted using a single RTX 4090 (24GB) GPU with 12 vCPU Intel Xeon Platinum 8352V CPU \\n@ 2.10GHz, under PyTorch 1.10.0, Python 3.8 (Ubuntu 20.04), and CUDA 11.3. \\nFor heuristic algorithms, (Bogyrbayeva et al., 2023) extended the \"TSP-ep-all\" algorithm by \\n(Agatz et al., 2018) to DPS by adopting the \"divide-and-conquer heuristic\" (DCH) from (Poikonen \\net al., 2019) . We compared the performance of \"TSP-ep-all\" and DPS. (Mahmoudinazlou and Kwon, \\n2024) proposed a Hybrid Genetic Algorithm with Type-Aware Chromosomes that integrates local \\nsearch and dynamic programming to solve TSP-D, showing strong performance on “Amsterdam” \\n(\\n)\\n*\\n*\\nGap\\n100%\\ni\\ni\\ni\\ni\\nZ\\nZ\\nZ\\n=\\n-\\n´\\ni\\nZ\\n*\\niZ\\ni\\nGap\\n \\nGap\\n\\n \\n23 \\n \\nand “Random” datasets, which we also compared against. The key innovation in their approach lies \\nin observing that escape strategies improve solution quality while increasing runtime. This \\ndemonstrates effective mitigation of metaheuristic methods\\' inherent local optima issues. \\nConsequently, we only compared HGA-TAC+ implementations employing escape strategies. All \\nthree heuristic algorithms were implemented in Julia and evaluated using an AMD EPYC 9654 96-\\ncore processor. \\nFor the greedy policy in reinforcement learning, instances are not evaluated in batches but \\nsequentially processed one-by-one on the GPU. Heuristic methods follow the same strategy but \\nutilize a single CPU thread instead of GPU. For batch sampling strategies, HM (n_samples) (where \\nn_samples denotes batch size) demonstrates that increasing n_samples generally improves solution \\naccuracy while reducing computational speed. Each algorithm was executed 10 times, with the best \\nreward and computation time per instance averaged across trials. \\nWe initially evaluated using the \"Random\" dataset. To rigorously evaluate the performance of \\nour proposed model for the TSP-D, we executed each algorithm 10 times and calculated the average \\nof the cost and computation times for each instance. we conducted extensive computational \\nexperiments on benchmark instances with varying network sizes (N=11, 20, 50, and 100 nodes). \\nPerformance was assessed based on two primary metrics: the total solution cost (lower values \\nindicate better performance) and the computational time required (in seconds, lower values indicate \\ngreater efficiency). We employed TSP-ep-all, an approach known for generating high-quality \\nsolutions, as the baseline for comparison. The percentage gap (Gap) relative to this baseline is \\nreported to quantify the deviation in solution quality.  \\nWe first examine the performance of rapid decision-making strategies, including DPS/10, HM \\n(greedy), and our own greedy implementation (Ours(greedy)). As show in Table 1, These methods \\nprioritize computational speed, consistently delivering solutions within fractions of a second across \\nall instance sizes (e.g., ≤ 0.01s for greedy methods, ≤ 0.27s for DPS/10 at N=100). However, \\nthis velocity is achieved at the expense of solution quality. Both HM (greedy) and Ours (greedy) \\nexhibit similar performance, yielding positive Gaps ranging from approximately 0.7% to 5.4% \\ncompared to the baseline. DPS/10 shows comparable Gaps (3.70% to 5.90%). While extremely fast, \\nthese methods demonstrate the limitations of purely greedy strategies in navigating the complex \\nTSP-D solution landscape. \\nThe core of our evaluation lies in comparing Ours (sampling_k) against the HM (sampling_k) \\nusing equivalent number of samples for batch sampling (k=100, 1200, 2400, 4800). For any given \\nnumber of samples and problem size, ours (sampling_k) consistently achieves lower costs than HM \\n(sampling_k). This advantage is particularly evident in larger instances. For example, at N=50 and \\n\\n \\n24 \\n \\nk=4800, our method yields a cost of 391.51 (Gap: -1.42%), significantly outperforming HM\\'s \\n396.31 (Gap: -0.22%). Similarly, at N=100 and k=4800, our method achieves a 1.22% Gap (Cost: \\n542.21) compared to HM\\'s 1.63% Gap (Cost: 544.42). This suggests that our RL framework \\nintegrates the sampled information more effectively to guide the policy towards higher-quality \\nsolutions.  \\nConcurrent with superior solution quality, our algorithm exhibits enhanced computational \\nefficiency. When comparing Ours (sampling_k) and HM (sampling_k) at identical N and k, our \\nmethod consistently requires less computational time. For instance, solving the N=100 instance with \\nk=4800 takes 8.43s using our method, whereas HM requires 9.54s, representing an approximate \\n11.6% reduction in runtime.  \\nIncreasing the sampling width (k) predictably improves solution quality for RL method, when \\ncompared against the strong heuristic HGVAC+, our sampling method presents a compelling \\nalternative. While HGVAC+ performs well, particularly on smaller instances (N=11, 20) where it \\nachieves negative Gaps relatively quickly, our method (Ours (sampling_k) with sufficient k) often \\nmatches or surpasses its solution quality on larger instances while offering significant computational \\nsavings. For N=50, Ours(sampling_4800) achieves a better Gap (-1.42%) in nearly half the time \\n(2.41s vs 4.65s) compared to HGVAC+ (Gap 0.39%). For N=100, Ours (sampling_4800) delivers a \\nmarginally better Gap (1.22% vs 1.53%) while being substantially faster (8.43s vs 14.83s). This \\nsuggests our method scales more effectively in terms of maintaining high-quality solutions within \\nreasonable time limits as problem complexity increases. \\nTable 1 \\nTSP-D results on “Random” locations dataset \\n  \\nN=11 \\nN=20 \\nN=50 \\nN=100 \\nMethod \\ncost \\nGap \\nTime(s) \\ncost \\nGap \\nTime(s) \\ncost \\nGap \\nTime(s) \\ncost \\nGap \\nTime(s) \\nTSP-ep-all(Baseline) \\n230.07  0.00% \\n0.01  \\n281.62  0.00% \\n0.11  \\n397.17  \\n0.00% \\n23.95  \\n535.67  0.00% \\n2352.53  \\nDPS/10 \\n  \\n  \\n  \\n292.05  3.70% \\n0.02  \\n420.61  \\n5.90% \\n0.05  \\n565.14  5.50% \\n0.27  \\nHGVAC\\n＋ \\n227.45  -1.14% \\n0.52  \\n279.88  -0.62% \\n1.22  \\n398.72  \\n0.39% \\n4.65  \\n543.88  1.53% \\n14.83  \\nHM(greedy) \\n233.21  1.36% \\n0.00  \\n285.54  1.39% \\n0.00  \\n408.84  \\n2.94% \\n0.01  \\n564.42  5.37% \\n0.01  \\nOurs(greedy) \\n231.67  0.69% \\n0.00  \\n285.80  1.48% \\n0.00  \\n407.04  \\n2.49% \\n0.00  \\n564.36  5.36% \\n0.01  \\nHM(sampling_100) \\n230.10  0.01% \\n0.11  \\n282.93  0.46% \\n0.13  \\n399.59  \\n0.61% \\n0.35  \\n550.13  2.70% \\n0.76  \\nOurs(sampling_100) \\n229.05  -0.45% \\n0.10  \\n282.10  0.17% \\n0.13  \\n396.41  \\n-0.19% \\n0.35  \\n550.89  2.84% \\n0.74  \\nHM(sampling_1200) \\n229.22  -0.37% \\n0.14  \\n282.13  0.18% \\n0.18  \\n397.38  \\n0.05% \\n0.70  \\n546.01  1.93% \\n2.57  \\nOurs(sampling_1200) \\n228.55  -0.66% \\n0.13  \\n280.80  -0.29% \\n0.17  \\n392.94  \\n-1.06% \\n0.68  \\n544.41  1.63% \\n2.21  \\nHM(sampling_2400) \\n229.12  -0.42% \\n0.18  \\n281.84  0.08% \\n0.28  \\n397.01  \\n-0.04% \\n1.41  \\n545.13  1.77% \\n4.90  \\nOurs(sampling_2400) \\n228.36  -0.74% \\n0.15  \\n280.67  -0.34% \\n0.23  \\n392.07  \\n-1.28% \\n1.27  \\n543.18  1.40% \\n4.22  \\nHM(sampling_4800) \\n228.93  -0.50% \\n0.29  \\n281.67  0.02% \\n0.50  \\n396.31  \\n-0.22% \\n2.59  \\n544.42  1.63% \\n9.54  \\nOurs(sampling_4800) \\n228.30  -0.77% \\n0.21  \\n280.44  -0.42% \\n0.42  \\n391.51  \\n-1.42% \\n2.41  \\n542.21  1.22% \\n8.43  \\n \\n\\n \\n25 \\n \\nAs show in Table 2. On the uniform dataset, our proposed reinforcement learning algorithm \\ndemonstrates superior performance compared to the HM method across TSP-D instances of varying \\nscales (N=10, 20, 50). Utilizing the sampling_4800, our approach consistently yielded lower costs, \\nachieving negative Gaps of -0.12%, -1.10%, and -0.58% for N=10, N=20, and N=50, respectively, \\nrelative to the computationally intensive HM (4800) baseline. This signifies a notable improvement \\nin solution quality. Crucially, this enhancement was realized with comparable or marginally reduced \\ncomputational times (e.g., 2.47s vs. 2.57s for N=50). While both algorithms improve solution \\nquality with increased sampling, our method exhibits greater effectiveness, achieving superior \\nsolutions faster. Notably, for N=20 and N=50, our algorithm with only 1200 or 2400 samples already \\nsurpasses the cost performance of the HM (4800) baseline (indicated by negative Gaps: -0.89%/-\\n1.10% for N=20, -0.31%/-0.50% for N=50). Fig. 4 presents detailed routing visualizations on the \\n20-node uniform dataset, clearly demonstrating that our model with sampling strategy outperforms \\nHM in nearly every instance. \\nTable 2 \\nTSP-D results on “uniform” locations dataset \\n  \\nN=11 \\nN=20 \\nN=50 \\nMethod \\ncost \\nGap \\nTime \\ncost \\nGap \\nTime \\ncost \\nGap \\nTime \\nHM(greedy) \\n228.38  0.28% \\n0.00  277.87  0.66% \\n0.00  426.93  4.26% \\n0.01  \\nHM(sampling_100) \\n228.38  0.28% \\n0.09  276.95  0.33% \\n0.12  413.22  0.91% \\n0.35  \\nHM(sampling_1200) \\n227.98  0.10% \\n0.15  276.09  0.02% \\n0.17  410.57  0.27% \\n0.69  \\nHM(sampling_2400) \\n227.75  0.00% \\n0.18  276.09  0.02% \\n0.26  409.94  0.11% \\n1.31  \\nHM(sampling_4800)(Baseline) 227.75  0.00% \\n0.29  276.05  0.00% \\n0.50  409.48  0.00% \\n2.57  \\nOurs(greedy) \\n228.38  0.28% \\n0.00  279.23  1.15% \\n0.00  425.68  3.96% \\n0.01  \\nOurs(sampling_100) \\n227.76  0.00% \\n0.11  275.51  -0.19% \\n0.14  412.42  0.72% \\n0.35  \\nOurs(sampling_1200) \\n227.49  -0.12% \\n0.14  273.60  -0.89% \\n0.19  408.23  -0.31% \\n0.74  \\nOurs(sampling_2400) \\n227.49  -0.12% \\n0.18  273.00  -1.10% \\n0.24  407.43  -0.50% \\n1.28  \\nOurs(sampling_4800) \\n227.49  -0.12% \\n0.29  273.00  -1.10% \\n0.46  407.09  -0.58% \\n2.47  \\n \\n \\nSampling_100 \\nSampling_2400 \\n \\nHM \\nOurs \\nHM \\nOurs \\n0 \\n \\nBest reward=233.24 \\n \\nBest reward=241.15 \\n \\nBest reward=233.24 \\n \\nBest reward=228.73 \\n\\n \\n26 \\n \\n1 \\n \\nBest reward=281.72 \\n \\nBest reward=273.12 \\n \\nBest reward=281.72 \\n \\nBest reward=267.86 \\n2 \\n \\nBest reward=275.20 \\n \\nBest reward=267.71 \\n \\nBest reward=275.19 \\n \\nBest reward=267.71 \\n3 \\n \\nBest reward=239.17 \\n \\nBest reward=238.75 \\n \\nBest reward=239.17 \\n \\nBest reward=238.75 \\n4 \\n \\nBest reward=298.38 \\n \\nBest reward=296.13 \\n \\nBest reward=298.38 \\n \\nBest reward=293.62 \\n5 \\n \\nBest reward=284.89 \\n \\nBest reward=284.90 \\n \\nBest reward=284.89 \\n \\nBest reward=284.89 \\n6 \\n \\nBest reward=300.33 \\n \\nBest reward=300.12 \\n \\nBest reward=300.13 \\n \\nBest reward=300.12 \\n\\n \\n27 \\n \\n7 \\n \\nBest reward=300.16 \\n \\nBest reward=300.03 \\n \\nBest reward=298.12 \\n \\nBest reward=299.29 \\n8 \\n \\nBest reward=279.25 \\n \\nBest reward=278.35 \\n \\nBest reward=279.25 \\n \\nBest reward=274.22 \\n9 \\n \\nBest reward=277.19 \\n \\nBest reward=274.80 \\n \\nBest reward=270.82 \\n \\nBest reward=274.80 \\nFig. 4. Test results of our model and HM on the \"uniform\" dataset position with 20 nodes \\nWe also compared the model performance on the Amsterdam dataset, As show in Table 3, since \\nthe Amsterdam dataset has 10 nodes, the performance differences of multiple models on 11 nodes \\nare not compared. For the N=20 instances, our proposed algorithm (Ours(sampling_4800)) \\ndemonstrates highly competitive performance. It achieves a solution cost of 2.34, marginally \\noutperforming the TSP-ep-all baseline (Gap: -0.70%) and matching the heuristic HGVAC+. Notably, \\nits computation time (0.48s) is significantly faster than HGVAC+ (1.81s) and comparable to the HM \\nreinforcement learning method (0.50s), while being only moderately slower than the baseline\\'s 0.12s. \\nAlthough DPS methods offer extreme speed (≤0.05s), their solution quality is substantially \\ncompromised (Gap: 33.20%), highlighting a clear trade-off. Our method effectively balances near-\\noptimal solution quality with efficient computation time at this scale. \\nThis advantage in computational efficiency becomes more pronounced for the larger N=50 \\ninstances. Our algorithm yields a cost of 3.29, representing the best performance among the RL \\nmethods (Gap: 0.76%) and remaining highly competitive with the baseline cost (3.27). Crucially, \\nour execution time (2.47s) remains efficient and similar to HM (2.58s), offering a nearly 10-fold \\nspeed advantage over the baseline (24.53s) and a 2.5-fold advantage over HGVAC+ (6.20s). These \\nresults underscore our method\\'s strong scalability, delivering high-quality solutions with \\nsignificantly reduced computational burden compared to both the heuristic baseline and other \\n\\n \\n28 \\n \\ncompetitive approaches as problem size increases. \\nTable 3 \\nTSP-D results on “Amsterdam” locations dataset \\n  \\nN=20 \\nN=50 \\nMethod \\ncost \\nGap \\nTime \\ncost \\nGap \\nTime \\nTSP-ep-all(Baseline) \\n2.36  \\n0.00% \\n0.12  \\n3.27  \\n0.00% \\n24.53  \\nDPS/10 \\n3.14  \\n33.20% \\n0.05  \\n3.80  \\n16.49% \\n0.08  \\nDPS/25 \\n  \\n  \\n0.00  \\n4.23  \\n29.46% \\n0.07  \\nHGVAC\\n＋ \\n2.34  \\n-0.79% \\n1.81  \\n3.33  \\n2.10% \\n6.20  \\nHM(sampling_4800) \\n2.38  \\n1.00% \\n0.50  \\n3.31  \\n1.37% \\n2.58  \\nOurs(sampling_4800) \\n2.34  \\n-0.70% \\n0.48  \\n3.29  \\n0.76% \\n2.47  \\n5.2. Model Training Performance \\nWe found that compared to the improvement in inference speed, the model\\'s improvement in \\ntraining speed and convergence speed is more significant. Specifically, our model can achieve better \\nresults within the same number of epochs, and the training speed per epoch is also significantly \\nincreased. We compared the differences in training speed and convergence speed with HM on a \\nsingle 4090 GPU. \\nTo evaluate the training characteristics of the proposed model (Ours) and the baseline model \\nHM in solving TSP-D problems of different scales, we conducted detailed comparative experiments \\non instances with 11, 20, 50 and 100 nodes respectively. As show in Fig. 5 and Fig. 6, both models \\nwere trained for 10,000 epochs, and key performance indicators, including the reward value \\n(Reward, representing path cost here, lower is better) and training time, were recorded every 200 \\nepochs. Analyzing the training curves (reward value change over epochs), both models exhibit the \\ntypical characteristics of reinforcement learning, i.e., the reward value shows an overall downward \\ntrend as training progresses, indicating continuous optimization of the model\\'s policy. However, our \\nmodel (Ours) demonstrates significant advantages in terms of convergence speed and final solution \\nquality. Taking n=11 as an example, the Ours model obtained a much lower reward value than the \\nHM model in the initial stage (Epoch 0) (311.7 vs 409.5), and reached a reward value close to the \\nfinal level (~253) around 1200 epochs, while the HM model\\'s reward value was still above 260 \\nduring the same period and required more epochs to stabilize. This advantage of rapid convergence \\nis even more pronounced on larger-scale problems (n=20, 50, 100). For example, at n=100, the Ours \\nmodel could rapidly reduce the reward value from about 958 to around 700 in the early training \\nstages (around 1000-1400 epochs), while the HM model required a longer exploration period to \\nreach a similar level. At the end of training (Epoch 10000), for all tested node scales, the final \\naverage reward value obtained by the Ours model was lower than that of the HM model (n=11: \\n\\n \\n29 \\n \\n242.7 vs 245.3; n=20: 304.4 vs 312.5; n=50: 477.5 vs 487.8; n=100: 677.5 vs 688.0), which \\ndemonstrates the effectiveness of our model in finding better TSP-D solutions. Furthermore, our \\nproposed model exhibits training stability significantly superior to the HM baseline model. Across \\nall tested node scales (n=11, 20, 50, 100), the evolution of the Ours model\\'s reward value over \\ntraining epochs shows smaller fluctuations, especially in the middle and later stages of training, its \\nlearning curve is smoother, and rarely exhibits the drastic performance oscillations or significant \\ntemporary policy degradation phenomena observed in the HM model. \\nIn terms of computational efficiency, our model (Ours) also shows significant superiority. By \\ncomparing the recorded average training time per epoch, it can be found that under all node scales, \\nthe time consumption per epoch of the Ours model is significantly lower than that of the HM model. \\nFor example, for n=11, the average epoch time of the Ours model is about 0.13 seconds, while the \\nHM model is about 0.27-0.30 seconds. When the scale increases to n=100, the epoch time of the \\nOurs model is about 0.9-1.1 seconds, while the HM model requires 1.3-1.7 seconds. The cumulative \\neffect of this improvement in per-epoch efficiency is significant, leading to the total cumulative time \\nof the Ours model being much less than the HM model when completing the same number of \\ntraining epochs (10,000 Epochs). Specifically, for n=11, Ours total time consumption is about 1438 \\nseconds, HM time consumption is about 2838 seconds. For n=100, Ours total time consumption is \\nabout 10241 seconds (about 2.8 hours), while HM time consumption is 16730 seconds (about 4.6 \\nhours). This indicates that the Ours model not only trains faster but also has lower computational \\ncosts. From the perspective of scalability, although the per-epoch time consumption and total time \\nconsumption of both models inevitably increase with the increase in problem size, the Ours model \\nalways maintains a relative time efficiency advantage. Overall, our model can not only converge \\nfaster to higher-quality solutions during the training process but also has significant advantages in \\ncomputational resource consumption, especially when dealing with larger-scale problems, this dual \\nimprovement in efficiency and effectiveness makes it a more potential reinforcement learning \\nsolution for solving TSP-D problems. \\n \\n \\n\\n \\n30 \\n \\nFig. 5. The training learning curves and training time graphs of our model and the HM model at 11, 20 nodes \\n \\nFig. 6. The training learning curves and training time graphs of our model and the HM model at 50, 100 nodes \\n\\n \\n31 \\n \\n6 Conclusions \\nIn this study, a novel end-to-end deep reinforcement learning framework was proposed to solve \\nTSP-D. The contributions of this study are as follows: First, a Transformer-inspired encoder \\narchitecture with optimized k-nearest neighbors sparse attention was developed to efficiently extract \\ncrucial structural patterns within TSP-D instances. Second, we introduced dynamic expander graph \\nMasking and global node features to capture the intricate spatial and collaborative dynamics of the \\nproblem. Third, an efficient MGU based decoder was implemented alongside an advanced \\nasynchronous advantage actor-critic training strategy, incorporating prioritized experience replay \\nand adaptive learning rate adjustments to enhance training efficiency. \\nWe evaluated the performance of the proposed model in solving the TSP-D problem using three \\nsets of benchmark instances. Compared with state-of-the-art heuristic algorithms (TSP-ep-all, DPS, \\nHGVAC+) and the reinforcement learning algorithm (HM) in the field. Experimental results \\ndemonstrate that our proposed method generates high-quality solutions across various TSP-D \\ninstances, exhibiting competitive solution quality compared to leading heuristic algorithms while \\ndemonstrating superior generalization capabilities on large-scale problems. Notably, when handling \\nmedium to large scale instances with 20, 50 and 100 nodes, our method achieves high-quality \\nsolutions across nearly all datasets while maintaining low computational costs. Furthermore, it \\nsignificantly reduces training time compared to state-of-the-art reinforcement learning algorithms \\nduring model training. The proposed method offers valuable insights for combinatorial optimization \\nproblems in multi-agent collaborative decision-making. It also introduces novel ideas and technical \\nsupport for developing intelligent decision-making systems in logistics. \\nThis study had some limitations. While demonstrating strong performance up to N=100, the \\nframework\\'s scalability and performance on significantly larger or more densely connected \\ninstances warrant further investigation. Moreover, the current model focuses primarily on core \\nrouting optimization without incorporating more nuanced real-world operational constraints. \\nTherefore, in future work, we will focus on enhancing the scalability and robustness of the proposed \\narchitecture, possibly through exploring more advanced graph representation techniques or \\nhierarchical learning approaches. Additionally, validating the model\\'s performance on diverse real-\\nworld datasets, potentially incorporating operational constraints such as varying drone battery \\nendurance, payload capacities, or specific customer time windows, will be critical for bridging the \\ngap towards practical deployment in logistics systems. Extending this framework to tackle other \\ncomplex multi-agent or heterogeneous fleet routing problems also represents a valuable direction \\nfor future research. \\n\\n \\n32 \\n \\nAcknowledgments \\nThis work was supported by the National Natural Science Foundation of China (Nos: 52105507 and \\n52275476) and Specialized Research Fund for Chongqing Technology Innovation and Application \\nDevelopment (No: CSTB2022TIAD-KPX0061). \\n \\nDeclaration of Interest statement \\nAuthors declare that there is no conflict of interest due to the publication of this paper. \\n \\n \\n\\n \\n33 \\n \\nReferences \\nAgatz, N., Bouman, P., Schmidt, M., 2018. Optimization Approaches for the Traveling Salesman \\nProblem with Drone. Transportation Science 52(4), 965-981. \\nBello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S., 2016. Neural combinatorial optimization with \\nreinforcement learning. arXiv preprint arXiv:1611.09940. \\nBengio, Y., Lodi, A., Prouvost, A., 2021. Machine learning for combinatorial optimization: A \\nmethodological tour d\\'horizon. European Journal of Operational Research 290(2), 405-421. \\nBogyrbayeva, A., Yoon, T., Ko, H.B., Lim, S., Yun, H.Y.K., Kwon, C., 2023. A deep reinforcement \\nlearning approach for solving the Traveling Salesman Problem with Drone. Transp. Res. Pt. C-\\nEmerg. Technol. 148, 19. \\nCho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y., 2014. \\nLearning phrase representations using RNN encoder-decoder for statistical machine translation. \\narXiv preprint arXiv:1406.1078. \\nCosta, L., Contardo, C., Desaulniers, G., 2019. Exact Branch-Price-and-Cut Algorithms for Vehicle \\nRouting. Transportation Science 53(4), 946-985. \\nDeudon, M., Cournut, P., Lacoste, A., Adulyasak, Y., Rousseau, L.M., 2018. Learning Heuristics for the \\nTSP by Policy Gradient, 15th International Conference on the Integration of Constraint \\nProgramming, Artificial Intelligence, and Operations Research (CPAIOR). Springer International \\nPublishing Ag, Delft, NETHERLANDS, pp. 170-181. \\nDuan, J., Luo, H., Wang, G., 2025. Approaches to the truck-drone routing problem: A systematic review. \\nSwarm and Evolutionary Computation 92, 101825. \\nGebreyesus, G., Fellek, G., Farid, A., Fujimura, S., Yoshie, O., 2023. Gated-Attention Model with \\nReinforcement Learning for Solving Dynamic Job Shop Scheduling Problem. IEEJ Trans. Electr. \\nElectron. Eng. 18(6), 932-944. \\nGu, R.X., Poon, M., Luo, Z.H., Liu, Y., Liu, Z., 2022. A hierarchical solution evaluation method and a \\nhybrid algorithm for the vehicle routing problem with drones and multiple visits. Transp. Res. Pt. \\nC-Emerg. Technol. 141, 23. \\nHaider, Z., Charkhgard, H., Kim, S.W., Kwon, C., 2019. Optimizing the relocation operations of free-\\nfloating electric vehicle sharing systems. Available at SSRN 3480725. \\nHam, A.M., 2018. Integrated scheduling of <i>m</i>-truck, <i>m</i>-drone, and <i>m</i>-depot \\nconstrained by time-window, drop-pickup, and <i>m</i>-visit using constraint programming. \\nTransp. Res. Pt. C-Emerg. Technol. 91, 1-14. \\nHopfield, J.J., Tank, D.W., 1985. “Neural” computation of decisions in optimization problems. Biological \\nCybernetics 52(3), 141-152. \\nJoshi, C.K., Laurent, T., Bresson, X., 2019. An efficient graph convolutional network technique for the \\ntravelling salesman problem. arXiv preprint arXiv:1906.01227. \\nKarimi-Mamaghan, M., Mohammadi, M., Meyer, P., Karimi-Mamaghan, A.M., Talbi, E., 2022. Machine \\nlearning at the service of meta-heuristics for solving combinatorial optimization problems: A state-\\nof-the-art. European Journal of Operational Research 296(2), 393-422. \\nKitjacharoenchai, P., Ventresca, M., Moshref-Javadi, M., Lee, S., Tanchoco, J.M.A., Brunese, P.A., 2019. \\nMultiple traveling salesman problem with drones: Mathematical model and heuristic approach. \\nComputers & Industrial Engineering 129, 14-30. \\nKong, F.H., Jiang, B., 2024. Delivery optimization for collaborative truck-drone routing problem \\nconsidering vehicle obstacle avoidance. Computers & Industrial Engineering 198, 14. \\nKool, W., Van Hoof, H., Welling, M., 2018. Attention, learn to solve routing problems! arXiv preprint \\narXiv:1803.08475. \\nKuo, R.J., Lu, S.H., Lai, P.Y., Mara, S.T.W., 2022. Vehicle routing problem with drones considering time \\nwindows. Expert Systems with Applications 191, 19. \\nLaporte, G., 1992. The traveling salesman problem: An overview of exact and approximate algorithms. \\nEuropean Journal of Operational Research 59(2), 231-247. \\nLi, K., Zhang, T., Wang, R., 2021. Deep Reinforcement Learning for Multiobjective Optimization. IEEE \\nTransactions on Cybernetics 51(6), 3103-3114. \\nLi, Y., Liu, M., Jiang, D.D., 2022. Application of Unmanned Aerial Vehicles in Logistics: A Literature \\nReview. Sustainability 14(21), 18. \\nLiu, M., Wang, Z., Li, J., 2022. A deep reinforcement learning algorithm for large-scale vehicle routing \\nproblems, International Conference on Electronic Information Technology (EIT 2022). SPIE, pp. \\n\\n \\n34 \\n \\n824-829. \\nLu, H., Zhang, X., Yang, S., 2019. A learning-based iterative method for solving vehicle routing problems, \\nInternational conference on learning representations. \\nLu, J., Liu, Y.M., Jiang, C.M., Wu, W.W., 2025. Truck-drone joint delivery network for rural area: \\nOptimization and implications. Transp. Policy 163, 273-284. \\nMa, Q., Ge, S., He, D., Thaker, D., Drori, I., 2019. Combinatorial optimization by graph pointer networks \\nand hierarchical reinforcement learning. arXiv preprint arXiv:1911.04936. \\nMahmoudinazlou, S., Kwon, C., 2024. A hybrid genetic algorithm with type-aware chromosomes for \\nTraveling Salesman Problems with Drone. European Journal of Operational Research 318(3), 719-\\n739. \\nMontaña, L.C., Malagon-Alvarado, L., Miranda, P.A., Arboleda, M.M., Solano-Charris, E.L., Vega-\\nMejía, C.A., 2021. A novel mathematical approach for the Truck-and-Drone Location-Routing \\nProblem, 3rd International Conference on Industry 4.0 and Smart Manufacturing (ISM). Elsevier \\nScience Bv, Upper Austria Univ Appl Sci, Hagenberg Campus, Linz, AUSTRIA, pp. 1378-1391. \\nMurray, C.C., Chu, A.G., 2015. The flying sidekick traveling salesman problem: Optimization of drone-\\nassisted parcel delivery. Transp. Res. Pt. C-Emerg. Technol. 54, 86-109. \\nNazari, M., Oroojlooy, A., Snyder, L., Takác, M., 2018. Reinforcement learning for solving the vehicle \\nrouting problem. Advances in neural information processing systems 31. \\nNguyen, M.A., Dang, G.T.H., Hà, M.H., Pham, M.T., 2022. The min-cost parallel drone scheduling \\nvehicle routing problem. European Journal of Operational Research 299(3), 910-930. \\nPeng, B., Wang, J., Zhang, Z., 2019. A deep reinforcement learning algorithm using dynamic attention \\nmodel for vehicle routing problems, International Symposium on Intelligence Computation and \\nApplications. Springer, pp. 636-650. \\nPoikonen, S., Golden, B., Wasil, E.A., 2019. A Branch-and-Bound Approach to the Traveling Salesman \\nProblem with a Drone. INFORMS J. Comput. 31(2), 335-346. \\nPoikonen, S., Wang, X.Y., Golden, B., 2017. The vehicle routing problem with drones: Extended models \\nand connections. Networks 70(1), 34-43. \\nPourmohammadreza, N., Jokar, M.R.A., Van Woensel, T., 2025. Last-mile logistics with alternative \\ndelivery locations: A systematic literature review. Results in Engineering 25, 104085. \\nRave, A., Fontaine, P., Kuhn, H., 2023. Drone location and vehicle fleet planning with trucks and aerial \\ndrones. European Journal of Operational Research 308(1), 113-130. \\nSacramento, D., Pisinger, D., Ropke, S., 2019. An adaptive large neighborhood search metaheuristic for \\nthe vehicle routing problem with drones. Transp. Res. Pt. C-Emerg. Technol. 102, 289-315. \\nSchermer, D., Moeini, M., Wendt, O., 2018. Algorithms for Solving the Vehicle Routing Problem with \\nDrones, In: Nguyen, N.T., Hoang, D.H., Hong, T.-P., Pham, H., Trawiński, B. (Eds.), Intelligent \\nInformation and Database Systems. Springer International Publishing, Cham, pp. 352-361. \\nShi, C., 2022. Pointer Network Solution Pool : Combining Pointer Networks and Heuristics to Solve TSP \\nProblems, 2022 3rd International Conference on Computer Vision, Image and Deep Learning & \\nInternational Conference on Computer Engineering and Applications (CVIDL & ICCEA), pp. 1236-\\n1242. \\nShirzad, H., Velingker, A., Venkatachalam, B., Sutherland, D.J., Sinop, A.K., 2023. EXPHORMER: \\nSparse Transformers for Graphs, 40th International Conference on Machine Learning. Jmlr-Journal \\nMachine Learning Research, Honolulu, HI. \\nSitek, P., Wikarek, J., Jagodzinski, M., 2022. A Proactive Approach to Extended Vehicle Routing Problem \\nwith Drones (EVRPD). Applied Sciences-Basel 12(16), 21. \\nSutskever, I., Vinyals, O., Le, Q., 2014. Sequence to Sequence Learning with Neural Networks, 28th \\nConference on Neural Information Processing Systems (NIPS). Neural Information Processing \\nSystems (Nips), Montreal, CANADA. \\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., \\n2017. Attention Is All You Need, 31st Annual Conference on Neural Information Processing \\nSystems (NIPS). Neural Information Processing Systems (Nips), Long Beach, CA. \\nWang, X.Y., Poikonen, S., Golden, B., 2017. The vehicle routing problem with drones: several worst-\\ncase results. Optimization Letters 11(4), 679-697. \\nWang, Y., Wang, Z., Hu, X.P., Xue, G.Q., Guan, X.Y., 2022. Truck-drone hybrid routing problem with \\ntime-dependent road travel time. Transp. Res. Pt. C-Emerg. Technol. 144, 27. \\nWang, Y., Yang, X.X., Chen, Z.B., 2023. An Efficient Hybrid Graph Network Model for Traveling \\nSalesman Problem with Drone. Neural Process. Lett. 55(8), 10353-10370. \\nWu, G.H., Mao, N., Luo, Q.Z., Xu, B.J., Shi, J.M., Suganthan, P.N., 2022. Collaborative Truck-Drone \\n\\n \\n35 \\n \\nRouting for Contactless Parcel Delivery During the Epidemic. Ieee Transactions on Intelligent \\nTransportation Systems 23(12), 25077-25091. \\nXu, Y.Q., Fang, M., Chen, L., Xu, G.Y., Du, Y.L., Zhang, C.Q., 2022. Reinforcement Learning With \\nMultiple Relational Attention for Solving Vehicle Routing Problems. Ieee Transactions on \\nCybernetics 52(10), 11107-11120. \\nYu, V.F., Lin, S.W., Jodiawan, P., Lai, Y.C., 2023. Solving the Flying Sidekick Traveling Salesman \\nProblem by a Simulated Annealing Heuristic. Mathematics 11(20), 21. \\nZhang, K., He, F., Zhang, Z.C., Lin, X., Li, M., 2020. Multi-vehicle routing problems with soft time \\nwindows: A multi-agent reinforcement learning approach. Transp. Res. Pt. C-Emerg. Technol. 121, \\n14. \\nZhang, S., Liu, S.L., Zhang, W.Y., 2023. Vehicle routing problems with time windows under the \\ncollaborative delivery mode of electric vehicle-drone. Chinese Journal of Management Science. \\nZhao, J.X., Mao, M.J., Zhao, X., Zou, J.H., 2021. A Hybrid of Deep Reinforcement Learning and Local \\nSearch for the Vehicle Routing Problems. Ieee Transactions on Intelligent Transportation Systems \\n22(11), 7208-7218. \\nZhou, J., Yu, Q., Xue, Z.M., Yang, L.B., 2025. Research on the Route Planning Problem of Drone and \\nTruck Collaborative Delivery in Restricted Areas. IEEE Access 13, 33062-33073. \\nZhuang, J.T., Tang, T., Ding, Y.F., Tatikonda, S., Dvornek, N., Papademetris, X., Duncan, J.S., 2020. \\nAdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients, 34th Conference on \\nNeural Information Processing Systems (NeurIPS). Neural Information Processing Systems (Nips), \\nElectr Network. \\n \\n \\n',\n",
       " 'The Causal Round Trip: Generating Authentic\\nCounterfactuals by Eliminating Information Loss\\nRui Wu\\nwurui22@mail.ustc.edu.cn\\nSchool of Management, University of Science and Technology of China\\n96 Jinzhai Road, Hefei, 230026, Anhui, China\\nLizheng Wang\\nlzwang@ustc.edu.cn\\nSchool of Management, University of Science and Technology of China\\n96 Jinzhai Road, Hefei, 230026, Anhui, China\\nYongjun Li ∗\\nlionli@ustc.edu.cn\\nSchool of Management, University of Science and Technology of China\\n96 Jinzhai Road, Hefei, 230026, Anhui, China\\nAbstract\\nJudea Pearl’s vision of Structural Causal Models (SCMs) as engines for counterfactual\\nreasoning hinges on faithful abduction: the precise inference of latent exogenous noise.\\nFor decades, operationalizing this step for complex, non-linear mechanisms has remained\\na significant computational challenge. The advent of diffusion models, powerful universal\\nfunction approximators, offers a promising solution. However, we argue that their standard\\ndesign, optimized for perceptual generation over logical inference, introduces a fundamen-\\ntal flaw for this classical problem: an inherent information loss we term the Structural\\nReconstruction Error (SRE). To address this challenge, we formalize the principle of\\nCausal Information Conservation (CIC) as the necessary condition for faithful abduc-\\ntion. We then introduce BELM-MDCM, the first diffusion-based framework engineered\\nto be causally sound by eliminating SRE by construction through an analytically in-\\nvertible mechanism. To operationalize this framework, a Targeted Modeling strategy\\nprovides structural regularization, while a Hybrid Training Objective instills a strong\\ncausal inductive bias. Rigorous experiments demonstrate that our Zero-SRE framework\\nnot only achieves state-of-the-art accuracy but, more importantly, enables the high-fidelity,\\nindividual-level counterfactuals required for deep causal inquiries. Our work provides a\\nfoundational blueprint that reconciles the power of modern generative models with the\\nrigor of classical causal theory, establishing a new and more rigorous standard for this\\nemerging field.\\nKeywords:\\nCausal Inference, Diffusion Models, Causal Information Conservation, Struc-\\ntural Causal Models, Counterfactual Generation, BELM, Structural Reconstruction Error\\n1 Introduction\\nThe fundamental challenge of causal inference, as articulated by Rubin (1974), is our in-\\nability to simultaneously observe an individual’s potential outcomes. Generating authentic\\ncounterfactuals is thus the field’s grand challenge. Structural Causal Models (SCMs), in-\\ntroduced by Pearl (2009), provide the formal language for this pursuit. An SCM posits\\nthat an outcome Vi is generated by a function of its parents Pai and a unique exogenous\\n∗. Corresponding author.\\n©2025 Rui Wu, Lizheng Wang, and Yongjun Li.\\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/.\\narXiv:2511.05236v1  [cs.LG]  7 Nov 2025\\n\\nWu, Wang, and Li\\nnoise variable Ui. This noise, Ui, represents the primordial causal information—the\\ncollection of unobserved factors unique to an individual. This concept aligns directly with\\nthe long-standing focus in econometrics on unobserved individual heterogeneity, a\\ncentral challenge in structural modeling for decades (Heckman, 2001). Pearl’s framework\\nfor causal reasoning, the Abduction-Action-Prediction cycle, hinges on the fidelity of the\\nfirst step: abduction. To answer any ”what if” question, one must first perfectly infer this\\nprimordial information Ui from an observed outcome vi. For decades, while this theoretical\\nblueprint was clear, its practical realization for complex, non-linear mechanisms remained\\na major computational hurdle, often addressed in econometrics through strong parametric\\nassumptions or linear approximations (Angrist and Pischke, 2008).\\nThe advent of deep generative models, particularly diffusion models (Ho et al., 2020),\\noffers a powerful new hope for bridging this gap. As near-universal function approxima-\\ntors, they possess the expressive power to learn the complex, non-linear functions that\\nhave long challenged classical methods (Chao et al., 2023; Sanchez and Tsaftaris, 2022).\\nHowever, this promise is shadowed by a critical, yet overlooked, ”impedance mismatch.”\\nThese models were engineered for perceptual tasks like image synthesis, where visual plau-\\nsibility is paramount, not for the logical rigor demanded by causal abduction. We argue\\nthat their standard design, which relies on approximate inversion schemes like DDIM (Song\\net al., 2021a), is fundamentally at odds with the strict requirements of this classical causal\\nproblem.\\nIn this work, we diagnose and resolve this conflict. We begin by giving the classic re-\\nquirement for faithful abduction a modern name: Causal Information Conservation\\n(CIC)1. Our core contribution is the identification that standard diffusion models system-\\natically violate this principle due to an inherent algorithmic flaw. We formalize this flaw\\nas the Structural Reconstruction Error (SRE)—a quantifiable information loss that\\nimposes a hard theoretical ceiling on the fidelity of any counterfactual generated by such\\nmethods. The SRE is not an estimation error to be solved with more data, but a structural\\ndefect in the tool itself.\\nTo solve the long-standing challenge of operationalizing faithful abduction, we introduce\\nBELM-MDCM. It is not merely a new model, but the first diffusion-based framework re-\\nengineered from first principles to be causally sound. Architected around an analytically\\ninvertible sampler (Liu et al., 2024), it is the first Zero-SRE causal framework by\\nconstruction.\\nThis design choice reconciles the expressive power of modern diffusion\\nmodels with the logical rigor of Pearl’s causal theory, ensuring the abduction step is lossless.\\nOur primary contributions are therefore:\\n(i) Diagnosing a Fundamental Barrier in a Classic Problem. We are the first to\\nidentify that standard diffusion models, when applied to the classic problem of SCM\\nabduction, suffer from a structural flaw we term the Structural Reconstruction\\nError (SRE), which violates the foundational principle of Causal Information\\nConservation.\\n1. In this work, ’Causal Information Conservation’ is defined operationally as the lossless, deterministic\\nrecovery of the exogenous noise variable U.\\nIts novelty lies in its application as a design principle\\nand diagnostic tool for the diffusion model paradigm in causality, rather than as a formal information-\\ntheoretic quantity. Connecting this operational principle to formal measures, such as mutual information,\\nis a compelling avenue for future research.\\n2\\n\\nThe Causal Round Trip\\n(ii) Proposing the First Causally-Sound Diffusion Framework.\\nWe introduce\\nBELM-MDCM, the first framework to eliminate SRE by design. By leveraging an\\nanalytically invertible mechanism, it ensures that the power of diffusion models can\\nbe applied to causality without compromising the integrity of the abduction process.\\n(iii) Developing a Principled Methodology to Operationalize the Framework.\\nTo make our Zero-SRE framework practical and robust, we introduce two synergistic\\ninnovations: a Targeted Modeling strategy to manage complexity and a Hybrid\\nTraining Objective to provide a strong causal inductive bias, both supported by\\nour theoretical analysis.\\nThrough a comprehensive experimental evaluation, we demonstrate that BELM-MDCM\\nnot only sets a new state-of-the-art in estimation accuracy but, more critically, unlocks\\nthe generation of authentic individual-level counterfactuals for deep causal inquiries. By\\nproviding a foundational blueprint that resolves a core tension between modern machine\\nlearning and classical causal theory, our work establishes a new, more rigorous standard for\\nthis research direction.\\n1.1 The Inversion Challenge in Diffusion-Based Causality\\nDiffusion models (Ho et al., 2020) are powerful generative models that learn to reverse a\\nfixed, gradual noising process. They train a neural network, ϵθ(xt, t), to predict the noise\\ncomponent of a corrupted sample xt by optimizing a simple mean-squared error objective:\\nLsimple(θ) = Et,x0,ϵ\\n\\x14\\r\\r\\rϵ −ϵθ\\n\\x00√¯αtx0 +\\n√\\n1 −¯αtϵ, t\\n\\x01\\r\\r\\r\\n2\\x15\\n(1)\\nwhere ¯αt defines the noise schedule and ϵ ∼N(0, I). This trained network is then used to\\niteratively denoise a variable from pure noise back to a clean sample. A standard determin-\\nistic method for this generative process is the Denoising Diffusion Implicit Model (DDIM)\\n(Song et al., 2021a):\\nxt−1 = √¯αt−1\\n\\x12xt −√1 −¯αtϵθ(xt, t)\\n√¯αt\\n\\x13\\n+\\np\\n1 −¯αt−1 · ϵθ(xt, t)\\n(2)\\nHowever, causal abduction requires the inverse operation: encoding an observed data point\\nx0 into its latent noise code xT . Standard frameworks (Chao et al., 2023) use the DDIM\\ninversion, which only approximates this path:\\nxt+1 = √¯αt+1\\n\\x12xt −√1 −¯αtϵθ(xt, t)\\n√¯αt\\n\\x13\\n+\\np\\n1 −¯αt+1 · ϵθ(xt, t)\\n(3)\\nThis inversion is approximate because it relies on the noise prediction ϵθ(xt, t) remain-\\ning constant across the step, which introduces discretization errors that accumulate (Liu\\net al., 2022). This structural flaw, which we term the Structural Reconstruction Er-\\nror (SRE), systematically corrupts the inferred exogenous noise Ui. The initial error in\\nthe abduction step then propagates through the entire Abduction-Action-Prediction cycle,\\ncompromising the fidelity of the final counterfactual.\\n3\\n\\nWu, Wang, and Li\\n1.2 Our Solution: A Zero-SRE Causal Framework\\nTo eliminate SRE by construction, we build our framework upon an analytically invertible\\nsampler: the Bidirectional Explicit Linear Multi-step (BELM) sampler (Liu et al., 2024).\\nBELM overcomes the ”memoryless” limitation of single-step samplers like DDIM by using\\na history of noise predictions, a principle grounded in classical theory for solving ODEs\\n(Hairer and Wanner, 2006).\\nSpecifically, we employ a second-order BELM. During decoding, it computes a more\\nstable effective noise, ϵeff, using predictions from the current and previous timesteps:\\nϵeff = 3\\n2ϵθ(xt, t) −1\\n2ϵθ(xt+1, t + 1)\\n(4)\\nThis improved estimate is then used in a DDIM-like update. The key innovation is that\\nthe corresponding encoding process is constructed to be the exact algebraic inverse of this\\ndecoding process, guaranteeing that the round-trip is lossless, i.e., H(T(x0)) = x0. While\\nthe original work on BELM focused on general generative tasks, we are the first to iden-\\ntify, leverage, and theoretically justify its analytical invertibility as the key to\\nsatisfying the principle of Causal Information Conservation for rigorous counter-\\nfactual generation. Our choice of a second-order BELM represents a deliberate trade-off,\\nproviding substantial accuracy gains over single-step methods while maintaining practical\\nefficiency (Liu et al., 2024), making it ideal for our causal framework.\\n1.3 Methodological Gaps in Applying Invertible SCMs\\nHowever, achieving high-fidelity causal inference requires more than a simple substitution of\\none sampler for another. The principle of analytical invertibility, while theoretically sound,\\nexposes new challenges in practical SCM implementation that our framework is designed\\nto address.\\nThe Challenge of Model Specification: Targeted Modeling.\\nA key decision in SCM\\nconstruction is assigning a causal mechanism to each node. Naively applying a complex,\\ncomputationally expensive BELM-based diffusion model to every node in the causal graph\\nis suboptimal. This motivates our Targeted Modeling strategy, where model complexity\\nis treated as a resource to be allocated judiciously across the graph.\\nThe Challenge of Downstream Tasks:\\nHybrid Training.\\nThe second challenge\\narises from a fundamental mismatch in objectives. A diffusion model is trained on a gen-\\nerative objective, Ldiffusion(θ), while a downstream predictive task is optimized using a dis-\\ncriminative loss, Ltask(ϕ). These two objectives are not aligned. This ”objective mismatch”\\nmotivates our Hybrid Training strategy, which seeks to unify these two goals.\\n2 Theoretical Analysis: An Operator-Theoretic Framework\\nTo formalize our thesis that Causal Information Conservation is paramount and its\\nviolation via Structural Reconstruction Error is a fundamental barrier, we develop\\na rigorous operator-theoretic framework.\\nThis perspective is essential for analyzing the\\nfidelity of the causal mapping process itself, moving beyond simple prediction errors. We\\n4\\n\\nThe Causal Round Trip\\npresent the first formal analysis that decomposes the counterfactual error in diffusion-based\\ncausal models to explicitly isolate the SRE, proving how our Zero-SRE design eliminates\\nthis critical structural limitation.\\nOur analysis first establishes the conditions for perfect counterfactual generation (§2.1-\\n§2.3) and proves that standard methods produce a non-zero SRE, which our sampler elimi-\\nnates by construction (Proposition 5-6; §2.4). The centerpiece is a novel error decomposition\\ntheorem that isolates the SRE, motivating our Zero-SRE design (§2.5-§2.7). We conclude\\nwith learnability guarantees and a discussion of implications for advanced causal tasks like\\ntransportability (§2.8-§2.10).\\n2.1 Problem Formulation and Causal Operators\\nLet (Ω, F, P) be a probability space. We consider endogenous variables V as elements of\\nthe Hilbert space of square-integrable random variables, X := L2(Ω, Rd). Unless otherwise\\nspecified, all vector norms ∥· ∥in the subsequent analysis refer to the standard Euclidean\\n(L2) norm.\\nDefinition 1 (Functional SCM Operator) A Structural Causal Model is defined by a\\nset of unknown, true functional operators {Fi}d\\ni=1, where each Fi : X pai × Ui →Xi is a\\nmap such that Vi := Fi(Pai, Ui), with Pai being the set of parent random variables and Ui\\nan exogenous noise variable. We establish the convention that the corresponding lowercase\\nbold letter, pai, denotes a specific vector of observed values for these parents.\\nOur goal is to learn a model parameterized by θ that approximates this SCM. Our model\\nconsists of a pair of conditional operators for each variable Vi:\\n1. A decoder (generative) operator Hθ : U × X p →X, which aims to approximate\\nF.\\n2. An encoder (inference) operator Tθ : X × X p →U, which aims to perform\\nabduction by inferring the latent noise.\\nThese operators are realized by solving the probability flow ODE (Appendix A). The decoder\\nHθ solves the ODE from t = T to t = 0, while the encoder Tθ solves it from t = 0 to t = T.\\nOur BELM sampler is a high-fidelity numerical solver designed such that these forward and\\nbackward operations are exact algebraic inverses.\\n2.2 Identifiability and Exact Counterfactual Generation\\nWe adapt principles from identifiable generative modeling (Chao et al., 2023) to formalize\\nthe conditions for exact counterfactuals. This requires assuming the SCM is invertible with\\nrespect to its noise term, a condition discussed in Section 2.11.\\nTheorem 2 (Identifiability via Statistical Independence) Given an SCM operator\\nX := F(Pa, U) where U ⊥⊥Pa and F is invertible w.r.t. U. If a learned encoder Tθ\\n(with sufficient capacity) yields a latent representation Z = Tθ(X, Pa) that is statistically\\nindependent of the parents Pa, then Z is an isomorphic representation of the exogenous\\nnoise U.\\n5\\n\\nWu, Wang, and Li\\n2.3 Geometric Inductive Bias for Identifiability\\nThe score-matching objective’s geometric inductive biases strengthen our identifiability ar-\\ngument. We leverage the principle of implicit regularization, where optimizers favor\\n”simpler” functions (Hochreiter and Schmidhuber, 1997; Neyshabur et al., 2018).2\\nThis\\nsuggests the model learns the most parsimonious geometric transformation required to ex-\\nplain the data.\\nConsidering the local geometry of the data density p(x) provides powerful intuition. In\\na local region R, if the data is isotropic (spherically symmetric), the simplest score function\\nis a radial vector field, yielding a conformal map. If the structure is simply anisotropic (e.g.,\\nellipsoidal), the model is biased towards learning a local affine map. This refines the notion\\nof a purely conformal bias and leads to the following proposition.\\nProposition 3 (Implicit Bias towards Simple Geometric Maps) Assume (A1) the\\ntrue data density p(x) is smooth (C2) and (A2) the optimization process has a simplicity\\nbias (e.g., favoring low-complexity solutions, see Appendix H).\\n(i) If there exists a local region R where p(x) is isotropic, the optimal learned score\\nfunction is a radial vector field, and the flow map it generates is a conformal map\\non R.\\n(ii) If we relax the condition to a local region R where p(x) has an ellipsoidal structure,\\nthe optimal learned score function is normal to the ellipsoidal iso-contours, and the\\nflow map it generates is a local affine transformation on R.\\nThe formal argument is detailed in Appendix H. This proposition is significant: it suggests\\nthat the model defaults to learning the most parsimonious, well-behaved, and locally in-\\nvertible map that can explain the data’s geometry. This bias is crucial for the abduction\\nstep, as it prevents the pathological distortions that would corrupt the inferred causal noise\\nU.\\nTheorem 4 (Operator Isomorphism Guarantees Exact Counterfactuals) Let the con-\\nditions of Theorem 2 hold. If the learned operator pair(Tθ, Hθ) constitutes a conditional\\nisomorphism (i.e., Hθ(Tθ(·, pa), pa) = I, the identity operator), then the model’s prediction\\nunder an intervention do(Pa := α) is exact.\\nProof A full proof, covering cases for different dimensions of the exogenous noise variable,\\nis provided in Appendix B.\\n2.4 Analysis of Inversion Fidelity\\nWe now formally analyze the inversion error. We prove that standard approximate schemes\\nproduce a non-zero SRE (Proposition 5), whereas our chosen sampler eliminates it by\\nconstruction (Proposition 6).\\n2. We adopt the principle of simplicity bias, a cornerstone of modern deep learning theory that, while\\nempirically supported, remains an active and not yet universally proven area of research. Our conclusions\\nare conditioned on its validity, as discussed further in Section 2.11.\\n6\\n\\nThe Causal Round Trip\\nProposition 5 (Structural Error of Approximate Inversion) Let TDDIM be the op-\\nerator for one step of DDIM inversion from xt to xt+1, and HDDIM be the generative step\\noperator from xt+1 to xt. The single-step reconstruction error is non-zero and of second\\norder in the time step ∆t:\\n(HDDIM ◦TDDIM)(xt) −xt = O((∆t)2)\\nThis error accumulates over the full trajectory, leading to a non-zero Structural Reconstruc-\\ntion Error.\\nProof See Appendix C for a rigorous proof.\\nProposition 6 (Analytical Invertibility of the Sampler) Let TBELM and HBELM be\\nthe operators corresponding to the full-trajectory BELM sampler for inference and genera-\\ntion, respectively. For a fixed noise prediction network ϵθ, the operators are exact algebraic\\ninverses:\\nHBELM ◦TBELM = I\\nProof\\nThe proof follows from the algebraic construction of the BELM update rules, as\\ndetailed in Appendix C.\\n2.5 Error Decomposition for Counterfactual Estimation\\nThis brings us to our central theoretical result: an error decomposition theorem that rig-\\norously partitions the total counterfactual error. This decomposition isolates the SRE and\\nmathematically demonstrates why its elimination is critical.\\nDefinition 7 (Counterfactual Error Components) We formally define the two pri-\\nmary sources of error in counterfactual estimation for the invertible case:\\n1. The Structural Reconstruction Error (ESR) measures the information loss from\\nthe model’s abduction-action cycle on a given sample X:\\nESR(X) := ∥(Hθ ◦Tθ −I)X∥2\\n2. The Latent Space Invariance Error (ELSI) measures the failure of the learned\\nlatent space to remain invariant under interventions on parent variables:\\nELSI := ∥Tθ(X, Pa) −Tθ(Xtrue\\nα\\n, α)∥2\\nTheorem 8 (Counterfactual Error Bound) Let a model be defined by (Tθ, Hθ) and\\nthe true SCM by F. Assume the decoder Hθ is LH-Lipschitz. The expected squared error\\nof the model’s counterfactual prediction ˆXα is bounded by the expectation of the two error\\ncomponents:\\nE\\nh\\n∥ˆXα −Xtrue\\nα\\n∥2i\\n≤2E\\n\\x02\\nESR(Xtrue\\nα\\n)\\n\\x03\\n+ 2L2\\nHE [ELSI]\\n7\\n\\nWu, Wang, and Li\\nProof The proof is in Appendix D.\\nRemark 9 (Elimination of Structural Error) By Proposition 6, the Structural Re-\\nconstruction Error for BELM-MDCM is identically zero. This is the central theoretical\\nadvantage of our framework. It disentangles the error sources, allowing us to isolate the\\nentire modeling challenge to learning a high-quality score function (ϵθ) without the con-\\nfounding factor of an imperfect inversion algorithm. Any remaining error is now purely a\\nfunction of statistical estimation, not a structural bias of the model itself.\\nProposition 10 (Bound on Latent Space Invariance Error) We assume the learned\\nscore network, ϵθ, is Lipschitz continuous, ensuring the existence and uniqueness of the\\nprobability flow ODE solution via the Picard-Lindel¨of theorem. Under standard integrability\\nconditions (Fubini’s theorem), the Latent Space Invariance Error is bounded by the expected\\nscore-matching loss:\\nE [ELSI] ≤C′ · E\\n\\x02\\n∥ϵθ −ϵ∗∥2\\x03\\nfor some constant C′, where ϵ∗is the true score function.\\nProof The proof is in Appendix D.\\nThis proposition formally establishes that by eliminating structural error, the causal fidelity\\nof BELM-MDCM is directly and provably controlled by its ability to accurately learn the\\ndata’s score function.\\n2.6 Decomposing Error: A Motivation for Empirical Validation\\nThe error decomposition in Theorem 8 provides a clear strategy for empirical validation\\nby isolating two distinct error sources: the Structural Reconstruction Error (ESR)\\nand the Latent Space Invariance Error. While developing a single score combining\\nthese is future work, these components directly motivate our empirical investigations. Our\\nablation study (Section 5.4.2) is designed to measure the impact of a non-zero ESR, while\\nour stress-test (Section 5.4.1) probes robustness when latent space invariance is challenged\\nby a non-invertible SCM.\\n2.7 Theoretical Roles of Targeted Modeling and Hybrid Training\\nWith algorithmic error eliminated by our Zero-SRE design, the challenge becomes minimiz-\\ning the modeling error (ELSI). Our two methodological innovations, Targeted Modeling\\nand Hybrid Training, are principled strategies for this purpose.\\nTargeted Modeling as Formal Complexity Control.\\nOur Targeted Modeling strat-\\negy acts as a form of structural regularization. The finite sample bound in Theorem 15\\nis governed by the Rademacher complexity Rn(FΘ) of the entire SCM’s hypothesis space.\\nBy assigning low-complexity models to a subset of nodes, we directly constrain the overall\\ncomplexity.\\n8\\n\\nThe Causal Round Trip\\nRemark 11 (Effect on Generalization Bound) Our Targeted Modeling strategy is for-\\nmally justified as a complexity control mechanism. The Rademacher complexity of a com-\\nposite SCM is bounded by the sum of the complexities of its individual mechanisms (Mohri\\net al., 2018). By strategically substituting a high-complexity diffusion model Fdiff with a\\nlower-complexity alternative Fsimple for non-critical nodes, Targeted Modeling directly min-\\nimizes this upper bound.\\nThis leads to a tighter generalization bound and improves the\\nstatistical efficiency of the overall SCM.\\nHybrid Training as a Weighted Score-Matching Objective.\\nThe Hybrid Training\\nObjective, Ltotal = Ldiffusion+λ·Ltask, imparts a crucial inductive bias for learning a causally\\nsalient score function. The task-specific loss acts as a conductor’s baton, forcing the model\\nto prioritize learning an accurate score function in regions of the data manifold most critical\\nto the causal question. We formalize this by proposing that the auxiliary loss implicitly\\nimplements a weighted score-matching objective.\\nProposition 12 (Hybrid Objective as a Weighted Score-Matching Regularizer)\\nThe auxiliary task loss Ltask provides a lower bound for the model’s error, weighted by a\\nfunction reflecting the causal salience of the data manifold. Minimizing the hybrid objec-\\ntive Ltotal is thereby equivalent to solving a weighted score-matching problem that prioritizes\\naccuracy in causally salient regions, leading to a smaller effective Latent Space Invariance\\nError. (A rigorous proof is provided in Appendix E.)\\nThis proposition formally grounds our hybrid training strategy, revealing that the task-\\nspecific loss intelligently forces the diffusion model to prioritize accuracy in regions of the\\ndata manifold most critical to the causal question. This reinforces the CIC principle by\\navoiding information loss where it matters most, effectively implementing the simplicity\\nbias principle from Section 2.3.\\nWe can deepen this insight by analyzing its information-theoretic implications.\\nProposition 13 (Disentanglement via Hybrid Objective) Information-theoretically,\\nthe hybrid objective provides a strong inductive bias towards learning a disentangled la-\\ntent representation.\\nIt encourages a ”division of labor” where the task-specific compo-\\nnent explains variance from the parents Pa, while the diffusion component’s latent code\\nZ = Tθ(V, Pa) models the residual information. This implicitly pushes Z towards being\\nindependent of Pa, a crucial step towards satisfying the identifiability conditions.\\nProof A detailed information-theoretic argument is provided in Appendix E.\\n2.8 BELM-MDCM as a Unifying Framework\\nThe principle of Causal Information Conservation also unifies our framework with classical\\nmodels. Simpler models like Additive Noise Models (ANMs) can be seen as special cases\\nwhere this principle is met trivially, positioning our work as a generalization of established\\ncausal principles. For instance, in a classic ANM (Hoyer et al., 2009), Vi = fi(Pai) + Ui,\\nthe noise is recovered by a direct, lossless inversion: Ui = Vi −fi(Pai). Our framework gen-\\neralizes this principle to arbitrarily complex, non-additive mechanisms, offering a flexible,\\n9\\n\\nWu, Wang, and Li\\nnon-parametric extension to classical structural equation models (Wooldridge, 2010). The\\nimportance of noise distributions, particularly non-Gaussianity, for identifiability in linear\\nmodels is also a well-established principle (Shimizu et al., 2006).\\n2.9 Learnability and Statistical Guarantees\\nWe now provide finite-sample learnability guarantees for our SCM framework.\\nProposition 14 (Asymptotic Consistency) Under standard regularity conditions, as\\nthe number of data samples n →∞and model capacity N →∞, the learned operators\\n(ˆTn, ˆHn) are consistent estimators of the ideal operators (T∗, H∗): ˆTn\\np−→T∗and ˆHn\\np−→H∗.\\nTheorem 15 (Finite Sample Bound for Causal Diffusion SCMs) Let an SCM con-\\nsist of d endogenous nodes, with a causal graph having a maximum in-degree of dmax\\nin\\n. As-\\nsume each causal mechanism is implemented by a score network ϵθ that is an L-layer MLP\\nwith ReLU activations, and the spectral norm of each weight matrix is bounded by B. Let\\nthe input space be appropriately normalized. Let the loss function be bounded by M. Then,\\nfor the parameters ˆθn learned from n samples, the excess risk is bounded with probability at\\nleast 1 −δ:\\nR(ˆθn) −R(θ∗) ≤C · d · L · BL ·\\np\\ndmax\\nin\\n+ dembed + 1\\n√n\\n+ M\\nr\\nlog(1/δ)\\n2n\\nwhere C is a constant independent of the network architecture and sample size, and dembed\\nis the dimension of the time embedding.\\nProof The proof, which combines the sub-additivity of Rademacher complexity over the\\nSCM with standard bounds for deep neural networks (Bartlett et al., 2017; Neyshabur et al.,\\n2018), is detailed in Appendix G.\\nRemark 16 (Interpretation of the Bound) This refined bound quantitatively links the\\ngeneralization error to:\\n(i) Causal Complexity (d ·\\np\\ndmax\\nin\\n): The error scales with the number of causal mech-\\nanisms (d) and the graph’s complexity (dmax\\nin\\n), formalizing the intuition that more\\ncomplex causal systems are harder to learn.\\n(ii) Network Complexity (L · BL): The error scales with the depth and spectral norm\\nof the score networks.\\nThis provides direct theoretical grounding for our Targeted\\nModeling strategy, as using simpler models tightens this generalization bound.\\n2.10 Implications for Causal Transportability\\nCausal Information Conservation also provides a foundation for transportability—applying\\nknowledge from a source domain S to a target domain T (Pearl and Bareinboim, 2014).\\nTransportability requires separating invariant causal knowledge from domain-specific mech-\\nanisms. By losslessly recovering the exogenous noise U (the invariant ”causal essence”), our\\nframework achieves this separation by design; the decoders Hθ represent the domain-specific\\nmechanisms. This insight is formalized in the following theorem.\\n10\\n\\nThe Causal Round Trip\\nTheorem 17 (Condition for Lossless Causal Transport) Let a source domain S and\\na target domain T be described by SCMs MS and MT , respectively. Assume the following\\nconditions hold:\\n(i) Shared Structure: Both domains share the same causal graph G and the same ex-\\nogenous noise distributions {pi(Ui)}. The domains differ only in a subset of causal\\nmechanisms Kchanged.\\n(ii) Noise Independence: The exogenous noise variables {Ui}d\\ni=1 are mutually indepen-\\ndent.\\n(iii) Information Conservation: A model (Tθ, Hθ) trained on data from S satisfies the\\nCausal Information Conservation principle, achieving zero Structural Reconstruction\\nError.\\nThen, causal knowledge can be losslessly transported from S to T by re-learning only the\\noperators {Tθk, Hθk} corresponding to the changed mechanisms k ∈Kchanged, while directly\\nreusing all operators for invariant mechanisms.\\nProof The proof is provided in Appendix F.\\n2.11 Discussion of Assumptions\\nOur framework rests on several key assumptions, which we now critically examine.\\nOur geometric inductive bias argument (Proposition 3) rests on the principle of sim-\\nplicity bias.\\nWhile this principle is a cornerstone of modern deep learning theory with\\nsubstantial empirical backing, it remains an active area of research and is not a universally\\nproven theorem. Our conclusions are therefore conditioned on the validity of this powerful\\nbut conjectural assumption.\\nThe cornerstone of our identifiability theory (Theorem 2) is the SCM’s invertibility\\nwith respect to its noise term U. This is a strong assumption; when violated (e.g., by a\\nmany-to-one function), the abduction task becomes ill-posed.\\nTo address this foundational challenge, we provide an exhaustive theoretical treatment\\nin Appendix C. There, we formalize the irreducible ”representational error” and derive a\\ntighter, more general error bound (Theorem 21). More importantly, we propose a concrete\\nmitigation strategy: a novel prior-matching regularizer (Definition 23), theoretically shown\\nto reduce the error by encouraging the learned encoder to approximate the ideal Maximum\\na Posteriori (MAP) solution (Proposition 24).\\nThis highlights a primary contribution:\\neven in the challenging non-invertible case, BELM-MDCM’s zero-SRE design eliminates\\nthe algorithmic error, thereby isolating the more fundamental representational challenge.\\nOur stress-test in Section 5.4.1 empirically confirms this advantage, while validating our\\nregularizer provides a clear direction for future work.\\nOur identifiability proof is dimension-dependent, leveraging Liouville’s theorem for d ≥3\\nand requiring stronger assumptions like asymptotic linearity for the special case of d = 2.\\nOther assumptions, such as Lipschitz continuity of the score network, are mild regularity\\nconditions standard in deep generative model analysis and can be encouraged through\\narchitectural choices like spectral normalization.\\n11\\n\\nWu, Wang, and Li\\n3 Architectural Design and Training\\nThe BELM-MDCM architecture embodies our core principles through a non-monolithic,\\ntheoretically-motivated design. Its central philosophy is Targeted Modeling: judiciously\\nallocating the expressive power of our Zero-SRE CausalDiffusionModel to nodes of causal\\ninterest (e.g., Treatment T, Outcome Y), while using simpler, efficient mechanisms for\\nconfounders, as illustrated in Figure 1. This strategy provides practical complexity control,\\ntightening the generalization bound as established in Theorem 15.\\nW\\nX\\nT\\nY\\nEmpirical Distribution\\nAdditive Noise Model\\nCausalDiffusionModel\\n(BELM-MDCM)\\nCausalDiffusionModel\\n(BELM-MDCM)\\nTargeted Modeling Principle:\\nThe expressive power of the CausalDif-\\nfusionModel is judiciously allocated to\\nkey causal nodes (Treatment T, Outcome\\nY) for high-fidelity counterfactual genera-\\ntion.\\nSimpler, eﬀicient mechanisms (e.g., ANM,\\nEmpirical Distribution) are used for con-\\nfounder nodes (W, X) to ensure stability\\nand eﬀiciency.\\nFigure\\n1:\\nIllustration\\nof\\nthe\\nTargeted\\nModeling\\nPrinciple.\\nThe\\nexpressive\\nCausalDiffusionModel is judiciously allocated to key causal nodes (Treatment T, Out-\\ncome Y) for high-fidelity counterfactual generation. Simpler, efficient mechanisms (e.g.,\\nANM, Empirical Distribution) are used for confounder nodes (W, X) to ensure stability\\nand efficiency.\\nThe internal architecture of the CausalDiffusionModel itself, depicted in Figure 2, is\\nengineered to learn the complex, non-linear mapping vi := fi(pai, ui) with high fidelity.\\n12\\n\\nThe Causal Round Trip\\nǜ푛ǙǑ\\nǜ푐ǅǘ1\\nM\\nǜ푐ǅǘ2\\n50\\n257\\n-3.0\\nwoman\\nǜ푐ǅǘ3\\nStandardScal\\ner\\nOneHotEncod\\ner\\nǜ푛ǙǑ⨁ǜ푐ǅǘ\\nConnection\\nTimestep\\nembedding\\nSelect\\nBELM-\\nMDCM \\nmodule\\nNoisy Target \\nVariable\\nInverse \\nTransformati\\non\\nPre-Processing\\nEmbedding\\nTrain\\nPost-Processing\\nCausal \\nIdentification\\nResults\\nFigure 2: The detailed internal architecture of the CausalDiffusionModel. This diagram\\nillustrates the end-to-end workflow of the causal mechanism designed for key nodes like\\nTreatment T and Outcome Y, detailing the pre-processing, embedding, training, and post-\\nprocessing stages.\\n3.1 Mechanism for Exogenous Nodes\\nExogenous nodes (without parents in the causal graph G) are modeled non-parametrically\\nvia the Empirical Distribution of the observed data. This approach avoids distributional\\nassumptions and provides a robust foundation for the Structural Causal Model (SCM).\\n3.2 Mechanism for Endogenous Nodes: The CausalDiffusionModel\\nFor endogenous nodes Vi, particularly those central to the causal query (treatment, outcome,\\nkey mediators), we employ our bespoke CausalDiffusionModel to learn the functional\\nmapping vi := fi(pai, ui).\\n3.2.1 Conditioning via Parent Node Transformation\\nThe denoising process is conditioned on the parent nodes pai, which are transformed into\\na fixed-dimensional conditioning vector c ∈Rdc. A ColumnTransformer handles heteroge-\\nneous data types: continuous parents are standardized (StandardScaler) to unify scales,\\nwhile categorical parents are one-hot encoded (OneHotEncoder) to prevent artificial or-\\ndinality. The resulting vectors are concatenated into c, which remains constant for a given\\nsample’s diffusion trajectory.\\n3.2.2 The Denoising Process\\nThe core of the CausalDiffusionModel is a denoising network ϵθ(vt, t, c), implemented as a\\nResidual MLP (He et al., 2016). It takes as input the noisy variable vt, a sinusoidal Time\\nEmbedding of timestep t, and the conditioning vector c. Before the diffusion process, the\\ntarget variable Vi is also preprocessed (standardized for continuous values or label-encoded\\n13\\n\\nWu, Wang, and Li\\nfor categorical ones). The denoising process is driven by the BELM sampler, ensuring a\\nmathematically exact and stable inversion path as established in Section 2.\\n3.2.3 Hybrid Training Objective\\nWe introduce a Hybrid Training Objective to reconcile generative fidelity with predictive\\naccuracy. As established in our theoretical analysis (Proposition 12), this is more than a\\nstandard multi-task learning scheme; it acts as a powerful inductive bias, creating a weighted\\nscore-matching objective that prioritizes accuracy in causally salient regions of the data\\nmanifold. The total loss is a linearly weighted combination:\\nLtotal = Ldiffusion + λ · Ltask\\n(5)\\nwhere Ldiffusion is the noise prediction error (Equation 1). The auxiliary loss Ltask is a Mean\\nSquared Error for continuous nodes (Lregression) and a Cross-Entropy loss for discrete nodes\\n(Lclassification).\\n3.2.4 Decoding and Counterfactual Generation\\nFor generation, the BELM sampler produces an output in the normalized space. This is\\nthen mapped back to the original data domain using the inverse transformations of the pre-\\nfitted preprocessors (StandardScaler for continuous, LabelEncoder for categorical). For\\ncategorical outputs, the continuous value is rounded and clipped to the valid class range\\nbefore the inverse mapping, ensuring that generated (counterfactual) data is interpretable\\nand resides in the correct space.\\n4 New Evaluation Metrics for Generative Causal Models\\nThe principle of Causal Information Conservation demands new evaluation dimensions that\\ntraditional metrics like ATE and PEHE cannot capture. An accurate ATE score, for in-\\nstance, could arise from a model with high SRE where individual errors fortuitously cancel\\nout at the population level. To move beyond mere outcome accuracy and directly assess a\\nmodel’s adherence to our foundational principle, we propose a new, theoretically-grounded\\nevaluation framework.\\n4.1 Causal Information Conservation Score (CIC-Score)\\nThe Causal Information Conservation Score (CIC-Score) is a direct empirical di-\\nagnostic for the Structural Reconstruction Error. It quantifies a framework’s adherence to\\nthe CIC principle by disentangling algorithmic information loss (from an imperfect inver-\\nsion process) from modeling error (from the statistical challenge of learning the true causal\\nmechanism). We define the score, bounded in [0, 1], using an exponential formulation:\\nCIC-Score = exp (−(δU + δSRE))\\nThe error components are designed to isolate distinct failure modes:\\n• δU, the Relative Noise Recovery Error, quantifies the modeling error. It measures\\nhow well the trained network approximates the true score function, reflected in the fidelity\\n14\\n\\nThe Causal Round Trip\\nof the recovered noise ˆU versus the ground-truth Utrue:\\nδU = E[∥ˆUscaled −Utrue, scaled∥2]\\nE[∥Utrue, scaled∥2]\\nThis term captures all inaccuracies from finite data and imperfect optimization.\\n• δSRE, the Normalized Structural Error, exclusively quantifies the algorithmic error\\ninherent to the inversion process itself. Its definition is model-dependent to allow for fair\\ncomparisons:\\n– For frameworks with analytical invertibility (e.g., our BELM-MDCM, ANMs), the\\nalgorithm introduces no information loss, so we set δSRE ≡0 by construction. Any\\nobserved reconstruction error is a symptom of modeling error, already captured by\\nδU.\\n– For frameworks relying on approximate inversion (e.g., DDIM), δSRE is empirically\\nmeasured to quantify this inherent algorithmic flaw:\\nδSRE = E[∥(Hθ ◦Tθ −I)X∥2]\\nE[∥X∥2]\\nThis principled decomposition allows the CIC-Score to fairly assess different frameworks by\\nisolating structural design advantages from the universal challenge of model training.\\n4.2 Causal Mechanism Fidelity Score (CMF-Score)\\nA generative causal model’s core promise is to learn true causal mechanisms, not just\\noutcomes. Na¨ıve metrics like pairwise correlations fail to capture the non-linear, multi-\\nvariable, and directional nature of causality. We therefore propose the Causal Mechanism\\nFidelity (CMF) score, a hierarchical framework with two levels of increasing rigor.\\n4.2.1 Level 1 (Pragmatic): The Conditional Mutual Information Score\\n(CMI-Score)\\nThe Conditional Mutual Information (CMI), I(Vi; Vj|Paj \\\\ {Vi}), is a non-parametric, non-\\nlinear measure of the direct influence a parent Vi has on its child Vj after accounting for all\\nother parents. The CMI-Score evaluates whether this influence is preserved. For a single\\nmechanism Vj, it is the average consistency across all parent-child edges:\\nCMI-Score(Vj) =\\n1\\n|Paj|\\nX\\nVi∈Paj\\n\\uf8eb\\n\\uf8ed1 −\\n\\x0c\\x0c\\x0cIobs(Vi; Vj|·) −Icf(Vi; V ′\\nj |·)\\n\\x0c\\x0c\\x0c\\nIobs(Vi; Vj|·) + ϵ\\n\\uf8f6\\n\\uf8f8\\nwhere Iobs and Icf are the CMI values from observational and counterfactual data, respec-\\ntively. The final CMI-Score is the average over all SCM mechanisms.\\n4.2.2 Level 2 (Gold Standard): The Kernelized Mechanism Discrepancy\\n(KMD) Score\\nTo rigorously compare entire conditional distributions, we use the Maximum Mean Dis-\\ncrepancy (MMD) (Gretton et al., 2012), a kernel-based statistical test for distributional\\n15\\n\\nWu, Wang, and Li\\nequality. The KMD-Score applies this test to the conditional distributions p(Vj|Paj) that\\ndefine each causal mechanism, measuring the discrepancy between the learned and observed\\nconditionals. The final score is mapped to a similarity measure in [0, 1]:\\nKMD-Score = exp(−γ · Epaj∼p(Paj)[MMD(p(Vj|paj), pθ(Vj|paj))])\\nwhere γ is a scaling parameter. A score of 1 indicates that the learned conditional mecha-\\nnism is statistically indistinguishable from the observed one.\\nComplementary and Validated Evaluation Metrics.\\nOur proposed metrics comple-\\nment, rather than replace, traditional ones like ATE and PEHE. They evaluate distinct\\nfacets of performance: while ATE/PEHE measure outcome accuracy, the CMF-Score as-\\nsesses mechanism fidelity. This distinction is critical, as a model can achieve a high ATE\\nvia fortuitous error cancellation despite failing to learn the true data-generating process.\\nTo ensure our metrics are practically reliable, we conducted a controlled micro-simulation\\nstudy, detailed in Appendix J. The results provide strong empirical evidence for their\\nvalidity and complementary roles: the CIC-Score acts as a high-sensitivity SRE detector;\\nthe CMI-Score robustly tracks the fidelity of causal associations; and the KMD-Score serves\\nas a final arbiter of distributional similarity. This validation confirms that our evaluation\\nframework offers a more complete, nuanced, and reliable assessment of generative causal\\nmodels.\\n5 Experiments\\nOur empirical evaluation is designed as a comprehensive test of our central thesis: that\\neliminating SRE is a necessary condition for generating authentic counterfactuals and un-\\nlocks analytical capabilities beyond the reach of conventional methods. We structured the\\nstudy as a four-act narrative to rigorously test our claims. Act I establishes our model’s\\nstate-of-the-art predictive fidelity on standard benchmarks. Act II provides a deep diag-\\nnostic analysis, using our proposed metrics as empirical evidence for the destructive effect\\nof SRE. Act III showcases the unique capabilities unlocked by an information-conserving\\nframework. Finally, Act IV validates the framework’s robustness through a series of stress\\ntests and a full ablation study.\\nEvaluation Protocol.\\nFor a rigorous evaluation, we employ two complementary proto-\\ncols. This distinction is crucial, as it separates the assessment of our methodology’s peak\\nperformance from the diagnostic analysis of its components.\\n(a) Ensemble Evaluation for SOTA Performance: To benchmark against state-of-\\nthe-art methods (specifically, ITE estimation in Section 5.1.3), we adopt the standard\\nDeep Ensemble methodology. We train N=5 independent models and report the final\\nmetric (e.g., PEHE) on the ensembled prediction.\\n(b) Individual Model Evaluation for Diagnostic Analysis: In all other experiments\\nwhere the goal is a fair, apples-to-apples architectural comparison or stability assess-\\nment, we report the mean and standard deviation of metrics from individual\\nmodel instances across N=5 runs. This isolates the effect of design choices from the\\ngains of ensembling.\\n16\\n\\nThe Causal Round Trip\\nWe estimate the Average Treatment Effect (ATE) throughout our experiments using a\\nstandard counterfactual imputation procedure, the pseudo-code for which is detailed in\\nAlgorithm 1 in Appendix K.\\nBaseline Estimators.\\nThe Directed Acyclic Graphs (DAGs) for our experiments are\\nshown in Figure 3. We benchmark BELM-MDCM against a suite of baselines from the\\nDoWhy library (Sharma and Kiciman, 2022), spanning classical statistical methods to state-\\nof-the-art machine learning estimators to ensure a comprehensive comparison.\\nW1\\n(Covariate)\\nC1\\n(Categorical Confounder)\\nT\\n(Treatment)\\nY\\n(Outcome)\\nW2\\n(Covariate)\\n(a) PSM Failure Scenario\\nX1\\n(Confounder)\\nT\\n(Treatment)\\nY\\n(Outcome)\\nX2\\n(Confounder)\\nM\\n(Mediator)\\nZ\\n(Instrumental\\nVariable)\\n(b) Ablation Study Scenario\\nConfounders\\n(age, educ, re74, etc.)\\nTreatment\\n(treat)\\nOutcome\\n(re78)\\n(c) Lalonde Confounding\\nStructure\\nFigure 3: Directed Acyclic Graphs (DAGs) for key experiments. (a) A structure designed to\\nchallenge propensity score methods. (b) A mediation structure used for the ablation study.\\n(c) The standard confounding structure assumed for both Lalonde-based experiments.\\n5.1 Act I: Establishing State-of-the-Art Predictive Fidelity\\nWe first establish that our principled design achieves superior predictive fidelity on standard\\ncausal inference benchmarks.\\n5.1.1 Robustness in Non-Linear Confounding Scenarios\\nWe tested our model in a challenging synthetic scenario (Figure 3a) designed with highly\\nnon-linear confounding to cause propensity-based methods to fail. Table 1 shows the results.\\nWhile Causal Forest is exceptionally accurate on this specific DGP, our BELM-MDCM\\nframework secures its position as the second most accurate method, delivering a highly\\nstable and competitive ATE estimate.\\nCrucially, it significantly outperforms the entire\\nsuite of propensity-based methods and powerful estimators like DML in accuracy. The high\\nstandard deviation of DML highlights its unreliability in this context, validating our model\\nas a robust estimator where traditional approaches are compromised.\\n17\\n\\nWu, Wang, and Li\\nTable 1: ATE Estimation on the PSM Failure Scenario (True ATE = 5000). We report the\\nmean ATE and standard deviation across multiple runs.\\nMethod\\nMean ATE ± Std Dev\\nAbsolute Error\\nBELM-MDCM\\n5266.87 ± 197.14\\n266.87\\nCausal Forest\\n4895.77 ± 69.26\\n104.23\\nPropensity Score Stratification\\n5309.38 ± 185.36\\n309.38\\nLinear Regression\\n5348.82 ± 23.23\\n348.82\\nPropensity Score Matching\\n5353.93 ± 191.36\\n353.93\\nInverse Propensity Weighting\\n5385.68 ± 52.03\\n385.68\\nDouble Machine Learning\\n4285.63 ± 550.97\\n714.37\\n5.1.2 Accuracy and Robustness on Real-World Observational Data\\nWe next evaluated our framework on the canonical Lalonde dataset (Lalonde, 1986), a\\nchallenging real-world benchmark with a known RCT ground truth. Table 2 demonstrates\\nthe comprehensive superiority of our BELM-MDCM framework.\\nIt achieved a mean\\nATE estimate of 1567.36 ± 201.62, the lowest error among all methods that correctly\\nidentified the treatment effect’s positive direction.\\nMore critically, the results highlight\\na stark contrast in reliability. Classical methods failed entirely, while the powerful Causal\\nForest baseline suffered from extreme instability (Std Dev of 785.59). In contrast, BELM-\\nMDCM exhibited remarkable robustness, with a standard deviation approximately four\\ntimes lower. This outstanding performance on a canonical benchmark validates that our\\nframework delivers accurate estimates with the consistency essential for trustworthy causal\\ninference.\\nTable 2: ATE Estimation Stability on the Lalonde Dataset (RCT Benchmark ATE ≈1794).\\nResults for all models are reported as Mean ± Standard Deviation across 5 independent\\nruns.\\nMethod\\nATE (Mean ± Std)\\nAbs. Error (Mean)\\nBELM-MDCM\\n1567.36 ± 201.62\\n226.64\\nCausal Forest\\n1085.30 ± 785.59\\n708.70\\nLinear Regression\\n46.33 ± 76.80\\n1747.67\\nPropensity Score Matching\\n-3.96 ± 118.37\\n1797.96\\nPropensity Score Stratification\\n-35.54 ± 81.44\\n1829.54\\nPropensity Score Weighting\\n-122.55 ± 50.51\\n1916.55\\nDouble Machine Learning\\nnan ± nan\\nnan\\n5.1.3 High-Fidelity ITE Estimation and Stability Analysis\\nObjective. We evaluate performance at the individual level using a semi-synthetic version\\nof the Lalonde dataset. This experiment leverages real-world covariates and assumes the\\ncausal structure depicted in Figure 3c. To rigorously assess both accuracy and reliability, we\\n18\\n\\nThe Causal Round Trip\\nfollow our Individual Model Evaluation protocol, reporting the mean and standard deviation\\nof performance across 5 independent runs for each method.\\nResults. The PEHE results, presented in Table 3, confirm the exceptional fidelity and\\nrobustness of our framework. BELM-MDCM achieves the lowest average PEHE score of\\n537.84 and demonstrates remarkable stability with the lowest standard deviation of just\\n60.11. This performance is closely followed by Causal Forest. However, the results also\\nhighlight the instability of other meta-learners; X-Learner, in particular, exhibits extremely\\nhigh variance, with a standard deviation more than three times larger than its competitors,\\nrendering its single-run estimates unreliable.\\nThis highlights the dual advantage of our\\nframework: superior accuracy combined with consistent, trustworthy performance. Figure 4\\nprovides visual confirmation, showing the tight clustering of our model’s ensembled ITE\\nestimates around the ground truth.\\n0\\n1000\\n2000\\n3000\\n4000\\nTrue ITE\\n0\\n1000\\n2000\\n3000\\n4000\\nEstimated ITE (Ensemble)\\nAccuracy of Individual Treatment Effect (ITE) Estimation\\nIndividual Samples (BELM-MDCM Ensemble)\\nPerfect Match (y=x)\\nFigure 4: Accuracy of Individual Treatment Effect (ITE) Estimation on the semi-synthetic\\nLalonde dataset. The plot shows the ensembled estimated ITE from our model versus the\\ntrue ITE. The tight clustering of our model’s estimates (blue dots) around the perfect-match\\nline (red dash) visually demonstrates its low PEHE score.\\n19\\n\\nWu, Wang, and Li\\nTable 3: ITE Estimation Accuracy (PEHE) on the Semi-Synthetic Lalonde Dataset. Results\\nare reported as Mean ± Standard Deviation across 5 independent runs. Lower is better.\\nMethod\\nPEHE Score (Mean ± Std)\\nBELM-MDCM\\n537.84 ± 60.11\\nCausal Forest\\n563.90 ± 73.66\\nS-Learner\\n816.26 ± 79.17\\nX-Learner\\n1546.38 ± 679.09\\nTable 4: Causal Mechanism Fidelity (CMI-Score) on the Semi-Synthetic Lalonde Dataset.\\nResults are reported as Mean ± Standard Deviation across 5 runs. Higher is better.\\nMethod\\nCMI-Score (Mean ± Std)\\nS-Learner\\n0.9905 ± 0.0062\\nBELM-MDCM\\n0.9824 ± 0.0092\\nCausal Forest\\n0.9786 ± 0.0099\\nX-Learner\\n0.9782 ± 0.0145\\nT-Learner\\n0.9555 ± 0.0113\\n5.2 Act II: Uncovering the Accuracy-Invertibility Trade-off\\nWe now conduct the pivotal experiment of our study: a deep diagnostic analysis using\\nour novel CIC-Score to reveal the trade-off between predictive accuracy and mechanism\\ninvertibility. This provides the core empirical evidence for our thesis by comparing three\\nparadigms: our BELM-MDCM (Learned Invertibility), a DDIM variant (Flawed Invertibil-\\nity), and a classic RF-ANM (Assumed Invertibility).\\nThe results in Table 5 decisively validate our framework’s principles.\\nOur BELM-\\nMDCM is the clear leader, achieving the lowest PEHE score (1071.95) with high stabil-\\nity. Critically, its CIC-Score of 0.3679 is orders of magnitude higher than the alternatives,\\nproving its unique ability to learn an invertible mapping that conserves causal informa-\\ntion. In stark contrast, the DDIM-MDCM model exemplifies the failure predicted by our\\ntheory: its near-zero CIC-Score confirms a near-total collapse of causal information due to\\nSRE, leading to unreliable predictions (high PEHE and variance). The classical RF-ANM,\\nwhile structurally invertible, lacks the capacity to learn the true mechanism, resulting in a\\nzero CIC-Score and poor accuracy. This ”Golden Table” experiment underscores that both\\nstructural integrity and powerful modeling capacity are essential for high-fidelity causal\\ninference.\\nThe Likelihood-Fidelity Dilemma: Why Natively Invertible Models Can Fail.\\nTo rigorously test the limits of models that are natively invertible, we conducted a com-\\nprehensive stability analysis on a Conditional Normalizing Flow (NF) baseline, a model\\nclass that satisfies the Causal Information Conservation principle by construction (SRE\\n≡0). Across five independent runs with different random seeds, the NF model consistently\\ndemonstrated successful statistical learning, with its training loss stably converging to a\\nhigh log-likelihood in each instance (a representative example is shown in Figure 5).\\n20\\n\\nThe Causal Round Trip\\nTable 5: The ”Ultimate Golden Table”: A comparative analysis of model classes on pre-\\ndictive accuracy (PEHE) and structural integrity (CIC-Score).\\nThis table includes the\\nNF-SCM baseline, which empirically validates the likelihood-fidelity dilemma. Results are\\nreported as Mean ± Standard Deviation across 5 runs. Lower PEHE is better; higher CIC-\\nScore is better.\\nModel\\nPEHE (Mean ± Std)\\nCIC-Score (Mean ± Std)\\nRF-ANM\\n1533.18 ± 134.24\\n0.0000 ± 0.0000\\nDDIM-MDCM\\n2085.98 ± 788.12\\n0.0065 ± 0.0130\\nNF-SCM\\n442229.96 ± 66963.73\\n0.1572 ± 0.0232\\nBELM-MDCM\\n1071.95 ± 152.11\\n0.3679 ± 0.0000\\n0\\n25\\n50\\n75\\n100\\n125\\n150\\n175\\n200\\nEpoch\\n2\\n0\\n2\\n4\\n6\\nNegative Log-Likelihood Loss\\nTraining Loss Curve\\nFigure 5: The training loss curve for the Conditional Normalizing Flow (NF) baseline.\\nThe smooth, stable convergence to a low negative log-likelihood value indicates a successful\\nstatistical training run. However, this did not correspond to learning the true causal mech-\\nanism, as evidenced by its extremely high PEHE score.\\nHowever, this statistical success was starkly contrasted by a systematic and catas-\\ntrophic failure in the causal task. The model yielded an average PEHE score of 442,229.96\\n± 66,963.73, confirming that its generated counterfactuals were fundamentally incor-\\nrect.\\nThis consistent result provides decisive evidence for a critical challenge we term\\nthe likelihood-fidelity dilemma: a model can perfectly learn to replicate a data distribution\\nwhile remaining completely ignorant of the underlying causal mechanism.\\nThe root of this dilemma is the fundamental mismatch between the optimization ob-\\njective and the causal goal. The maximum likelihood objective incentivizes the NF to find\\nany invertible mapping that transforms the data to a simple base distribution. While an\\ninfinite number of such mappings may be statistically equivalent, only one corresponds\\nto the true, unique causal data-generating process. Without a guiding signal, the NF is\\nmathematically predisposed to learn a causally-incorrect ”statistical shortcut.” This find-\\ning powerfully underscores the contribution of our Hybrid Training Objective. It acts\\n21\\n\\nWu, Wang, and Li\\nas the crucial causal inductive bias that resolves this dilemma, compelling the model to\\nlearn the unique, causally salient structure and enabling valid causal inference where pure\\nlikelihood-based methods, even those with zero SRE, are destined to fail.\\n5.3 Act III: Unlocking Deeper Causal Inquiry with a High-Fidelity Model\\nAn information-conserving model serves as a trustworthy ”world model” for deep causal\\ninquiry. We showcase three applications uniquely enabled by our framework’s high-fidelity\\ncounterfactuals.\\nHeterogeneity Analysis: Conditional ATE (CATE).\\nA reliable ITE model can act\\nas a “causal microscope.” We use it to explore treatment effect heterogeneity by estimat-\\ning the Conditional Average Treatment Effect (CATE) for subpopulations. By averaging\\nthe results from our five independently trained models, we obtain stable and robust esti-\\nmates. Our model’s mean CATE estimates track the true CATE trends with high fidelity\\nacross different education levels, a capability crucial for policy-making. For instance, for\\nindividuals with an education level of 3.0, the estimated CATE was $2562.79 (true CATE:\\n$2280.79). For levels 8.0, 12.0, and 16.0, the estimates were $2092.91 (true: $2118.61),\\n$2253.76 (true: $2384.44), and $2434.89 (true: $2490.21), respectively, demonstrating a\\nclose correspondence to the ground truth.\\nVictim (Index 201)\\nActual Outcome\\nVictim (Index 201)\\nwith Top Responder\\'s \"Luck\"\\n0\\n5000\\n10000\\n15000\\n20000\\n25000\\n30000\\nOutcome (Y)\\n$10,993.98\\n$33,328.30\\nCausal Attribution: The Impact of Exogenous \"Luck\"\\nTop Responder (Index 281)\\nActual Outcome ($4,296.47)\\nFigure 6: Causal Attribution Analysis. This chart shows the counterfactual outcome for\\nthe ’Victim’ if they had possessed the individual-specific exogenous factors of the ’Top\\nResponder’.\\nCausal Attribution: Isolating the Effect of Exogenous Factors.\\nWe conducted a\\ncausal attribution experiment via a counterfactual intervention of the form do(Uvictim :=\\n22\\n\\nThe Causal Round Trip\\nuresponder). Figure 6 shows that our framework can losslessly recover these factors, reveal-\\ning that unobserved exogenous ’luck’3 had a massive and stable causal effect, averaging a\\n+22,334.32 change in the ’Victim’s’ outcome. This capacity for reliable attribution is a\\nunique advantage of our information-conserving framework.\\n0\\n20000\\n40000\\n60000\\n80000\\n100000\\nOutcome (Y)\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\nDensity\\n1e\\n5\\nAudited Group: black=1 (N=371)\\nAverage Fairness Gap: $-1,678.16\\nCounterfactual Fairness Audit for Attribute: \"black\"\\nActual Outcome Distribution (black=1)\\nMean Actual: $8,829.19\\nCounterfactual Outcome Distribution (if black=0)\\nMean Counterfactual: $7,151.03\\n(a) Fairness audit for attribute ’black’.\\n20000\\n10000\\n0\\n10000\\n20000\\n30000\\n40000\\n50000\\n60000\\nOutcome (Y)\\n0\\n1\\n2\\n3\\n4\\n5\\nDensity\\n1e\\n5\\nAudited Group: hisp=1 (N=39)\\nAverage Fairness Gap: $1,862.12\\nCounterfactual Fairness Audit for Attribute: \"hisp\"\\nActual Outcome Distribution (hisp=1)\\nMean Actual: $5,145.18\\nCounterfactual Outcome Distribution (if hisp=0)\\nMean Counterfactual: $7,007.30\\n(b) Fairness audit for attribute ’hisp’.\\nFigure 7: Counterfactual fairness audits reveal significant outcome disparities based on\\nsensitive attributes. The plots show the distribution of actual outcomes for each group\\nversus the distribution of their counterfactual outcomes had their sensitive attribute been\\ndifferent.\\nCounterfactual Fairness Audit.\\nFinally, we applied our framework to a counterfactual\\nfairness audit.\\nOnly a model that faithfully represents the data generating process can\\nreliably answer questions about fairness. Figure 7 reveals significant and stable disparities:\\nour model estimates an average fairness gap of -1,678.16 for the ’black’ attribute and\\n+1,862.12 for the ’hisp’ attribute, demonstrating its capacity as a powerful tool for ethical\\naudits.\\n5.4 Act IV: Final Validation: Stress Tests and Ablation Study\\nWe conclude by subjecting the framework to two final tests: a stress test on a non-invertible\\nSCM and a comprehensive ablation study.\\n5.4.1 Stress Test on a Non-Invertible SCM\\nWe tested our framework’s robustness when the theoretical assumption of an invertible SCM\\nis violated, using a DGP where Y ∝U2\\nY . The results in Table 6 and Figure 8 decisively\\nvalidate our hypothesis. On the definitive metric of individual-level fidelity (PEHE), our\\nzero-SRE BELM framework achieves an error of 0.77, a 44% reduction compared to the\\nSRE-prone DDIM model. This empirically proves that even when the true SCM is non-\\ninvertible, eliminating algorithmic SRE provides a substantial advantage. This result is\\nfully consistent with our theoretical analysis in Appendix C (Theorem 21), where the total\\n3. In this context, the term ’luck’ serves as an intuitive shorthand for the exogenous noise variable U.\\nWithin the Structural Causal Model (SCM) framework, U represents all unobserved, individual-specific\\nfactors (e.g., intrinsic ability, random chance, measurement errors) that, together with the observed\\nparent variables (Pa), determine the final outcome for an individual.\\n23\\n\\nWu, Wang, and Li\\nerror is decomposed into algorithmic, modeling, and representational errors. By eliminating\\nthe algorithmic error (ESR ≡0), our framework’s performance approaches the theoretical\\nlimit set by the other two components.\\nInterestingly, the DDIM sampler achieves a slightly higher KMD-Score. We hypoth-\\nesize that its inherent inversion noise acts as a form of implicit regularization, making\\nthe marginal generated distribution appear closer to the truth, even while individual-level\\ncounterfactuals are less accurate. This highlights the important distinction between distri-\\nbutional fidelity and individual-level causal accuracy.\\nDDIM\\nBELM\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nScore\\n0.973\\n0.740\\nGroup-Level Accuracy (ATE Error)\\nLower is Better\\nDDIM\\nBELM\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\n1.376\\n0.766\\nIndividual-Level Fidelity (PEHE)\\nLower is Better\\nDDIM\\nBELM\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nScore\\n0.980\\n0.994\\nMechanism Fidelity (CMI-Score)\\nHigher is Better\\nDDIM\\nBELM\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n0.907\\n0.830\\nDistributional Fidelity (KMD-Score)\\nHigher is Better\\nRobustness to Many-to-One SCM: BELM vs. DDIM\\nFigure 8: Robustness comparison on the many-to-one SCM. Our zero-SRE framework\\n(BELM) demonstrates significantly superior performance on PEHE.\\nTable 6: Performance on the non-invertible SCM (Y ∝U2). Results are averaged over 5\\nruns (± std). Lower PEHE is better.\\nSampler\\nPEHE(Mean ± Std)\\nKMD-Score(Mean ± Std)\\nBELM (Zero SRE)\\n0.77 ± 0.16\\n0.830 ± 0.009\\nDDIM (with SRE)\\n1.38 ± 0.19\\n0.907 ± 0.023\\n5.4.2 Ablation Study: Deconstructing the Framework’s Success\\nWe conducted a comprehensive ablation study on a challenging synthetic dataset (Figure 3b)\\nto validate the contribution of each core component. The findings, presented in Table 7 and\\nFigure 9, provide unequivocal evidence for our integrated design. The full BELM-MDCM\\n24\\n\\nThe Causal Round Trip\\nmodel establishes the gold standard for both accuracy and stability. The study reveals three\\ncritical insights:\\n• Decisive Role of the Hybrid Objective: Removing it causes a catastrophic decline\\nin performance (400%+ error increase), demonstrating it is a core driver of the causal\\ninductive bias, not merely a fine-tuning mechanism.\\n• Critical Importance of Targeted Modeling: Removing it leads to a complete collapse\\nin model stability (Std Dev explodes from 4.57 to 138.01), validating our theoretical\\nanalysis on complexity control. A judicious allocation of model complexity is paramount\\nfor reproducibility.\\n• Robust Advantage of Exact Invertibility: Replacing BELM with DDIM leads to a\\nclear degradation in both accuracy and stability, confirming that SRE from approximate\\ninversion systematically erodes the quality of the final causal estimate.\\n125\\n150\\n175\\n200\\n225\\n250\\n275\\nMean Estimated ATE (Error bars show ±1 Std Dev)\\nw/o Hybrid Objective\\nw/o Targeted Modeling\\nw/o Exact Invertibility (DDIM)\\nBELM-MDCM (Full Model)\\nATE Estimation Accuracy\\nTrue ATE = 202.29\\n100\\n50\\n0\\n50\\n100\\n150\\n200\\nMean Absolute Error (Lower is Better)\\nw/o Hybrid Objective\\nw/o Targeted Modeling\\nw/o Exact Invertibility (DDIM)\\nBELM-MDCM (Full Model)\\nImpact of Ablation on Absolute Error\\nAblation Study Results (5 Runs) on Challenge Dataset\\nFigure 9: Visualization of the ablation study results. The top panel shows the mean es-\\ntimated ATE relative to the true value (red dashed line), with error bars indicating ±1\\nstandard deviation. The bottom panel highlights the mean absolute error for each configu-\\nration. The full BELM-MDCM model is demonstrably the most accurate and stable.\\nConclusion.\\nThe ablation study confirms that our framework’s three core design prin-\\nciples: analytical invertibility, a hybrid training objective, and targeted model-\\ning—work in synergy. The removal of any single component creates a significant vulnera-\\nbility, validating the integrity and effectiveness of our integrated architectural design.\\n25\\n\\nWu, Wang, and Li\\nTable 7: Ablation study results on a challenging synthetic dataset (True ATE = 202.29),\\nvalidating the necessity of each framework component. The mean and standard deviation\\nare computed over 5 runs.\\nModel Configuration\\nMean Est. ATE (± Std)\\nAbs. Error\\nBELM-MDCM (Full Model)\\n190.16 ± 4.57\\n12.14\\nw/o Exact Invertibility (DDIM)\\n219.98 ± 7.48\\n17.68\\nw/o Hybrid Objective\\n137.77 ± 23.99\\n64.53\\nw/o Targeted Modeling\\n253.20 ± 138.01\\n50.90\\n6 Causal Information Conservation as a Unifying Principle\\nThe principle of Causal Information Conservation extends beyond a foundation for our\\nmodel; it offers a unifying lens—a new taxonomy—for analyzing the suitability of any\\ngenerative model for individual-level causal inference. Applying this principle and its oper-\\national metric, SRE, allows us to situate our work and clarify its unique advantages within\\nthe broader landscape.\\n6.1 Normalizing Flows: Natively Information-Conserving\\nNormalizing Flows (NFs) (Dinh et al., 2014, 2017; Kingma and Dhariwal, 2018) are designed\\naround an invertible mapping. By construction, their Structural Reconstruction Error\\nis identically zero, making NFs a native implementation of the Causal Information Con-\\nservation principle. However, their strengths come with limitations: the requirement\\nof a tractable Jacobian imposes heavy architectural constraints, which can limit expressive\\npower and introduce strong topological assumptions on the data manifold.\\n6.2 VAEs and GANs: Architecturally High-SRE\\nVariational Autoencoders (VAEs) (Kingma and Welling, 2014) and Generative Adversar-\\nial Networks (GANs) (Goodfellow et al., 2014) are fundamentally ill-suited for this task.\\nTheir Structural Reconstruction Error is large and non-zero due to a fundamental ar-\\nchitectural mismatch, as their separate encoder and decoder networks lack any structural\\nguarantee of being inverses. Furthermore, their sources of information loss are inher-\\nent to their design; optimization objectives like the ELBO’s KL-divergence term actively\\nencourage lossy compression, theoretically impeding the recovery of precise causal informa-\\ntion.\\n6.3 A Comparative Perspective on Diffusion Models\\nViewed through our principle, diffusion models occupy a unique and compelling space in\\nthis taxonomy. Standard diffusion-based approaches, using samplers like DDIM, aspire\\nto conserve information, but their reliance on approximate inversion results in a non-zero\\nSRE; they are ”aspirational” but ultimately lossy. In contrast, BELM-MDCM (this\\nwork) achieves zero SRE by integrating an analytically invertible sampler, matching the\\ntheoretical purity of Normalizing Flows but without their rigid architectural constraints.\\n26\\n\\nThe Causal Round Trip\\nFurthermore, unlike NFs trained with a generic likelihood objective, our framework’s Hybrid\\nTraining objective provides a causally-oriented inductive bias. BELM-MDCM thus uniquely\\ncombines the rigorous invertibility of NFs with the modeling flexibility and task-specific\\npower of the diffusion paradigm, making it ideally suited for principled, high-fidelity causal\\ninference.\\n7 Conclusion and Future Work\\nThis paper introduced Causal Information Conservation as a guiding principle for\\nthe emerging field of diffusion-based causal inference.\\nOur primary contribution is not\\nthe concept of invertibility itself, but framing it as a foundational design requirement and\\nidentifying the Structural Reconstruction Error (SRE) as the precise, quantifiable\\ncost of its violation.\\nOur proposed framework, BELM-MDCM, serves as a constructive proof of this prin-\\nciple. By being architected around analytical invertibility, it is the first to achieve zero SRE\\nby design, shifting the focus from mitigating numerical errors to upholding a fundamental\\ncausal principle. This work provides a foundational blueprint and a more rigorous standard\\nfor applying the power of diffusion models to the profound challenges of causal inference,\\nreconciling their flexibility with the logical rigor demanded by classical theory.\\n7.1 Limitations and Future Work\\nOur work highlights several avenues for future research:\\n• Handling Non-Invertible SCMs: Our framework excels when the true SCM is\\ninvertible. While our stress test shows robustness when this is violated, developing\\nmodels inherently resilient to such misspecifications is a key challenge. Empirically\\nvalidating the proposed prior-matching regularizer (Appendix C) is a concrete next\\nstep.\\n• Robustness to Graph Misspecification: Like most SCM-based methods (Peters\\net al., 2017), our framework assumes a correctly specified causal graph. Analyzing\\nhow structural errors (e.g., omitted confounders) propagate through our error decom-\\nposition framework is a significant future research direction.\\n• Formalizing CIC within Information Theory: Our work defines CIC as an\\noperational principle.\\nA compelling direction is to formalize it within a rigorous\\ninformation-theoretic framework, for instance by proving that zero SRE maximizes\\nthe mutual information I(U; ˆU) between the true and recovered noise, connecting our\\nwork to rate-distortion theory.\\n• Scalability and Generalizability: While powerful, the BELM sampler is com-\\nputationally intensive.\\nImproving its efficiency for high-dimensional settings is an\\nimportant practical challenge.\\nFurthermore, extending the principles of CIC and\\nzero-SRE modeling to other data modalities, such as time-series or images, represents\\nan exciting frontier.\\n27\\n\\nWu, Wang, and Li\\nAcknowledgments and Disclosure of Funding\\nWe thank the anonymous reviewers for their insightful feedback which significantly improved\\nthe clarity and rigor of this paper.\\nAppendix A. Core Diffusion Model Equations\\nThis appendix provides the essential equations for the diffusion models referenced in this\\nwork. Diffusion models learn a data distribution by training a neural network ϵθ to reverse\\na fixed, gradual noising process.\\nThe model is trained by optimizing a simplified score-matching objective (Ho et al.,\\n2020):\\nLsimple(θ) = Et,x0,ϵ\\n\\x14\\r\\r\\rϵ −ϵθ\\n\\x00√¯αtx0 +\\n√\\n1 −¯αtϵ, t\\n\\x01\\r\\r\\r\\n2\\x15\\n(A.1)\\nwhere ¯αt is a predefined noise schedule. For deterministic generation and inversion, we use\\nthe Denoising Diffusion Implicit Model (DDIM) update step (Song et al., 2021a):\\nxt−1 = √¯αt−1\\n\\x12xt −√1 −¯αtϵθ(xt, t)\\n√¯αt\\n\\x13\\n+\\np\\n1 −¯αt−1ϵθ(xt, t)\\n(A.2)\\nThis process can be viewed as a discretization of a continuous-time probability flow Ordinary\\nDifferential Equation (ODE) (Song et al., 2021b):\\ndx =\\n\"\\n1\\n2\\nd log α(s)\\nds\\nx(s) −1\\n2\\nd log(1 −α(s))\\nds\\np\\n1 −α(s)\\np\\nα(s)\\nϵθ(x(s), s)\\n#\\nds\\n(A.3)\\nWithin our theoretical framework, the encoder operator Tθ corresponds to solving this ODE\\nforward in time (from s = 0 to s = 1), while the decoder operator Hθ corresponds to solving\\nit backward in time (from s = 1 to s = 0).\\nAppendix B. Detailed Proofs for Identifiability (Theorems 2 & 4)\\nThis appendix provides detailed, dimension-specific proofs for the identifiability of the ex-\\nogenous noise U and the subsequent correctness of counterfactual generation. The core\\nchallenge lies in showing that the statistical independence of the latent code Z from the\\nparents Pa is a sufficient condition to establish an isomorphic relationship between Z and\\nU. The mathematical tools required differ based on the dimensionality of U.\\nB.1 The High-Dimensional Case (d ≥3)\\nFor cases where the exogenous noise U has a dimensionality d ≥3, the proof leverages\\nLiouville’s theorem on conformal mappings.\\nProof [Proof of Theorems 2 and 4 for d ≥3] We adapt the proof from identifiable generative\\nmodeling (Chao et al., 2023) to our conditional operator setting.\\nLet Qpa(U) := Tθ(F(pa, U), pa) be the composite function mapping the noise U to the\\nlatent code Z for a given context pa. By assumption, Qpa is invertible and differentiable.\\n28\\n\\nThe Causal Round Trip\\nThe core assumption, Z ⊥⊥Pa, implies that the conditional density pZ(z|pa) must equal a\\nmarginal density pZ(z) that is independent of pa.\\nUsing the change of variables formula, we relate the density of Z to that of U:\\npZ(z|pa) =\\npU(Q−1\\npa(z))\\n\\x0c\\x0cdet JQpa(Q−1\\npa(z))\\n\\x0c\\x0c\\n(B.1)\\nwhere JQpa is the Jacobian of Qpa. Since the left-hand side is independent of pa and pU(·)\\nis a fixed distribution, this imposes a strong constraint on the Jacobian determinant. Under\\nregularity conditions, this implies that the Jacobian JQpa(u) must be a scaled orthogonal\\nmatrix, making Qpa a conformal map.\\nBy Liouville’s theorem, for dimensions d ≥3, any conformal map must be a M¨obius\\ntransformation (a composition of translations, scalings, orthogonal transformations, and\\ninversions). For the map to be well-behaved, it must exclude the inversion component,\\nwhich would introduce singularities.\\nThis is consistent with the regularity of functions\\nrepresentable by neural networks, simplifying the map to an affine form:\\nQpa(u) = Apau + dpa\\n(B.2)\\nwhere Apa is a scaled orthogonal matrix. The argument then uses the independence of the\\ndistribution’s moments and support to show that Apa and dpa must be constant w.r.t. pa.\\nThis leads to the isomorphic relationship Tθ(F(Pa, U), Pa) = AU + d = g(U).\\nThe proof of counterfactual correctness follows directly, as detailed in Chao et al.\\n(2023).\\nThe conditional isomorphism Hθ(Tθ(·, pa), pa) = I combined with the identi-\\nfiability result Tθ(F(pa, u), pa) = g(u) implies that Hθ(g(u), pa) = F(pa, u).\\nThe de-\\ncoder thus perfectly mimics the true causal mechanism, making an intervention exact:\\nˆXα = Hθ(g(u), α) = F(α, u) = Xtrue\\nα\\n.\\nB.2 The One-Dimensional Case (d = 1)\\nFor the one-dimensional case where Liouville’s theorem does not apply, this section provides\\na dedicated proof leveraging properties of 1D functions and a uniform noise assumption from\\n(Chao et al., 2023) that does not sacrifice generality.\\nWe first establish a helper lemma characterizing a specific class of 1D functions.\\nLemma 18 For U, Z ⊂R, consider a family of invertible functions qpa : U →Z for\\npa ∈Xpa ⊂Rd. The derivative expression dqpa\\ndu (q−1\\npa(z)) is a function of z only, i.e., c(z), if\\nand only if qpa(u) can be expressed as\\nqpa(u) = q(u + r(pa))\\nfor some function r and invertible function q.\\nProof\\nThe proof is provided in (Chao et al., 2023). The reverse direction follows from\\ndirect differentiation, while the forward direction uses the inverse function theorem to show\\nthat the inverses spa(z) = q−1\\npa(z) must have the same derivative 1/c(z), implying they can\\n29\\n\\nWu, Wang, and Li\\nonly differ by an additive constant, which yields the desired form after inversion.\\nThis lemma enables the proof of the theorem for the 1D case.\\nTheorem 19 (Identifiability for d = 1) Assume for X ∈X ⊂R and exogenous noise\\nU ∼Unif[0, 1], the SCM is X := f(Pa, U). Assume an encoder-decoder model with encoding\\nfunction g and decoding function h. Assume the following conditions:\\n1. The encoding is independent of the parents, g(X, Pa) ⊥⊥Pa.\\n2. The structural equation f is differentiable and strictly increasing w.r.t. U.\\n3. The encoding g is invertible and differentiable w.r.t. X.\\nThen, g(f(Pa, U), Pa) = ˜q(U) for an invertible function ˜q.\\nProof\\nLet qpa(U) := g(f(pa, U), pa). The conditions on f and g ensure qpa is strictly\\nmonotonic and thus invertible. By the independence assumption, the conditional distri-\\nbution of Z = qpa(U) does not depend on pa. We assume U ∼Unif[0, 1] without loss of\\ngenerality.4 The change of density formula gives:\\npZ(z) =\\npU(q−1\\npa(z))\\n|dqpa\\ndu (q−1\\npa(z))|\\n(B.3)\\nSince pU(u) = 1 on its support and qpa is increasing, the denominator must be independent\\nof pa. This implies dqpa\\ndu (q−1\\npa(z)) = c(z) for some function c. This meets the condition\\nof Lemma 18, allowing us to express qpa(u) = q(u + r(pa)) for some invertible q. Since\\nZ ⊥⊥Pa, its support must also be independent of pa. The support is q([0, 1] + r(pa)),\\nwhich is constant only if the interval [r(pa), 1 + r(pa)] is constant. This requires r(pa)\\nto be a constant, r. Thus, qpa(u) = q(u + r). Defining ˜q(u) = q(u + r), we find that the\\nmapping is solely a function of U, which completes the proof.\\nB.3 The Two-Dimensional Case (d = 2)\\nThe two-dimensional case is a well-known geometric exception, as the group of conformal\\nmaps is infinite-dimensional. Consequently, the proof strategy used for higher dimensions\\nvia Liouville’s theorem does not directly apply. Here, we show that identifiability still holds\\nunder additional, plausible regularity assumptions aligned with our modeling framework.\\nAssumption 1 (Asymptotic Linearity) The composite mapping Qpa(u) : C →C is an\\nentire function (analytic on the whole complex plane) with at most linear growth. That is,\\nthere exist constants A, B such that |Qpa(u)| ≤A|u| + B for all u ∈C.\\n4. For any SCM with a continuous noise E and a strictly increasing CDF FE, X = f(Pa, E) can be\\nre-parameterized to an equivalent SCM X = ˜f(Pa, U), where U = FE(E) ∼Unif[0, 1] and ˜f(·, ·) =\\nf(·, F −1\\nE (·)). The modeling task is then to learn the potentially more complex function ˜f.\\n30\\n\\nThe Causal Round Trip\\nThis assumption reflects a fundamental inductive bias of neural network architectures.\\nStandard activation functions (e.g., ReLU, Tanh) produce functions that cannot exhibit\\nsuper-polynomial growth or essential singularities at infinity, aligning our analysis with the\\nfunction classes our model can represent.\\nAssumption 2 (Non-Rotationally Symmetric Base Noise) The base distribution of\\nthe exogenous noise U is not rotationally symmetric.5\\nProof The proof proceeds in three steps. First, as established previously, the statistical\\nindependence condition Z ⊥⊥Pa implies that the learned mapping Qpa(u) must be a\\nconformal map, and thus an analytic function on C. Second, by Assumption 1, Qpa(u) is\\nan entire function with at most linear growth. The Generalized Liouville’s Theorem states\\nthat an entire function whose growth is bounded by a polynomial of degree k must itself be\\na polynomial of degree at most k. In our case, this implies Qpa(u) must be a polynomial\\nof degree at most one, giving it the affine form:\\nQpa(u) = a(pa)u + b(pa)\\nwhere a, b are complex coefficients that can depend on pa. Third, we use the full statisti-\\ncal independence condition to show that the coefficients a and b must be constant. For the\\ndistribution of Z = a(pa)U + b(pa) to be independent of pa, all of its properties must be\\nconstant. Mean: The mean E[Z|pa] = a(pa)E[U] + b(pa) must be constant. Assuming\\nE[U] = 0 without loss of generality implies b(pa) must be a constant, b. Covariance:\\nBy Assumption 2, the distribution of U is not rotationally symmetric, so its covariance\\nmatrix is not proportional to the identity. Any rotation induced by the phase of a(pa)\\nwould alter the covariance structure of Z. For the distribution of Z to be invariant, the\\nrotation angle (phase) of a(pa) must be constant. Scale: The change of variables formula\\nimplies that the magnitude |a(pa)| must also be constant. Since both the magnitude and\\nphase of a(pa) must be constant, a(pa) must be a constant complex number, a. Therefore,\\nQpa(u) = au + b, which is an isomorphic mapping of u. This completes the proof.\\nAppendix C. Extended Analysis of Inversion Fidelity and Non-Invertible\\nSCMs\\nThis appendix provides a unified analysis of inversion errors.\\nWe first provide rigorous\\nproofs for inversion fidelity (Propositions 5 and 6), then extend our error decomposition\\nframework to the more challenging non-invertible SCM setting.\\nC.1 Proofs for Inversion Fidelity (Propositions 5 & 6)\\nProof [Proof of Proposition 5] The proof proceeds by deriving the explicit one-step recon-\\nstruction error and then analyzing its order of magnitude.\\n5. This assumption is made without loss of generality. For any arbitrary continuous noise E = (E1, E2),\\none can define a new noise U = (FE1(E1), E2), where FE1 is the CDF of the first component. The\\nresulting distribution of U is uniform along its first axis, thus breaking any rotational symmetry. The\\nstructural function F then absorbs this transformation.\\n31\\n\\nWu, Wang, and Li\\n1. Derivation of the One-Step Reconstruction Error.\\nLet the single-step DDIM\\ninversion operator be Tt, mapping an observation xt to x′\\nt+1 using the noise prediction\\nϵt = ϵθ(xt, t). The corresponding generative operator is Ht, which reconstructs x′\\nt from\\nx′\\nt+1 using a new prediction at the new state, ϵ′\\nt+1 = ϵθ(x′\\nt+1, t + 1).\\nBy substituting the formula for x′\\nt+1 into the update for x′\\nt, the single-step reconstruction\\nerror x′\\nt −xt is found to be:\\nx′\\nt −xt =\\n\\x12√\\n1 −¯αt −\\n√¯αt\\n√1 −¯αt+1\\n√¯αt+1\\n\\x13\\n(ϵ′\\nt+1 −ϵt)\\n(C.1)\\nThis error is non-zero if and only if the noise prediction changes after one inversion step,\\ni.e., ϵ′\\nt+1 ̸= ϵt.\\n2. Analysis of the Error’s Order of Magnitude.\\nIn the continuous-time limit with\\ntime step ∆s = 1/T, a Taylor expansion shows that both the coefficient term and the\\ndifference in noise predictions (ϵ′\\nt+1 −ϵt) are of order O(∆s). The one-step reconstruction\\nerror is therefore the product of these two terms:\\nErrorstep = O(∆s) × O(∆s) = O((∆s)2) = O(1/T 2)\\n(C.2)\\nThis local error accumulates over the T steps of the trajectory, resulting in a total accumu-\\nlated error of order O(1/T). For any finite T, this global error is non-zero, constituting the\\nStructural Reconstruction Error (SRE) for DDIM.\\nProof [Proof of Proposition 6] The proof is constructive, following from the exact algebraic\\ninvertibility of the BELM sampler (Liu et al., 2024). For the second-order BELM used in\\nthis work, the one-step decoder is an affine transformation of the form:\\nxt−1 = Atxt + Btϵt + Ctϵt+1\\n(C.3)\\nwhere At, Bt, Ct are schedule-dependent coefficients, ϵt = ϵθ(xt, t), and ϵt+1 = ϵθ(xt+1, t +\\n1).\\nThe full-trajectory decoder, HBELM, is a composition of these one-step affine maps.\\nThe BELM encoder, TBELM, is constructed using a symmetric update rule designed to be\\nthe exact algebraic inverse of the decoder. As rigorously shown by Liu et al. (2024), this\\nconstruction ensures that the composite operator TBELM is the exact inverse of HBELM,\\nassuming the same sequence of noise function evaluations is used for both processes.\\nTherefore, by its algebraic construction, the BELM sampler guarantees a lossless round\\ntrip:\\nHBELM ◦TBELM = I\\n(C.4)\\nThe Structural Reconstruction Error is thus identically zero by construction.\\nC.2 Exhaustive Analysis for the Non-Invertible SCM Setting\\nThis section rigorously extends our framework to the non-invertible case, providing a theo-\\nretical underpinning for the stress test results in §5.4.1.\\n32\\n\\nThe Causal Round Trip\\nC.2.1 Assumptions and Definitions\\nWe formalize the problem with the following.\\nAssumption 3 (Well-Posed Abduction) For any observed (v, pa), the inverse image\\nset U(v,pa) = {u′ ∈U | F(pa, u′) = v} is non-empty, and the Maximum a Posteriori\\n(MAP) solution over this set, given the prior p(U), is unique. This solution defines the\\nideal amortized inverse operator, T∗(v, pa) = arg maxu′∈U(v,pa) p(u′).\\nDefinition 20 (Tripartite Error Sources) In the non-invertible case, we refine the er-\\nror sources into three distinct components:\\n(i) Algorithmic Error (SRE): The error from an imperfect inversion algorithm, ESR :=\\n∥(Hθ ◦Tθ −I)X∥2. For our framework, ESR ≡0.\\n(ii) Modeling Error: The error from imperfectly learning the ideal amortized inverse,\\nEModeling := ∥Tθ(V, Pa) −T∗(V, Pa)∥2.\\n(iii) Representational Error: The fundamental, irreducible error from the SCM’s non-\\ninvertibility, ERep := ∥T∗(V, Pa) −Utrue∥2.\\nC.2.2 A Tighter Error Decomposition\\nWe now present a tighter error bound for the non-invertible case.\\nTheorem 21 (Tighter Counterfactual Error Bound) Let the conditions of Theorem 8\\nhold (Hθ is LH-Lipschitz). The expected squared error of the counterfactual prediction is\\nbounded by:\\nE\\nh\\n∥ˆXα −Xtrue\\nα\\n∥2i\\n≤2E[ESR] + 4L2\\nHE[EModeling] + 4L2\\nHE[ERep]\\n(C.5)\\nProof We decompose the total error ∥ˆXα −Xtrue\\nα\\n∥using the triangle inequality and the\\nideal amortized inverse T∗as an intermediate step. The result follows from applying the\\ninequality (a + b + c)2 ≤2(a2 + b2 + c2), bounding terms with the LH-Lipschitz property of\\nHθ, and taking expectations.\\nC.2.3 Information-Theoretic Interpretation of Representational Error\\nThe Representational Error (ERep) is deeply connected to information conservation. The\\nnon-invertibility of the SCM F means that observing (V, Pa) is not sufficient to uniquely\\ndetermine U. Information-theoretically, this implies the conditional entropy H(U|V, Pa)\\nis greater than zero.\\nRemark 22 (Representational Error as Information Loss) The ideal amortized in-\\nverse T∗yields the mode of the posterior p(U|V, Pa). The expected representational error,\\nE[ERep], can be seen as the expected squared error of this MAP estimator, which is related\\nto the variance and shape of the posterior. Thus, ERep is a direct consequence of the infor-\\nmation about U that is fundamentally lost in the forward causal process, a loss captured by\\nH(U|V, Pa) > 0.\\n33\\n\\nWu, Wang, and Li\\nC.2.4 Theoretical Guarantee for the Mitigation Strategy\\nWe now provide a theoretical justification for the ‘Prior-Matching Regularizer‘ proposed in\\n§7.1.\\nDefinition 23 (Prior-Matching Regularizer) The regularizer is defined as R(Tθ) =\\nE(v,pa)\\n\\x02\\n∥sp(Tθ(v, pa))∥2\\x03\\n, where sp(u) = ∇u log p(u) is the score function of the prior\\ndistribution p(U).\\nProposition 24 (Regularizer Induces Convergence to MAP Solution) Minimizing\\nthe regularizer R(Tθ) provides an inductive bias that encourages the encoder output, ˆu =\\nTθ(v, pa), to lie on a mode of the prior distribution p(U).\\nProof [Proof Sketch] The objective is a form of score matching on the prior. The score\\nfunction sp(u) is zero if and only if u is a stationary point of the log-prior. Minimizing\\nthe expected squared norm of the score at the encoder’s output penalizes the production of\\nlatent codes ˆu in low-probability regions of the prior. This incentivizes the encoder to map\\nobservations to the most probable latent code, thereby encouraging Tθ to approximate the\\nideal MAP estimator T∗.\\nAppendix D. Main Proofs for Theoretical Framework\\nThis appendix provides the proofs for the main theoretical results presented in the text.\\nProof [Proof of Theorem 8] This bound is a direct specialization of the more general bound\\nfor non-invertible SCMs derived in Theorem 21 (Appendix C).\\nFor an invertible SCM, abduction is perfect, meaning the ideal amortized inverse T∗is\\nthe true inverse of the SCM function F. Consequently, the recovered noise is the true noise,\\nT∗(V, Pa) = Utrue, which implies that the Representational Error is identically zero:\\nE[ERep] = 0.\\nIn this context, the Latent Space Invariance Error, ELSI, becomes equivalent to the\\nModeling Error, EModeling. Applying the inequality (a + b)2 ≤2a2 + 2b2, the general bound\\nfrom Theorem 21 reduces to the two terms presented in Theorem 8.\\nProof [Proof of Proposition 10] This proof relies on the identifiability of the true SCM\\n(Theorem 2) and the Lipschitz continuity of the score network ϵθ, which guarantees unique\\nODE solutions via the Picard-Lindel¨of theorem.\\nWe also assume standard integrability\\nconditions (Fubini’s theorem).\\nThe encoder Tθ maps an initial condition x(0) to the terminal state x(T) of the prob-\\nability flow ODE (Eq. A.3). Let xθ(t; x0) and x∗(t; x0) denote the ODE solutions with the\\nlearned score ϵθ and the true score ϵ∗, respectively. The Latent Space Invariance Error is\\nE[ELSI] = E[∥xθ(T; X) −xθ(T; Xtrue\\nα\\n)∥2]. By the triangle inequality:\\n∥xθ(T; X) −xθ(T; Xtrue\\nα\\n)∥≤∥xθ(T; X) −x∗(T; X)∥\\n+ ∥x∗(T; X) −x∗(T; Xtrue\\nα\\n)∥\\n+ ∥x∗(T; Xtrue\\nα\\n) −xθ(T; Xtrue\\nα\\n)∥\\n34\\n\\nThe Causal Round Trip\\nThe middle term is zero under ideal identifiability, as the true encoder T∗maps both an\\nobservation and its true counterfactual to the same underlying noise. We thus only need to\\nbound terms of the form ∥xθ(T; x0) −x∗(T; x0)∥.\\nLet z(t) = xθ(t; x0) −x∗(t; x0).\\nSince the ODE vector field is Lipschitz, applying\\nGr¨onwall’s inequality to the differential of z(t) yields:\\n∥z(T)∥≤\\nZ T\\n0\\neLf(T−t)Cf∥ϵθ(x∗(t), t) −ϵ∗(x∗(t), t)∥dt\\nwhere Lf and Cf are constants from the ODE coefficients. Squaring, taking expectations,\\nand applying Jensen’s inequality leads to:\\nE[ELSI] ≤C′ · Ex,t[∥ϵθ −ϵ∗∥2]\\nwhere the final expectation is the score-matching loss.\\nAppendix E. Proofs for Theoretical Roles of Hybrid Training\\nE.1 Proof of Proposition 12 (Weighted Score-Matching)\\nThis section provides a rigorous proof that the auxiliary task loss Ltask provides a lower\\nbound on a weighted score-matching objective.\\nE.1.1 Preliminaries and Setup\\nProbability Flow ODE.\\nThe generative process is the solution to the reverse-time prob-\\nability flow ODE from t = T to t = 0:\\ndxt = f(t, xt, ϵ(t, xt))dt,\\nxT ∼N(0, I)\\n(F.1)\\nwhere the vector field f is determined by the diffusion scheduler.\\nCore Objects.\\n• ϵθ, ϵ∗: Learned and true score functions.\\n• xθ\\nt, x∗\\nt : ODE trajectories driven by ϵθ and ϵ∗.\\n• xθ\\n0, x∗\\n0: Generated and true (counterfactual) data points at t = 0.\\n• g : Rd →Rk: Downstream prediction function.\\n• Ypred = g(xθ\\n0), Ytrue = g(x∗\\n0).\\n• Ltask = ExT [∥Ypred −Ytrue∥2].\\nAssumptions.\\nWe assume standard regularity conditions for the proof:\\n(A1) The vector field f(t, x, ϵ) is Lipschitz continuous in x and ϵ.\\n(A2) The downstream task function g(x) is Lipschitz continuous and differentiable.\\n(A3) The learned score ϵθ and true score ϵ∗are well-behaved.\\n35\\n\\nWu, Wang, and Li\\nE.1.2 Formal Proposition Statement\\nProposition 25 (Hybrid Objective as a Weighted Score-Matching Regularizer)\\nUnder the regularity assumptions (A1-A3), the auxiliary task loss Ltask provides a lower\\nbound on a weighted score-matching objective:\\nLtask ≥C · ExT ,t\\n\\x02\\nw(x∗\\nt ) · ∥ϵθ(x∗\\nt ) −ϵ∗(x∗\\nt )∥2\\x03\\nwhere C > 0 and the weight function w(x∗\\nt ) measures the sensitivity of the final prediction\\nY to score perturbations along the ideal data generation trajectory.\\nProof The proof proceeds in three main steps.\\nStep 1: Bounding Sample Error by Score Error (Error Propagation).\\nLet z(t) =\\nxθ\\nt −x∗\\nt be the error between the two ODE trajectories. By linearizing the vector field f\\naround the true trajectory, we can analyze the impact of the score perturbation ∆ϵt =\\nϵθ(x∗\\nt ) −ϵ∗(x∗\\nt ).\\nFrom Duhamel’s Principle, the final sample error ∆x0 = z(0) can be\\nexpressed as an integral over the perturbation:\\n∆x0 =\\nZ 0\\nT\\nK(s)∆ϵsds\\n(F.2)\\nwhere the kernel K(s) captures the influence of a score perturbation at time s on the final\\nsample at time 0.\\nStep 2: Linearizing the Task Error.\\nThe error in the downstream prediction is ∆Y =\\ng(xθ\\n0) −g(x∗\\n0). A first-order Taylor expansion gives:\\n∆Y ≈∇xg(x∗\\n0) · ∆x0\\n(F.3)\\nThe task loss is the expected squared norm, Ltask = E[∥∆Y ∥2].\\nStep 3: Deriving the Weighted Relationship.\\nSubstituting the integral form of ∆x0\\ninto the task error approximation and applying the Cauchy-Schwarz inequality for integrals\\nyields a lower bound for the task loss:\\nLtask ≥C · ExT\\n\\x14Z T\\n0\\n∥W(x∗\\n0, s)∥2\\nF · ∥∆ϵs∥2ds\\n\\x15\\n(F.4)\\nwhere ∥· ∥F is the Frobenius norm and the influence operator W(x∗\\n0, s) captures the\\nend-to-end sensitivity from a score perturbation at time s to the final prediction. Rewriting\\nthe expectation gives the final form:\\nLtask ≥C · Et∼U[0,T]Ex∗\\nt\\n\\x02\\nw(x∗\\nt ) · ∥ϵθ(x∗\\nt ) −ϵ∗(x∗\\nt )∥2\\x03\\n(F.5)\\nwhere the weight function is w(x∗\\nt ) := T · ExT |x∗\\nt [∥W(x∗\\n0, t)∥2\\nF ].\\nInterpretation and Conclusion.\\nThe weight function w(x∗\\nt ) is large when the gradient\\nnorm ∥∇xg(x∗\\n0)∥is large—i.e., in causally salient regions where the outcome is highly sensi-\\ntive to the features. The inequality therefore shows that minimizing Ltask provides a lower\\nbound on a score-matching error that is weighted to prioritize accuracy in these causally\\nsalient regions.\\n36\\n\\nThe Causal Round Trip\\nE.2 Argument for Proposition 13 (Latent Space Disentanglement)\\nThis section provides a qualitative, information-theoretic argument for Proposition 13,\\nshowing how the hybrid objective encourages a ”division of labor” that promotes disentan-\\nglement. The SCM posits that an observation V is determined by (Pa, U), and the encoder\\nmaps (V, Pa) to a latent code Z = Tθ(V, Pa), where a perfect encoder would yield Z = U.\\nThe process works as follows: the diffusion loss (Ldiffusion) maximizes the log-likelihood\\nlog pθ(V |Pa), forcing the pair (Pa, Z) to contain all information to reconstruct V , thus max-\\nimizing the mutual information I(V ; (Pa, Z)). Simultaneously, the task loss (Ltask) learns\\na prediction from the parents, capturing all predictive information that Pa has about V and\\nthus maximizing I(V ; Pa). The dual objective must satisfy both constraints. From the\\nchain rule of mutual information, we know I(V ; (Pa, Z)) = I(V ; Pa) + I(V ; Z|Pa). Since\\nLdiffusion maximizes the left-hand side and Ltask captures the first term on the right, the op-\\ntimization incentivizes the latent code Z to model the remaining information, I(V ; Z|Pa).\\nThis leads to a connection to disentanglement: the ideal exogenous noise U is, by\\ndefinition, independent of Pa. By forcing Z to model the residual information, the opti-\\nmization process actively encourages the learned representation Z to be independent of Pa.\\nIn summary, the hybrid objective creates a division of labor: the task-specific head explains\\nthe variance from Pa, while the diffusion process’s latent code Z models the residual. This\\nresidual is, by construction, the information in V orthogonal to Pa, forcing Z to be an\\nempirical approximation of the true, disentangled exogenous noise U, thereby serving the\\nidentifiability condition of Theorem 2.\\nAppendix F. Proof of Theorem on Causal Transportability\\nThis appendix provides the proof for the theorem on lossless causal transportability.\\nProof [Proof of Theorem 17] This proof operates under an idealized setting, assuming a\\nperfectly trained model (i.e., zero statistical error), to isolate the structural properties of\\ntransportability. We assume the learned decoder Hθi is identical to the true mechanism Fi,\\nand its encoder Tθi is its perfect inverse.\\nLet the source and target SCMs be MS and MT , with mechanisms {Fi} and {F′\\ni}\\nrespectively. The set of changed mechanisms is indexed by Kchanged.\\nThe proof relies on the modularity of the SCM, which is guaranteed by the mutually\\nindependent exogenous noises (Condition ii of the theorem). This independence ensures\\nthat a change in one mechanism Fk to F′\\nk does not affect the conditional distributions of\\nother nodes Vj (j ̸= k), given their parents.\\nWe analyze the transportability of operators for each mechanism:\\n1. For invariant mechanisms (j /∈Kchanged): By definition, the true mechanism is\\nunchanged, F′\\nj = Fj. Since the model operators (Tθj, Hθj) perfectly learned Fj in\\nthe source domain, they remain valid for the target domain T and can be directly\\nreused.\\n2. For changed mechanisms (k ∈Kchanged): The original operators (Tθk, Hθk) are\\nnow invalid as they model Fk, not the new mechanism F′\\nk. However, a new operator\\npair (T′\\nθk, H′\\nθk) can be learned from target domain data. This training only requires\\n37\\n\\nWu, Wang, and Li\\nsamples (v′\\nk, pa′\\nk) from domain T and the shared noise distribution pk(Uk) (Condition\\ni). Because the noises are independent, this re-learning process for mechanism k is\\nmodular and does not affect the other invariant mechanisms.\\nThe procedure for adapting the model is therefore modular: freeze all invariant operators\\n{(Tθj, Hθj)}j /∈Kchanged and re-train only those for the changed mechanisms {k ∈Kchanged}\\non target domain data. The resulting adapted model is valid for the target domain T and\\ncan perform abduction on a target individual by applying the correct (reused or re-trained)\\nencoders, thereby losslessly recovering the full vector of exogenous noises. This fulfills the\\ncondition for lossless transport.\\nAppendix G. Proof of the Specific Finite Sample Bound (Theorem 15)\\nThis derivation combines standard generalization bounds with known complexity bounds\\nfor deep neural networks. We first state a key lemma regarding the complexity of neural\\nnetworks.\\nLemma 26 (Rademacher Complexity of Neural Networks) Let Fϵ be the function\\nclass of an L-layer MLP with ReLU activations, input dimension p, and weight matrices\\n{Wj}L\\nj=1. Assume the input data X is contained within a ball of radius RX. If the spectral\\nnorm of each weight matrix is bounded, ∥Wj∥2 ≤Bj, the Rademacher complexity of the\\nfunction class is bounded by:\\nRn(Fϵ) ≤Cnet\\nRXL√p\\n√n\\n\\uf8eb\\n\\uf8ed\\nL\\nY\\nj=1\\nBj\\n\\uf8f6\\n\\uf8f8\\nFor simplicity and under normalization, we often consider RX = 1. This result is a sim-\\nplified form derived from (Bartlett et al., 2017; Neyshabur et al., 2018), where Cnet is a\\nuniversal constant.\\nProof\\nThe proof proceeds by combining the standard excess risk bound with the two\\nlemmas above.\\nFirst, from standard learning theory, the excess risk is bounded by the\\nRademacher complexity of the total loss function class:\\nR(ˆθn) −R(θ∗) ≤4Rn(FLSCM) + M\\nr\\nlog(1/δ)\\n2n\\nNext, by the sub-additivity property of Rademacher complexity, we decompose the SCM’s\\ncomplexity into the sum of its individual components:\\nRn(FLSCM) ≤\\nd\\nX\\ni=1\\nRn(FLi)\\nThe next step is to relate the loss complexity to the network complexity. The score-matching\\nloss for mechanism i is Li = ∥ϵ −ϵθi(·)∥2. Since this loss is Lipschitz with respect to the\\n38\\n\\nThe Causal Round Trip\\noutput of ϵθi, its Rademacher complexity is upper-bounded by a constant multiple of the\\ncomplexity of the score network’s function class Fϵi via Talagrand’s contraction lemma,\\ni.e., Rn(FLi) ≤C1 · Rn(Fϵi). We then apply Lemma 26 to bound the complexity of each\\nscore network.\\nThe input dimension pi for mechanism i is pi = dim(xt) + dim(pai) +\\ndim(time embedding). Assuming univariate variables, this is pi = 1 + |Pai| + dembed. Let\\ndmax\\nin\\n= maxi |Pai|, so the maximum input dimension is pmax = 1+dmax\\nin\\n+dembed. Assuming\\nall networks have depth L and a uniform spectral norm bound B, we have:\\nRn(Fϵi) ≤Cnet\\nL√pmax\\n√n\\nBL = Cnet\\nL\\np\\n1 + dmax\\nin\\n+ dembed\\n√n\\nBL\\nFinally, we assemble the complete bound by substituting all components back into the\\ninitial inequality:\\nR(ˆθn) −R(θ∗) ≤4\\nd\\nX\\ni=1\\nRn(FLi) + M\\nr\\nlog(1/δ)\\n2n\\n≤4\\nd\\nX\\ni=1\\nC1 · Rn(Fϵi) + M\\nr\\nlog(1/δ)\\n2n\\n= (4C1Cnet) d · L · BL ·\\np\\n1 + dmax\\nin\\n+ dembed\\n√n\\n+ M\\nr\\nlog(1/δ)\\n2n\\nBy defining C = 4C1Cnet as a generic constant independent of the network architecture and\\nsample size, we arrive at the final form stated in the theorem.\\nAppendix H. Formal Analysis of the Geometric Inductive Bias\\nThis section provides the formal argument for Proposition 3, demonstrating how the score-\\nmatching objective, under a simplicity bias, compels the learned generative map to adopt\\nthe local data geometry, yielding a parsimonious and well-behaved transformation.\\nH.1 Preliminaries and Definitions\\nLet M ⊂Rd be the data manifold with a smooth probability density p(x). The true score\\nfunction is the vector field s∗(x) := ∇x log p(x). We learn a parameterized score network\\nsθ(x) by minimizing the score-matching objective LSM(θ) = Ex∼p(x)[∥sθ(x) −s∗(x)∥2].\\nThe generative process is described by the probability flow ODE, whose vector field\\nf(x, t) is a function of the score. The map Hθ : Rd →M is the flow map of this ODE\\nintegrated from t = 1 to t = 0. A map is conformal if its Jacobian is a scaled orthogonal\\nmatrix, and affine if it is a linear transformation plus a translation.\\nH.2 Assumptions\\nAssumption 4 (Smoothness of the Data Density) The true data density p(x) is at\\nleast twice continuously differentiable (C2) on M.\\n39\\n\\nWu, Wang, and Li\\nAssumption 5 (Adoption of the Simplicity Bias Principle) Our analysis relies on\\nthe principle of implicit regularization: the conjecture that optimizers like SGD favor solu-\\ntions with a simplicity bias. We formalize this as a preference for score functions sθ with\\nlower complexity (e.g., lower Dirichlet Energy or smaller spectral norms) (Hochreiter and\\nSchmidhuber, 1997; Neyshabur et al., 2018).\\nH.3 Proof of Proposition 3\\nThe proof proceeds in four steps:\\nStep 1: The Irrotational Property of the True Score Field. By definition, the\\ntrue score s∗(x) is the gradient of the scalar potential log p(x). By a fundamental theorem\\nof vector calculus, the curl of any gradient field is zero. Thus, s∗is an irrotational (or\\nconservative) vector field.\\nStep 2: The Variational Perspective of Score Matching. The simplicity bias (As-\\nsumption 5) reinforces the irrotational nature of the learned field sθ. This is understood via\\nthe Helmholtz decomposition, which splits any vector field into irrotational (curl-free) and\\nsolenoidal (divergence-free) components. The simplicity bias conjectures that the optimiza-\\ntion preferentially minimizes the energy of the solenoidal component, driving the learned\\nfield sθ towards a purely irrotational solution (∇×sθ ≈0) to match the conservative nature\\nof the true score s∗.\\nStep 3: The Geometry of the Score Field under Local Structural Assump-\\ntions. We analyze the score field’s structure under two local geometric cases for the density\\np(x) in a region R. Case (i): Local Isotropy. If p(x) is locally spherically symmet-\\nric around a center c, its value depends only on the radius r = ∥x −c∥. The gradient\\ns∗= ∇log p(x) must point along the radial direction, making the true score a radial vector\\nfield. The learned field converges to this simple structure. Case (ii): Local Ellipsoidal\\nStructure. If the iso-contours of p(x) are concentric ellipsoids, the gradient s∗must be\\northogonal to these ellipsoidal surfaces at every point. Such a field is still irrotational but\\nno longer radial.\\nStep 4:\\nThe Geometry of the Flow Map.\\nThe ODE vector field f(x, t) is a\\nlinear combination of the score field sθ∗(x) and the position vector x. The geometry of\\nthe resulting flow map Hθ∗depends on this vector field’s geometry.\\nResult for Case\\n(i): In the isotropic case, f(x, t) is also a radial vector field. A flow generated by a radial\\nvector field is, by its rotational symmetry, necessarily a conformal map, as it preserves\\nangles. Result for Case (ii): In the ellipsoidal case, the flow must transform the isotropic\\nlatent space into the anisotropic data space. The simplest such transformation is a local\\naffine transformation, involving direction-dependent scaling and rotation. While this\\ngeometric argument is standard, a fully rigorous proof would require directly computing\\nthe Jacobian of the integrated flow map Hθ∗to verify it satisfies the required mathematical\\nconditions.\\nThe model’s inductive bias thus compels it to learn the most parsimonious\\ngeometric transformation necessary to explain the local data geometry, promoting the well-\\nbehaved, locally invertible mappings that are crucial for abduction.\\n40\\n\\nThe Causal Round Trip\\nAppendix I. Experimental Details\\nThis appendix provides comprehensive details for all experiments presented in Section 5,\\nensuring full transparency and reproducibility.\\nI.1 General Setup\\nSoftware Environment.\\nAll experiments were conducted in a unified software environ-\\nment to ensure full reproducibility. Key library versions used were: dowhy (0.12), econml\\n(0.16.0), numpy (1.26.4), pandas (1.3.5), scikit-learn (1.6.1), torch (1.10.0), lightgbm\\n(4.6.0), and networkx (3.2.1). All stochastic processes were controlled with a fixed global\\nrandom seed (42), except for the ensemble runs, which used a set of distinct seeds for\\ntraining.\\nBaseline Estimator Configurations.\\nAll baseline estimators were implemented using\\nthe dowhy library. For machine learning-based methods like Causal Forest and DML, we\\nutilized their robust implementations from the econml library. To ensure a fair and repro-\\nducible comparison against established benchmarks, we used their default hyperparameter\\nsettings, which are widely recognized and have been optimized by the library authors to\\nprovide strong performance across a broad range of tasks. Our proposed model, in turn, un-\\nderwent a systematic grid search; the final parameters and a detailed analysis are provided\\nin Appendix I.2.\\nDetails for PSM Failure Scenario (Act I).\\nThe Data Generation Process (DGP) for\\nthis experiment (N = 5000, true ATE τ = 5000) follows the graph in Figure 3a and is\\ndefined by:\\nW1, W2 ∼N(0, 1)\\nC1 ∼Categorical(softmax(z)),\\nwhere\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nzA = W1 −W2\\nzB = cos(πW1) + sin(πW2)\\nzC = W 2\\n1 −W 2\\n2\\nlogit(P(T = 1)) = 2 sin(πW1) + 1.5W 2\\n2 + 2W1W2 −1.5 · I[C1 = A] + 2.5 · I[C1 = B] + UT\\nY = 5000 · T + 60 · (15W1 −25W2 + 10W1W2)\\n+ 60 · (−40 · I[C1 = A] + 50 · I[C1 = C]) + UY\\nwith exogenous noises UT ∼Logistic(0, 1) and UY ∼N(0, 60002).\\nDetails for Lalonde Benchmark (Act I).\\nTo evaluate performance on real-world data,\\nwe used the canonical Lalonde dataset, analyzing the effect of the NSW job training pro-\\ngram (treat) on 1978 real earnings (re78). The assumed causal structure is the standard\\nconfounding model (Figure 3c), a DAG structure that reflects the broad consensus in the\\ncausal inference community for this benchmark. In line with our Targeted Modeling prin-\\nciple, we applied the expressive CausalDiffusionModel only to the key treat and re78\\nnodes, modeling all confounders non-parametrically via their empirical distributions.\\nDetails for Semi-Synthetic Analysis (Act II).\\nThis dataset uses the real-world co-\\nvariates from the Lalonde dataset as a foundation. The outcome Y and the ground-truth\\n41\\n\\nWu, Wang, and Li\\nIndividual Treatment Effect (ITEtrue) are then synthetically generated according to the\\nfollowing structural equations:\\nYbase = 2 · Xre74 + 1.5 · Xre75 + 100 · Xeduc −50 · Xage\\n+ 2000 · Xblack −1000 · Xhisp + Ubase\\nITEtrue = 1500 + 350 log(1 + Xeduc) −3(Xage −40)2\\n+ 1200 · (1 −Xnodegr) · (1 −Xblack)\\n−1000 tanh\\n\\x12Xre74 −µre74\\n1000\\n\\x13\\nY = Ybase + ITEtrue · Xtreat\\nwhere Ubase ∼N(0, 5002) and µre74 is the mean of the re74 covariate.\\nDetails for the Stress Test on Non-Invertible SCMs (Act IV).\\nThis experiment’s\\nprimary objective is to evaluate the framework’s robustness when the core theoretical as-\\nsumption of SCM invertibility is explicitly violated. The DGP (N = 2000) is defined as\\nfollows:\\nW ∼U(−2, 2)\\nP(T = 1|W) = σ(W + 0.5W 2)\\n(where σ(·) is the sigmoid function)\\nUY ∼N(0, 1.52),\\nY = 5T + 2W + U2\\nY\\nThe true ATE is exactly τ = 5.0.\\nThe SCM was structured with W as an Empirical-\\nDistribution, and T and Y as CausalDiffusionModel.\\nDetails for the Ablation Study (Act IV).\\nThe ablation study was conducted on\\na challenging synthetic mediation dataset (N = 4000) designed to highlight the benefits\\nof generative models. The causal graph is shown in Figure 3b, with the following data\\ngeneration process:\\nX1 ∼N(0, 1),\\nX2 ∼U(−2, 2),\\nZ ∼Bernoulli(0.5)\\nT ∼Bernoulli (σ (2.0 sin(πX1)X2 −1.5Z + UT ))\\nM = 5 tanh(X2) + I[T = 1](15 cos(2πX2) + 5X1) + I[T = 0](−10|X1|) + UM\\nY = 25M + 10 sinc(2X1) + UY\\nwhere σ(·) is the sigmoid function, UT ∼Logistic(0, 0.3), UM ∼N(0, 1.52), and UY is\\ndrawn from a Gaussian Mixture Model conditioned on Z. The true ATE for this DGP is\\napproximately 202.29.\\nBELM-MDCM (Full Model): The complete proposed framework. The nodes T, M, and\\nY are all modeled by a CausalDiffusionModel with sampler type=’belm’ and a hybrid\\nobjective weight of λ = 5.0.\\nw/o Analytical Invertibility: Identical to the full model, but the sampler type for all\\ndiffusion models was set to ’ddim’.\\n42\\n\\nThe Causal Round Trip\\nw/o Hybrid Objective: Identical to the full model, but the hybrid objective weight λ\\nfor all diffusion models was set to 0.0.\\nw/o Targeted Modeling: The key mediator node M was modeled with a simpler gcm.\\nAdditiveNoiseModel (backed by an LGBMRegressor), while T and Y remained as Causal-\\nDiffusionModels with λ = 5.0 and sampler type=’belm’.\\nI.2 Model Hyperparameter Justification\\nThe hyperparameters for our BELM-MDCM model, reported in Table 9, were identified\\nthrough a systematic grid search for each experiment. The variation in these parameters\\nreflects principled adaptations to different data characteristics, guided by the trade-off be-\\ntween generative fidelity and discriminative accuracy, as well as the signal-to-noise ratio\\n(SNR) of the underlying causal relationships. Below, we provide a holistic analysis for each\\nscenario.\\nI.2.1 Hyperparameter Search Details\\nTo ensure full reproducibility, we detail the hyperparameter search space used to arrive at\\nthe configurations in Table 9. The final values for each experiment were selected based on the\\nbest performance on a held-out validation set, typically comprising 20-30% of the training\\ndata. The primary selection criterion was the lowest PEHE score for experiments with a\\nground-truth ITE (e.g., Semi-Synthetic), or the best balance of low absolute ATE error and\\nlow estimation variance for observational data scenarios. For the Lalonde experiment in\\nparticular, we prioritized a configuration that demonstrated superior stability, as detailed\\nin the analysis below.\\nTable 8: Hyperparameter Search Space and Selection Criteria.\\nHyperparameter\\nSearch Space (Grid Search)\\nSelection Criterion\\nHybrid Weight λ\\n{0.0, 0.1, 0.3, 1.0, 2.0, 5.0, 10.0}\\nBest balance of low\\nerror and low variance\\non the validation set.\\nGuidance Weight w\\n{0.0, 0.1, 0.2, 0.3, 1.0, 5.0, 10.0}\\nDiffusion Timesteps T\\n{50, 100, 200, 500}\\nLearning Rate\\n{5e-5, 1e-4, 1.1e-4, 1.2e-4, 2e-4}\\nScenario 1: The ”Perfectionist Learner” (PSM Failure Experiment)\\nData Pro-\\nfile: This synthetic dataset features complex, non-linear functions (e.g., sin, cos) but is\\ncharacterized by a pure, high-SNR signal. The main challenge is to perfectly learn this\\nintricate generative process. Hyperparameter Strategy: The strategy prioritizes gener-\\native modeling. A low hybrid weight (λ = 0.1) directs the model to focus on the diffusion\\nloss. Since the conditional signal is strong, classifier-free guidance is disabled (w = 0.0).\\nA large model capacity and extended training are employed to capture the ground-truth\\nfunctions.\\nScenario 2:\\nThe ”Pragmatic Signal Extractor” (Lalonde Experiment)\\nData\\nProfile: This real-world data is characterized by a weak, noisy signal and a small sam-\\n43\\n\\nWu, Wang, and Li\\nTable 9: Consolidated BELM-MDCM Hyperparameters for All Key Experiments. Column\\nheaders correspond to the following experiments: PSM (PSM Failure), Lalonde, Semi-\\nSynth (Semi-Synthetic), Ablation (Ablation Study), and Stress Test (Act IV).\\nHyperparameter\\nPSM\\nLalonde\\nSemi-Synth\\nAblation\\nStress Test\\nNumber of Epochs\\n1500\\n1000\\n1200\\n700\\n500\\nBatch Size\\n128\\n64\\n64\\n128\\n128\\nHidden Dimension\\n512\\n512\\n768\\n768\\n256\\nLearning Rate\\n1 × 10−4\\n1 × 10−4\\n1.1 × 10−4\\n1 × 10−4\\n1 × 10−4\\nDiffusion Timesteps (T)\\n200\\n200\\n50\\n200\\n200\\nHybrid Weight λ\\n0.1\\n2.0\\n2.0\\n5.0\\n0.5\\nGuidance Weight (w)\\n0.0\\n1.0\\n0.1\\n0.2\\n0.0\\nple size. The primary challenge is to robustly extract the causal signal. Hyperparameter\\nStrategy: The priority shifts from pure accuracy to a balance of accuracy and stability. A\\nhigh hybrid weight (λ = 2.0) provides a strong inductive bias towards the predictive task.\\nCrucially, our systematic grid search revealed that high guidance weights (w > 1.0) dra-\\nmatically increased estimation variance, leading to unreliable results. We therefore selected\\na moderate guidance weight of w = 1.0. This configuration provides a stabilizing effect,\\nguiding the model towards the causal signal without amplifying noise, thereby achieving\\nthe optimal balance between accuracy and the robustness required for reliable inference on\\nreal-world data.\\nScenario 3: The ”Precision Artist” (Semi-Synthetic Experiment)\\nData Profile:\\nThis setup presents a hybrid-SNR environment, using noisy real-world covariates but a\\npure, synthetic outcome function. The task demands high precision. Hyperparameter\\nStrategy: The diffusion timesteps are reduced (T = 50), plausibly because a shorter\\ngenerative path better preserves the fine-grained details of the synthetic ITE function. The\\nhybrid weight is high (λ = 2.0) to focus on the estimation task, while guidance is light\\n(w = 0.1) as the ITE signal is cleaner than in the purely observational Lalonde case.\\nScenario 4: The ”Component Analyst” (Ablation Study)\\nData Profile: This\\nis a complex, synthetic mediation structure with a clean, high-SNR signal, specifically\\ndesigned to isolate the performance impact of individual framework components. Hyper-\\nparameter Strategy: The strategy is to create a strong, stable baseline. A high hybrid\\nweight (λ = 5.0) heavily orients the model towards the predictive task, making any per-\\nformance degradation from ablations more pronounced. A large model capacity (Hidden\\nDim=768) ensures the model can learn the complex functions, while light guidance (w = 0.2)\\nprovides a minor stabilizing effect.\\nScenario 5: The ”Robustness Tester” (Stress Test)\\nData Profile: A simple DGP\\nfeaturing a non-invertible causal mechanism (Y ∝U2), designed to test the framework’s\\nbehavior when its core assumption is violated. Hyperparameter Strategy: The goal\\nis stable learning of a simple function.\\nA moderate hybrid weight (λ = 0.5) balances\\ngenerative and predictive learning, while a smaller model (Hidden Dim=256) is sufficient for\\n44\\n\\nThe Causal Round Trip\\nthe simpler DGP and helps prevent overfitting. Guidance is turned off (w = 0.0) as the\\nsignal is clean.\\nTable 10: Summary of Adaptive Hyperparameter Strategies.\\nAspect\\nPSM\\nFailure\\nLalonde\\nSemi-\\nSynthetic\\nAblation\\nStudy\\nStress Test\\nData Signal\\nPure &\\nComplex\\nNoisy &\\nWeak\\nHybrid &\\nComplex\\nPure &\\nMediated\\nPure & Non-\\nInvertible\\nGuidance (w)\\n0.0 (Off)\\n1.0 (Balanced\\nGuidance)\\n0.1 (Slight\\nNudge)\\n0.2 (Light)\\n0.0 (Off)\\nTimesteps (T)\\n200\\n(Standard)\\n200\\n(Standard)\\n50 (Short\\nPath)\\n200\\n(Standard)\\n200\\n(Standard)\\nHybrid (λ)\\n0.1\\n(Generative)\\n2.0\\n(Predictive)\\n2.0\\n(Predictive)\\n5.0 (Strongly\\nPredictive)\\n0.5\\n(Balanced)\\nCore Strategy\\nPerfect\\nGeneration\\nRobust\\nPrediction\\nPrecision\\nEstimation\\nComponent\\nIsolation\\nStable\\nLearning\\nHolistic Comparison\\nIn conclusion, the variability in hyperparameters demonstrates the\\nflexibility of our framework. It showcases the model’s ability to deploy different tools—such\\nas prioritizing the generative loss or amplifying weak signals with guidance—to optimally\\nadapt to the specific challenges posed by diverse causal inference problems.\\nI.3 CMF Metric Implementation Details\\nTo ensure the rigor and reproducibility of our evaluation, this section provides the imple-\\nmentation details for the CMF scores.\\nCMI-Score Estimation Method.\\nFor Conditional Mutual Information (CMI) estima-\\ntion, we adopt the widely-used k-Nearest Neighbors (k-NN) based estimator from Kraskov\\net al. (Kraskov et al., 2004). This method is chosen for its strong theoretical properties\\nand practical robustness, particularly for continuous and high-dimensional data, as it avoids\\nexplicit density estimation. Its key advantages include:\\n• Non-parametric: It makes no assumptions about the underlying data distributions.\\n• Data-adaptive: The estimation is based on local distances, adapting to the data\\nmanifold’s geometry.\\n• Robustness: It is more robust than methods that rely on fixed binning, which can\\nbe sensitive to bin size.\\nFollowing common practice, we set the number of neighbors to k = 5 for all our experiments\\nto ensure a stable and reliable estimation.\\nKMD-Score Kernel Parameter Selection.\\nThe performance of the MMD test is sen-\\nsitive to kernel parameter choices. For the KMD-Score, we employed a standard Radial\\nBasis Function (RBF) kernel, k(x, y) = exp(−∥x −y∥2/(2σ2)). The bandwidth parameter\\n45\\n\\nWu, Wang, and Li\\nσ is critical. Following best practices, we set the bandwidth using the median heuristic, a\\nrobust and common data-driven approach. For the analysis on the Lalonde dataset’s re78\\nmechanism, this heuristic yielded a bandwidth of σ = 0.1, a value confirmed as effective in\\npreliminary experiments. All KMD-Scores are reported using this configuration.\\nAppendix J. Empirical Validation of Proposed Evaluation Metrics\\nTo empirically validate the reliability, sensitivity, and complementary nature of our proposed\\nevaluation metrics (CIC-Score, CMI-Score, and KMD-Score), we conducted a controlled\\nmicro-simulation study assessing how each metric responds to a spectrum of increasingly\\nsevere model errors.\\nExperimental Setup.\\nWe designed a simple ground-truth Structural Causal Model\\n(SCM) and created five simulated models (A through E) representing a clear ”degrada-\\ntion gradient” of model quality:\\n• Model A (Oracle): Represents a theoretically perfect model with zero SRE and a\\nperfectly learned causal mechanism. This serves as our gold standard, with expected\\nscores of 1.0.\\n• Model B (Lossy Inverter): Simulates a model with a non-zero SRE. It uses the\\ncorrect causal mechanism but introduces a systematic error during the reconstruction\\ncycle, mimicking the core flaw of DDIM-based approaches.\\n• Model C (Wrong Mechanism): Represents a model that fails to learn the correct\\nfunctional form of the causal mechanism (e.g., learning a linear instead of a sinusoidal\\nrelationship).\\n• Model D (Maximal Error): Simulates a more severe failure where both the causal\\nmechanism and the inferred noise distribution are fundamentally incorrect.\\n• Model E (Total Mismatch): Represents the worst-case scenario where the model\\nignores the causal graph and inputs, generating outputs from an unrelated distribu-\\ntion.\\nAnalysis of Results.\\nThe results (Figure 10) demonstrate the distinct and complemen-\\ntary roles of our proposed metrics:\\n1. The CIC-Score acts as a high-sensitivity ”SRE detector.” It exhibits a dra-\\nmatic drop from a perfect 1.0 (Model A) to approximately 0.23 (Model B) the moment\\nSRE is introduced, while showing less sensitivity to the specific form of mechanism\\nerror.\\nThis confirms its primary role as a diagnostic for adherence to the Causal\\nInformation Conservation principle.\\n2. The CMI-Score serves as a robust ”mechanism association tracker.” It\\ndegrades gracefully and monotonically as the learned causal mechanism deviates from\\nthe ground truth (from 0.99 to 0.76). This demonstrates its utility in quantifying\\nthe fidelity of learned parent-child conditional dependencies.\\n3. The KMD-Score functions as the ”final arbiter” of distributional fidelity.\\nPossessing the widest dynamic range, it is sensitive to all forms of error and provides\\na holistic judgment of the similarity between the generated and true counterfactual\\ndistributions. As the most rigorous metric, it correctly assigns the lowest score to the\\ncompletely mismatched Model E.\\n46\\n\\nThe Causal Round Trip\\nA: Oracle\\nB: Lossy Inverter\\n(SRE Error)\\nC: Wrong Mechanism\\nD: Maximal Error\\nE: Total Mismatch\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nScore Value\\n1.000\\n0.154\\n0.158\\n0.157\\n0.136\\nCIC-Score\\nA: Oracle\\nB: Lossy Inverter\\n(SRE Error)\\nC: Wrong Mechanism\\nD: Maximal Error\\nE: Total Mismatch\\n0.999\\n0.994\\n0.949\\n0.836\\n0.758\\nCMI-Score\\nA: Oracle\\nB: Lossy Inverter\\n(SRE Error)\\nC: Wrong Mechanism\\nD: Maximal Error\\nE: Total Mismatch\\n1.000\\n0.971\\n0.822\\n0.790\\n0.570\\nKMD-Score\\nEmpirical Validation of Proposed Metrics via Micro-simulation\\nFigure 10: Results of the micro-simulation study for metric validation. The three plots\\nshow the response of the CIC-Score, CMI-Score, and KMD-Score to five models (A-E) of\\nprogressively decreasing quality. The scores demonstrate a clear monotonic degradation,\\nconfirming their ability to reliably track model fidelity. Note the CIC-Score’s sharp drop\\nfrom Model A to B, highlighting its specific sensitivity to the Structural Reconstruction\\nError (SRE).\\nThis simulation thus validates our proposed metrics as a reliable and nuanced evaluation\\nframework. They work in synergy to diagnose specific model failings (CIC-Score), assess\\nrelational accuracy (CMI-Score), and provide an overall quality judgment (KMD-Score),\\noffering a more insightful assessment than traditional metrics alone.\\nDiscussion on the Non-Zero Lower Bound of Scores.\\nNotably, even for the worst-\\nperforming model (Model E), the CMI and KMD scores do not fall to zero. This behavior\\nis not a limitation but a desirable feature that reflects their ability to capture ”residual\\nstatistical structure” in the evaluation setting.\\n• For the KMD-Score: The MMD compares the joint distributions Pmodel(Y, W, T)\\nand Poracle(Y, W, T). Crucially, as all models operate on the same observed parent\\ndata, the marginal parent distribution P(W, T) is identical between the two. The\\ndifference lies only in the conditional P(Y |W, T). Because the joint distributions share\\na substantial common subspace, their MMD will be finite, preventing the KMD-Score\\nfrom reaching zero. Furthermore, using a ‘StandardScaler‘ maps both distributions\\nto a similar feature space, which is necessary for a fair, scale-invariant comparison.\\n• For the CMI-Score: This metric quantifies the I(Y ; parent|other parents). Even\\nan incorrect mechanism (like in Model C or D) still generates an output Y as a\\ndeterministic function of its parents, resulting in a non-zero CMI. For Model E, where\\nY is independent of its parents, the theoretically zero CMI is not observed due to the\\ninherent finite-sample variance of the non-parametric k-NN estimator.\\nThis behavior is advantageous, ensuring the metrics provide a meaningful, continuous gra-\\ndient of failure rather than a simplistic binary judgment. This enhances their diagnostic\\npower, allowing for fine-grained distinctions between types and degrees of model imperfec-\\ntion.\\n47\\n\\nWu, Wang, and Li\\nAppendix K. Algorithm for ATE Estimation\\nThis appendix provides the detailed pseudo-code for the counterfactual imputation proce-\\ndure used to estimate the Average Treatment Effect (ATE) in our experiments.\\nAlgorithm 1: ATE Estimation with Invertible SCMs via Counterfactual Imputa-\\ntion\\nInput: Observational data D = {v(j)}N\\nj=1 where v(j) ∈Rd; Causal graph G.\\nOutput: Estimated Average Treatment Effect ( ˆ\\nATE).\\nDefine Invertible SCM Mθ:\\nMθ = {fi(·; θi)}d\\ni=1 based on G, where vi = fi(pai, ui);\\n// 1.\\nTrain the invertible SCM on observational data\\nˆθ ←arg minθ\\nPN\\nj=1\\nPd\\ni=1 Li\\n\\x10\\nfi(pa(j)\\ni ; θi), v(j)\\ni\\n\\x11\\n;\\n// 2.\\nGenerate counterfactual outcomes for each individual\\nfor j ←1 to N do\\ntj ←v(j)\\nT\\n;\\n// Observed treatment\\nu(j) ←M−1\\nˆθ (v(j)) ;\\n// Abduction via invertible BELM encoder\\nyj(1 −tj) ←Predict\\n\\x00Mˆθ, u(j), do(T := 1 −tj)\\n\\x01\\n;\\n// Action & Prediction\\n// Store factual and counterfactual outcomes\\nYj(tj) ←v(j)\\nY ;\\nYj(1 −tj) ←yj(1 −tj);\\nend\\n// 3.\\nCompute the ATE from factual and counterfactual outcomes\\nˆ\\nATE ←1\\nN\\nPN\\nj=1 (Yj(1) −Yj(0));\\nreturn\\nˆ\\nATE;\\nReferences\\nJoshua D. Angrist and J¨orn-Steffen Pischke. Mostly Harmless Econometrics: An Empiri-\\ncist’s Companion. Princeton University Press, 2008.\\nPeter L Bartlett, Dylan J Foster, and Matus Telgarsky.\\nSpectrally-normalized margin\\nbounds for neural networks.\\nIn Advances in Neural Information Processing Systems,\\nvolume 30, 2017.\\nPatrick Chao, Patrick Bl¨obaum, and Shiva Prasad Kasiviswanathan. Interventional and\\ncounterfactual inference with diffusion models. arXiv preprint arXiv:2302.00860, 2023.\\nLaurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent compo-\\nnents estimation. arXiv preprint arXiv:1410.8516, 2014.\\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP.\\nIn International Conference on Learning Representations (ICLR), 2017.\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in\\nNeural Information Processing Systems, volume 27, 2014.\\n48\\n\\nThe Causal Round Trip\\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexan-\\nder Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(25):\\n723–773, 2012.\\nErnst Hairer and Gerhard Wanner. Solving Ordinary Differential Equations II: Stiff and\\nDifferential-Algebraic Problems, volume 14 of Springer Series in Computational Mathe-\\nmatics. Springer Science & Business Media, 2nd edition, 2006.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for\\nimage recognition.\\nIn Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 770–778, 2016.\\nJames J Heckman. Micro data, heterogeneity, and the evaluation of public policy: Nobel\\nlecture. Journal of Political Economy, 109(4):673–748, 2001.\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In\\nAdvances in Neural Information Processing Systems, volume 33, pages 6840–6851, 2020.\\nSepp Hochreiter and J¨urgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42,\\n1997.\\nPatrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Sch¨olkopf.\\nNonlinear causal discovery with additive noise models. In Advances in Neural Information\\nProcessing Systems, volume 21, 2009.\\nDiederik P. Kingma and Prafulla Dhariwal.\\nGlow: Generative flow with invertible 1x1\\nconvolutions. In Advances in Neural Information Processing Systems, volume 31, 2018.\\nDiederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International\\nConference on Learning Representations (ICLR), 2014.\\nAlexander Kraskov, Harald St¨ogbauer, and Peter Grassberger. Estimating mutual informa-\\ntion. Physical Review E, 69(6):066138, 2004.\\nRobert J. Lalonde. Evaluating the econometric evaluations of training programs with ex-\\nperimental data. The American Economic Review, 76(4):604–620, 1986.\\nLuping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion\\nmodels on manifolds. arXiv preprint arXiv:2202.09778, 2022.\\nZimeng Liu, Ziqiong Li, Jian Liu, and Wei Su. BELM: Bidirectional explicit-form linear\\nmulti-step samplers for diffusion models. In The Twelfth International Conference on\\nLearning Representations (ICLR), 2024.\\nMehdi Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learn-\\ning. The MIT Press, second edition, 2018.\\nBehnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. A PAC-\\nBayesian approach to spectrally-normalized margin bounds for neural networks. In In-\\nternational Conference on Learning Representations (ICLR), 2018.\\n49\\n\\nWu, Wang, and Li\\nJudea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press,\\nCambridge, second edition, 2009.\\nJudea Pearl and Elias Bareinboim. Transportability of causal and statistical relations: A\\nformal approach. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial\\nIntelligence, pages 1149–1157, 2014.\\nJonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of Causal Inference:\\nFoundations and Learning Algorithms. Adaptive Computation and Machine Learning\\nseries. The MIT Press, 2017.\\nDonald B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized\\nstudies. Journal of Educational Psychology, 66(5):688–701, 1974.\\nPedro Sanchez and Sotirios A. Tsaftaris. DCMs: Diffusion causal models for counterfactual\\nestimation. arXiv preprint arXiv:2202.10166, 2022.\\nAmit Sharma and Emre Kiciman. DoWhy: An end-to-end library for causal inference. In\\nProceedings of the First Conference on Causal Learning and Reasoning, volume 177 of\\nProceedings of Machine Learning Research, pages 734–747. PMLR, 2022.\\nShohei Shimizu, Patrik O. Hoyer, Aapo Hyv¨arinen, and Antti Kerminen. A linear, non-\\ngaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7:\\n2003–2030, 2006.\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In\\nInternational Conference on Learning Representations (ICLR), 2021a.\\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon,\\nand Ben Poole. Score-based generative modeling through stochastic differential equations.\\nIn International Conference on Learning Representations (ICLR), 2021b.\\nJeffrey M Wooldridge. Econometric Analysis of Cross Section and Panel Data. MIT press,\\n2010.\\n50\\n',\n",
       " 'AI Assisted AR Assembly: Object Recognition and Computer\\nVision for Augmented Reality Assisted Assembly\\nAlexander Htet Kyaw\\nMassachusetts Institute of Technology\\nCambridge, MA, United States\\nalexkyaws@mit.edu\\nHaotian Ma\\nCornell University\\nCornell, NY, United States\\nhm443@mit.edu\\nSasa Zivkovic\\nCornell University\\nIthaca, NY, United States\\nsz382@cornell.edu\\nJenny Sabin\\nCornell University\\nIthaca, NY, United States\\njes557@cornell.edu\\nAbstract\\nWe present an AI-assisted Augmented Reality assembly workflow\\nthat uses deep learning-based object recognition to identify different\\nassembly components and display step-by-step instructions. For\\neach assembly step, the system displays a bounding box around the\\ncorresponding components in the physical space, and where the\\ncomponent should be placed. By connecting assembly instructions\\nwith the real-time location of relevant components, the system\\neliminates the need for manual searching, sorting, or labeling of\\ndifferent components before each assembly. To demonstrate the\\nfeasibility of using object recognition for AR-assisted assembly, we\\nhighlight a case study involving the assembly of LEGO sculptures.\\nCCS Concepts\\n• Human-centered computing →Interaction techniques; Aug-\\nmented reality; • Computing methodologies →Computer vision\\nproblems; Machine learning approaches.\\nKeywords\\nAugmented Reality, Object Recognition, Assembly Instruction, As-\\nsembly Tracking, Feedback-Based Fabrication\\nACM Reference Format:\\nAlexander Htet Kyaw, Haotian Ma, Sasa Zivkovic, and Jenny Sabin. 2025.\\nAI Assisted AR Assembly: Object Recognition and Computer Vision for\\nAugmented Reality Assisted Assembly. In Proceedings of ACM Symposium\\non Computational Fabrication (SCF ’25). ACM, New York, NY, USA, 3 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1\\nIntroduction\\nBy using Augmented Reality (AR), we can overlay digital informa-\\ntion directly onto the physical world, providing 3D instructions\\nthroughout the assembly process. For example, prior work has\\nexplored combining AR assembly tasks with gesture recognition\\n[Kyaw et al. 2024], computer vision [Jahn et al. 2022] and physics\\nsimulation to enhance interaction [Kyaw et al. 2023a].\\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\\nSCF ’25, Cambridge, MA, USA\\n© 2025 Copyright held by the owner/author(s).\\nACM ISBN 978-1-4503-XXXX-X/2025/06\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nFigure 1: Object Recognition for AR Assisted Assembly\\nPrevious research has utilized AI algorithms such as object recog-\\nnition to recognize assembly states, automatically advance AR in-\\nstructions [Stanescu et al. 2023], align virtual model registration\\n[Canadinc and Yan 2024], and verify whether a part has reached its\\ntarget placement location [Kästner et al. 2021].\\nBuilding upon this body of work, our system incorporates ob-\\nject recognition to detect individual parts, associate them with\\ncorresponding digital instructions, and dynamically highlight both\\ntheir current and target spatial regions. This research advances\\nthe field of AR-assisted assembly by introducing a framework that\\ndelivers contextual, stepwise digital guidance aligned with physical\\ncomponents within the assembly space [Ma and Kyaw 2023].\\n2\\nSystem Overview\\nLeveraging deep learning for object recognition, our system is\\ncapable of identifying each component within the assembly area and\\nassociating it with the relevant assembly instruction. The system\\nhighlights the necessary component for every assembly step by\\nshowing a bounding box around the component’s current location\\nand its intended placement location. It then automatically advances\\nto the subsequent step once it detects that the current step has been\\ncompleted. For the purpose of this research, the Microsoft Hololens\\n2 AR headset and Lego components were chosen as the case study\\nto demonstrate the system’s capabilities.\\narXiv:2511.05394v1  [cs.CV]  7 Nov 2025\\n\\nSCF ’25, November 20–21, 2025, Cambridge, MA, USA\\nKyaw, Ma, Zivkovic, and Sabin\\nFigure 2: System overview diagram illustrating the various software components and data -flow of the system\\n2.1\\nObject Recognition in Augmented Reality\\nThe object recognition algorithm is trained on synthetic data repre-\\nsenting eight distinct primitive yellow LEGO components using the\\nYOLOv5 model. These synthetic image datasets are generated using\\na rendering engine that simulates various orientations and lighting\\nconditions to mimic real-world scenarios and improve detection ro-\\nbustness. The HoloLens 2 camera captures the physical workspace\\nand streams video data to a server, where it is split into individual\\nframes for object detection using the YOLOv5 model. The model\\nidentifies 2D bounding boxes around LEGO components in each\\nframe. These bounding boxes are then projected into the AR envi-\\nronment using a homography-based 2D-to-3D planar projection,\\ncomputed from the camera’s pose and field of view. See figure 2 for\\nsoftware implementation details.\\nFigure 3: Synthetic data generation for training object recog-\\nnition algorithm\\nFigure 4: Bounding Box Corresponding to Assembly Step\\n2.2\\nStep by Step 3D Assembly Instruction\\nThe interface displays step-by-step assembly instructions, indicat-\\ning where the user should pick up each component and where to\\nplace it in relation to the rest of the assembly. For each instruction\\nstep, a corresponding 3D bounding box is generated to visually link\\nthe digital instruction with the physical component, highlighting\\nboth its current position and its intended placement. 3D bounding\\nboxes are shown only for components relevant to the current as-\\nsembly step, and each box is annotated with the component type,\\nallowing users to identify both the location and the nature of the\\npart. Since LEGO assemblies are built layer by layer, only the geom-\\netry of the current layer is visualized to reduce cognitive load. In\\nour demonstration, the assembly parts are positioned on the right\\nside of the workspace, while the assembly area is on the left.\\n3\\nResults\\nWe demonstrated the assembly of two distinct LEGO sculptures: an\\nellipsoidal egg and a twisted wall. Using our system, we success-\\nfully assembled both sculptures without referencing any 2D paper\\ndrawings or 3D digital models. Further user studies are planned to\\nquantitatively evaluate the system’s effectiveness. While the cur-\\nrent study is limited to tabletop assemblies, future research could\\nexplore more complex assembly tasks or improve the accuracy of\\nthe AR projections [Kyaw et al. 2023b]. Additionally, future work\\ncould explore using this system to assemble designs generated by\\n3D generative AI. [??]. By bridging assembly instructions with the\\nreal-time localization of components in physical space, this research\\ndemonstrates the potential of object recognition for AI-assisted AR\\nassembly.\\nFigure 5: Artifacts made using AI-Assisted AR Assembly\\n\\nAI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly\\nSCF ’25, November 20–21, 2025, Cambridge, MA, USA\\nReferences\\nSeda Tuzun Canadinc and Wei Yan. 2024.\\nMulti-3D-Models Registration-Based\\nAugmented Reality Instructions for Assembly. In 2024 IEEE Conference on Vir-\\ntual Reality and 3D User Interfaces Abstracts and Workshops (VRW). 997–998.\\ndoi:10.1109/VRW62533.2024.00295\\nGwyllim Jahn, Cameron Newnham, and Nick Berg. 2022. Depth Camera Feedback for\\nGuided Fabrication in Augmented Reality. doi:10.52842/conf.acadia.2024.1.684\\nAlexander Htet Kyaw, Jack Otto, and Leslie Lok. 2023a. Active Bending in Physics-Based\\nMixed Reality: The design and fabrication of a reconfigurable modular bamboo\\nsystem. Graz, Austria, 169–178. doi:10.52842/conf.ecaade.2023.1.169\\nAlexander Htet Kyaw, Lawson Spencer, Sasa Zivkovic, and Leslie Lok. 2024. Gesture\\nRecognition for Feedback Based Mixed Reality and Robotic Fabrication: A Case\\nStudy of the UnLog Tower. In Phygital Intelligence, Chao Yan, Hua Chai, Tongyue\\nSun, and Philip F. Yuan (Eds.). Springer Nature, Singapore, 331–345. doi:10.1007/978-\\n981-99-8405-3_28\\nAlexander Htet Kyaw, Arvin HaoCheng Xu, Gwyllim Jahn, Nick van den Berg, Cameron\\nNewnham, and Sasa Zivkovic. 2023b. Augmented Reality for high precision fabri-\\ncation of Glued Laminated Timber beams. Automation in Construction 152 (Aug.\\n2023), 104912. doi:10.1016/j.autcon.2023.104912\\nLinh Kästner, Leon Eversberg, Marina Mursa, and Jens Lambrecht. 2021. Integrative Ob-\\nject and Pose to Task Detection for an Augmented-Reality-based Human Assistance\\nSystem using Neural Networks. In 2020 IEEE Eighth International Conference on Com-\\nmunications and Electronics (ICCE). 332–337. doi:10.1109/ICCE48956.2021.9352121\\nHaotian Ma and Alexander Htet Kyaw. 2023. AI ASSEMBLY: OBJECT RECOGNITION,\\nCOMPUTER VISION, AND DIGITAL TWIN FOR MIXED REALITY ASSEMBLY.\\nCornell University (May 2023). doi:10.7298/ha3k-4e73\\nAna Stanescu, Peter Mohr, Mateusz Kozinski, Shohei Mori, Dieter Schmalstieg, and\\nDenis Kalkofen. 2023. State-Aware Configuration Detection for Augmented Re-\\nality Step-by-Step Tutorials. In 2023 IEEE International Symposium on Mixed and\\nAugmented Reality (ISMAR). 157–166. doi:10.1109/ISMAR59233.2023.00030 ISSN:\\n2473-0726.\\nReceived 27 Sep 2025\\n',\n",
       " 'SiamMM: A Mixture Model Perspective on Deep\\nUnsupervised Learning\\nXiaodong Wang1, Jing Huang1, Kevin J Liang1\\n1Meta AI\\nRecent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised\\nand unsupervised learning. However, the application of clustering is often heuristic, and the optimal\\nmethodology remains unclear. In this work, we establish connections between these unsupervised\\nclustering methods and classical mixture models from statistics. Through this framework, we demon-\\nstrate significant enhancements to these clustering methods, leading to the development of a novel\\nmodel named SiamMM. Our method attains state-of-the-art performance across various self-supervised\\nlearning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground\\ntruth labels, uncovering potential instances of mislabeling.\\nDate: October 24, 2024\\nCorrespondence: First Author at xiaodongwang@meta.com\\n1\\nIntroduction\\nRecent self-supervised and unsupervised learning methods have demonstrated the ability to learn represen-\\ntations that are surprisingly competitive with fully supervised learning Caron et al. (2020); Chen and He\\n(2021); Misra and van der Maaten (2020); Chen et al. (2020b); Zbontar et al. (2021). Rather than relying on\\nhuman annotations, which can be costly or biased, recent approaches have used Siamese network architectures\\nwith implicit labels: augmentations of the same image instance are considered positive samples, while any\\nother image is considered to be negative. This instance-based approach has proven successful, but ignores the\\nsemantic similarities present in the image data. For example, ImageNet Deng et al. (2009), a commonly used\\ndataset for unsupervised learning, is actually a supervised dataset purposefully collected from a hierarchical set\\nof labels Miller (1995), and even non-curated image data contain examples with similar characteristics Pope\\net al. (2021). Disregarding this structure can lead to similar images repelling each other in the embedding\\nspace, hampering the self-supervised learning process. To address this, several recent works have incorporated\\nclustering strategies Caron et al. (2018, 2020); Li et al. (2021a); Huang et al. (2022b). Although learned\\nclusters have been shown to lead to strong performance in various downstream tasks, several design choices of\\nthese past clustering methods, often based somewhat heuristically on K-means Lloyd (1982), remain unclear.\\nTo enhance our understanding of clustering methods, we leverage a principled statistical foundation and\\nconceptualize clustering-based self-supervised learning as a mixture model (MM). Mixture models characterize\\ndata as a combination of component distributions, with each component representing a distinct sub-population\\nwithin the overall distribution. Given the semantic structure inherent in the image data, we posit that\\nMMs serve as a natural fit for unsupervised representation learning. Depending on whether embeddings are\\nnormalized, we cast clustering in representation learning as either a Gaussian mixture model (GMM) or von\\nMises-Fisher mixture model (vMFMM).\\nExamining previous methodologies through this lens allows us to discern the underlying techniques and\\nperceive improvements within more generalized mixture models. For example, K-means, often employed\\nin cluster-based self-supervised learning through contrastive loss with a large number of clusters, can be\\nviewed as an empirical execution within this broader framework. By relaxing some of these constraints or\\ndrawing analogies to common Expectation Maximization (EM) procedures for learning mixture models, we\\ngain insights into more effective ways of utilizing clustering in self-supervised learning. The mixture model\\nformulation also suggests alternative procedures, such as consistent clustering, cluster variance, and soft\\nassignment, as opposed to the prevalent methods that involve re-initialized clustering, equal spherical clusters,\\n1\\narXiv:2511.05462v1  [cs.LG]  7 Nov 2025\\n\\nFigure 1\\nSiamMM model architecture.\\nAssuming all the embeddings are L2 normalized, we cast clustering in\\nrepresentation learning as a von Mises-Fisher mixture model (vMFMM). The optimization objective tends to minimize\\nthe distance between an embedding and its clustering centroid (or nearest centroids) without negative samples.\\nand hard assignment.\\nAnother fundamental question in the clustering or mixture model is determining the number of clusters,\\nwhich is often picked by heuristic intuition, cross-validation, or information criterion Sugar and James (2003).\\nIn the context of deep representation learning, previous unsupervised learning methods tend to overcluster\\nthe data population to achieve high model performance Li et al. (2021a), which results in quite a lot of\\nclusters where some of them only capture a single point inside. We argue that this phenomenon contrasts\\nwith the original motivation of clustering semantically similar subpopulations. In order to disentangle the\\nuncertainty in determining the number of cluster components, we seek a non-parametric solution to merge\\nclusters throughout the training process. Although a simpler model is obtained, the performance achieves\\nstate-of-the-art performance on a series of self-supervised learning benchmarks.\\nWe suggest changes to clustering in representation learning to reflect the insights of a mixture model perspective.\\nBecause of this inspiration, we name our method SiamMM. We conducted a number of self-supervised learning\\nexperiments on ImageNet Deng et al. (2009) with ResNet architectures He et al. (2016), demonstrating that\\nour perspective of the mixture model indeed leads to empirical improvements. Our main contributions are\\nsummarized as follows:\\n1. We introduce a new representation learning approach, SiamMM, which interprets clustering as a statistical\\nmixture model and applies its insights in a self-supervised manner. Our MLE-based loss function is more\\nadaptive and improves the accuracy of Siamese networks without the need to sample negative samples.\\n2. We propose a novel scheme that dynamically reduces the number of clusters during pretraining. This\\nreduction not only cuts down on training time but also facilitates interpretation. The learned clusters exhibit\\na strong resemblance to the unseen ground truth labels for ImageNet.\\n3. SiamMM achieves advancements over previous methods across various Self-Supervised Learning (SSL)\\nbenchmarks. We further explore the impact of negative samples in clustering, consistent centroid updates,\\ncluster concentration estimation, and soft cluster assignment through an extensive ablation study. We hope\\nour valuable insights can propel the field of unsupervised learning forward.\\n2\\nRelated work\\nSelf-supervised contrastive learning. Contrastive learning Hadsell et al. (2006) approaches to self-supervised\\nlearning in computer vision aim to learn an embedding space where views or augmented crops of an image\\nhave more similar embeddings than those of different images. Many contrastive methods rely on mechanisms\\nof sampling a large number of negative samples Chen et al. (2020a); He et al. (2020); Tian et al. (2020); Wu\\net al. (2018) to prevent the model from learning trivial solutions with unlabeled data. For example, SimCLR\\nChen et al. (2020a) requires a large batch size to have enough negative samples; MoCo He et al. (2020)\\n2\\n\\nmaintains negative samples in a memory bank Wu et al. (2018) and consistently updates the embeddings via\\na separate momentum encoder.\\nNegative samples were initially thought to prevent representation collapse: without something to counteract\\nthe gradients pulling embeddings together, it was thought that self-supervised methods would lead to the\\ntrivial solution of all inputs mapping to a constant embedding. However, a recent series of methods Grill et al.\\n(2020); Chen and He (2021); Zbontar et al. (2021); Bardes et al. (2022) have shown it possible to directly\\nlearn invariant features from distorted versions of an image without any negative samples. BYOL Grill et al.\\n(2020) and SimSiam Chen and He (2021) introduce asymmetry in the network and learn updates using one\\ndistorted version and a stop gradient operation on the other version. Barlow Twins Zbontar et al. (2021)\\nand VICReg Bardes et al. (2022) add regularization in the cross-correlation matrix of embeddings of positive\\nsamples to reduce over-fitting of redundant parameters.\\nDeep clustering. Clustering-based methods for self-supervised learning have seen success in self-supervised\\nlearning YM. et al. (2020); Caron et al. (2019, 2018, 2020); Li et al. (2021a). Though negative samples are\\nnot directly used, pseudo-labels or cluster centroids play a role in making a contrastive loss. DeepCluster\\nCaron et al. (2018) and SeLa YM. et al. (2020) cluster representation embeddings from previous iterations\\nand train a classifier based on the cluster index as the pseudo-label. SwAV Caron et al. (2020), ProPosHuang\\net al. (2022b), and PCL Li et al. (2021a) incorporate negative cluster centroids in a contrastive loss, though\\nSwAV and ProPos pulls the centroid embeddings of two distorted versions of an image closer, while PCL uses\\none distorted version to predict its cluster centroid computed from another version. Another difference among\\nthese clustering-based methods is that DeepCluster and PCL adopt K-means in cluster analysis, while SeLa\\nand SwAV perform online clustering via Sinkhorn-Knopp Cuturi (2013).\\nMixture Models. Mixture models are common statistical models that have been in use for more than a century,\\nwith Gaussian mixture models (GMMs) a popular choice McLachlan et al. (2000); Day (1969). These models\\nare particularly adept at modeling data with sub-populations, which tend to form clusters within the overall\\ndistribution. As an unsupervised learning approach, GMM and its variant can lie well in Deep Neural Network\\nfor density estimation van den Oord and Schrauwen (2014a), clustering Yang et al. (2019), sampling Gepperth\\nand Pfülb (2021), and anomaly detection Zong et al. (2018), and has been successfully applied in multiple\\nfields of computer vision, such as image processing Zoran and Weiss (2011); van den Oord and Schrauwen\\n(2014b), segmentation Gupta and Sortrakul (1998), generation Gepperth (2022), and face verification Hasnat\\net al. (2017). The interpretation of clustering in self-supervised representation learning as a mixture model\\nare relatively limited in the literature.\\n3\\nBackground\\n3.1\\nSiamese Networks\\nRepresentation learning methods seek to learn a mapping from an image x ∈X to a d-dimensional embedding\\nv ∈V, such that the embedding space V captures useful information and structure applicable to downstream\\ntasks. This mapping is often parameterized as a deep neural network E(·), with v = E(x).\\nRecent works often adopt a Siamese framework, seeking to maximize the embedding similarity between two\\naugmentations x1 and x2 of a single image x. An encoder followed by a small multilayer perceptron (MLP)\\npredictor g generates embedding v1 = g(E(x1)) on one branch, while on the other branch, momentum encoder\\nEm(·) generates a second embedding vm\\n2 = Em(x2). The loss is defined as\\nL(v1, vm\\n2 ) = D(v1, stopgrad(vm\\n2 ))\\n(1)\\nwhere D is a distance metric defined in the embedding space and stopgrad(·) is a stop-gradient operation\\napplied to the second view’s embedding and D.\\n3.2\\nClustering\\nAs posed in (1), each instance is considered to be an independent entity, with the loss only enforcing similarity\\nbetween augmented views for a particular example. For methods with contrastive losses, embeddings of all\\n3\\n\\nother samples are additionally pushed away. Since this may not be ideal if there exists many subpopulations\\nwithin the data distribution, clustering methods allow grouping samples by semantic similarity. Given N\\nsamples xi ∈X, 1 ≤i ≤N and their corresponding embedding vectors vi = E(xi) ∈V, a clustering method\\nwill produce a cluster assignment A(xi) ∈{1, 2, ..., K}, and Ck := {vi ∈V|A(xi) = k, 1 ≤i ≤N}, 1 ≤k ≤K\\nindicates the embedding point cloud of the k-th cluster.\\n3.3\\nMixture Models\\nThe Gaussian Mixture Model (GMM) is a statistical method commonly used in modeling probabilistic\\nclustering and is widely applied in data mining and machine learning Murphy (2013). Given the cluster Ck,\\nassume that the image embeddings v ∈Ck follow a Gaussian distribution centered at µk with variance Σk.\\nThe mixture distribution of K Gaussian clusters is given by the summation:\\nf(v) =\\nK\\nX\\nk=1\\nπkfk(v|µk, Σk)\\n(2)\\nwhere fk is the density function of a d-dimensional Gaussian distribution of the k-th cluster Ck; πk is the\\nprior probability of Ck.\\nDue to the common practice of L2 normalization, the embedding v lies on a hypersphere and thus can\\nmore appropriately be modeled with a von Mises-Fisher mixture model (vMFMM). Suppose that v is a\\nd−dimensional unit random variable, i.e. v ∈Rd, ||v|| = 1. The likelihood function that v is generated from\\nfk(.) is\\nfk(v|µk, κk) = cd(κk) exp (κkµT\\nk v)\\n(3)\\nwhere µ is a mean direction with ||µ|| = 1, κ is the concentration, and normalization term cd(κ) is given by\\ncd(κ) =\\nκd/2−1\\n(2π)d/2Id/2−1(κ)\\n(4)\\nwhere Id(.) is the modified Bessel function of the first kind and order d.\\nExpectation Maximization (EM) algorithm. The parameters of mixture models are commonly estimated with the\\nEM algorithm, by maximizing the log-likelihood function. In the E-step, the probability that v is assigned to\\nthe cluster index k is calculated, denoted as p(k|v). Let the unnormalized mean vector of cluster k be defined\\nas\\nrk =\\nPN\\ni=1 vip(k|vi)\\nPN\\ni=1 p(k|vi)\\n(5)\\nThen, µ and κ can be approximated by Maximum Likelihood Estimation (MLE) Banerjee et al. (2005):\\nˆµk =\\nrk\\n||rk||\\n(6)\\nˆκk = ||rk||d −||rk||3\\n1 −||rk||2\\n(7)\\nfor k = 1, 2, ..., K.\\n4\\nSiamMM for Clustering Representations\\nWe propose viewing clustering in unsupervised representation learning through the lens of mixture models.\\nUnder such a paradigm, each cluster can be viewed as a mixture component, representing a subpopulation of\\nthe data distribution. Though inspired by mixture models, we do not directly model the learned embeddings\\nas a standard mixture model, as there are several key differences of the deep representation setting. Firstly, the\\n4\\n\\ngoal of representation learning is to learn the encoder E, whose parameter updates must occur in conjunction\\nwith the EM learning of the mixture model parameters. Learning the mapping of X 7→V also means that the\\ndeep feature distribution modeled by the MM is constantly evolving, in contrast to the fixed data features\\ncommonly modeled by mixture models in statistics. Instead, we primarily use mixture models to motivate\\nchanges in the common clustering methodology.\\nTo adapt mixture models to deep representation learning, we adopt a two-tier EM algorithm to model cluster\\ndistributions and learn representation embeddings simultaneously. In the E step, the assignment function is\\nupdated based on the current centroids, and so is the mean vector of (5). This is followed by an M step to\\nestimate the mixture model parameters µ and κ via MLE (6, 7), given the current parameters of E. These E\\nand M steps are iterated, updating the cluster model. We then perform an outer loop M step, updating E to\\nmaximize the likelihood of the data. We provide a deeper dive into changes to individual components below.\\n4.1\\nMLE: Non-negative Soft-assignment Loss\\nPrevious methods have posed a variety of loss objectives for clustering, including pseudo cluster label\\nprediction Caron et al. (2018), swapped assignment prediction Caron et al. (2020), or directly replacing one of\\nthe views Li et al. (2021a) or both views Huang et al. (2022b) with cluster centroids in the contrastive loss.\\nSeveral of these methods utilize negative centroids as an additional repelling force between clusters.\\nAdditionally, many previous clustering approaches assume a hard-assignment strategy, where the assignment\\nprobability πk is implicitly treated as a binary 0-1 variable given by\\nπk(v) =\\n(\\n1\\nif v ∈Ck\\n0\\notherwise\\n(8)\\nIn the case that v ∈Ck, the density function of mixture models (2) reduces to a single distribution fk.\\nWhile hard assignment simplifies clustering, it is also an assumption that limits the model expressivity.\\nAlternatively, we can generalize and allow the mixture model to have soft assignment. Within the context of\\nrepresentation learning, this allows a single sample to be driven towards multiple prototypes, with varying\\nprobability. In practice, we limit soft assignment to a set of H nearest centroids around v. Suppose that the\\nset of nearest centroids is M with size |M| = H, so πk(v) = 0 for k /∈M.\\nIn mixture models, the M step of the EM algorithm typically estimates the model parameters with maximum\\nlikelihood estimation (MLE). The negative log-likelihood can be expressed as the following weighted summation:\\nLH(v|ˆκA(x), ˆµA(x)) = −log\\nX\\nk∈M\\nπk exp(ˆκkˆµT\\nk v)\\n(9)\\nwhere ˆκ and ˆµ are now fixed estimators and carry no gradient operation; πk is the weight scale for the k-th\\nassignment. We define πk by considering the relative similarity score between v and its centroids by\\nπk(v) =\\nαk exp(µT\\nk v/τ)\\nPH\\nl αl exp(µT\\nl v/τ)\\n(10)\\nwhere αk is a measure of the size of cluster; τ > 0 is a tuning parameter; for a large temperature τ, the weight\\ntends to be evenly distributed over H; for a small τ, the weight lies more on the closest centroids. Note that\\nin the special case where H = 1, this soft assignment reduces to the hard assignment of (8).\\nNotably, MLE does not rely on any negative sampling; instead, one only needs to maximize the similarity\\nbetween embeddings and their nearest clusters. By removing any notion of negatives from our objective\\nfunctions, we can improve computational efficiency by avoiding computing additional negatives terms in the\\nloss. Note that this corroborates recent works finding that negative sampling is unnecessary for non-clustering\\nself-supervised methods Grill et al. (2020); Chen and He (2021); Zbontar et al. (2021); Bardes et al. (2022).\\n5\\n\\n4.2\\nCluster Merging\\nOne of the prevalent challenges in many mixture models or clustering algorithms is the determination of the\\nnumber of clusters, often unknown a priori and requiring estimation through methods such as cross-validation\\nor analytic criteria. In the realm of self-supervised learning, this challenge is compounded by the presence of\\nembedding shift: as the encoder is trained, the distribution and clustering behavior of embeddings change over\\ntime. Although employing a large number of clusters (K) tends to yield strong empirical performance, it can\\ncompromise the interpretability of clustering results. For instance, advocating for 100k clusters, as suggested\\nin Li et al. (2021a), is a hundred times larger than the number of classes in ImageNet. Furthermore, a fixed\\nhigh value of K increases computational costs, underscoring the importance of minimizing K when feasible.\\nInstead of manually specifying the number of clusters, we propose a merging strategy that dynamically\\nadapt the parameter K to the evolving embedding space. In the initial stages, when the representations lack\\nsubstantial meaning, it is advantageous to commence with a larger number of clusters. Subsequently, at each\\niteration, we refine the clustering by merging the closest clusters by a predefined threshold ζ. Suppose the\\npair-wise distance between cluster index i and j is Zij := ||µi −µj||2, then the two cluster are merged if\\nZij −E{Zij}\\nSE{Zij}\\n< ζ\\n(11)\\nwhere E{.} and SE{.} are the expectation and standard deviation of the pair-wise distance, respectively. This\\napproach, characterized by the initial use of a large number of clusters followed by a gradual reduction to a\\nsmaller set of key centroids, resonates with the principles of the Lottery Ticket Hypothesis Frankle and Carbin\\n(2019): by initially overseeding clusters, there are increased chances of identifying the “correct” centroids,\\nthereby enhancing clustering performance. Such an approach mitigates the computational costs associated\\nwith a fixed large number of clusters, ultimately converging to a number on par with the number of classes in\\nImageNet without sacrificing accuracy.\\n4.3\\nConsistent Centroids Update\\nThere is a well-known tendency of K-means to fall into poor local minima, and the final result is often sensitive\\nto centroid initialization Pena et al. (1999). In practice, it is common to run many re-initializations and pick\\nthe best result Jain et al. (1999); Li et al. (2021a).\\nThe practice of re-initializing clustering after each epoch results in several negative side effects in self-supervised\\nlearning. Firstly, these re-initializations mean that the cluster centroids are constantly changing, beyond just\\nthe embeddings being updated by backpropagation; the very clustering structure, which is meant to represent\\nlearned semantic structure in the data, is inconsistent, leading to a learning signal full of thrash. In contrast,\\nwe observe that the mixture model formulation suggests that the mixture distributions should be learned\\nconsistently. Rather than re-initializing from scratch multiple times each epoch, centroids should be initialized\\nexactly once, at the start of training; subsequent clustering per epoch should be initialized with the previous\\nepoch’s centroids. By doing so, we provide the algorithm with more consistent cluster targets, and removing\\nthe need for multiple restarts significantly speeds up training. The difference between one round vs. five\\nrandom initializations Li et al. (2021a) per epoch is an approximately 25% increase in training time, even\\nwith an efficient implementation like Faiss Johnson et al. (2019).\\n4.4\\nNon-uniform Cluster Size\\nK-means is well known for producing clusters of roughly equal size, as every point in V is assigned to the\\nnearest centroid, without any notion of the learned per-cluster size. This may not be a good assumption in\\ncertain applications. For example, while the commonly used ImageNet-1K dataset Deng et al. (2009) has a\\nrelatively balanced number of samples per class, the classes themselves are not necessarily evenly distributed\\nsemantically (e.g., over 10% of the categories are breeds of dogs), nor are the clusters one might recover when\\nlearning unsupervised structure.\\nIn the context of the von Mises-Fisher mixture model, each component of the mixture is characterized not\\nonly by a mean µk (i.e., the centroid) but also by a concentration parameter per cluster κk. The ability to\\nlearn per-cluster variance allows for a more expressive representation of the embedding distribution.\\n6\\n\\n4.5\\nFinal Loss Function\\nClustering algorithms like K-means may fail to converge at the beginning of the training process, when\\nthe representations have yet to learn much meaning. Empirically, we found that adding an instance-wise\\nloss can boost clustering efficiency and improve performance. This finding is consistent to the additive loss\\nsuggested by Li et al. (2021a); Khorasgani et al. (2022); Huang et al. (2022b); Li et al. (2021b). We define the\\nsymmetrized instance-wise loss from (1) as\\nLinst = L(v1, vm\\n2 ) + L(v2, vm\\n1 )\\n(12)\\nThis instance-wise loss is combined with the cluster-wise loss, specifically the soft-assignment loss LH in (9).\\nThe final loss for SiamMM takes an additive form\\nLfinal = LH + Linst\\n(13)\\nImportantly, neither of these terms relies on negative samples.\\n5\\nExperiments\\n5.1\\nImplementation details\\nWe pre-train SiamMM on the ImageNet-1k dataset Deng et al. (2009), with a standard ResNet-50 He et al.\\n(2016) as encoder E. The projection head is a 3-layer MLP, and the prediction head is a 2-layer MLP. Following\\nChen et al. (2021), all MLP layers have Batch Normalization Ioffe and Szegedy (2015), with 4096 dimensional\\nhidden layers with ReLU activations, and the output layers are 256 dimensional without ReLU.\\nOur training procedure proceeds as follows: First, we update the cluster assignments using the centroids\\ncomputed from the previous epoch, and estimate the mixture model parameters µ and κ using equations (6)\\nand (7), respectively. Next, we backpropagate through the encoder E using the loss function in (13). Finally,\\nwe apply the cluster merging strategy described in Section 4.2 and update the cluster centroids accordingly.\\nWe use Faiss Johnson et al. (2019) for clustering and the nearest centroid search, with a default initial\\nnumber of K = 100k for cluster merging. We find that setting the merging threshold ζ = −1.2 in (11), which\\ncorresponds to the 10-th percentile of the normalized pairwise distance between cluster centroids, yields a\\npromising merging curve. This threshold works well for large cluster numbers, though it may vary with\\ndata distribution and initial cluster count. We recommend that researchers adjust this parameter through\\nmethods like grid search to suit their specific datasets and clustering needs. We set α = 1 and τ = 0.02 in\\n(10) according to soft assignment when H > 1; by default, H = 5 unless otherwise stated. In the estimation\\nof concentration parameter κ, we find it beneficial to apply principal component analysis (PCA) to reduce\\ndimensional correlations in the features Jing et al. (2022). More details on this process can be found in\\nAppendix C.1.\\nWe adopt the same configuration of data augmentations as MoCo v3 Chen et al. (2021). We use the LARS\\noptimizer Ginsburg et al. (2018) by default with a weight decay of 1e-6, a momentum of 0.9, batch size 4096,\\nand a base learning rate lrbase = 0.6 and 0.5 for 100-epoch and 200-epoch, respectively. The learning rate is\\nadjusted for different batch size lr = lrbase × batch_size/256 and follows a cosine decay schedule.\\n5.2\\nImage classification benchmarks\\nWe evaluated the pre-trained ResNet-50 backbone with SiamMM for 100 epochs and 200 epochs on ImageNet.\\nImageNet Linear Evaluation. We compare SiamMM’s learned representations with recent state-of-art methods\\nby training a linear classifier on top of a frozen ResNet-50 backbone. The validation set top-1 accuracies are\\nreported in Table 1. We use an SGD optimizer with cosine learning rate with lrbase = 0.1, momentum of 0.9,\\nweight decay of 0.0, and batch size of 1024, for 90 epochs. The 200-epoch pretraining results of BYOL Grill\\net al. (2020) and SwAV Caron et al. (2020) are from the implementation in Chen and He (2021), which\\nreproduces the results with two 224×224 crops of each image for a fair comparison. We observe that SiamMM\\noutperforms the previous methods when considering a similar number of pre-training epochs. In particular,\\n7\\n\\nTable 1 Linear classification on ImageNet. We report top-1 accuracy (in %) with a frozen pretrained ResNet-50. †: result\\nreproduced by us.\\nMethod\\nEpochs\\nTop-1\\nEpochs\\nTop-1\\nSimCLRChen et al. (2020a)\\n100\\n64.6\\n200\\n66.8\\nMoCo v2Chen et al. (2020b)\\n100\\n-\\n200\\n67.5\\nPCLLi et al. (2021a)\\n100\\n-\\n200\\n67.6\\nSwAVCaron et al. (2020)\\n100\\n66.5\\n200\\n69.1\\nSimSiamChen and He (2021)\\n100\\n68.1\\n200\\n70.0\\nBYOLGrill et al. (2020)\\n100\\n66.5\\n200\\n70.6\\nAll4OneEstepa et al. (2023)\\n100\\n66.6\\n200\\n-\\nNNCLRDwibedi et al. (2021)\\n100\\n69.2\\n200\\n70.7\\nMoCo v3Chen et al. (2021)\\n100\\n68.9\\n200\\n71.0 †\\nProPosHuang et al. (2022b)\\n100\\n-\\n200\\n72.2\\nSNCLRGe et al. (2023)\\n100\\n69.6\\n200\\n72.4\\nLEWELHuang et al. (2022a)\\n100\\n71.9\\n200\\n72.8\\nSiamMM (ours)\\n100\\n71.9\\n200\\n73.2\\nTable 2 Other downstream evaluation tasks. (left) semi-supervised evaluation for ResNet-50 backbone fine-tuned on 1%\\nand 10% ImageNet data; the top-5 accuracy are reported (in %). (right) transfer learning results for other downstream\\ntasks, Places205 Zhou et al. (2014) scene classification with top-1 accuracy, and VOC07 Everingham et al. (2009)\\nmulti-label image classification with mAP.\\nMethod\\nEpochs\\nImageNet\\nPlaces205\\nVOC07\\n1%\\n10%\\nSimCLRChen et al. (2020a)\\n200\\n56.5\\n82.7\\n-\\n-\\nMoCo v2Chen et al. (2020b)\\n200\\n66.3\\n84.4\\n-\\n-\\nPCLLi et al. (2021a)\\n200\\n73.9\\n85.0\\n50.3\\n85.4\\nNNCLRDwibedi et al. (2021)\\n400\\n79.2\\n88.6\\n-\\n-\\nSNCLRGe et al. (2023)\\n400\\n80.1\\n89.1\\n-\\n-\\nPIRLMisra and van der Maaten (2020)\\n800\\n57.2\\n83.8\\n49.8\\n81.1\\nSimCLRChen et al. (2020a)\\n800\\n75.5\\n87.8\\n53.3\\n86.4\\nBYOLGrill et al. (2020)\\n1000\\n78.4\\n89.0\\n54.0\\n86.6\\nSwAVCaron et al. (2020)\\n1000\\n78.5\\n89.9\\n-\\n-\\nBarlow TwinsZbontar et al. (2021)\\n1000\\n79.2\\n89.3\\n54.1\\n86.2\\nVICRegBardes et al. (2022)\\n1000\\n79.4\\n89.5\\n54.3\\n86.6\\nSiamMM (ours)\\n200\\n79.4\\n89.2\\n53.2\\n87.3\\nSiamMM outperforming other clustering methods such as SwAV Caron et al. (2020), PCL Li et al. (2021a),\\nand ProPos Huang et al. (2022b) demonstrate the value of our perspective of the mixture model.\\nSemi-supervised Learning. We also evaluate the performance of the pretrained backbone by fine-tuning it with\\na linear classifier using 1% and 10% of the labels. We use the split of Chen et al. (2020a) to select a subset of\\nImageNet training data with labels and report the top-5 accuracy on the ImageNet validation set in Table 2.\\nOur method is competitive with the state-of-the-art even compared with the previous methods pre-trained on\\na larger number of epochs.\\nTransfer Learning. Following the setup of Misra and van der Maaten (2020), we train a linear classifier on top\\nof the frozen SiamMM backbone on other downstream tasks: Places205 Zhou et al. (2014) scene classification\\nand VOC07 Everingham et al. (2009) multi-label image classification. For Places205 dataset, we train a\\nfully-connected layer followed by softmax and report the top-1 accuracy (in %); for VOC07, we train a linear\\nSVM and report mAP in Table 2. Our SiamMM achieves results on par with the state-of-the-art on both\\ndownstream tasks.\\nClustering Evaluation. We evaluate the clustering quality by computing the Adjusted Mutual Information\\n(AMI) Vinh et al. (2009) between clustering index obtained by different methods and the ground truth label\\n8\\n\\nTable 3 Evaluation of Quality of Clustering Results. Following Li et al. (2021a), Adjusted Mutual Information (AMI) are\\nreported under 200 epochs pretraining on ImageNet with 25k clusters. Following Tian et al. (2021), every clusters\\nmapped to the most frequent true class label to compute top-1 accuracy (%).\\nMethod\\nAMI (K = 25k)\\nAcc (K = 1k)\\nDeepClusterCaron et al. (2018)\\n0.28\\n-\\nMoCoHe et al. (2020)\\n0.29\\n-\\nSwAVCaron et al. (2020)\\n0.40\\n-\\nPCLLi et al. (2021a)\\n0.41\\n-\\nProPosHuang et al. (2022b)\\n0.53\\n-\\nSimCLRChen et al. (2020a)\\n-\\n33.3\\nBYOLGrill et al. (2020)\\n-\\n51.0\\nMoCLRTian et al. (2021)\\n-\\n51.6\\nSiamMM (ours)\\n0.60\\n56.3\\nTable 4 Instance-wise and cluster-wise losses ablation (row 1-2); effect of instance-wise contrastive loss of Moco v3 Chen\\net al. (2021) (row 3-5).\\nMethod\\ninst.-wise\\nneg. inst.\\nclus.-wise\\nTop-1\\nSimSiam\\nX\\n70.0\\nSiamMM(w/o i)\\nX\\n71.8\\nMoco v3\\nX\\nX\\n71.0\\nSiamMM(w/ n)\\nX\\nX\\nX\\n73.1\\nSiamMM\\nX\\nX\\n73.2\\non ImageNet. In the second experiment, we map to the most frequent true class label to every cluster and\\nevaluate the top-1 accuracy by following Tian et al. (2021). Table 3 demonstrates the higher quality of the\\nclusters generated by SiamMM that significantly exceeds the previous unsupervised learning methods.\\n6\\nAblation Experiments\\n6.1\\nInstance-wise v.s Cluster-wise Losses\\nTo evaluate the importance of the clustering objective, we ablate the instance loss and only implement the\\nloss (9). The linear evaluation top-1 accuracy drops to 71. 8% under 200-epoch pre-training, though it is still\\nbetter than that of SimSiam (see row #1 & #2 in Table 4).\\n6.2\\nInstance-wise Negative Samples\\nThe proposed method can incorporate instance-wise self-supervised learning objectives. We replace the Linst\\nin (12) with contrastive loss of MoCo v3 Chen et al. (2021) and denote the new additive loss as “SiamMM w/\\nnegative inst” (see row#4 in Table 4). It shows that the MoCo v3 can get boosted by more than 2% after\\nadding our cluster-wise loss (9). Besides, as “SiamMM w/ negative inst” achieves similar result to SiamMM,\\nit shows that negative instance is not needed.\\n6.3\\nCluster Merging\\nWe evaluated the performance and training time of the model with and without the cluster-merging strategy.\\nTable 5 presents the top-1 accuracy of linear evaluation for various numbers of clusters. Initiating with a\\nsubstantial number like 100k and progressively merging down to a smaller cluster count markedly enhances\\nmodel performance compared to fixed-number clustering algorithms when the final number of clusters is\\nsimilar. Furthermore, in comparison to employing a fixed large number of clusters, the training time is\\napproximately halved without a significant loss in accuracy. In comparison, SimSiam requires only 65% of\\n9\\n\\nTable 5 Fix or merge the number of clusters during training. Top-1 accuracy (%) of linear evaluation on ImageNet, and\\ntraining time compared with the baseline (row #1) are reported.\\nfinal # of clusters\\nTOP-1\\nTrain Time\\n1k\\nfixed\\n71.6\\n1x\\nmerged\\n73.0\\n1.15x\\n5k\\nfixed\\n72.4\\n1.1x\\nmerged\\n73.2\\n1.20x\\n100k\\nfixed\\n73.2\\n2.33x\\nTable 6 Initial number of clusters. Initiating with different number of clusters (25k, 50k, 100k) and merging down to a\\nsimilar number of cluster count. Top-1 accuracy (%) of linear evaluation on ImageNet.\\ninit # of clusters\\nhard assign\\nsoft assign\\ntrain time\\n25k\\n72.6\\n72.8\\nx\\n50k\\n72.9\\n73.1\\n1.1x\\n100k\\n73.0\\n73.2\\n1.3x\\nthe training time of SiamMM (starting from 100k clusters), but it results in a 3.2% drop in performance\\ncompared to SiamMM.\\n6.4\\nInitial Number of Clusters for Merging\\nWe conduct a comparative analysis of the performance of the model based on three different numbers of initial\\nclusters, considering both hard and soft cluster assignments. Regardless of the initial number of clusters,\\nthe final number of clusters converges to a to almost the same number of classes included in ImageNet, as\\ndepicted in Figure 2. We extended this analysis to two subsets of ImageNet—ImageNet100Tian et al. (2019)\\nand TinyImageNetLe and Yang (2015), which contain 100 and 200 classes, respectively—and found similar\\nconvergence patterns, underscoring the robustness of our approach across different datasets.\\nTable 6 illustrates that initiating with a larger number of clusters and subsequently merging down increases\\nthe top-1 accuracy. This finding supports our hypothesis that overseeding clusters initially provides more\\nopportunities to discover the “correct” centroids, resulting in enhanced clustering. However, increasing the\\ncluster count from 50k to 100k yielded 0.1% returns in accuracy, while largely increasing time complexity. Based\\non these findings, we recommend using 50k clusters as a balanced choice, offering substantial computational\\nefficiency while maintaining strong performance. Additionally, we observed that the soft-assignment strategy\\nimproved top-1 accuracy by 0.2% compared to the hard-assignment approach.\\n6.5\\nPer-cluster Concentration Parameter\\nWe ablate the effect of having per cluster concentration parameter by replacing κk with a constant number.\\nWithout taking into account the concentration scale per cluster, model performance drops by 0.3% under\\n200-epoch pre-training (see Table 7).\\n6.6\\nConsistent Centroid Updating\\nAs discussed in Section 4.3, we consistently update cluster centroids, propagating the learned centroids from\\nthe previous iteration to the next. In the ablation study, we adhere to the default setting Li et al. (2021a);\\nHuang et al. (2022b); Caron et al. (2018), randomly initializing cluster centroids when applying clustering per\\niteration.\\nAppendix Figure 7 illustrates the training time for varying numbers of cluster re-initializations and with\\ndifferent fixed numbers of clusters. The training time for 200k clusters doubles when the number of re-\\ninitializations increases from 1 to 5. Furthermore, we observe a slight drop in model performance without\\n10\\n\\nFigure 2 Starting from different initial numbers, the number of clusters converge almost to the true number of cluster\\nin the datasets (left: ImageNet1kDeng et al. (2009); right: ImageNet100Tian et al. (2019)).\\nTable 7 Concentration Parameter and Consistent Update. Top-1 accuracy (in %) of linear evaluation on ImageNet under\\n200 epochs: w/ and w/o the per-cluster concentration parameter κ (row #1 & #3); w/ and w/o the consistent update\\n(row #2 & #3).\\nMethod\\nConsist Update\\nConcent. κ\\nTOP-1\\nSiamMM\\nX\\n72.9\\nSiamMM\\nX\\n73.1\\nSiamMM\\nX\\nX\\n73.2\\napplying the consistency strategy, as shown in Table 7. This highlights a major advantage of consistent update\\nto inherit centroids from the previous epoch as initialization and avoiding re-initialization.\\n6.7\\nNegative Sample is Unnecessary for SiamMM\\nMany previous unsupervised learning methods borrow from contrastive methods and use large numbers of\\nnegative cluster centroids to form a contrastive or cross-entropy loss Li et al. (2021a); Huang et al. (2022b);\\nCaron et al. (2018, 2020). As discussed in Section 4.1, these negative centroids are not needed from a mixture\\nmodel point of view. We thus reframe the cluster-wise loss (9) (with H = 1 for simplicity) by contrasting v\\nfrom its negative centroids as suggested by PCL Li et al. (2021a),\\nLnce1 = −log\\nexp(ˆκA+ ˆµT\\nA+v)\\nexp(ˆκA+ ˆµT\\nA+v)+\\nX\\nA−\\nexp(ˆκA−ˆµT\\nA−v)\\n(14)\\nwhere A+ and A−indicates the positive centroid and negative centroids, respectively.\\nAlternatively, another sampling strategy is to sample negative image embedding v−that are out of the cluster\\nCA. So, the variant loss contrasts v+ from the negative embeddings v−by\\nLnce2 = −log\\nexp(ˆκA ˆµT\\nAv+)\\nexp(ˆκA ˆµT\\nAv+)+\\nX\\nv−\\nexp(ˆκAˆµT\\nAv−)\\n(15)\\nwhere v+ and v−are positive and negative image embedding, respectively. The evaluation results reported\\non Appendix Table 8 show that the two variations perform similarly to SiamMM. We conclude that simply\\nintegrating more negative samples does not help, possibly due to noise or redundant information.\\n6.8\\nCluster visualization\\nWe showcase visualizations of samples from the approximately 1000 clusters learned by SiamMM, along with\\nthe corresponding ground truth labels for each image. Each cluster is characterized by the most common\\n11\\n\\nFigure 3 Visualization of Merged 1000 Clusters. Left: Images grouped under the same cluster label, where each row\\ncorresponds to a true class label in ImageNet. Right: Images grouped under class label “eggnog”, where each row\\ncorresponds to a predicted clustering label.\\nground truth label within it in Figure 3. In particular, despite the absence of truth labels during training,\\nSiamMM learns clusters that exhibit a coherent alignment with ground truth labels. Furthermore, SiamMM\\nreveals subtle texture or background differences that are consistently present across various image classes.\\nThese observations underscore the potential of SiamMM to improve data quality by identifying mislabeled\\nimages Northcutt et al. (2021) and highlighting more challenging negative samples.\\n7\\nConclusion\\nWe have presented a mixture model perspective on clustering in self- and unsupervised learning methods.\\nThis framework allows us to develop a generalized framework and propose refined techniques for clustering in\\nrepresentation learning. Empirical evaluations confirm our individual insights and show that the combination\\nof our parts result in learned representations performing better on a number of downstream tasks. For future\\nwork, we would like to see how the mixture model perspective generalizes on classification datasets with class\\nimbalance, detection, and video.\\n12\\n\\nReferences\\nArindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit hypersphere using von\\nmises-fisher distributions. Journal of Machine Learning Research, 6(46):1345–1382, 2005. http://jmlr.org/papers/\\nv6/banerjee05a.html.\\nAdrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised\\nlearning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April\\n25-29, 2022. OpenReview.net, 2022. https://openreview.net/forum?id=xm6YD62D1Ub.\\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of\\nvisual features. In Computer Vision – ECCV 2018, pages 139–156, Cham, 2018. Springer International Publishing.\\nMathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image features on\\nnon-curated data. In Proceedings of the International Conference on Computer Vision (ICCV), 2019.\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised\\nlearning of visual features by contrasting cluster assignments. In Advances in Neural Information Processing Systems,\\nvolume 33, pages 9912–9924. Curran Associates, Inc., 2020.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of\\nProceedings of Machine Learning Research, pages 1597–1607. PMLR, 13–18 Jul 2020a.\\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. In 2021 IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR), pages 15745–15753, 2021. doi: 10.1109/CVPR46437.2021.01549.\\nXinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning.\\nCoRR, abs/2003.04297, 2020b. https://arxiv.org/abs/2003.04297.\\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv\\npreprint arXiv:2104.02057, 2021.\\nMarco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information\\nProcessing Systems, volume 26. Curran Associates, Inc., 2013.\\nNeil E Day. Estimating the components of a mixture of normal distributions. Biometrika, 56(3):463–474, 1969.\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image\\ndatabase. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2009.\\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from\\nmy friends: Nearest-neighbor contrastive learning of visual representations, 2021. https://arxiv.org/abs/2104.14548.\\nImanol G. Estepa, Ignacio Sarasúa, Bhalaji Nagarajan, and Petia Radeva. All4one: Symbiotic neighbour contrastive\\nlearning via self-attention and redundancy reduction, 2023. https://arxiv.org/abs/2303.09417.\\nMark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual\\nobject classes (voc) challenge. International journal of computer vision, 88:303–308, 2009.\\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.\\nInternational Conference on Learning Representations, 2019.\\nChongjian Ge, Jiangliu Wang, Zhan Tong, Shoufa Chen, Yibing Song, and Ping Luo. Soft neighbors are positive\\nsupporters in contrastive visual representation learning. arXiv preprint arXiv:2303.17142, 2023.\\nAlexander Rainer Tassilo Gepperth. A new perspective on probabilistic image modeling. 2022 International Joint\\nConference on Neural Networks (IJCNN), pages 1–10, 2022.\\nAlexander Rainer Tassilo Gepperth and Benedikt Pfülb. Image modeling with deep convolutional gaussian mixture\\nmodels. 2021 International Joint Conference on Neural Networks (IJCNN), pages 1–9, 2021.\\nBoris Ginsburg, Igor Gitman, and Yang You. Large batch training of convolutional networks with layer-wise adaptive\\nrate scaling, 2018. https://openreview.net/forum?id=rJ4uaX2aW.\\nJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\\nBernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos,\\n13\\n\\nand Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In Advances in Neural\\nInformation Processing Systems, volume 33, pages 21271–21284. Curran Associates, Inc., 2020.\\nLalit Gupta and Thotsapon Sortrakul. A gaussian-mixture-based image segmentation algorithm. Pattern Recognition, 31\\n(3):315–325, 1998. ISSN 0031-3203. doi: https://doi.org/10.1016/S0031-3203(97)00045-9. https://www.sciencedirect.\\ncom/science/article/pii/S0031320397000459.\\nR. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE\\nComputer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), volume 2, pages 1735–1742,\\n2006. doi: 10.1109/CVPR.2006.100.\\nAbul Hasnat, Julien Bohné, Jonathan Milgram, Stéphane Gentric, and Liming Chen. von mises-fisher mixture model-\\nbased deep learning: Application to face verification. CoRR, abs/1706.04264, 2017. http://arxiv.org/abs/1706.04264.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings\\nof the IEEE conference on computer vision and pattern recognition, 2016.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual\\nrepresentation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\\npages 9726–9735, 2020. doi: 10.1109/CVPR42600.2020.00975.\\nLang Huang, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, and Toshihiko Yamasaki. Learning where to learn in\\ncross-view self-supervised learning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR), pages 14431–14440, 2022a. doi: 10.1109/CVPR52688.2022.01405.\\nZhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. Learning representation for clustering via prototype\\nscattering and positive sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022b.\\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal\\ncovariate shift. In International conference on machine learning, pages 448–456. pmlr, 2015.\\nAnil K Jain, M Narasimha Murty, and Patrick J Flynn. Data clustering: a review. ACM computing surveys (CSUR),\\n31(3):264–323, 1999.\\nLi Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian.\\nUnderstanding dimensional collapse in contrastive\\nself-supervised learning. In International Conference on Learning Representations, 2022. https://openreview.net/\\nforum?id=YevsQ05DEN7.\\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big\\nData, 7(3):535–547, 2019.\\nSalar Hosseini Khorasgani, Yuxuan Chen, and Florian Shkurti. Slic: Self-supervised learning with iterative clustering\\nfor human action videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR), pages 16091–16101, June 2022.\\nYa Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.\\nJunnan Li, Pan Zhou, Caiming Xiong, and Steven C.H. Hoi. Prototypical contrastive learning of unsupervised\\nrepresentations. In ICLR, 2021a.\\nYunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. Proceedings\\nof the AAAI Conference on Artificial Intelligence, 35(10):8547–8555, May 2021b. doi: 10.1609/aaai.v35i10.17037.\\nhttps://ojs.aaai.org/index.php/AAAI/article/view/17037.\\nStuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129–137, 1982.\\nGeoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite mixture models. Wiley, 2000.\\nGeorge A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, 1995.\\nIshan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In 2020\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6706–6716, 2020.\\ndoi:\\n10.1109/CVPR42600.2020.00674.\\nKevin\\nP.\\nMurphy.\\nMachine\\nlearning\\n:\\na\\nprobabilistic\\nperspective.\\nMIT\\nPress,\\nCam-\\nbridge,\\nMass.\\n[u.a.],\\n2013.\\nISBN\\n9780262018029\\n0262018020.\\nhttps://www.amazon.com/\\nMachine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref=sr_1_2?ie=UTF8&qid=\\n1336857747&sr=8-2.\\n14\\n\\nCurtis G Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine learning\\nbenchmarks. arXiv preprint arXiv:2103.14749, 2021.\\nJosé M Pena, Jose Antonio Lozano, and Pedro Larranaga. An empirical comparison of four initialization methods for\\nthe k-means algorithm. Pattern recognition letters, 20(10):1027–1040, 1999.\\nPhillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images\\nand its impact on learning. International Conference on Learning Representations, 2021.\\nCatherine A. Sugar and Gareth M. James. Finding the number of clusters in a data set: An information theoretic\\napproach. Journal of the American Statistical Association, 98:750–763, 2003.\\nY. Tian, O. J. Henaff, and A. Van Den Oord. Divide and contrast: Self-supervised learning from uncurated data. In\\n2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 10043–10054, Los Alamitos, CA,\\nUSA, oct 2021. IEEE Computer Society. doi: 10.1109/ICCV48922.2021.00991. https://doi.ieeecomputersociety.\\norg/10.1109/ICCV48922.2021.00991.\\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849,\\n2019.\\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision – ECCV 2020,\\npages 776–794, Cham, 2020. Springer International Publishing.\\nAaron van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep gaussian mixture\\nmodels. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural\\nInformation Processing Systems, volume 27. Curran Associates, Inc., 2014a. https://proceedings.neurips.cc/paper/\\n2014/file/8c3039bd5842dca3d944faab91447818-Paper.pdf.\\nAäron van den Oord and Benjamin Schrauwen. The student-t mixture as a natural image patch prior with application\\nto image compression. Journal of Machine Learning Research, 15(60):2061–2086, 2014b. http://jmlr.org/papers/\\nv15/vandenoord14a.html.\\nNguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings comparison: is a\\ncorrection for chance necessary? In Proceedings of the 26th annual international conference on machine learning,\\npages 1073–1080, 2009.\\nZhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance\\ndiscrimination. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3733–3742, 2018.\\nLinxiao Yang, Ngai-Man Cheung, Jiaying Li, and Jun Fang.\\nDeep clustering by gaussian mixture variational\\nautoencoders with graph embedding. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV),\\npages 6439–6448, 2019. doi: 10.1109/ICCV.2019.00654.\\nAsano YM., Rupprecht C., and Vedaldi A. Self-labelling via simultaneous clustering and representation learning. In\\nInternational Conference on Learning Representations, 2020.\\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via\\nredundancy reduction. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24\\nJuly 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12310–12320. PMLR,\\n2021.\\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene\\nrecognition using places database. Advances in neural information processing systems, 27, 2014.\\nBo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep\\nautoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on Learning\\nRepresentations, 2018. https://openreview.net/forum?id=BJJLHbb0-.\\nDaniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration. In 2011\\nInternational Conference on Computer Vision, pages 479–486, 2011. doi: 10.1109/ICCV.2011.6126278.\\n15\\n\\nAppendix\\nA\\nAlgorithm Pseudocode\\nAlgorithm 1 SiamMM Pseudocode\\n# E: backbone + projection MLP\\n# Em: momentum encoder of E\\n# g: prediction MLP\\nInput: training data X, number of cluster K\\nInitial: cluster centroids {rk}K\\nk=1\\nwhile not Max Epochs do\\nE-step: Update assignment A(.) by clustering embeddings on the centroid {rk}K\\nk=1 from the last\\niteration\\nM-step: Update µk and κk for k = 1, 2, ..., K\\nfor x in X do\\nx1, x2 = aug(x), aug(x) # image augmentation\\nv1, v2 = g(E(x1)), g(E(x2))\\nvm\\n1 , vm\\n2 = Em(x1), Em(x2)\\nL(v1, v2, vm\\n1 , vm\\n2 |µA(x), κA(x)) # loss in (13)\\nupdate(E,g) # optimization update\\nupdate(Em) # momentum update\\nend for\\nMerge: Merge centroids {rk}K\\nk=1 based on (11)\\nUpdate number of clusters K\\nend while\\nB\\nRelation to other methods\\nWe compare the proposed method with the previous state-of-the-art unsupervised learning method such as\\nPCL Li et al. (2021a), SwAV Caron et al. (2020), and ProPos Huang et al. (2022b).\\nPCL. Both PCL Li et al. (2021a) and SiamMM attempt to improve the within-cluster compactness in an EM\\nmanner. However, PCL is based on an InfoNCE instance-wise contrastive loss requiring negative samples. As\\na result, PCL can suffer from the class collision issue Huang et al. (2022b) from contrasting false negative\\ninstances. SiamMM does not require negative centroids (Section 4.1).\\nSwAV. Though SwAV Caron et al. (2020) and SiamMM can be implemented by soft-assignment clustering, they\\nrepresent different conceptual approaches to representation learning. SwAV poses clustering as a prediction\\nproblem, using the embedding of one augmented view to fit the assignment distribution of the other view,\\nwhile SiamMM directly models the clustering distribution by maximizing the likelihood of a mixture model.\\nProPos. ProPos Huang et al. (2022b) and SiamMM have similarities in using non-contrastive losses as the\\ninstance-wise objective. The primary difference is that ProPos implements clustering on two augmented views\\nto compute paired cluster centroids, with an infoNCE contrastive loss requiring a large number of negative\\ncentroids. However, SiamMM has a more straightforward interpretation of the mixture model and is more\\nsample efficient in not requiring negative centroids.\\nC\\nAdditional Implementation Details\\nC.1\\nDetails on parameter estimation\\nThe loss function of SiamMM attempts to pull image embeddings closer to the nearest cluster centroids on a\\nsphere. The mean estimator ˆµ is a normalized cluster centroid. The concentration parameter κ scales the\\n16\\n\\ngradient for each cluster, respectively. It tries to make the loosely distributed cluster more compact compared\\nwith the densely distributed clusters. Based on (7), the estimator includes the length of centroids vector ||r||\\nand the dimensionality d. However, as the dimension of embedding might be correlated, it is non-trivial to\\nestimate the dimensionality d.\\nOne simple way to address this problem is to use PCA to reduce the dimension of r and re-measure it length\\nin a low-dimensional space. Based on our experiments, with only 150 principle components the rP CA can\\nreserve more than 80% norm out of the original 256-dim r, so we rewrite (7) by\\nˆκk\\nP CA = ||rP CA\\nk\\n||dP CA −||rP CA\\nk\\n||3\\n1 −||rP CA\\nk\\n||2\\n(16)\\nwhere dP CA is the number of principle components selected to form rP CA\\nk\\n. Note that the dimension reduction\\nhas only been done in the estimation of κ. We still keep the original dimensionality of image representations.\\nThe discussion of dimension reduction in representation learning is out of the scope of this paper.\\nD\\nVariants of SiamMM loss\\nFigure 4 Illustration of the differenct negative sampling strategies; the dot and cross indicate the embedding of a data\\npoint and a cluster centroid, respectively; the blue arrow illustrates a positive pair, while the red arrow illustrates a\\nnegative pair. (Left) sampling negative cluster centroids (14) either restored in a memory bank Li et al. (2021a) or\\nfrom a batch size Caron et al. (2020); (Right) sampling negative data points out of a target cluster, introduced in (15)\\n.\\nTable 8 Linear evaluation on ImageNet for the variant losses of SiamMM; top-1 accuracy is reported (in %); Compared\\nwith SiamMM, SiamMM+Lnce1 (14) introduces negative centroids whose number is chosen as |A−| = 16000 as\\nsuggested in Li et al. (2021a); SiamMM+Lnce2 (15) introduces additional negative instances.\\nMethod\\nNeg. inst.\\nNeg. cent.\\nTop-1\\nSiamMM Lnce1\\nX\\n72.3\\nSiamMM Lnce2\\nX\\n72.2\\nSiamMM\\n72.2\\n17\\n\\nE\\nAdditional Figures\\nFigure 5 Training time per 100 epochs for different numbers of cluster re-initializations and with different number of\\nclusters.\\nFigure 6 The number of clusters converge to almost the same number of classes included in TinyImageNet.\\nFigure 7 Read from right to left, the top-1 accracy increases linearly as the number of clusters converges on par with\\nthe true number of classes during the training process.\\n18\\n\\nFigure 8 Visualization of Merged 1000 Clusters. Left: Images grouped under the same cluster label, where each row\\ncorresponds to a true class label in ImageNet. Right: Images grouped under class label “whiskey jar”, where each row\\ncorresponds to a predicted clustering label.\\nFigure 9 Visualization of Merged 1000 Clusters. Left: Images grouped under the same cluster label, where each row\\ncorresponds to a true class label in ImageNet. Right: Images grouped under class label “spiny lobster”, where each row\\ncorresponds to a predicted clustering label. Identifiable faces are blurred in the images.\\nFigure 10 Visualization of Merged 1000 Clusters. Left: Images grouped under the same cluster label, where each row\\ncorresponds to a true class label in ImageNet. Right: Images grouped under class label “mosque”, where each row\\ncorresponds to a predicted clustering label.\\n19\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.text_extraction import extract_text_from_pdf_threaded\n",
    "\n",
    "\n",
    "documents = []\n",
    "for files in discover_dirs:\n",
    "    logging.debug(files)\n",
    "    data = extract_text_from_pdf_threaded(str(files))\n",
    "    documents.append(data)\n",
    "\n",
    "# extract_text_from_pdf_threaded(\"data/downloads/pdf/2511.04981.pdf\")\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf73013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Deep Progressive Training: scaling up depth capacity of zero/one-layer models Zhiqi Bu Meta FAIR Model depth is a double-edged sword in deep learning: deeper models achieve higher accuracy but require higher computational cost. To efficiently train models at scale, an effective strategy is the progressive training, which scales up model capacity during training, hence significantly reducing computation with little to none performance degradation. In this work, we study the depth expansion of large models through the lens of optimization theory and feature learning, offering insights on the initialization of new layers, hyperparameter transfer, learning rate schedule, and timing of model expansion. Specifically, we propose zero/one-layer progressive training for the optimal tradeoff between computation and loss. For example, zero/one-layer progressive training on GPT2 can save ≈80% compute, or equivalently accelerate ≈5× while achieving almost the same loss, compared to to a fully trained 60-layer model with 7B parameters. Correspondence: zhiqibu@meta.com 1 Introduction Strong performance of deep learning models is highly correlated to model sizes, with larger model having higher accuracy but also incurring higher computation cost to train, e.g. LLAMA-4 training costs over 7M GPU hours and an estimated 2,000 tons of carbon emissions. This phenomenon leads to a tradeoff between model utility (measured by loss or accuracy) and computational cost (measured by floating point operation, or FLOP), and has motivated scaling laws to train compute-optimal large language models Hoffmann et al. (2022); Kaplan et al. (2020). To accelerate the training of large models, one direction is known as progressive training or model growth, which initially trains a small model (a.k.a. teacher or source model) and then scales up to large models (a.k.a. student or grown model) during training. In contrast to the fixed-size training, the progressive training formulates the model size as a time-dependent variable, and it is clearly more efficient because the compute is 6BTN, proportional to the model size N. For example, consider a progressive training that scales up the model size at iteration τ: N(t) = ( Nsmall if t ≤τ Nlarge if t > τ (1.1) The fixed-size training requires 6BTNlarge FLOPs, whereas the progressive training requires 6B(τNsmall + (T −τ)Nlarge), which is significantly less if (I) τ is close to T and (II) Nsmall ≪Nlarge. As a brief preview, we will develop techniques to push τ ≈0.8T and to train zero/one-layer small models with Nsmall ≈0.02 · Nlarge, hence accelerating ≈5× in Figure 1. A long list of research has contributed to the development of progressive training, especially on initialization of large models, multi-stage training, training regime, and theory. Initialization from precedented small models. Chen et al. (2015); Wang et al. (2023b); Yao et al. study the function-preserving initialization, such that the large model has the same loss and function as the small model at the moment of depth expansion. These works scale up the depth of convolution networks and BERT by 2× and reduce the computation to ≈70% computation. However, while function-preserving guarantees nice behavior during depth expansion, it does not guarantee fast convergence after the expansion. Alternatively, 1 arXiv:2511.04981v1 [cs.LG] 7 Nov 2025 without function-preserving,',\n",
       "  'of depth expansion. These works scale up the depth of convolution networks and BERT by 2× and reduce the computation to ≈70% computation. However, while function-preserving guarantees nice behavior during depth expansion, it does not guarantee fast convergence after the expansion. Alternatively, 1 arXiv:2511.04981v1 [cs.LG] 7 Nov 2025 without function-preserving, Chen et al. (2021) linearly combines two layers to initialize a new layer; Gong et al. (2019); Yang et al. (2020); Du et al. (2024) stacks the old layers multiple times; Qin et al. (2021); Wang et al. (2023a) propose learning-based methods that require extra training. These methods empirically scale up the depth by 2 ∼4× and reduce the “grown v.s. target” computation to ≈55 ∼70% (e.g. Wang et al. (2023a), Figure 1 in Chen et al. (2021), Figure 6 in Pan et al. (2024), and Figure 1 in Du et al. (2024)). In contrast, we scale up the depth by 60× and reduce the computation to 20%. Multi-stage training. Most works in progressive training expand the small models once like (1.1). However, many works study multi-stage training and gradual stacking Reddi et al. (2023). For instance, Gong et al. (2019); Shen et al. (2022); Qin et al. (2021); Pan et al. (2024); Yao et al. scale up the sizes of BERT for 3 ∼4× during training, optionally freezing some of the layers at some stages Agarwal et al. (2024); Yang et al. (2020); Yano et al. (2025a). We will leverage our discovery of the mixing behaviors to show that multi-stage training is less efficient and can be unnecessary. Training regime. While most progressive training methods are tested on classification models like BERT Devlin et al. (2019) and ViT Dosovitskiy et al. (2020), some recent papers have evaluated on generative language models like GPT2 and reported 1.4 ∼2× speedup Du et al. (2024). As shown in Figure 1 and Figure 7, our method enjoys 5× speedup across different model sizes. We also note some papers that scale up MoE (not in terms of depth though) but the speedup seems transient as discussed in Section 7. Theory. Theoretical analysis in progressive training is largely lacking, except Agarwal et al. (2024) on strongly-convex and smooth loss. In contrast, we give a convergence theory of convex and Lipschitz continuous (non-smooth) loss and empirically validate its insights. Besides a convergence theory, we also study feature learning and hyperparameter transfer for progressive training. 1.1 Related work In addition to previous works in progrssive training, this work is closely related to convex optimization in Section 4, feature learning theory in Section 3.2, and learning rate schedules (especially warmup-stable-decay; WSD Xing et al. (2018); Hägele et al. (2024)). 1.2 Contributions To our best knowledge, we are the first to advocate zero/one-layer depth expansion and to explore WSD schedule in progressive training. We summarize our main contributions here and provide extra insights on optimizer choice, optimizer states, model size, batch size, multi-stage expansion, and other topics in Section C. 1. We analyze the depth expansion as an initialization problem and ensure feature learning. This approach allows hyperparameter transfer (e.g. learning',\n",
       "  'progressive training. We summarize our main contributions here and provide extra insights on optimizer choice, optimizer states, model size, batch size, multi-stage expansion, and other topics in Section C. 1. We analyze the depth expansion as an initialization problem and ensure feature learning. This approach allows hyperparameter transfer (e.g. learning rate) throughout the progressive training, in contrast to extra hyperparameter tuning Gu et al. (2020); Yano et al. (2025b). 2. We reveal the important role of learning rate schedule, especially WSD schedule which theoretically and empirically improves the convergence. 3. We discover the mixing behaviors of progressive training, which supports the mixing time transfer and single-stage training, in contrast to multi-stage expansion Gong et al. (2019); Yang et al. (2020); Qin et al. (2021); Shen et al. (2022); Pan et al. (2024). 4. We show that zero/one-layer progressive training has the best tradeoff between computational cost and loss, compared to multi-layer progressive or fixed-size training (see Figure 8). Specifically, zero/one-layer depth expansion is easy to implement and devoid of ordering issues (see Section 3.3 and Section A.3). 5. We theoretically analyze the convergence of progressive training under convex optimization to give insights on initialization, learning rate schedule, and projected gradient descent. 2 0 100000 200000 300000 400000 500000 600000 Iterations 2 4 6 8 10 12 Validation loss GPT 0 12-layer (random) GPT 0 12-layer (early stopped) GPT 1 12-layer (copying) GPT 1 12-layer (random) GPT 1 12-layer (early stopped) GPT 12-layer (fixed-size) 0.0 0.5 1.0 1.5 2.0 FLOPs 1e20 2.75 3.00 3.25 3.50 3.75 4.00 4.25 4.50 4.75 Validation loss X speedup 2.0 2.2/ GPT 0 12-layer (random) GPT 1 12-layer (copying) GPT 1 12-layer (random) GPT 12-layer (fixed-size) 0 100000 200000 300000 400000 500000 600000 Iterations 2 4 6 8 10 12 Validation loss GPT 0 60-layer (random) GPT 0 60-layer (early stopped) GPT 1 60-layer (copying) GPT 1 60-layer (random) GPT 1 60-layer (early stopped) GPT 60-layer (fixed-size) 0.00 0.25 0.50 0.75 1.00 1.25 1.50 FLOPs 1e21 2.5 3.0 3.5 4.0 4.5 Validation loss X speedup 4.4 4.8/ GPT 0 60-layer (random) GPT 1 60-layer (copying) GPT 1 60-layer (random) GPT 60-layer (fixed-size) Figure 1 Zero-layer (red, 39M or 0.15B) and one-layer (blue and green, 46M or 0.27B) progressive training can achieve significant speedup over fixed-sized training (black, 12-layer 124M or 60-layer 7B) on GPT2 pre-trained on OpenWebText under WSD schedule. The difference in final validation loss is < 0.5% for 124M runs and < 0.2% for 7B runs. The depth expansion takes place at 80% of iterations for full runs, and at 2% of iterations immediately after warmup for early stopped runs. 2 Experiment settings We use the GPT2 Radford et al. (2019), Mixture of Experts (MoE, Fedus et al. (2022)), and ResNet He et al. (2016) model families as testbeds1. For GPT2, we train on OpenWebText dataset Gokaslan et al. (2019) with 1024 sequence length, following nanoGPT codebase. For ResNet, we train on ImageNet dataset with 224× 224 resolutions for 100 epochs. Our main optimizer is Muon-NSGD, with 0.1 weight decay and without gradient clipping. The Muon-NSGD',\n",
       "  'families as testbeds1. For GPT2, we train on OpenWebText dataset Gokaslan et al. (2019) with 1024 sequence length, following nanoGPT codebase. For ResNet, we train on ImageNet dataset with 224× 224 resolutions for 100 epochs. Our main optimizer is Muon-NSGD, with 0.1 weight decay and without gradient clipping. The Muon-NSGD is adapted from the original Muon Jordan et al. (2024) by (1) optimizing all 2D tensors with Muon and other tensors with normalized SGD (NSGD), and (2) using a single learning rate for both Muon and NSGD. We use the cosine learning rate schedule and WSD schedule, that decay to 0 with 2% warm-up. Additional details are in Section B. 3 How to expand depth? 3.1 Depth expansion approaches We introduce multiple approaches to expand the depth of a residual neural network. 1For ResNet, the models are configured by 4 stages, e.g. ResNet50 with [3,4,6,3] and ResNet101 with [3,4,23,3]. In each stage, the first layer has one shape, and each of the other layers has the same shape which is different to the first layer. Hence the zero-layer analogy corresponds to ResNet14 with [1,1,1,1] and the one-layer analogy corresponds to ResNet26 with [2,2,2,2]. 3 • [copying]: New layers are copied from the small model Chang et al. (2018); Gong et al. (2019); Li et al. (2020). • [random]: New layers are randomly initialized Wang et al. (2017); Chen et al. (2021). • [zero]: New layers are initialized as zeros. This approach kills the gradient flow and makes the new layers untrainable, hence invalidating the progressive training. • [copying_zero]: New layers are copied from the small model, except some sub-layers are zero Shen et al. (2022); Wang et al. (2023b); Tan et al. (2024); Wu et al. (2024); Du et al. (2024). To test these approaches in a minimalist manner, we expand zero/one-layer versions of GPT2 and ResNet to multiple layers in Figure 2. The experiments of copying_zero approach can be found in Section A. 0 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Loss GPT 0-layer (fixed size) GPT 0 12-layer (random) GPT 1-layer (fixed size) GPT 1 12-layer (zero) GPT 1 12-layer (random) GPT 1 12-layer (copying) GPT 12-layer (fixed size) 50000 100000 150000 200000 250000 Iterations 100 101 Loss ResNet14 (fixed size) ResNet14 50(random) ResNet26 (fixed size) ResNet26 50(zero) ResNet26 50(random) ResNet26 50(copying) ResNet50 (fixed size) Figure 2 Convergence of zero/one-layer progressive training and fixed-size training. Left: ResNet with depth expansion at 32-th epoch. Right: GPT2 with depth expansion at 50k iterations. Takeaway 1: For zero/one-layer progressive training, random and copying are empirically the best initializations of new layers. 3.2 Feature learning and Hyperparameter transfer To ensure feature learning and keep the representations non-trivial and stable, each layer’s activation needs to have consistent element sizes: denoting l-th layer’s activation as Al ∈Rnl, then ∥Al∥2/√nl ∼∥Al+1∥2/√nl+1 Mei et al. (2019); Yang and Hu (2020); Chizat and Bach (2018); Yang et al. (2022, 2023). For linear layers Al+1 = AlWl, this translates to the spectral scaling condition by muP theory, i.e. the spectral norm',\n",
       "  'consistent element sizes: denoting l-th layer’s activation as Al ∈Rnl, then ∥Al∥2/√nl ∼∥Al+1∥2/√nl+1 Mei et al. (2019); Yang and Hu (2020); Chizat and Bach (2018); Yang et al. (2022, 2023). For linear layers Al+1 = AlWl, this translates to the spectral scaling condition by muP theory, i.e. the spectral norm ∥Wl∥∗∼ p nl+1/nl for all layers. Importantly, muP allows zero-shot hyperparameter transfer across model sizes, so that the optimal hyper- parameters (e.g. learning rate) are the same for small and large models. In Figure 3, we illustrate this on the Muon-NSGD optimizer with muP-scaling learning rate. We highlight that hyperparameter transfer is particularly desirable in progressive training, where model sizes change significantly before and after the model expansion. In fact, without the residual connection, only copying and random satisfy muP condition, but not zero or copying_zero, since any zero sub-layer will have ∥Wl∥∗= 0 ̸∼ p nl+1/nl. Consequently, there is a conflict between muP condition and function-preserving (FPI) outside the residual path. Here function-preserving means the large model has exactly the same loss as the small model Chen et al. (2015); Shen et al. (2022); Wang et al. (2023b), hence no loss spikes. Takeaway 2: For layers outside the residual path, zero initialziation =⇒FPI =⇒not muP, which does not enjoy fast convergence and feature learning. 4 10 3 10 2 10 1 learning rate 3.0 3.5 4.0 4.5 5.0 5.5 6.0 Loss GPT 0-layer GPT 1-layer GPT 2-layer GPT 6-layer GPT 12-layer 10 3 10 2 10 1 learning rate 0.5 1.0 1.5 2.0 2.5 Loss ResNet14 ResNet26 ResNet50 ResNet101 Figure 3 Validation (solid) and training loss (dashed) at different learning rates of Muon-NSGD. Table 1 Summary of initialization approaches in progressive training. function-preserving trainability feature learning copying no high yes random no high yes zero yes low no 3.3 Where to expand? For zero-layer expansion, only random initialization works; for one-layer expansion, random and copying both work; however, for multi-layer expansion, we must consider the ordering in depth expansion. We consider three variants of copying initialization: suppose we expand 3 to 6 layers, • [copying_last], copying only the last layer, e.g. [1, 2, 3] →[1, 2, 3, 3, 3, 3]. • [copying_stack], copying and stacking all layers, e.g. [1, 2, 3] →[1, 2, 3, 1, 2, 3]. • [copying_inter], copying and interpolating all layers, e.g. [1, 2, 3] →[1, 1, 2, 2, 3, 3]. We note that copying_inter (interpolating) is adopted by Chang et al. (2018); Pan et al. (2024); Dong et al. (2020); Qin et al. (2022), as well as Wang et al. (2023b) if some sub-layers are zeros; copying_stack (stacking) is adopted by Gong et al. (2019); Li et al. (2020); Fu et al. (2023), as well as Shen et al. (2022); Du et al. (2024) if some sub-layers are zeros. To test these variants, we experiment with deeper models such as ResNet50 and GPT 6-layer in Figure 4. We observe that copying all layers is consistently better than only copying one layer (copying_last), whereas copying_inter and copying_stack are almost indistinguishable. 0 20000 40000 60000 80000 100000 Iterations 2.9',\n",
       "  'are zeros. To test these variants, we experiment with deeper models such as ResNet50 and GPT 6-layer in Figure 4. We observe that copying all layers is consistently better than only copying one layer (copying_last), whereas copying_inter and copying_stack are almost indistinguishable. 0 20000 40000 60000 80000 100000 Iterations 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 Loss GPT 6-layer copying_last copying_stack copying_inter GPT 12-layer 0 20000 40000 60000 80000 100000 Iterations 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 Loss GPT 6-layer copying_last copying_stack copying_inter GPT 12-layer 50000 100000 150000 200000 250000 Iterations 100 6 × 10 1 2 × 100 3 × 100 4 × 100 Loss ResNet50 copying_last copying_stack copying_inter ResNet101 50000 100000 150000 200000 250000 Iterations 100 6 × 10 1 2 × 100 3 × 100 4 × 100 Loss ResNet50 copying_last copying_stack copying_inter ResNet101 Figure 4 Convergence of multi-layer progressive training and fixed-size training. Left: ResNet with depth expansion at 32/64-th epoch. Right: GPT2 with depth expansion at 30/70k iterations. Takeaway 3: Copying_inter and copying_stack are similarly performant for multi-layer depth expansion, but they are invalid for zero-layer depth expansion and equivalent for one-layer depth expansion (e.g. from [1] →[1, 1, 1, 1, 1, 1]). 5 4 A convergence theory for progressive training We analyze the convergence of progressive training under convex and G-Lipschitz loss L, so as to offer useful insights to the training recipe in Section 7. In fact, although deep learning is non-convex, its training dynamics is similar to convex optimization Schaipp et al. (2025); Defazio and Mishchenko (2023); Lee et al. (2019); Bu et al. (2021); Jacot et al. (2018); Allen-Zhu et al. (2019); Leclerc and Madry (2020); Bu and Xu (2024). We denote the small model before depth expansion as wt, the large model after depth expansion as Wt, and the corresponding minima as w∗and W∗. For any iteration trained with SGD, wt+1 = wt −ηt+1 ∂L ∂wt , the classical analysis gives ∥wt+1 −w∗∥2 ≤∥wt −w∗∥2 −2ηt+1(L(wt) −L(w∗)) + η2 t+1G2 (4.1) and equivalently, for the large model training with the same learning rate schedule, ∥Wt+1 −W∗∥2 ≤∥Wt −W∗∥2 −2ηt+1(L(Wt) −L(W∗)) + η2 t+1G2 (4.2) Now for the progressive training with depth expansion at t = τ, we use telescoping sum (4.1) from t = 0 →τ −1 and (4.2) from t = τ →T −1 to obtain ∥wτ −w∗∥2 + ∥WT −W∗∥2 ≤∥w0 −w∗∥2 + ∥Wτ −W∗∥2 + T −1 X t=0 η2 t+1G2 + 2 τ−1 X t=0 ηt+1(L(w∗) −Lt) + 2 T −1 X t=τ ηt+1(L(W∗) −Lt) where Lt = L(wt) for t < τ, and Lt = L(Wt) otherwise. Dividing by 2 PT −1 t=0 ηt+1 and re-arranging give PT −1 t=0 ηt+1Lt PT −1 t=0 ηt+1 ≤ Pτ−1 t=0 ηt+1L(w∗) + PT −1 t=τ ηt+1L(W∗) PT −1 t=0 ηt+1 + G2 PT −1 t=0 η2 t+1 2 PT −1 t=0 ηt+1 + ∥w0 −w∗∥2 −∥WT −W∗∥2 + ∥Wτ −W∗∥2 −∥wτ −w∗∥2 2 PT −1 t=0 ηt+1 On the left hand side, we apply the Jensen’s inequality to get L( ¯ Wprogressive T ) ≤ PT',\n",
       "  'PT −1 t=0 ηt+1 + G2 PT −1 t=0 η2 t+1 2 PT −1 t=0 ηt+1 + ∥w0 −w∗∥2 −∥WT −W∗∥2 + ∥Wτ −W∗∥2 −∥wτ −w∗∥2 2 PT −1 t=0 ηt+1 On the left hand side, we apply the Jensen’s inequality to get L( ¯ Wprogressive T ) ≤ PT −1 t=0 ηt+1Lt PT −1 t=0 ηt+1 , where ¯ Wprogressive T = Pτ−1 t=0 ηt+1[wt, 0] + PT −1 t=τ ηt+1Wt PT −1 t=0 ηt+1 is the running average of iterates, and we have used L([wt, 0]) = L(wt) for residual networks. On the right hand side, we throw away −∥WT −W∗∥2 because it is small and negative. We obtain L( ¯ Wprogressive T ) ≤ Pτ−1 t=0 ηt+1L(w∗) + PT −1 t=τ ηt+1L(W∗) PT −1 t=0 ηt+1 + G2 PT −1 t=0 η2 t+1 2 PT −1 t=0 ηt+1 + ∥w0 −w∗∥2 + ∥Wτ −W∗∥2 −∥wτ −w∗∥2 2 PT −1 t=0 ηt+1 (4.3) We can easily recover the fixed-size large model training from scratch by setting τ = 0: L( ¯ Wfixed-size T ) ≤L(W∗) + G2 PT −1 t=0 η2 t+1 2 PT −1 t=0 ηt+1 + ∥W0 −W∗∥2 2 PT −1 t=0 ηt+1 (4.4) 6 Subtracting (4.3) from (4.4), we obtain the difference which is desired to be ⪅0, Pτ t=1 ηt(L(w∗) −L(W∗)) PT t=1 ηt + ∥w0 −w∗∥2 −∥W0 −W∗∥2 + ∥Wτ −W∗∥2 −∥wτ −w∗∥2 2 PT t=1 ηt We view the large model as the concatenation of a small model and extra parameters Wt = [wt, xt] for t = 0, τ, and simplify the analysis by assuming W∗= [w∗, x∗]. We now obtain L( ¯ Wprogressive T ) −L( ¯ Wfixed-size T ) ≤ Pτ t=1 ηt PT t=1 ηt (L(w∗) −L(W∗)) + ∥xτ −x∗∥2 −∥x0 −x∗∥2 2 PT t=1 ηt (4.5) From the viewpoint of large model, we can mathematically view the progressive training as projected gradient descent (PGD) that projects the deeper layers to zero, followed by an instant teleportation of xτ from zero to some good initialization, then continued with SGD. In words, the effectiveness of progressive training comes from both optimizers (PGD and SGD) and initialization of new layers. Taking a closer look at (4.5), we can optimize this difference via the following factors. • Initialization strategy of xτ: given that x0 is randomly initialized, (1) if we randomly initialize new layers, then the second term is zero; (2) if we initialize better than random, then the second term is negative and the difference is improved. This analysis is visualized in Figure 2. • Learning rate schedule ηt: to minimize Pτ t=1 ηt PT t=1 ηt , we prefer smaller ηt for t ≤τ than for t > τ, contrary to learning rate decay but consistent with WSD schedule, where ηt remains constant for most iterations (see Figure 5). To validate our insights on learning rate schedules, we experiment cosine and WSD schedules each with optimally tuned learning rate in Figure 5. We expand small models to large models at every 10% of total training horizon. For ResNet, the small model',\n",
       "  'remains constant for most iterations (see Figure 5). To validate our insights on learning rate schedules, we experiment cosine and WSD schedules each with optimally tuned learning rate in Figure 5. We expand small models to large models at every 10% of total training horizon. For ResNet, the small model can still catch up with large model when τ ≈0.8T under WSD schedule, but it fails to catch up around τ ≥0.7T under cosine schedule; for GPT, the small model can catch up until τ ≈0.8T under WSD schedule, but it fails around τ ≥0.5T under cosine schedule. 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 0-layer (cosine schedule) GPT 12-layer (cosine schedule) 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 0-layer (WSD schedule) GPT 12-layer (WSD schedule) 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 3.0 3.5 4.0 4.5 5.0 5.5 Validation loss cosine schedule WSD schedule 50000 100000 150000 200000 250000 Iterations 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 Validation loss ResNet14 (WSD schedule) ResNet50 (WSD schedule) 50000 100000 150000 200000 250000 Iterations 40 45 50 55 60 65 70 75 80 Validation accuracy ResNet14 (WSD schedule) ResNet50 (WSD schedule) 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00 Validation loss 75.25 75.50 75.75 76.00 76.25 76.50 76.75 77.00 77.25 Validation accuracy Figure 5 Performance of zero-layer progressive training and fixed-size training, where WSD schedule significantly enhances the progressive training. See one-layer results in Section C. Takeaway 4: Progressive training is indeed “PGD + initialization of new layers + SGD”, whose effectiveness relies on good initialization (e.g. random) and learning rate schedule (e.g. WSD). 7 5 When to expand depth? To determine the timing of depth expansion, we need to understand the mixing time, which is the time until the loss of progressive training is close to the fixed-size large model training. To be specific, we define tmix such that L(Wfixed-size τ+tmix ) ≈L(Wprogressive τ+tmix ). Clearly, if the mixing time is short, then we can expand the models at later stage and save more compute. 5.1 Perspectives matter to mixing behaviors We highlight that the mixing behaviors of progressive training (e.g. Figures 5, 9,13) have not been clearly observed in the literature, possibly due to the difference in perspectives of comparison. In figures of Wang et al. (2023a); Chen et al. (2021); Pan et al. (2024); Du et al. (2024), the comparison is between the grown model and the target model, while our comparison is based on the entire training (source plus grown models). Such a perspective omits the computational cost of small models and the stated speedup must be discounted in our context. We re-plot Figure 5 (GPT under WSD schedule) from their perspective and no longer observe the mixing behaviors in Figure 6. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 Validation loss GPT 12-layer',\n",
       "  'must be discounted in our context. We re-plot Figure 5 (GPT under WSD schedule) from their perspective and no longer observe the mixing behaviors in Figure 6. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 Validation loss GPT 12-layer 20000 40000 60000 80000 100000 Iterations 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 Validation loss GPT 0 12-layer ( /T = 0.5) GPT 12-layer Pre-growth Post-growth Figure 6 Different perspectives of Figure 5 to compare progressive training and fixed-size training. Left: only com- paring the grown model and target model. Right: matching the pre-growth loss of source model to target model. 0.0 0.5 1.0 1.5 2.0 FLOPs 1e20 2.8 3.0 3.2 3.4 3.6 3.8 Validation loss 5.0X speedup GPT 0 12-layer (random) GPT 1 12-layer (copying) GPT 1 12-layer (random) GPT 12-layer (fixed-size) 0.00 0.25 0.50 0.75 1.00 1.25 1.50 FLOPs 1e21 2.5 3.0 3.5 4.0 4.5 Validation loss 5.0X speedup GPT 0 60-layer (random) GPT 1 60-layer (copying) GPT 1 60-layer (random) GPT 60-layer (fixed-size) Figure 7 Comparing progressive training and fixed- size training via the grown model and target model of Figure 1. Left: 124M model. Right: 7B model. Another perspective in the literature is to overlay the loss curve for the grown model over the target model. Shen et al. (2022) suggest that the convergence rate of grown model is “the same as the target model trained from scratch\", and that the training dynamics is preserved. However, we claim that our method significantly improves the training dynamics instead of preserving it, as shown in Figure 6 by comparing the dashed orange curve and the solid black curve. Takeaway 5: The mixing behaviors of loss and training dynamics are the highlight of our depth expansion, which are only observable via the comparison between the entire progressive training and the fixed-size training from scratch. 5.2 Sensitivity to τ under different schedules Interestingly, in Figure 5, the mixing time tmix(τ) is highly sensitive to the timing of depth expansion τ for cosine schedule, but robust to τ for WSD schedule. For example, expanding GPT 1-layer at 10% horizon (blue curve) and expanding at 60% horizon (brown curve) both need ≈16B tokens or 30k iterations to mix with 12-layer training. However, expanding at 80% horizon (grey curve) cannot mix well as the learning rate has decayed. The same patterns hold for ResNet as well. As a consequence, we determine the timing of depth expansion as total duration of constant learning rates minus mixing time in Figure 1. To be more precise, our WSD uses 2% warmup, 10% decay, and 528k iterations with constant learning rate. We subtract ≈40k iterations of mixing time from it (derived from Figure 5 or the early stopped run), and set the timing of depth expansion at t =480k. Takeaway 6: During the stable phase of WSD schedule, the mixing time is almost unaffected by the timing of depth expansion. Hence we can transfer the mixing time at early iterations until the decaying phase (see Figure 1). 8',\n",
       "  'run), and set the timing of depth expansion at t =480k. Takeaway 6: During the stable phase of WSD schedule, the mixing time is almost unaffected by the timing of depth expansion. Hence we can transfer the mixing time at early iterations until the decaying phase (see Figure 1). 8 6 Which to expand depth? While we can expand the depth of any small model, we show the following through 150 runs (3 large model sizes, 5 small model sizes, 10 expansion times) in Figure 8. Takeaway 7: It is the most computationally efficient to (I) scale up from the zero/one-layer models and (II) scale up only once, i.e. use single-stage progressive training. As we see in Figure 8, the zero/one-layer progressive training almost captures the loss-compute tradeoff from a Pareto-optimal viewpoint, especially in contrast with the progressive training from more than 2 layers. Additionally, the latest timing of expansion that still allows the progressive training to mix with the fixed-size training is not sensitive to small model sizes. In other words, expanding from 1-layer or from 6-layer at τ/T ≈0.6 is similarly effective, but the latter is much more computationally expensive. 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 2.8 3.0 3.2 3.4 3.6 3.8 4.0 Loss GPT 0 12-layer GPT 1 12-layer GPT 2 12-layer GPT 6 12-layer GPT 9 12-layer 0.2 0.4 0.6 0.8 1.0 FLOPs 1e20 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 Loss GPT 0 24-layer GPT 1 24-layer GPT 2 24-layer GPT 6 24-layer GPT 12 24-layer 0.5 1.0 1.5 2.0 2.5 FLOPs 1e20 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 Loss GPT 0 36-layer GPT 1 36-layer GPT 2 36-layer GPT 6 36-layer GPT 18 36-layer 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 2.8 3.0 3.2 3.4 3.6 3.8 4.0 Loss GPT 0 12-layer GPT 1 12-layer fixed-size 1,2,6,9,12-layer 0.2 0.4 0.6 0.8 1.0 FLOPs 1e20 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 Loss GPT 0 24-layer GPT 1 24-layer fixed-size 1,2,6,12,24-layer 0.5 1.0 1.5 2.0 2.5 FLOPs 1e20 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 4.0 Loss GPT 0 36-layer GPT 1 36-layer fixed-size 1,2,6,18,36-layer Figure 8 Loss-compute tradeoff (validation loss v.s. FLOPs) of depth expansion from small models to {12, 24, 36}- layer GPT2 with {124M, 400M, 1B} parameters. Another insight from the loss-compute tradeoff is that, it suffices to use single-stage expansion, i.e. we do not need multi-stage expansion such as 0 →12 →24 (if our target model is 24-layer). The reason lies in the mixing behaviors such that 0 →12 →24 can be decomposed to two single-stage expansions 0 →12 and 12 →24. Therefore, the loss curve of 0 →12 →24 will mix with those of 0 →12 and 12 →24, in the first stage and the second stage, respectively. As a result (see Section C.3), multi-stage training achieves the same loss with worse efficiency than single-stage training, in sharp contrast to Gong et al. (2019); Yao et al. where the mixing behaviors are not observed. 7 Deep progressive training recipe We summarize our progressive',\n",
       "  'and the second stage, respectively. As a result (see Section C.3), multi-stage training achieves the same loss with worse efficiency than single-stage training, in sharp contrast to Gong et al. (2019); Yao et al. where the mixing behaviors are not observed. 7 Deep progressive training recipe We summarize our progressive training recipe, leveraging the theoretical insights and empirical evidences in previous sections. 1. Train zero/one-layer model and then expand depth by random initialization2. 2. Train models with Muon-NSGD (or other muP-scaled optimizers) and employ the same hyperparameters before and after depth expansion. 3. Train models with WSD learning rate schedule and expand depth during the stable phase. 2Alternatively, train one-layer model and expand by copying , e.g. w →[w, w, w]. 9 4. The timing of depth expansion τ (or equivalently the mixing time tmix) can be determined by two small-scale runs: one fixed-size training and one progressive training (τ at the end of warmup), both early stopped when their losses mix. We further validate our recipe with MoE Wolfe (2024); Xue et al. (2024) on OpenWebText dataset, and we observe the same patterns as dense models such as the mixing behaviors. We emphasize that our approach is different and orthogonal to existing works that upcycle MoE He et al. (2024), which scale up a small dense model to a large MoE without increasing the depth, rather than a shallow MoE to a deep MoE. This upcycling approach has reported some negative results Muennighoff et al.; Komatsuzaki et al.; Nakamura et al.; Liew et al.; Wei et al. (2024), because the grown MoE becomes worse than the MoE trained from scratch after a few hundred billion tokens. 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss nanoMOE 0-layer (random) nanoMOE 12-layer (fixed size) 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss nanoMOE 1-layer (random) nanoMOE 12-layer (fixed size) 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 3.0 3.5 4.0 4.5 5.0 5.5 Validation loss 0-layer (random) 1-layer (random) Figure 9 Convergence of zero/one-layer progressive training and fixed-size training for MoE, with random initialization of new layers. Takeaway 8: Zero/one-layer progressive training is effective on various model architectures including ResNet, GPT2 and MoE. 8 Conclusion We show that zero/one-layer progressive training can significantly accelerate large-scale training, if it is equipped with good initialization method and learning rate schedule. This strategy retains almost the same performance as fixed-size training thanks to the mixing behaviors. This work demonstrates the power of theoretical insights into progressive training, drawing tools from feature learning and optimization theory. We expect future works to continue pushing the efficiency frontier and the applicability of progressive training, e.g. by scaling up both width and depth. 10 References Naman Agarwal, Pranjal Awasthi, Satyen Kale, and Eric Zhao. Stacking as accelerated gradient descent. arXiv preprint arXiv:2403.04978, 2024. 2 Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242–252. PMLR, 2019. 6 Valentyn',\n",
       "  'and depth. 10 References Naman Agarwal, Pranjal Awasthi, Satyen Kale, and Eric Zhao. Stacking as accelerated gradient descent. arXiv preprint arXiv:2403.04978, 2024. 2 Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242–252. PMLR, 2019. 6 Valentyn Boreiko, Zhiqi Bu, and Sheng Zha. Towards understanding of orthogonalization in muon. In High-dimensional Learning Dynamics 2025, 2025. https://openreview.net/forum?id=ppmyFtr9EW. 15 Zhiqi Bu and Shiyun Xu. Gradient descent with generalized newton’s method. In The Thirteenth International Conference on Learning Representations, 2024. 6 Zhiqi Bu, Shiyun Xu, and Kan Chen. A dynamical view on optimization algorithms of overparameterized neural networks. In International conference on artificial intelligence and statistics, pages 3187–3195. PMLR, 2021. 6 Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert. Multi-level residual networks from dynamical systems view. In International Conference on Learning Representations, 2018. 4, 5 Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2bert: Towards reusable pretrained language models. arXiv preprint arXiv:2110.07143, 2021. 2, 4, 8 Tianqi Chen, Ian Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. arXiv preprint arXiv:1511.05641, 2015. 1, 4 Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. Advances in neural information processing systems, 31, 2018. 4 Aaron Defazio and Konstantin Mishchenko. Learning-rate-free learning by d-adaptation. In International Conference on Machine Learning, pages 7449–7479. PMLR, 2023. 6 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171–4186, 2019. 2 Chengyu Dong, Liyuan Liu, Zichao Li, and Jingbo Shang. Towards adaptive residual network training: A neural-ode perspective. In International conference on machine learning, pages 2616–2626. PMLR, 2020. 5 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2 Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, and Jie Fu. Stacking your transformers: A closer look at model growth for efficient llm pre-training. Advances in Neural Information Processing Systems, 37:10491–10540, 2024. 2, 4, 5, 8, 14 William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1–39, 2022. 3, 16 Cheng Fu, Hanxian Huang, Zixuan Jiang, Yun Ni, Lifeng Nai, Gang Wu, Liqun Cheng, Yanqi Zhou, Sheng Li, Andrew Li, et al. Triple: revisiting pretrained model reuse and progressive learning for efficient vision transformer scaling and searching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17153–17163, 2023. 5, 19 Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github. io/OpenWebTextCorpus, 2019. 3 Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and',\n",
       "  'progressive learning for efficient vision transformer scaling and searching. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17153–17163, 2023. 5, 19 Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http://Skylion007.github. io/OpenWebTextCorpus, 2019. 3 Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In International conference on machine learning, pages 2337–2346. PMLR, 2019. 2, 4, 5, 9, 19 Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. On the transformer growth for progressive bert training. arXiv preprint arXiv:2010.12562, 2020. 2 Alex Hägele, Elie Bakouch, Atli Kosson, Leandro Von Werra, Martin Jaggi, et al. Scaling laws and compute-optimal training beyond fixed training durations. Advances in Neural Information Processing Systems, 37:76232–76264, 2024. 2 11 Ethan He, Abhinav Khattar, Ryan Prenger, Vijay Korthikanti, Zijie Yan, Tong Liu, Shiqing Fan, Ashwath Aithal, Mohammad Shoeybi, and Bryan Catanzaro. Upcycling large language models into mixture of experts. arXiv preprint arXiv:2410.07524, 2024. 10 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 3 Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pages 30016–30030, 2022. 1 Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. 6 Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. https://kellerjordan.github.io/posts/muon/. 3, 15 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 1 Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. In The Eleventh International Conference on Learning Representations. 10 Guillaume Leclerc and Aleksander Madry. The two regimes of deep network training. arXiv preprint arXiv:2002.10376, 2020. 6 Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. Advances in neural information processing systems, 32, 2019. 6 Bei Li, Ziyang Wang, Hui Liu, Yufan Jiang, Quan Du, Tong Xiao, Huizhen Wang, and Jingbo Zhu. Shallow-to-deep training for neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 995–1005, 2020. 4, 5 Seng Pei Liew, Takuya Kato, and Sho Takase. Scaling laws for upcycling mixture-of-experts language models. In Forty-second International Conference on Machine Learning. 10 Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension- free bounds and kernel limit. In Conference on learning theory, pages 2388–2464. PMLR, 2019.',\n",
       "  'Pei Liew, Takuya Kato, and Sho Takase. Scaling laws for upcycling mixture-of-experts language models. In Forty-second International Conference on Machine Learning. 10 Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension- free bounds and kernel limit. In Conference on learning theory, pages 2388–2464. PMLR, 2019. 4 Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language models. In The Thirteenth International Conference on Learning Representations. 10 Taishi Nakamura, Takuya Akiba, Kazuki Fujii, Yusuke Oda, Rio Yokota, and Jun Suzuki. Drop-upcycling: Training sparse mixture of experts with partial re-initialization. In The Thirteenth International Conference on Learning Representations. 10 Yu Pan, Ye Yuan, Yichun Yin, Jiaxin Shi, Zenglin Xu, Ming Zhang, Lifeng Shang, Xin Jiang, and Qun Liu. Preparing lessons for progressive training on language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18860–18868, 2024. 2, 5, 8, 14 Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li, Maosong Sun, et al. Knowledge inheritance for pre-trained language models. arXiv preprint arXiv:2105.13880, 2021. 2 Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Elle: Efficient lifelong pre-training for emerging data. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2789–2810, 2022. 5 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3 Sashank J Reddi, Sobhan Miryoosefi, Stefani Karp, Shankar Krishnan, Satyen Kale, Seungyeon Kim, and Sanjiv Kumar. Efficient training of language models using few-shot learning. In International Conference on Machine Learning, pages 14553–14568. PMLR, 2023. 2 12 Fabian Schaipp, Alexander Hägele, Adrien Taylor, Umut Simsekli, and Francis Bach. The surprising agreement between convex optimization theory and learning-rate scheduling for large model training. arXiv preprint arXiv:2501.18965, 2025. 6 Sheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for transformer language models. In International Conference on Machine Learning, pages 19893–19908. PMLR, 2022. 2, 4, 5, 8, 14, 19 Zhen Tan, Daize Dong, Xinyu Zhao, Jie Peng, Yu Cheng, and Tianlong Chen. Dlo: Dynamic layer operation for efficient vertical scaling of llms. arXiv preprint arXiv:2407.11030, 2024. 4, 14 Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. arXiv preprint arXiv:2303.00980, 2023a. 2, 8 Yite Wang, Jiahao Su, Hanlin Lu, Cong Xie, Tianyi Liu, Jianbo Yuan, Haibin Lin, Ruoyu Sun, and Hongxia Yang. Lemon: Lossless model expansion. In The Twelfth International Conference on Learning Representations, 2023b. 1, 4, 5, 14 Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by increasing model capacity. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2471–2480, 2017. 4 Tianwen Wei, Bo Zhu, Liang Zhao, Cheng Cheng, Biye Li, Weiwei Lü, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Liang Zeng, et',\n",
       "  'Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by increasing model capacity. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2471–2480, 2017. 4 Tianwen Wei, Bo Zhu, Liang Zhao, Cheng Cheng, Biye Li, Weiwei Lü, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Liang Zeng, et al. Skywork-moe: A deep dive into training techniques for mixture-of-experts language models. arXiv preprint arXiv:2406.06563, 2024. 10 Cameron Wolfe. nanomoe: Minimal mixture of experts implementation. https://github.com/wolfecameron/nanoMoE, 2024. 10 Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, and Ping Luo. Llama pro: Progressive llama with block expansion. arXiv preprint arXiv:2401.02415, 2024. 4, 14 Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. A walk with sgd. arXiv preprint arXiv:1802.08770, 2018. 2 Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739, 2024. 10 Cheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progressively stacking 2.0: A multi-stage layerwise training method for bert training speedup. arXiv preprint arXiv:2011.13635, 2020. 2 Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020. 4 Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. 4 Greg Yang, James B Simon, and Jeremy Bernstein. A spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023. 4 Kazuki Yano, Takumi Ito, and Jun Suzuki. STEP: Staged parameter-efficient pre-training for large language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 374–384, Albuquerque, New Mexico, April 2025a. Association for Computational Linguistics. ISBN 979-8-89176-190-2. doi: 10.18653/v1/2025.naacl-short.32. https://aclanthology.org/2025.naacl-short.32/. 2 Kazuki Yano, Sho Takase, Sosuke Kobayashi, Shun Kiyono, and Jun Suzuki. Efficient construction of model family through progressive training using model expansion. arXiv preprint arXiv:2504.00623, 2025b. 2 Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language model pre-training. In The Twelfth International Conference on Learning Representations. 1, 2, 9, 14 Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. 16 13 A Depth expansion approaches A.1 Applicability of random, copying, and zero initialization We summarize the depth expansion approaches with respect to the depth of source models. Table 2 Applicability of depth expansion approaches. Merged cells indicate that multiple approaches are equivalent for a source model. zero-layer one-layer multi-layer random Yes Yes Yes copying_inter No Yes Yes copying_stack Yes copying_last Yes zero Yes Yes Yes We note that zero-layer or one-layer depth expansion significantly simplifies the copying approaches, as there is no ordering such as stacking or interpolation. In Appendix H of Du et al. (2024), the search space of',\n",
       "  'Yes Yes Yes copying_inter No Yes Yes copying_stack Yes copying_last Yes zero Yes Yes Yes We note that zero-layer or one-layer depth expansion significantly simplifies the copying approaches, as there is no ordering such as stacking or interpolation. In Appendix H of Du et al. (2024), the search space of ordering is enormous but necessary, since a proper ordering indeed improves the progressive training. This aligns with our results that copying all layers is better than copying only the last layer. However, it is debatable which of copying_inter and copying_stack is more advantageous, e.g. Pan et al. (2024) claims that copying_inter is more stable but Du et al. (2024) demonstrates that copying_stack converges better. We highlight that such debate is completely avoided for zero-layer or one-layer progressive training. A.2 Copying_zero initialization Completely zero initialization renders new layers not trainable, despite the depth expansion is function- preserving. It has been shown in the literature that copying with partially zero initialization has better trainability and is still function-preserving. There are two known methods that copy all sub-layers except (1) the normalization sub-layers are initialized as zeros Shen et al. (2022), or (2) the last linear sub-layer is initialized as zero (Wang et al. (2023b); Tan et al. (2024); Wu et al. (2024) and Du et al. (2024) Gzero), or masked with zeros Yao et al.. These approaches are termed as copying_zeroN and copying_zeroL, which enforce zero output of a new layer and are function-preserving. We experiment in the same setting as in Figure 2. Empirically, copying_zeroN has weak trainability, but copying_zeroL converges as fast as copying (without any zero sub-layers) and avoids any loss spike unlike copying. 0 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Loss GPT 1-layer (fixed size) GPT 1 12-layer (zero) GPT 1 12-layer (copying_zeroN) GPT 1 12-layer (copying_zeroL) GPT 1 12-layer (copying) GPT 12-layer (fixed size) Figure 10 Convergence of one-layer progressive training and fixed-size training, with 2 different approaches of copying_zero initialization. 14 A.3 On the performance and ordering of random initialization We observe that random initialization of new layers works well on GPT2 and MoE, but slightly less so on ResNet. We think the reason is the location of insertion: for GPT2 and MoE, the insertion is at the bottom, e.g. [1,2,3] to [1,2,3,R,R,R]; however, for ResNet with 4 stages, the insertion is intermittent since the model architecture is inhomogeneous, e.g. ResNet26 to ResNet50 is like [[1],[2],[3],[4]] to [[1,R],[2,R], [3,R,R,R],[4,R]]. On a related note, we analyze the ordering of random initialization. We train 6-layer or 12-layer GPT2 and expand the depth at τ = 0.1T. We insert randomly initialized layers on top or bottom of old layers, i.e. [1, 2, 3, 4, 5, 6] →[R, ..., R, 1, ..., 6] or [1, 2, 3, 4, 5, 6] →[1, ..., 6, R, ..., R]. 0 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 1-layer (fixed size) GPT 1 12-layer (random top) GPT 1 12-layer (random bottom) GPT 12-layer (fixed',\n",
       "  '6] or [1, 2, 3, 4, 5, 6] →[1, ..., 6, R, ..., R]. 0 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 1-layer (fixed size) GPT 1 12-layer (random top) GPT 1 12-layer (random bottom) GPT 12-layer (fixed size) 0 20000 40000 60000 80000 100000 Iterations 2.9 3.0 3.1 3.2 3.3 3.4 3.5 Validation loss GPT 1-layer (fixed size) GPT 1 12-layer (random top) GPT 1 12-layer (random bottom) GPT 12-layer (fixed size) 0 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 6-layer (fixed size) GPT 6 12-layer (random top) GPT 6 12-layer (random bottom) GPT 12-layer (fixed size) 0 20000 40000 60000 80000 100000 Iterations 2.9 3.0 3.1 3.2 3.3 3.4 3.5 Validation loss GPT 6-layer (fixed size) GPT 6 12-layer (random top) GPT 6 12-layer (random bottom) GPT 12-layer (fixed size) Figure 11 Convergence of progressive training and fixed-size training with different insertion of random initialization (right plot is zoom-in of the left). Adding new layers at the bottom of old layers works best, with much smaller loss spikes. We highlight that appending new layers at the bottom is empirically the best approach, and has much smaller loss spike than inserting at the top. However, such choice is completely avoided for zero-layer progressive training. B Experiment settings Default batch size is 512 and decaying is 20% for WSD schedule, except for the long runs in Figure 1 where decaying is 10% and batch size=512 for 1B models and 64 for 7B models. For WSD schedule, the learning rate is 0.01 as shown to be optimal in Figure 3 (only here GPT2 are trained for 25k iterations); for cosine schedule, the learning rate is 0.05. Regarding the optimizer, Muon-NSGD uses Muon Jordan et al. (2024) and NSGD as in Boreiko et al. (2025) 15 in order to orthogonalize all tensors: denoting W as a layer’s parameter, we apply Muon: Wt+1 = (1 −ηλ)Wt −η · NS(mt) NSGD: Wt+1 = (1 −ηλ)Wt −η · mt/∥mt∥2 where NS is the Newton-Schulz matrix iteration, mt is the momentum, and λ is the weight decay. For GPT2 models, we always keep n_embd/n_head=64. Different depth uses different n_head: full 12-layer uses 12 heads; full 24-layer uses 16 heads; full 36-layer uses 20 heads; full 60-layer uses 48 heads. For MoE models, we use the same configurations as GPT2 (12-layer). Additionally, we use 8 experts, auxiliary loss Fedus et al. (2022), and router z loss Zoph et al. (2022). C Additional experiments C.1 Comparing progressive training to shorter fixed-size training In Figure 1, we have observed that progressive training can achieve similar (though sometimes slightly higher) validation loss than fixed-size training under the same number of iterations. To make sure that progressive training has actual advantage instead of just moving along the loss-compute tradeoff, we launch another fixed-size training with shorter training horizon. To be concrete, the depth expansion happens at τ = 0.8T, meaning the grown model from progressive training is trained for',\n",
       "  'number of iterations. To make sure that progressive training has actual advantage instead of just moving along the loss-compute tradeoff, we launch another fixed-size training with shorter training horizon. To be concrete, the depth expansion happens at τ = 0.8T, meaning the grown model from progressive training is trained for 120k iterations. Our second fixed-size training runs for the same 120k iterations using the same learning rate schedule (60k in stable phase, 60k in decaying phase, no warmup, same peak learning rate). It is clear that the progressive training does inherit from the small model training, despite the loss spike seems to suggest that all progresses before model expansion are lost. This is obvious by comparing the red and grey curves. 0 1 2 3 4 5 FLOPs 1e19 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 0 12-layer (random), 480~600k iter GPT 1 12-layer (copying), 480~600k iter GPT 1 12-layer (random), 480~600k iter GPT 12-layer (fixed-size), 0~120k iter GPT 12-layer (fixed-size), last of 600k iter 0 1 2 3 4 5 FLOPs 1e19 2.8 2.9 3.0 3.1 3.2 3.3 Validation loss GPT 0 12-layer (random), 480~600k iter GPT 1 12-layer (copying), 480~600k iter GPT 1 12-layer (random), 480~600k iter GPT 12-layer (fixed-size), 0~120k iter GPT 12-layer (fixed-size), last of 600k iter Figure 12 Comparing progressive training and fixed-size training via the grown model and target model (right plot is zoom-in of the left). Under the same compute budget, progressive training converges much faster than fixed-size training, despite having high loss after the depth expansion. C.2 Mixing behaviors across model sizes In Figure 13, we consistently observe the mixing behaviors on hundreds of runs, from various small models to various large models. Specifically, the mixing time is empirically robust to model sizes. 16 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 0-layer GPT 12-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 0-layer GPT 24-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 0-layer GPT 36-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 1-layer GPT 12-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 1-layer GPT 24-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 1-layer GPT 36-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 2-layer GPT 12-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 2-layer GPT 24-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 2-layer GPT 36-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100',\n",
       "  '4 × 100 6 × 100 Validation loss GPT 2-layer GPT 24-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 2-layer GPT 36-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 6-layer GPT 12-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 6-layer GPT 24-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 6-layer GPT 36-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 9-layer GPT 12-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 12-layer GPT 24-layer 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 18-layer GPT 36-layer Figure 13 Scaling up by depth expansion from {0, 1, 2, 6, 18} layers to {12,24,36} layers GPT2 with {124M, 400M, 1B} parameters. 17 0.0 0.2 0.4 0.6 0.8 1.0 Timing of expansion /T 2.8 3.0 3.2 3.4 3.6 3.8 Validation loss GPT 0 12-layer GPT 1 12-layer GPT 2 12-layer GPT 6 12-layer GPT 9 12-layer GPT 12-layer (fixed size) 0.0 0.2 0.4 0.6 0.8 1.0 Timing of expansion /T 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 Validation loss GPT 0 24-layer GPT 1 24-layer GPT 2 24-layer GPT 6 24-layer GPT 12 24-layer GPT 24-layer (fixed size) 0.0 0.2 0.4 0.6 0.8 1.0 Timing of expansion /T 2.4 2.6 2.8 3.0 3.2 3.4 3.6 3.8 Validation loss GPT 0 36-layer GPT 1 36-layer GPT 2 36-layer GPT 6 36-layer GPT 18 36-layer GPT 36-layer (fixed size) Figure 14 Final loss of depth expansion at different timing, from {0, 1, 2, 6, 18} layers to {12,24,36} layers GPT2 with {124M, 400M, 1B} parameters. C.3 Single-stage progressive training is sufficient We experiment with single-stage progressive training (from 0-layer or 2-layer to 12-layer) and multi-stage training (from 0-layer to 2-layer then to 12-layer). As we expected, the mixing behaviors lead all runs to similar final losses, and multi-stage progressive training does not show improved efficiency: the multi-stage run has almost the same FLOPs as the 2-layer progressive training, worse than the 0-layer one. Additionally, multi-stage progressive training is hard to set up because of multiple timing of expansion. 0 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 0 2 12-layer GPT 0 12-layer ( = 0.5T) GPT 2 12-layer ( = 0.5T) GPT 0-layer (fixed size) GPT 2-layer (fixed size) GPT 12-layer (fixed size) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 2.8 3.0 3.2 3.4 3.6 3.8 Validation loss GPT 0 2 12-layer GPT 0 12-layer ( = 0.5T) GPT 2 12-layer ( = 0.5T) GPT 12-layer (fixed size) Figure 15 Multi-stage progressive training does not show better efficiency or loss,',\n",
       "  '0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 2.8 3.0 3.2 3.4 3.6 3.8 Validation loss GPT 0 2 12-layer GPT 0 12-layer ( = 0.5T) GPT 2 12-layer ( = 0.5T) GPT 12-layer (fixed size) Figure 15 Multi-stage progressive training does not show better efficiency or loss, due to the mixing behaviors. 18 C.4 Optimizer states We conduct an ablation study to explore how to deal with the optimizer states (e.g. momentum and variance in AdamW or Muon-NSGD) during the expansion. Previous works have mixed results: Shen et al. (2022); Fu et al. (2023) show that copying the old layers’ optimizer states to new layers can be helpful; Gong et al. (2019) resets the optimizer states of all layers. We consider the following methods for optimizer states (OS): denoting embedding as E, hidden layers as H, and last layer as L, • (inheriting OS) inheriting existing OS: [E, H, L] →[E, 0 × 12, L] • (copying OS) inheriting existing OS and copying hidden layers’ OS: [E, H, L] →[E, H × 12, L] • (no OS) not inheriting any OS: [E, L] or [E, H, L] →[0, 0 × 12, 0] We observe that all methods seem to work in terms of final losses and mixing behaviors, although copying OS is less stable. 0 20000 40000 60000 80000 100000 Iterations 2.9 3.0 3.1 3.2 3.3 3.4 3.5 Validation loss GPT 0 12-layer (no OS) GPT 1 12-layer (no OS) GPT 1 12-layer (copying OS) GPT 1 12-layer (inheriting OS) GPT 12-layer (fixed size) Figure 16 Validation loss of depth expansion at τ = 0.1T with different ways to set optimizer states. C.5 Choosing optimizer and learning rate schedule In Figure 17, we train 100k iterations with two optimizers and two learning rate schedules. The same schedule is used before and after expansion, without changing the learning rate. We observe that Muon-NSGD with WSD schedule achieves best loss at all FLOPs (also at any timing of expansion τ/T). This is consistent with our theory. 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 3.0 3.5 4.0 4.5 5.0 5.5 Validation loss Muon-NSGD, WSD schedule Muon-NSGD, cosine schedule AdamW, WSD schedule AdamW, cosine schedule Figure 17 Loss-compute tradeoff (validation loss v.s. FLOPs) of zero-layer depth expansion under different optimizers and learning rate schedules. The target model is 12-layer GPT2. For WSD schedule, AdamW uses 0.0005 learning rate and Muon-NSGD uses 0.01 learning rate. For cosine schedule, the learning rates are doubled. 19 C.6 Mixing needs data, not iterations Importantly, we observe that the mixing time is measured by data size, i.e. images or tokens processed, not by iterations. In Figure 18, we compare a progressive training with constant batch size to another one with 4× batch size after the depth expansion (τ = 0.1T). The final loss is similar although large-batch training takes much fewer iterations. 0 5000 10000 15000 20000 25000 Iterations 3 × 100 4 × 100 6 × 100 Validation loss GPT 6 12-layer(B=0.5M) GPT 6 12-layer(B=0.5 2M) GPT 12-layer (B=0.5M) 0 2 4 6 8 10 12',\n",
       "  '(τ = 0.1T). The final loss is similar although large-batch training takes much fewer iterations. 0 5000 10000 15000 20000 25000 Iterations 3 × 100 4 × 100 6 × 100 Validation loss GPT 6 12-layer(B=0.5M) GPT 6 12-layer(B=0.5 2M) GPT 12-layer (B=0.5M) 0 2 4 6 8 10 12 Tokens(B) 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 Validation loss GPT 6 12-layer(B=0.5M) GPT 6 12-layer(B=0.5 2M) GPT 12-layer (B=0.5M) Figure 18 Progressive training needs sufficient data to mix with fixed-size large model training, largely unaffected by batch size or iterations. C.7 One-layer model expansion figures We present the one-layer model expansion results in correspondence to Figure 5 and Figure 6 here, due to space limit. 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 1-layer (cosine schedule) GPT 12-layer (cosine schedule) 20000 40000 60000 80000 100000 Iterations 101 3 × 100 4 × 100 6 × 100 Validation loss GPT 1-layer (WSD schedule) GPT 12-layer (WSD schedule) 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 3.0 3.2 3.4 3.6 3.8 Validation loss cosine schedule WSD schedule Figure 19 Performance of one-layer progressive training and fixed-size training, where WSD schedule significantly enhances the progressive training. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 FLOPs 1e19 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 Validation loss GPT 12-layer 20000 40000 60000 80000 100000 Iterations 3.00 3.25 3.50 3.75 4.00 4.25 4.50 4.75 5.00 Validation loss GPT 1 12-layer ( /T = 0.5) GPT 12-layer Pre-growth Post-growth Figure 20 Different perspectives to compare (one-layer) progressive training and fixed-size training. Left: only comparing the grown model and target model. Right: matching the pre-growth loss of source model to target model. 20'],\n",
       " [\"Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques Esha Chowdhury Dept. of CSE Dhaka University of Engineering & Technology, Gazipur, Bangladesh eshachowdhury.cse@gmail.com Abstract— Accurate prediction of cardiovascular disease (CVD) risk is a crucial task for healthcare institutions. This work is important since diabetes is increasing and is strongly linked to heart disease. This study proposes an efficient CVD risk prediction model for diabetic patients using a combination of machine learning (ML) and hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by removing duplicates, handling missing values, identifying categorical and numerical features, and applying Principal Component Analysis (PCA) for feature extraction. Several ML models, including DT, RF, KNN, SVM, AdaBoost, and XGBoost, were implemented, with XGBoost achieving the highest accuracy of 0.9050. Various DL models (ANN, DNN, RNN, CNN, LSTM, BiLSTM, and GRU) and hybrid models combining CNN with LSTM, BiLSTM, and GRU variants were also explored, with some achieving perfect recall (1.00), while the LSTM model achieved the highest accuracy of 0.9050. Our research highlights the effectiveness of ML and DL models in predicting cardiovascular disease risk among diabetic patients, thereby automating and enhancing clinical decision-making. High accuracy and F1 scores demonstrate these models’ capability to improve personalized risk management and preventive strategies. Index Terms— Cardiovascular Disease, Diabetic, ML, DL, High Risk, Clinical Decisions. I. Introduction Increased prevalence of diabetes is leading to growth in cardiovascular disease in the whole world. This has added to the proactive risk management being a prerequisite health plan of all the people [1]. Due to the requirement to process a lot of complex patient data and profiles, healthcare practitioners cannot calculate or predict diabetes patients' heart disease risk in a timely manner [2]. Doctors in the past used to use traditional clinical scoring systems to ascertain the likelihood of a diabetic patient developing a heart attack or stroke. Such systems are not always absolutely correct and foresee the future events. More advanced procedures are thus desperately needed to ensure more accuracy and automation of cardiovascular risk prediction of this group of vulnerable population. Techniques of Ensemble Machine Learning have changed healthcare [3]. We use a machine learning-based analytic platform to make predictions. We use machine learning (ML) and deep learning (DL) to acquire useful information. We analyze data related to patient health variables and biomarkers. We achieve this by using algorithms such as random forest, decision tree, and AdaBoost. This technology enables users to make better decisions and anticipates more robust decision-making systems. We enhance methods of precise cardiovascular risk assessment through model training and validation. We employ cutting-edge deep learning architectural guidelines to comply with clinical guidelines. We process measures of clinical data and standardize physiological data. We codify categorical health data and measure its accuracy. We produce a reliable dataset for cardiovascular risk modeling. There is potential in deep learning in cardiology to enhance the process of cardiovascular risk assessment. These complex deep learning patterns can be studied using models applied to large, multidimensional datasets, which is typically challenging for traditional machine learning models [4].\",\n",
       "  'We produce a reliable dataset for cardiovascular risk modeling. There is potential in deep learning in cardiology to enhance the process of cardiovascular risk assessment. These complex deep learning patterns can be studied using models applied to large, multidimensional datasets, which is typically challenging for traditional machine learning models [4]. This study will address the drawback of this, eliminating shortcomings of traditional methods with an advancement of a more intelligent and reconfigurable system that is capable of managing the dynamic nature of health data [5] and disease progression of diabetic patients. The study on DL is also aimed at increasing the clarity and cognition of cardiovascular risk forecasts and building confidence in the use of such sophistications in clinical diabetic patient care decision-making. II. Related Work Several papers have examined the use of ML and DL models in predicting cardiovascular diseases, diabetic complications, and early intervention solutions. When the Linear Regression (LR) model was used to predict 10-year CVD risk, the highest results of 0.023 and 0.001 were achieved on the test set when utilizing the entire set of features [2]. Deep learning algorithms are built on a solid paradigm of prediction of the risk of coronary artery disease (CAD) and cardiovascular disease (CVD) risk classification [4]. Furthermore, meta-learning models, especially support vector machines (SVMs), have been developed to improve the prediction of CVD risk by identifying the complicated trends in the large datasets [6]. The effectiveness of such algorithms usually lies in the proper methods of selection of features, which play an important role in the accuracy and efficiency of predictions [7]. A Study indicates that the Deep Belief Network (DBN)-based model was found to have a better prediction accuracy of about 81.20% than traditional algorithms, including SVM and Artificial Neural Networks (ANN) models [8]. Basic risk factors had a greater predictive performance in nondiabetic women (AUC = 0.79) but had the same predictive performance in nondiabetic men (AUC = 0.69) [9]. Zarkogianni et al. [10] found that related risk prediction models based on hybrid wavelet neural networks and self- organizing maps had an AUC of 71.48. III. METHODS This study evaluates the performance of various ML and DL Models for CVD risk prediction in diabetic patients. The methodology includes several key stages (see figure 1). Epoch Accuracy Learning Rate Optimizer 8 95.00 0.01 Adam 8 95.00 0.001 Adam 8 95.00 0.0001 Adam A. Dataset Description The research is based on a large dataset from the Behavioral Risk Factor Surveillance System (BRFSS) to determine the risk of cardiovascular disease among diabetic patients. The data in the 2023 BRFSS dataset of the US Division of Population Health National Center of Chronic Disease Prevention and Health Promotion were used in the study. The table of attributes of the dataset is shown in TABLE II. The data was obtained on the Diabetes Health Indicators Dataset page in Kaggle. The dataset will also have lifestyle variables such as smoking and physical activity in addition to health variables, such as blood pressure and cholesterol levels. The most significant characteristics of the dataset are shown in TABLE',\n",
       "  'II. The data was obtained on the Diabetes Health Indicators Dataset page in Kaggle. The dataset will also have lifestyle variables such as smoking and physical activity in addition to health variables, such as blood pressure and cholesterol levels. The most significant characteristics of the dataset are shown in TABLE III. B. Data Preprocessing The data was critically handled and normalized to prepare it to be used in learning ML and DL models. This entailed some of the key steps, such as eliminating duplication, missing values, categorizing columns as numerical and categorical, and dividing the data into training and test data, as illustrated in Fig. 1. The dataset that was preprocessed had 17 chosen features and 433,324 entries. It was further divided into 80:20 to have 346,242 samples of the training set and 86,662 of the testing set. C. Feature Extraction In an attempt to simplify the data, this research paper used Principal Component Analysis (PCA), which transforms variables into unrelated elements and preserves the most important patterns at the same time. The method is particularly beneficial in the analysis of medical data because the information about patients is considerably more comprehensible. D. Hyper Parameter Tuning In this study, we optimized our approach to accurately predict the likelihood of cardiovascular disease among diabetic patients by utilizing hyperparameter values and cross-validation techniques. We have tuned parameters in the machine learning models, including the regularization strength (C) and the type of kernel, to achieve optimal accuracy and generalization. The objective was to enhance the accuracy of prediction and not to overfit to the training data. Each of the models was tested with the help of the Grid Search CV and k-fold cross-validation with various combinations of parameters. The result was a hyperparameter optimization that was performed on all our models. The best models, after tuning in Colab on the BRFSS dataset, were the XGBoost and LSTM models. The higher accuracy scores reflect the significant increase in performance that our hyperparameter adjustments produced. TABLE I HYPERPARAMETER SETTING E. Machine Learning Approach In this research, we investigated several machine learning models to forecast cardiovascular disease in diabetic patients. Decision Tree (DT) model is simple to comprehend, and it is applicable to both numerical and categorical data [11]. However, the Random Forest (RF) model is more capable of dealing with complex data and minimizing overfitting [11]. The K-Nearest Neighbors (KNN) algorithm ranks the data according to the proximity of data points, thereby acting well in identifying the complex patterns. AdaBoost is performed in stages; each time a new model is being trained, it corrects the errors made by the previous model. Finally, eXtreme Gradient Boosting (XGBoost), which integrates multiple classifiers, is especially good at boosting the performance of models and making them stronger. F. Deep Learning Approach The paper will analyze how deep learning models can be used to identify patients who are at risk of developing cardiovascular diseases when they have diabetes. CNN is particularly useful with grid-like data and is adept at finding patterns and features [12]. Artificial Neural Networks (ANN) are able to make',\n",
       "  \"The paper will analyze how deep learning models can be used to identify patients who are at risk of developing cardiovascular diseases when they have diabetes. CNN is particularly useful with grid-like data and is adept at finding patterns and features [12]. Artificial Neural Networks (ANN) are able to make forecasts, identify errors, and perform repetitive changes to their internal connections to enhance their performance. The Recurrent Neural Networks (RNN) are created with sequential dependencies and are especially applicable in predicting risk of disease. Long Short-Term Memory (LSTM) networks are better at dealing with long-term dependencies, and thus they are capable of storing important information that can be used to make accurate predictions. Deep Neural Networks (DNN) allow model learning of complex patterns of data that are on a multi- level, and they increase the accuracy of the model. Bidirectional LSTM (BiLSTM) is more reliable in recognizing the patterns as it processes the data in both directions. Finally, Gated Recurrent Units (GRU) are used to overcome the vanishing gradient issue and make models learn on long sequences. G. Hybrid Model To make better predictions, researchers have also looked into hybrid models that use more than one deep learning design. The study introduces numerous new hybrid deep learning algorithms for heart disease risk prediction. Each combination of CNN with LSTM, BiLSTM, and GRU, as well as LSTM with GRU and BiLSTM with GRU. It's beneficial that CNN can pick out features in space, and the recurrent networks make it more accurate, address the problem of disappearing gradients, and understand how things relate to each other over time. When looking for malware, for instance, a mix of LSTM and CNN has been used before [12]. Each hybrid model demonstrates its ability to enhance and streamline clinical decision-making. This makes it possible to create more personalized risk management and avoidance plans IV. RESULT AND ANALYSIS A. Confusion Matrix A confusion matrix is a table that visualizes the performance of an algorithm by showing actual versus predicted classifications [6]. The confusion matrix and ROC curve results of the models are given below (see figures 2, 3, 4, 5, 6, 7, and 8): Fig. 1. Proposed Method TABLE II DETAILS OF THE 2023 BRFSS DATASET TABLE III KEY FEATURES OF THE DATASET Attribute Details Title 2023 BRFSS Survey Data Author U.S. Centers For Disease Control and Prevention Features 303 variables covering different aspects Data Items Responses from approximately 433,324 participants across the United States Collection Method Computer-Assisted Telephone Interview systems Geographical Coverage All 50 states of the United States, the District of Columbia, Guam, Puerto Rico, and US Virgin Islands Categories of Features Demographics, Chronic Health Conditions, Behavioral Features, Preventive Health Services, Physical and Mental Health Status, Disability Status and others Feature Description DIABTYPE According to your doctor or other health professional, which type of diabetes do you have? DIABETES Diabetes is represented by three distinct values: 0 indicates no diabetes, 1 indicates pre-diabetes, and 2 indicates diabetes. HighBP Binary variable indicating high blood pressure. HighChol Binary variable indicating high cholesterol. CHOLCHK Indicates whether cholesterol levels have\",\n",
       "  'doctor or other health professional, which type of diabetes do you have? DIABETES Diabetes is represented by three distinct values: 0 indicates no diabetes, 1 indicates pre-diabetes, and 2 indicates diabetes. HighBP Binary variable indicating high blood pressure. HighChol Binary variable indicating high cholesterol. CHOLCHK Indicates whether cholesterol levels have been checked within the past five years. BMI Body Mass Index; weight in kilograms divided by height in meters squared. SMOKE 0 indicates never smoked; 1 indicates having smoked at least 100 cigarettes in a lifetime. CVDSTRK Indicates whether the patient has ever had a stroke. EXERANY During the past month, other than your regular job, did you participate in any physical activities or exercises? MEDCOST During the past 12 months, was there a time when you needed to see a doctor but could not because of cost? GENHLTH General health status. Categories: Excellent, Very good, Good, Fair, and Poor. MENTHLTH The number of days in the past month when mental health was poor. PHYSHLTH The number of days in the past month when physical health was poor. DIFFWALK Difficulty in walking or climbing stairs; 1 indicates difficulty and 0 indicates no difficulty. SEX 0 for Female, 1 for Male. AGE Categorized into 13 levels based on 5-year intervals. EDUCAG Education level on a scale from 1 (lowest) to 6 (highest). B. Machine Learning based prediction Results The classification results for various machine learning models in predicting cardiovascular disease risk in diabetic patients show that XGBoost achieved the highest accuracy at 90.50%, with precision, recall, and F1-score all at 0.95, indicating excellent performance (see table IV). RF and AdaBoost also performed well, with accuracies of 90.47% and 90.35%, respectively. Other models, such as DT and KNN, also showed high accuracy, though slightly lower. Overall, XGBoost is the top-performing model in this analysis, demonstrating the best balance of accuracy and predictive metrics. Fig. 2. Confusion matrix for XGBoost TABLE IV CLASSIFICATION RESULT FOR ML MODEL Model Accuracy Precision Recall F1-Score DT 0.9014 0.91 0.99 0.95 RF 0.9047 0.91 1.00 0.95 AdaBoost 0.9035 0.91 0.99 0.95 KNN 0.9009 0.91 0.99 0.95 XGBoost 0.9050 0.91 0.99 0.95 C. Deep Learning based prediction Results The classification results for DL models in predicting cardiovascular disease risk in diabetic patients reveal high performance across all models. The results indicate strong classification capabilities for each model. The results show that each model is very effective at classifying things. LSTM was the most accurate, with 90.50%. BiLSTM was next with 90.49%, while GRU was last with 90.47%. CNN also did well, getting 90.44% of the answers right and being the most accurate (0.9109). All three models, ANN, DNN, and RNN, had the same accuracy of 90.41% and perfect recall (1.0000). LSTM had the highest overall accuracy, and all of the models gave findings that were highly similar and reliable (see Table V). TABLE V CLASSIFICATION RESULT FOR DL MODEL Model Accuracy Precision Recall F1- Score ANN 0.9041 0.9041 1.0000 0.9496 DNN 0.9041 0.9041 1.0000 0.9496 RNN 0.9041 0.9041 1.0000 0.9496 LSTM 0.9050 0.9091 0.9944 0.9498 GRU 0.9047 0.9067 0.9973 0.9498',\n",
       "  'models gave findings that were highly similar and reliable (see Table V). TABLE V CLASSIFICATION RESULT FOR DL MODEL Model Accuracy Precision Recall F1- Score ANN 0.9041 0.9041 1.0000 0.9496 DNN 0.9041 0.9041 1.0000 0.9496 RNN 0.9041 0.9041 1.0000 0.9496 LSTM 0.9050 0.9091 0.9944 0.9498 GRU 0.9047 0.9067 0.9973 0.9498 BiLSTM 0.9049 0.9084 0.9952 0.9498 CNN 0.9044 0.9109 0.9912 0.9494 Fig. 3. Confusion matrix for LSTM D. Hybrid Model based prediction Results The classification outcomes for hybrid models in predicting cardiovascular disease risk in diabetic patients demonstrate significant predictive validity. With an accuracy of 90.46%, a precision of 0.9095, and a recall of 0.9934, the CNN+LSTM model was the best. This conclusion means that it could identify positive cases and be reliable at the same time. The LSTM+GRU model had similar results, with an accuracy of 90.46%, but slightly less precision and a higher recall of 0.9981. CNN+BiLSTM and BiLSTM+GRU both had 90.44% and 90.41% accuracy, respectively. BiLSTM+GRU had a perfect recall of 1.00. In most cases, CNN+LSTM was the most accurate overall. The hybrid models, in their turn, all were high. This indicates that they achieved good precision, recall, and F1 scores in the process of generating predictions. (see Table VI). TABLE VI CLASSIFICATION RESULT FOR HYBRID MODEL Model Accuracy Precision Recall F1- Score CNN+LSTM 0.9046 0.9095 0.9934 0.9496 CNN+LSTM 0.9044 0.9070 0.9965 0.9496 LSTM+GRU 0.9046 0.9060 0.9981 0.9498 BiLSTM+GRU 0.9041 0.9041 1.0000 0.9496 Fig. 4. Confusion matrix for CNN+LSTM Fig. 5. Confusion matrix for LSTM +GRU Fig.6: ROC curve of the XGBoost model Fig.7: ROC curve of the LSTM+GRU Fig.8: ROC curve of the LSTM model V. CONCLUSION AND FUTURE WORK This paper applies machine learning (ML) and deep learning (DL) methods to offer a holistic predictive model to evaluate the risk of cardiovascular disease among diabetics. We examined BRFSS data and used PCA to present additional information. We split it into 2 parts, 80% training and 20% testing. This strategy guaranteed the impartiality of the test and the effectiveness of the model. To maximize the accuracy and reliability of the predictions, this paper employed a number of different machine learning models, such as Decision Tree (DT), K-Nearest Neighbor (KNN), and XGBoost, along with artificial neural network (ANN), deep neural network (DNN), convolutional neural network (CNN), recurrent neural network (RNN), long short-term memory (LSTM), bidirectional long short-term memory (BiLSTM), gated recurrent unit (GRU), and hybrid architecture. The suggested procedure of evaluating the probability of heart problems in diabetics was always very precise, recalling F1 scores. The dataset may be refined in the future by adding more real-time patient data and better methods of ensemble and automated hyperparameter optimization. In addition, it can build a clinical support system that is understandable and user-friendly and hence increase its impact on healthcare. REFERENCES [1] H. Sang, H. Lee, M. Lee, J. Park, S. Kim, and H. G. Woo, et al., “Prediction model for cardiovascular disease in patients with diabetes using machine learning derived and validated in two independent Korean cohorts,” Scientific Reports, vol. 14, pp. 14966–14966, 2024. [2] H. Calero-Diaz, D. Chushig-Muzo, H.',\n",
       "  '[1] H. Sang, H. Lee, M. Lee, J. Park, S. Kim, and H. G. Woo, et al., “Prediction model for cardiovascular disease in patients with diabetes using machine learning derived and validated in two independent Korean cohorts,” Scientific Reports, vol. 14, pp. 14966–14966, 2024. [2] H. Calero-Diaz, D. Chushig-Muzo, H. Fabelo, I. Mora-Jiménez, C. Granja, and C. Soguero-Ruiz, “Data-driven cardiovascular risk prediction and prognosis factor identification in diabetic patients,” Proc. IEEE- EMBS Int. Conf. Biomed. Health Inform. (BHI), vol. 2022, pp. 1–4,2022. [3] M. Dorraki, Z. Liao, D. Abbott, P. J. Psaltis, E. Baker, N. Bidargaddi, A. van den Hengel, J. Narula, and J. W. Verjans, “Cardiovascular disease risk prediction via machine learning using mental health data,” European Heart Journal– Digital Health, vol. 3, pp. 2784–2784, 2022. [4] A. M. Johri, K. V. Singh, L. E. Mantella, L. Saba, A. Sharma, and J. R. Laird, et al., “Deep learning artificial intelligence framework for multiclass coronary artery disease prediction using combination of conventional risk factors, carotid ultrasound, and intraplaque neovascularization,” Computers in Biology and Medicine, vol. 150, pp. 106018–106018, 2022. [5] W. Yang, Y. Guo, and Y. Liu, “Improvement of auxiliary diagnosis of diabetic cardiovascular disease based on data oversampling and deep learning,” Applied Sciences, vol. 13,pp. 5449–5449, 2023. [6] N. S. Punn and D. K. Dewangan, “Ensemble meta-learning using SVM for improving cardiovascular disease risk prediction,” medRxiv, pp.1–10, 2024. [7] L. C. Jaffrin and J. Visumathi, “Impact of feature selection techniques on machine learning and deep learning techniques for cardiovascular disease prediction-an analysis,” Edelweiss Applied Science and Technology, Learning Gate, vol. 8, no. 5, pp. 1454–1471, 2024. [8] K. Vidhya and R. Shanmugalakshmi, “Deep learning based big medical data analytic model for diabetes complication prediction,” J. Ambient Intell. Human Comput., vol. 11, no. 4, pp. 5691–5702, 2020. [9] A. R. Folsom, L. E. Chambless, B. B. Duncan, A. C. Gilbert, and J. S. Pankow, “Prediction of coronary heart disease in middle-aged adults with diabetes,” Diabetes Care, vol. 26, no. 10, pp. 2777–2784, Oct. 2003. [10] K. Zarkogianni, M. Athanasiou, A. C. Thanopoulou, and K. S. Nikita, “Comparison of machine learning approaches toward assessing the risk of developing cardiovascular disease as a long-term diabetes complication,” IEEEJ. Biomed. Health Inform., vol. 22, no. 5, pp. 1637–1647, 2018. [11] M. H. Alshayeji, S. Abed, and S. C. B. Sindhu, “Two-stage framework for diabetic retinopathy diagnosis and disease stage screening with ensemble learning,” Expert Syst. Appl., vol. 225, p. 120206, 2023. [12] P. Thakur, V. Kansal, and V. Rishiwal, “Hybrid deep learning approach based on LSTM and CNN for malware detection,” Wireless Personal Communications, vol. 136, no. 3, pp. 1879–1901, Jun. 2024. [13] K. Papatheodorou, M. Banach, M. Edmonds, N. Papanas, and D. Papazoglou, “Complications of diabetes,” J. Diabetes Res., vol. 2015, pp. 1– 5, 2015. [14] A. S. Saldanha de Mattos Matheus, L. R. M. Tannus, R. A. Cobas, C. C. S. Palma, C. A. Negrato, and M. de B. Gomes, “Impact of diabetes on cardiovascular disease: an update,” Int. J. Hypertens., vol. 2013, pp. 1–15, 2013. [15] L. Langouche and G. Van den Berghe,',\n",
       "  '[14] A. S. Saldanha de Mattos Matheus, L. R. M. Tannus, R. A. Cobas, C. C. S. Palma, C. A. Negrato, and M. de B. Gomes, “Impact of diabetes on cardiovascular disease: an update,” Int. J. Hypertens., vol. 2013, pp. 1–15, 2013. [15] L. Langouche and G. Van den Berghe, “Glucose metabolism and insulin therapy,” Crit. Care Clin., vol. 22, no. 1, pp. 119–129, 2006. [16] J. A. Bluestone, K. Herold, and G. Eisenbarth, “Genetics, pathogenesis and clinical interventions in type 1 diabetes,” Nature, vol. 464, no. 7293, pp. 1293– 1300, 2010. [17] S. E. Kahn, M. E. Cooper, and S. Del Prato, “Pathophysiology and treatment of type 2 diabetes: perspectives on the past, present, and future,” The Lancet, vol. 383, no. 9922, pp. 1068–1083, 2014 [18] P. Z. Zimmet, D. J. Magliano, W. H. Herman, and J. E. Shaw,“Diabetes: a21stcentury challenge,” Lancet Diabetes Endocrinol., vol. 2, no. 1, pp. 56–64, 2014. [19] V. Lyssenko, A. Jonsson, P. Almgren, N. Pulizzi, B. Isomaa, and T. Tuomi, et al. ,“Clinical risk factors, DNA variants, and the development of type 2 diabetes,” New England Journal of Medicine, vol. 359, no. 21, pp. 2220–2232, 2008. [20] D. M. Lloyd-Jones, P. W. F. Wilson, M. G. Larson, A. Beiser, E. P. Leip, R. B. D’Agostino, and D. Levy, “Framingham risk score and prediction of lifetime risk for coronary heart disease,” Am. J. Cardiol., vol. 94, no. 1, pp. 20–24, Jul. 2004.'],\n",
       " [\"Graphical Abstract BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records in Depression and PTSD Cohorts Multi-Vocabulary EHR Codes Enhance Output Interpretability. Truncated ICD-10 Reduces Redundancy and Model Complexity. SPE + RoPE→ +9.2% AUROC, +20.4% AUPRC compared to SPE Only F35 264648 F48 DB1104 D40 F48 1 N34 DB1205 D40 D41 102301 DB1110 Convert to Three-Sequence BiPETE Inputs EHR Codes Days-ago Visit EHR Data Processing: BiPETE Input Generation Absolute & Relative Visit-Level Positional Encoding Integrated Gradient: Token-Level Attribution SPE (Absolute) Token Embeddings Multi-Head Attention Skip-Connection xN Encoder Blocks Layer-Norm K,Q V RoPE (Relative) Fully Connected Layer Skip-Connection Layer-Norm RNN-based Binary Classifier Prediction 4 4 3 3 0 0 5 5 3 3 2 2 5 5 20 20 1 2 1 1 35 20 35 35 1 35 Associated EHR Codes Patient Hospital Visits Train BiPETE on Binary Classification Task Evaluate Using 5-Fold Cross Validation Cohort Data Source Non-ASUD ASUD BiPETE Architecture Medication (DB ID) Lab Test (LOINC) ER Visit Diagnosis (ICD10) EHR Vocab Positive IGi(x): Attribution Towards ASUD Negative IGi(x): Attribution Away From ASUD |IGi(x)|: Reflects Influence Strength Aggregate by Token Across Test Datasets to Identify Tokens Contributing to BiPETE's Correct Predictions. 0 days ago (Latest Visit) 5 days ago 20 days ago 4th Visit 3rd Visit 2nd Visit 1st Visit 35 days ago Gradients of Prediction Output w.r.t Test Input Tokens Average ith Token Attribution IGi(x) Linear Path Actual Input's Gradients Baseline Input's Gradients Medications Associated with Decreased ASUD Risk: Hydroxyzine, Escitalopram, Duloxetine, and Dextroamphetamine Diagnosis Associated with Increased ASUD Risk: Chronic Pain, Musculoskeletal Injuries, Gastrointestinal Conditions AUROC & AUPRC: 96% & 93% (Depression Cohort) AUROC & AUPRC: 96% & 94% (PTSD Cohort) arXiv:2511.04998v1 [cs.LG] 7 Nov 2025 BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records Daniel S. Lee1, Mayra S. Haedo-Cruz2, Chen Jiang1, Oshin Miranda1, and LiRong Wang1 1Department of Pharmaceutical Sciences, University of Pittsburgh School of Pharmacy, Pittsburgh, PA, 15213, USA 2Division of Clinical and Translational Cancer Research, University of Puerto Rico Comprehensive Cancer Center, San Juan, PR, 00921, USA Abstract Transformer-based deep learning models have shown promise for disease risk prediction using electronic health records (EHRs), but modeling temporal de- pendencies remains a key challenge due to irregular visit intervals and lack of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder or BiPETE for single-disease prediction, which integrates rotary positional em- beddings to encode relative visit timing and sinusoidal embeddings to preserve visit order. Without relying on large-scale pretraining, BiPETE is trained on EHR data from two mental health cohorts–depressive disorder and post-traumatic stress disorder (PTSD)–to predict the risk of alcohol and substance use disorders (ASUD). BiPETE outperforms baseline models, improving the area under the precision-recall curve (AUPRC) by 34% and 50% in the depression and PTSD cohorts, respectively. An ablation study further confirms the effectiveness of the dual positional encoding strategy. We apply the Integrated Gradients method to interpret model predictions, identifying key clinical features associated with ASUD risk and protection, such as abnormal\",\n",
       "  'precision-recall curve (AUPRC) by 34% and 50% in the depression and PTSD cohorts, respectively. An ablation study further confirms the effectiveness of the dual positional encoding strategy. We apply the Integrated Gradients method to interpret model predictions, identifying key clinical features associated with ASUD risk and protection, such as abnormal inflammatory, hematologic, and metabolic markers, as well as specific medications and comorbidities. Overall, these key clinical features identified by the attribution methods contribute to a deeper understanding of the risk assessment process and offer valuable clues for mitigating potential risks. In summary, our study presents a practical and inter- pretable framework for disease risk prediction using EHR data, which can achieve strong performance. 1 Introduction In recent decades, the growing availability of electronic health records (EHRs), together with advances in machine learning and deep learning, has opened new avenues for en- hancing patient-outcome prediction and have achieved notable successes [1, 2, 3]. Hier- archical, longitudinal EHR data consists of large-scale, chronologically ordered records of patients’ diagnoses, treatments, laboratory tests, and health trajectories [4], and this structure presents significant opportunities for leveraging transformer-based deep learn- ing models to predict outcomes and understand disease progression [5]. Despite these advances, a major challenge in predictive modeling for healthcare is capturing tempo- ral dependencies and inter-visit correlations, as the disease progression, comorbidities, and treatment effects unfold over time [6, 7]. EHR data are primarily organized by visits, and their irregular and sparse intervals further complicate fine-grained temporal modeling in EHR data [8, 9]. To enable transformers to capture visit-level interactions among EHR tokens that reflect real-world clinical patterns, input EHR code sequences must be carefully modeled. Transformer models have demonstrated efficacy in learning complex interdependen- cies within sequence data, a strength attributed to their attention mechanism [5, 10, 11]. Consequently, several specialized transformer-based architectures have been introduced to model EHR data, each addressing temporal dependencies in distinct ways [12, 13]. BEHRT [10] and Med-BERT [6] both adopt the Bidirectional Encoder Representations from Transformers (BERT) [7] framework, pretraining on large-scale EHR data to learn contextual representation of the EHR tokens, with the goal of fine-tuning on a wide range of downstream tasks. To model the timing of EHR code occurrences, Med-BERT adds a learned positional embedding (LPE) applied to groups of codes within each pa- tient visit, with an optional ordinal LPE assigned to individual codes [6]. BEHRT adds two LPEs: age, and visit-segment embeddings. TransformEHR employs a transformer encoder-decoder structure to pretrain on the task of predicting ICD codes for future patient visits [14]. Similar to Med-BERT, it uses learned visit-level positional encoding, supplemented with sinusoidal positional embeddings (SPE) derived from visit dates to capture additional time information. These systems demonstrate the effectiveness of transformers for modeling EHR data and a reliance on large-scale pretraining. However, both LPE and SPE are forms of absolute positional encoding (APE) and do not explicitly encode relative position infor- mation of the tokens. While APE specifies the position of each token by adding a unique positional vector to the token embeddings, the burden of learning relative dependencies is left to',\n",
       "  'both LPE and SPE are forms of absolute positional encoding (APE) and do not explicitly encode relative position infor- mation of the tokens. While APE specifies the position of each token by adding a unique positional vector to the token embeddings, the burden of learning relative dependencies is left to the subsequent attention and linear layers in the transformer block. Another limitation of adding multiple APEs is the introduction of noise. Redundant positional information could lead to signal superposition that distorts rather than enriches token representations. As in transformer-based natural language models, transformer-based models that in- put EHR sequence can benefit from encoding relative visit-level temporal information using RoPE. RoPE encodes positional information through position-dependent rota- 2 tions of token embeddings. This rotation mechanism enables effective modeling of local relationships while reducing long-ranged dependency effects [15]. Leveraging these in- sights, we propose BiPETE, a Bi-Positional Embedding Transformer Encoder classifier that integrates both absolute and relative positional encodings–SPE and RoPE–which complement each other to capture temporal dynamics of longitudinal structured EHR data for disease prediction. BiPETE’s RoPE provides a positional encoding in which EHR token interactions are governed by their relative temporal separation in days be- tween visits, enabling the model to leverage irregular, yet clinically meaningful time intervals. In contrast, BiPETE’s SPE encodes absolute visit order, reinforcing sequenc- ing and grouping of the tokens. To enhance the representation of heterogeneous EHR data, we integrate multi- ple standard clinical vocabularies. Unlike BEHRT, MedBERT, and TransformerEHR, BiPETE is not pretrained on large-scale EHR data, but trained on a moderate-sized dataset for a single-disease prediction task (Table1). While our approach reduces the data and computation requirements for deployment, the high-cardinality of multi- vocabulary medical codes can complicate token representation and pattern learning [9, 16, 17]. To address this, we grouped diagnosis codes–which constitutes the largest proportion of our vocabulary–into broader categories that reflect common traits, related diseases, or conditions, thus preserving category-level information while reducing the vocabulary size. We evaluate BiPETE on predicting the risk of subsequent alcohol and substance use disorders (ASUD) in two mental health disorder (MHD) cohorts derived from the National Institute of Health (NIH) All of Us (AoU) EHR data [18]: depressive dis- orders and post-traumatic stress disorder (PTSD) cohorts. Balancing accuracy with explainability is another key challenge in applying deep learning to health-outcome prediction, as producing outputs that are interpretable and actionable for clinicians is essential [19]. To enhance clinical interpretability, we apply integrated gradients (IG), a gradient-based feature attribution method [20, 21], to aggregate token-level attri- butions, yielding global estimates of EHR tokens that drive BiPETE’s prediction for ASUD and non-ASUD classes. Methods 1 Data Source and Cohort Definition We sourced our cohort data from the AoU Program, a NIH-funded cloud-based biomed- ical data repository and analysis platform. AoU data repository comprises clinical, ge- nomic, demographic and other health-related data from de-identified participants aged 18 or older, collected across more than 50 healthcare organizations in the United States [18]. We extracted standardized EHR data curated under Observational Medical Out- comes Partnership (OMOP) Common Data Model (CDM) from controlled-access tier.',\n",
       "  'data repository comprises clinical, ge- nomic, demographic and other health-related data from de-identified participants aged 18 or older, collected across more than 50 healthcare organizations in the United States [18]. We extracted standardized EHR data curated under Observational Medical Out- comes Partnership (OMOP) Common Data Model (CDM) from controlled-access tier. The analyses were conducted using data repository version 8, which includes EHR data 3 Cohort Stats Depressive Disorder PTSD COHORT SIZE 65,643 9,310 CASES : CTRL 20.6% : 79.4% 24.7% : 75.3% VOCAB SIZE ICD10 DB ID LOINC ER 33.1% 34.6% 32.1% 0.2% % of N=4,904 ICD10 DB ID LOINC ER 41.6% 34.7% 23.4% 0.3% % of N=3,535 DATASET-WIDE FREQUENCY DISTRIBUTION ICD10 DB ID LOINC ER 65.8% 22.1% 11.2% 0.9% % of N=4,712,501 ICD10 DB ID LOINC ER 65.5% 25.2% 8.3% 1.0% % of N=749,261 Table 1. Characteristics of Depression and PTSD Cohorts. Cohort and data descriptions are generated after preprocessing EHR data into BiPETE input format. Vocabulary size denotes the size of EHR codes set for each standard vocabularies, whereas dataset-wide EHR code distribution refers to the frequency distribution of codes across the dataset. ICD10 statistics are derived after truncating codes to the first three characters. from 394,596 participants. Data is available at AoU Research Hub upon registration. Two cohorts of patients with MHD, depressive disorders and PTSD, were con- structed using AoU Cohort Builder. AoU patient data are organized using standardized medical information referred to as “concepts”. The custom medical concept sets used to build our cohorts are reported in Supplementary Table 3 and 4. Prior to preprocessing, the depression and PTSD cohorts comprised 84,163 and 15,334 patients, respectively (Supplementary Table 1). Following data preprocessing, cohort sizes were reduced to 65,643 for depression and 9,310 for PTSD (Table 1). For cohort EHR data preprocessing, we defined the following clinical scenario: a patient with MHD diagnosis—either depressive disorders or PTSD—but without a prior diagnosis of ASUD presents at a clinic. The current risk of new-onset ASUD is assessed based on the patient’s medical history from the preceding 15 months. The patient is labeled as a case if diagnosed with ASUD at a subsequent visit, and as a control if not diagnosed with ASUD (Figure 1). For control patients, the medical history from their most recent visits was used while ensuring no ASUD diagnosis was recorded during their entire observation period. 4 Processing Participant Electronic Health Record Data BiPETE ... MHD Dx Visit 1 Visit 2 Visit 3 ... ... MHD Dx Visit 1 Visit 2 Visit 3 ... Visit N Sinusoidal Positional Embedings Rotary Positional Embeddings Multi-Head Attention Skip-Connection Fully Connected xM Encoder Blocks EHR token Embeddings RNN-based Binary Classifier Raw Logit Prediction Layer-Norm Layer-Norm Skip-Connection K,Q V EHR Codes Days-ago Indices Visit Indices ixx mxx mxx dxx dbxxxxx 1234567 ... dbxxxxx 1 1 3 2 3 3 ... N 50 50 10 24 10 10 ... 0 Convert to BiPETE Input Format Extract EHR Codes from 1 Year & 3 Months Observation Period * Diagnosis (ICD10CM First 3 Letters) * Administered Drugs (Drugbank ID) * Abnormal Measurements (LOINC) * Number',\n",
       "  'dbxxxxx 1 1 3 2 3 3 ... N 50 50 10 24 10 10 ... 0 Convert to BiPETE Input Format Extract EHR Codes from 1 Year & 3 Months Observation Period * Diagnosis (ICD10CM First 3 Letters) * Administered Drugs (Drugbank ID) * Abnormal Measurements (LOINC) * Number of ER Visits 1 Year & 3 Months Observation Period Class 1 (MHD & ASUD) Class 0 (MHD) Last recorded visit Visit w/ ASUD Event Visit N Figure 1. Pipeline Flowchart Illustrating Data Preprocessing, Model Input Construction, and BiPETE Architecture. MHD Dx refers to diagnosis of Mental Health Disorder. For ASUD and non-ASUD classes, only the EHR codes from visits occurring after MHD diagnosis are extracted. Repeated values within the Visit and Days-ago index sequences indicate that the corresponding EHR codes were recorded during the same clinical visit. In the BiPETE architecture, the K, Q, V are the key, query and value embeddings, respectively, and M refers to the number of encoder blocks. 2 EHR Preprocessing and Input Representation 2.1 Preprocessing Multi-Vocabulary EHR Structured EHRs offer a wealth of patient information encompassing diagnosis, med- ication, laboratory tests and other clinical data. To capture this rich clinical con- text, we constructed input instances using standard vocabularies, including the Inter- national Classification of Diseases, Tenth Revision (ICD10) for diagnosis, DrugBank Identifiers (DBID) for medications, and Logical Observation Identifiers Names and Codes (LOINC) for laboratory measurements. Laboratory tests were restricted to those flagged abnormal–either high or low–emphasizing clinically significant recordings. Ad- ditionally, the number of emergency room (ER) visits associated with each visit was incorporated as an indicator of acute events. 5 OMOP CDM organizes clinical information using relational tables, requiring a series of preprocess steps to construct EHR input sequences. After defining our cohorts, we extracted medical concepts for diagnosis, medications and measurements for each pa- tient using visit occurrence identifiers and their corresponding visit date ranges, defined by a start and end date. To reduce time-series fragmentation, patient visits occurring within a 7-day window were merged into a single visit, grouping temporally related encounters and analyzing them as a single episode of care. Patients with fewer than three visits within the observation window were excluded to ensure sufficient clinical context per input instance. The extracted concepts were mapped to standardized vocabularies and consolidated to represent the clinical events occurring within visits to generate sequential EHR code data. There are visits with no recorded or extracted EHR codes, and a unique token was assigned to such visits to denote the absence of clinical information. To reduce model complexity and sparsity in the input representation, ICD10 diag- nosis codes were truncated to their three-character root categories, aggregating related diagnoses into broader groups. This grouping increased token frequency, mitigated re- dundancy and enhanced model efficiency by reducing vocabulary size. Furthermore, the patient’s EHR history was restricted to a 15-month observation window to focus on information most predictive of our classification task. 2.2 Temporal Modeling with Auxiliary Sequences BiPETE processes a sequence of EHR codes alongside two auxiliary sequences to encode temporality: visit indices and days-ago indices',\n",
       "  'reducing vocabulary size. Furthermore, the patient’s EHR history was restricted to a 15-month observation window to focus on information most predictive of our classification task. 2.2 Temporal Modeling with Auxiliary Sequences BiPETE processes a sequence of EHR codes alongside two auxiliary sequences to encode temporality: visit indices and days-ago indices (Figure 1). In line with prior studies [10, 12], visit indices follow the chronological order of a patient’s visits, with the earliest visit indexed as 0. Days-ago indices are computed as the time difference in days between each visit and the most recent recorded visit in the patient’s medical record, with codes from the most recent visit indexed as 0. The temporal information represented by visit indices and encoded using SPE is hereafter referred to as visit embeddings, while that represented by days-ago indices and encoded using RoPE is referred to as days-ago embeddings. Days-ago embeddings capture the relational dynamics of EHR code occurrences be- tween visits. RoPE applies rotation transformations to query and key embeddings of EHR tokens, such that the dot product between queries and keys depend on their rela- tive positions, decaying interdependence with increased relative distance between them [12]. In simpler terms, RoPE explicitly expresses short- and long-ranged token relation- ships, assisting the transformer’s attention mechanism to learn the interaction patterns. All codes within a single visit undergo the same rotational transformation, effectively grouping them together into a visit while capturing relative temporal relationships of visits. Patient visits are intermittent and visit gaps vary widely across patients. Applying RoPE directly to such varying days-ago indices can complicate model learning, as each index produces a different rotation based on the gap between visits. To strengthen visit 6 order and token grouping, we encode visit order using SPE, applying the same absolute positional encoding to the codes recorded within a visit. 3 BiPETE Architecture BiPETE is a transformer-based classifier adapted from BERT [11] with architectural modifications to the encoder and an addition of a bidirectional gated recurrent unit (BiGRU) classifier head (Figure 1). Unlike the original BERT, BiPETE employs the bidirectional self-attention encoder without leveraging pre-training. The model param- eters are randomly initialized, and the token relationships are learned solely in the context of the single-disease classification task. Two modifications are applied to the encoder block: RoPE embeddings are applied to the query and key representations of tokens, and Pre-LayerNorm configuration is used. After the input EHR token se- quence is processed by stacked encoder blocks, the GRU head generates a single output logit, which is passed through a sigmoid and optimized with Binary Cross-Entropy loss against the ASUD or non-ASUD label. 4 Model Training and Evaluation 4.1 Model Training and Cross-validation We evaluated BiPETE with the ASUD risk prediction task on depressive disorders and PTSD cohorts and employed five-fold cross-validation to assess model generalizability [22]. For each iteration, the data was partitioned into training, validation and test subsets in a 7:1:2 ratio. Across the iterations, each fold served once as a test set. A 6-encoder-layer architecture was used to train and evaluate the model on the depression cohort, resulting',\n",
       "  'cross-validation to assess model generalizability [22]. For each iteration, the data was partitioned into training, validation and test subsets in a 7:1:2 ratio. Across the iterations, each fold served once as a test set. A 6-encoder-layer architecture was used to train and evaluate the model on the depression cohort, resulting in a 48-million-parameter model. For the PTSD cohort, a reduced 3-encoder-layer version was employed due to its smaller dataset size, resulting in a 35-million-parameter model. The size of large language models must scale with dataset size to improve performance. A study on compute-optimal training suggested that, for a given compute budget, model parameters and training data should be scaled at equal proportion, with approximately 20 training tokens per parameter to achieve optimal balance between performance and compute efficiency [23]. In our case, although the models were larger relative to the dataset size, ablation studies indicated that our configuration, combined with early stopping based on average validation loss across folds, yielded the best performance. To assess the impact of the positional encodings on model performance, we con- ducted ablation experiments in the depression cohort by excluding visit or days-ago embedding, comparing training and test metrics for models configured with visit em- bedding alone, days-ago embedding alone, and both combined. For comparability, all models were trained using the same random seeds, deterministic computation settings and five-fold cross-validation protocol. 7 4.2 Evaluation Metrics We report area under receiver operator characteristics curve (AUROC) and area under precision-recall curve (AUPRC) as primary performance evaluation metrics. Both AU- ROC and AUPRC deliver threshold-independent assessments by summarizing model performance across a range of decision thresholds. AUROC measures the trade-off between true-positive rate and false-positive rate and is relatively robust to class im- balance, as it can emphasize the model’s ability to identify positives amidst a large proportion of negatives. AUPRC captures trade-off between precision and recall across all decision thresholds. It reflects the effect of missed cases via recall and is well-suited for evaluating disease risk prediction characterized by rare positive instances and high costs associated with missed detections. In addition to AUROC and AUPRC, we report positive predictive value (PPV), sensitivity, and specificity on three different thresholds to communicate interpretable evaluation of model performance. All metrics are reported as the mean and standard deviation across the five held-out test folds. Coefficient of variation (CV) across the thresholds is calculated across the models to compare the dispersion of the metrics. 4.3 Baseline Models Used for Benchmarking We compared BiPETE against three baseline models: BiGRU, Logistic Regression (LR) and Bernoulli Naive Bayes (BNB). For LR and BNB, the EHR data was represented as one-hot encoded vectors, whereas for BiGRU, EHR codes were transformed to token embeddings without visit and days-ago embeddings. LR is a robust classifier and was included to assess whether the data exhibits linear separability. BNB was used to evaluate whether the presence or absence of EHR codes could predict ASUD risk, assuming conditional independence among the codes as in the Naive Bayes framework. BiGRU served as a deep learning baseline capable of modeling sequential dependencies in EHR',\n",
       "  'assess whether the data exhibits linear separability. BNB was used to evaluate whether the presence or absence of EHR codes could predict ASUD risk, assuming conditional independence among the codes as in the Naive Bayes framework. BiGRU served as a deep learning baseline capable of modeling sequential dependencies in EHR data. All baseline models were trained and evaluated under the same five-fold cross-validation setup as BiPETE to ensure consistency. 5 Attribution Analysis 5.1 Integrated Gradients To interpret the contribution of the EHR codes to ASUD prediction within our cohorts, we employed Integrated Gradient (IG) to compute token-level attributions. IG provides feature attributions that quantify the model’s sensitivity to changes in each input, capturing variations in each feature that affect the prediction [20]. Grounded in integral calculus, IG follows axiomatic principles that enable instance-level attributions to be naturally aggregated into global, dataset-level insights. IG requires a predefined, uninformative baseline input to serve as a reference for measuring attribution. The completeness axiom ensures that the sum of all feature attributions equals the difference between the model output and the reference output, 8 while the sensitivity axiom guarantees that only features influencing the prediction receive nonzero attributions [20]. Tokens that consistently drive predictions away from the baseline yield positive or negative contributions, whereas uninformative features converge toward zero. In practice, IG computes the attribution for each token by integrating the gradient of the model output with respect to the input along a linear path from the baseline input to the actual input. Feature attribution IGi(x) of token i in input x is formally written as: IGi(x) = \\x00xi −x′ i \\x01 · Z 1 α=0 ∂F(x′ + α (x −x′)) ∂xi dα (1) where x′ denotes the baseline input vector, α ∈[0,1] is a scalar parameter tracing the straight-line path from x′ to x, and F represents the prediction function. For deep learning models, this path integral is numerically approximated by averaging gradients over interpolated inputs. The approximation of IGi(x) of token i in input x is formally written as: IGi(x) ≈(xi −x′ i) · 1 m m X k=1 ∂F \\x00x′ + k m (x −x′) \\x01 ∂xi (2) where x′ denotes the baseline input vector, x is the input being explained, F represents the prediction function, m is the number of interpolated steps and k is the index of the steps. In our study, the baseline input is defined as a sequence of padding tokens. To ensure consistency in the attribution of tokens across instances with varying sequence length and positions, we retain the visit and days-ago embeddings of the original input when constructing the baseline. IG token attributions quantify the extent to which each feature influences the pre- dicted label, with positive values increasing and negative values decreasing the predicted likelihood. In our ASUD classification task, the attributions of EHR codes reflect the model’s internal reasoning rather than absolute risk. 5.2 Aggregating Token Contribution: Relative Contribution We compute the Relative Contribution (RC) to quantify the directional attribution at the token level, defined as the ratio of a token’s average attribution in the',\n",
       "  'our ASUD classification task, the attributions of EHR codes reflect the model’s internal reasoning rather than absolute risk. 5.2 Aggregating Token Contribution: Relative Contribution We compute the Relative Contribution (RC) to quantify the directional attribution at the token level, defined as the ratio of a token’s average attribution in the true positive (TP) group to its average attribution in the true negative (TN) group. Only instances correctly predicted by the model are used in the RC calculation to extract meaningful attribution interpretation [10]. Tokens showing opposite attribution signs across TP and TN groups were excluded, as they indicate inconsistent directional effects. Denoting ATP(t) as the average attribution of token t in the true positive (TP) group, and ATN(t) as the average attribution in the true negative (TN) group, the RC of token t is then calculated as: RC(t) = ATP(t) ATN(t) (3) RC values greater than 1 indicate contribution toward the ASUD class, whereas values less than 1 indicate contribution toward the non-ASUD class. To ensure statistical 9 robustness and reduce the influence of outliers, we restrict our analysis on tokens that appear in at least 1% of instances within each TP and TN group. For tokens occurring multiple times within a single instance, attribution values are averaged. RC values are calculated using the attribution values from the five held-out test sets. Results 1 Cohort Characteristics After Preprocessing Cohort EHR vocabulary sizes and distributions are shown in Table 1. Within the depression cohort, ICD10, DBID, and LOINC codes contribute approximately equally to the total EHR vocabulary, whereas in the PTSD cohort, the distribution is more uneven. Nevertheless, the dataset-wide frequency distributions are similar across both cohorts. Truncating ICD10 codes reduced model complexity and redundancy in clinical information. While this introduces loss in diagnostic specificity, it reduced the ICD10 vocabulary size by 95%, from 32,662 unique codes to 1,625 in the depression cohort and by 93%, from 21,137 unique codes to 1,469 in the PTSD cohort. Most data loss occurred during the extraction and mapping of visit-level EHR codes to standardized vocabularies–ICD10, DBID, and LOINC–which reduced the number of usable visits in many patient records. As a result, 22% of samples in the depression cohort and 39% in the PTSD cohort were removed (Supplementary Table 1). 2 BiPETE Performance on ASUD Risk Prediction AUROC and AUPRC scores of BiPETE’s ASUD risk prediction in depression disor- der and PTSD cohorts are shown in Table 2. On the AUROC, BiPETE with both days-ago and visit embeddings obtain the best results in the depression cohort with a score of 96.46%, outperforming the baseline models–BiGRU, LR, and BNB–which scored 85.21%, 83.54% and 77.17%, respectively. The AUPRC score of 93.18% shows a performance gain of over 30% compared to the three baseline models. A similar pattern for both metrics is observed in the PTSD cohort. Supplementary Table 3 compares PPV, sensitivity and specificity of BiPETE, Bi- GRU and LR in the decision thresholds of 0.2, 0.5 and 0.8. As expected, increasing the decision threshold leads to higher PPV and specificity, but lower sensitivity for all models. Though this',\n",
       "  'metrics is observed in the PTSD cohort. Supplementary Table 3 compares PPV, sensitivity and specificity of BiPETE, Bi- GRU and LR in the decision thresholds of 0.2, 0.5 and 0.8. As expected, increasing the decision threshold leads to higher PPV and specificity, but lower sensitivity for all models. Though this trend is consistent, the CV values for BiPETE metrics remain consistently lower than those of BiGRU and LR, indicating BiPETE achieves more stable and robust class separation across thresholds. 3 Impact of Visit and Days-ago Embeddings on Performance In the depression cohort, we report AUROC and AUPRC scores of BiPETE and its variants configured with only days-ago embeddings or only visit embeddings to com- 10 Models AUROC (%) AUPRC (%) Depressive Disorder BiPETE 96.46 (±0.21) 93.18 (±0.27) RoPE Only 94.46 (±0.88) 90.73 (±1.17) SPE Only 88.33 (±0.35) 77.34 (±0.48) BiGRU 85.21 (±0.24) 69.42 (±0.35) Linear Regression 83.54 (±0.47) 67.79 (±0.30) Bernoulli Naive Bayes 77.17 (±0.51) 46.73 (±0.77) PTSD BiPETE 96.50 (±0.40) 94.04 (±0.63) BiGRU 80.13 (±0.53) 62.29 (±0.24) Linear Regression 83.97 (±0.68) 70.86 (±1.53) Bernoulli Naive Bayes 93.59 (±5.44) 87.42 (±9.33) Table 2. AUROC and AUPRC Scores for ASUD Risk Prediction using BiPETE and Baseline Models in Depression and PTSD Cohorts. The depres- sion cohort includes two additional rows for positional encoding variants: one using only days-ago embeddings (RoPE) and the other only visit embeddings (SPE). The baseline BiGRU shares the same configuration as the BiGRU used as the classifier head in BiPETE. ±indicates standard deviation across five-fold cross-validation. pare the impact of different positional embedding strategies. BiPETE outperforms the single positional embedding models in both metrics (Table 2). Compared to BiGRU, BiPETE, days-ago-embedding-only model, and visit-embedding-only model improved in AUPRC by 34.23%, 30.70% and 11.41%, respectively. In addition to achieving higher AUROC and AUPRC scores, BiPETE exhibits lower standard deviations than the single-positional-embedding models, indicating a more consistent performance than the models with a single positional encoding. The average ROC and PR curves of the cross-validation folds for the BiPETE and its positional encoding variant models are presented in Supplementary Figure 1. Figure 2 illustrates the learning behavior of the positional encoding variant models during training. We report training and validation metrics–including loss, accuracy and ROAUC–recorded over the course of 30 epochs. Among the configurations, BiPETE demonstrates the fastest convergence and exhibits the most stable training performance, indicated by the narrow error bars across all training metrics. BiPETE’s train accu- racy reaches close to 100%, showing near-perfect fit to the training data and signs of overfitting, as evidenced by the increase in validation loss after epoch 15. The days- ago-embedding-only model achieves validation metrics comparable to those of BiPETE, but the validation loss and accuracy exhibit high instability during training, reflected in the large standard deviations. The visit-embedding-only model shows training stability similar to the model with both embeddings, but underperforms across both training 11 0.18 0.14 0.10 0.06 0.02 Train Loss 70 75 80 85 90 95 100 Accuracy 0.7 0.8 0.9 1.0 AUROC 0.9 0.7 0.5 0.3 0.1 Validation 70 75 80 85 90 95 100 0.7 0.8 0.9',\n",
       "  'stability similar to the model with both embeddings, but underperforms across both training 11 0.18 0.14 0.10 0.06 0.02 Train Loss 70 75 80 85 90 95 100 Accuracy 0.7 0.8 0.9 1.0 AUROC 0.9 0.7 0.5 0.3 0.1 Validation 70 75 80 85 90 95 100 0.7 0.8 0.9 1.0 Both Emb Only RoPE Only SPE Figure 2. Training and Validation Performance of Classifiers with Different Positional Encoding Configurations. Training and validation loss, accuracy and AUROC are reported across 30 training epochs to compare model learning and general- ization. The error bars indicate the standard deviation across five-fold cross-validation folds. and validation metrics. Supplementary Figure 2 presents the attention matrices of the nine heads in the final BiPETE layer for a representative instance, illustrating the effect of visit and days-ago embeddings. Each square matrix shows token-to-token attention scores of the input sequence. Both axes are annotated with the EHR vocabulary type and the visit indices, with the y axis representing tokens attending to those on the x axis. Ideally, different heads capture distinct aspects of the input EHR sequence, enabling the model to extract meaningful patterns for prediction, and the visualization shows that individual heads attend to different visit pairs. Tokens tend to exhibit high or low attention in groups according to their visit indices. Attention scores vary within the visits as well. Together, inter- and intra-visit dynamics can be seen in the attention maps. 4 IG-Derived Indicators of Increased and Reduced ASUD Risk We performed token-level feature attribution using IG and calculated RC by averaging the token attributions of correctly predicted test samples. RC quantifies the influence of each token on BiPETE’s prediction in the true positive group relative to the true negative group. We identified potential markers that drove BiPETE’s prediction to- ward ASUD or Non-ASUD in both the Depression and PTSD cohorts. The identified indicators–including, abnormal lab test results, medication use, and comorbidities–are reported for each cohort. 12 4.1 Indicators Associated with Higher ASUD Risk in Depression Cohort–Table 3 Alterations in lymphocyte-related markers, including the neutrophil-to-lymphocyte ra- tio (NLR) and platelet-to-lymphocyte ratio reflect chronic inflammation that can influ- ence neurotransmission and brain function, with evidence suggesting both neurotoxic and neuroprotective effects depending on context [48, 49]. Depression and SUD fre- quently co-occur and share overlapping immune dysfunction, as substances like alcohol and opioids directly impair immune function, while elevated pro-inflammatory cytokines correlate with depressive symptoms. Similarly, coagulation markers, such as prolonged prothrombin time (PT) and altered international normalized ratio (INR), are affected by substance use particularly chronic alcohol use through liver dysfunction, and by depression via medication adherence challenges, highlighting the bidirectional interac- tion between immune and hemostatic pathways and psychiatric and substance-related outcomes [49]. Vancomycin, while not psychoactive, may intersect with ASUD risk in patients with recent substance use or complex medical conditions, as altered pharmacokinetics and hospitalization-related stressors can create vulnerabilities for maladaptive substance behaviors [50]. Metronidazole may increase susceptibility to ASUD via disulfiram-like reactions when alcohol is consumed during or shortly after therapy, producing aversive physiological effects such as nausea, tachycardia, and abdominal discomfort that',\n",
       "  'substance use or complex medical conditions, as altered pharmacokinetics and hospitalization-related stressors can create vulnerabilities for maladaptive substance behaviors [50]. Metronidazole may increase susceptibility to ASUD via disulfiram-like reactions when alcohol is consumed during or shortly after therapy, producing aversive physiological effects such as nausea, tachycardia, and abdominal discomfort that can promote maladaptive drinking or relapse [51]. Acyclovir carries potential for neurotoxic side effects, particularly in older adults, patients with renal impairment, or those with multiple comorbidities, with symptoms such as confusion, ataxia, and altered mental status linked to elevated drug or metabolite concentrations and blood–brain barrier disruption [52]. These neurocognitive effects may impair mood regulation and inhibitory control, creating conditions for self-medication with alcohol or other substances. Conditions such as peripheral vascular disease, thyroid disorders, flatulence and re- lated gastrointestinal conditions, hereditary neuropathies, and primary hypertension Indicators of High Risk Risk Marker Type RC N (Case/Ctrl) Lymphocytes / Total Leukocytes in Blood Abnormal Lab test result 50.7491 912/2703 International Norm Ratio (Platelet Poor Plasma) Abnormal Lab test result 22.8152 408/938 Prothrombin Time (PT) Abnormal Lab test result 11.1948 560/1262 Vancomycin Medication use 5.2091 262/982 Metronidazole Medication use 2.4416 522/1515 Acyclovir Medication use 2.2308 156/520 Other Peripheral Vascular Diseases Comorbidities 40.5746 358/1575 Other Disorders of Thyroid Comorbidities 21.1796 175/717 Flatulence and Related Conditions Comorbidities 0.0644 300/1317 Hereditary and Idiopathic Neuropathy Comorbidities 15.3311 360/811 Essential (primary) Hypertension Comorbidities 9.7897 4974/20857 Table 3. Key Indicators Associated with Increased Risk of ASUD in Patients with Depressive Disorder. 13 often require ongoing management and can involve chronic pain, fatigue, or functional impairment. These stressors may dysregulate the hypothalamic-pituitary-adrenal axis and alter dopaminergic and serotonergic signaling in reward pathways, increasing sus- ceptibility to substance misuse as a coping strategy. Additionally, chronic illness and comorbidities can contribute to systemic inflammation, oxidative stress, and neuro- plasticity alterations, which may amplify depressive symptoms and further predispose patients to SUD. The cumulative physiological burden, along with potential medica- tion side effects and impaired autonomic or gastrointestinal function, can create a cycle of heightened stress and vulnerability, highlighting the importance of comprehensive medical and psychiatric management to mitigate substance misuse risk in these popu- lations. 4.2 Indicators Associated with Lower ASUD Risk in Depression Cohort–Table 4 Platelets and red blood cell indices, such as mean corpuscular hemoglobin concentration (MCHC), may play interconnected roles in depression and ASUD through mechanisms involving serotonin signaling, inflammation, and oxygen delivery [53, 54]. Platelets, which store the majority of the body’s serotonin, reflect central serotonergic activity and may be hyperactivated in depression [24]. Chronic use of substances such as alcohol, cocaine, heroin, and methamphetamine can induce thrombocytopenia via bone marrow suppression, immune-mediated destruction, or increased consumption, further impair- ing neurocognitive function and contributing to mood dysregulation. Similarly, low MCHC, often resulting from iron deficiency anemia or chronic substance use, reduces oxygen delivery to neurons, disrupting neurotransmitter synthesis and exacerbating de- pressive symptoms [53, 54]. Platelet abnormalities and low MCHC typically reflect the physiological consequences of substance use and depression, highlighting the potential benefit of interventions targeting these parameters to support mood stabilization and reduce indirect risk factors',\n",
       "  'substance use, reduces oxygen delivery to neurons, disrupting neurotransmitter synthesis and exacerbating de- pressive symptoms [53, 54]. Platelet abnormalities and low MCHC typically reflect the physiological consequences of substance use and depression, highlighting the potential benefit of interventions targeting these parameters to support mood stabilization and reduce indirect risk factors for SUD. Naloxone, a rapid opioid antagonist, remains a cornerstone of harm reduction, with community-based distribution programs achieving survival rates above 92–98% [55, 56, 57, 58]. β-lactam antibiotics such as cefazolin [59, 60, 61], amoxicillin [62, 63], and clavulanic acid [64, 65, 66, 67] modulate glutamatergic signaling by upregulat- ing GLT-1 and the cystine-glutamate exchanger in addiction-relevant brain regions, attenuating ethanol or cocaine reward, reducing cue-induced reinstatement, and facil- itating extinction of drug-seeking behaviors. Other agents, including hydroxyzine (es- pecially in combination with 5-HT3 antagonists), famotidine [68], and dexamethasone [69, 70], show promise in mitigating withdrawal symptoms, stress, or neuroinflamma- tion, which are key risk factors for relapse. Psychostimulants like dextroamphetamine [69, 71, 72, 73], when administered under clinical supervision, reduce substance use risk by addressing underlying neurochemical and behavioral deficits, supported by both population-level and preclinical data. Conditions such as other specified health status, intracranial injury, extrapyramidal and movement disorders, and male erectile dysfunction are generally chronic but man- 14 Indicators of Low Risk Type of Risk Marker RC N (Case/Ctrl) MCHC [Mass/volume] Abnormal Lab test result 0.4432 1152/4102 Platelets [#/volume] in Blood Abnormal Lab test result 0.9242 290/606 Naloxone Medication use 0.0115 253/928 Cefazolin Medication use 0.0236 418/2721 Amoxicillin Medication use 0.0457 867/4141 Famotidine Medication use 0.0483 498/3312 Dexamethasone Medication use 0.3031 521/2962 Dextroamphetamine Medication use 0.3056 169/903 Clavulanic acid Medication use 0.3480 440/2260 Duloxetine Medication use 0.3664 648/2621 Other Specified Health Status Comorbidities 0.0009 403/2811 Intracranial Injury Comorbidities 0.0031 216/680 Other Extrapyramidal and Movement Disorders Comorbidities 0.0067 536/2429 Male Erectile Dysfunction Comorbidities 0.0095 368/1168 Table 4. Key Indicators Associated with Decreased Risk of ASUD in Pa- tients with Depressive Disorder. ageable, often requiring structured medical oversight or ongoing outpatient care [74]. These conditions may promote regular engagement with healthcare providers, facili- tate adherence to treatment routines, and provide early opportunities for monitoring mental health, all of which can reduce reliance on maladaptive coping strategies such as substance use. While these disorders may impact daily functioning, their gener- ally predictable course allows patients to maintain relative emotional and physiological stability, limiting the severity of depressive symptoms that often drive self-medication. 4.3 Indicators Associated with Higher ASUD Risk in PTSD Cohort–Table 5 We found the presence of abnormalities in hematological and metabolic markers which may highlight stress-related dysregulation in PTSD and comorbid substance use dis- order (SUD). Serum albumin and albuminuria reflect hepatic and renal reserve, with hypoalbuminemia and elevated albuminuria signaling systemic inflammation and vas- cular risk [25]. Mean platelet volume reflects systemic stress and shows mixed patterns in PTSD, with elevated levels indicating platelet activation and inflammation linked to SUD [24]. Mean corpuscular hemoglobin abnormalities, often worsened by alcohol use, contribute to fatigue and cognitive impairment. Together, these markers highlight the compounded biological burden of PTSD and SUD. Several',\n",
       "  'volume reflects systemic stress and shows mixed patterns in PTSD, with elevated levels indicating platelet activation and inflammation linked to SUD [24]. Mean corpuscular hemoglobin abnormalities, often worsened by alcohol use, contribute to fatigue and cognitive impairment. Together, these markers highlight the compounded biological burden of PTSD and SUD. Several commonly prescribed medications can influence the risk of SUD in patients with PTSD, with effects varying based on neurobiological vulnerability and prior sub- stance use history. Hydrocodone, widely used for pain management, is strongly asso- ciated with increased risk of opioid misuse and dependence in trauma-exposed adults, particularly among women, younger adults (18–34 years), and those with prior SUD, likely due to dysregulated dopaminergic and endogenous opioid pathways that amplify its reinforcing euphoric effects [26, 27, 28]. Oxybutynin has also been reported in case 15 Indicators of High Risk Risk Marker Type RC N (Case/Ctrl) Albumin [Mass/Volume] in Serum/Plasma Abnormal Lab test result 80.5480 133/231 Platelet mean volume [Entitic Volume] in Blood Abnormal Lab test result 38.7960 40/154 Mean corpuscular hemoglobin [Entitic Mass] Abnormal Lab test result 3.6885 171/358 Hydrocodone Medication use 138.1751 192/364 Oxybutynin Medication use 30.3633 35/97 Trimethoprim/Sulfamethoxazole Medication use 16.4505 69/182 Aripiprazole Medication use 10.1848 85/215 Mononeuropathies of Lower Limb Comorbidities 270.3382 50/146 Dislocation/Sprain of Ankle, Foot, or Toe Comorbidities 26.4504 60/109 Dental Caries Comorbidities 20.9523 169/345 Fibroblastic Disorders Comorbidities 19.0740 73/214 Table 5. Key Indicators Associated with Increased Risk of ASUD in Patients with PTSD. series to carry abuse potential because of hallucinogenic and euphoric CNS effects, suggesting heightened vulnerability in patients with prior SUD [29, 30]. Trimetho- prim–sulfamethoxazole (TMP-SMX) has been implicated in neuropsychiatric adverse effects such as hallucinations, particularly in older adults, which may indirectly elevate SUD risk [31, 32]. Aripiprazole exhibits mixed effects: long-acting injectable formula- tions have been associated with improvements in psychiatric symptoms and reductions in alcohol and cocaine use among patients with PTSD or schizophrenia and co-occurring SUD, while other reports caution that it may exacerbate compulsive substance use urges in some individuals, consistent with FDA warnings on impulse control disorders. Neurological conditions such as mononeuropathies of the lower limb and muscu- loskeletal injuries (dislocations and sprains of the ankle, foot, and toe) contribute to chronic pain and functional impairment, often leading to self-medication. Dental caries and fibroblastic disorders reflect ongoing disease burden that may exacerbate stress and promote maladaptive coping. Alongside psychiatric comorbidities and high healthcare utilization, these conditions collectively amplify the biological and psychological burden seen in these patients [33, 34, 35]. 4.4 Indicators Associated with Lower ASUD Risk in PTSD Cohort–Table 6 Iron, particularly ferritin, and Vitamin B12, which are critical micronutrients, were found to influence the risk of ASUD in patients with PTSD. Vitamin B12 deficiency has been linked to both PTSD and SUD, likely due to its essential role in monoamine neurotransmitter synthesis and mood regulation. Lower B12 levels in individuals with alcohol or methamphetamine use have been associated with increased relapse risk, and supplementation may help restore energy, cognitive function, and emotional regula- tion, indirectly mitigating ASUD risk by reducing self-medication behaviors. Similarly, iron homeostasis, especially',\n",
       "  'essential role in monoamine neurotransmitter synthesis and mood regulation. Lower B12 levels in individuals with alcohol or methamphetamine use have been associated with increased relapse risk, and supplementation may help restore energy, cognitive function, and emotional regula- tion, indirectly mitigating ASUD risk by reducing self-medication behaviors. Similarly, iron homeostasis, especially ferritin levels, is vital for dopaminergic neurotransmission, oxidative stress regulation, and myelination. Disruption of iron balance can lead to 16 Indicators of Low Risk Risk Marker Type RC N (Case/Ctrl) Ferritin [Mass/volume] in Serum/Plasma Abnormal Lab test result 0.4231 48/108 Vitamin B12 [Mass/volume] in Serum/Plasma Abnormal Lab test result 0.5072 70/86 Lidocaine Medication use 0.0121 210/1248 Hydroxyzine Medication use 0.0838 152/529 Lamotrigine Medication use 0.1219 73/240 Escitalopram Medication use 0.2128 78/407 Duloxetine Medication use 0.5460 111/489 Ergocalciferol Medication use 0.6546 40/222 Ceftriaxone Medication use 0.7809 51/168 Disorder of Continuity of Bone Comorbidities 0.0010 35/ 87 Other Disorders of Ear Comorbidities 0.0064 104/393 COVID-19 Comorbidities 0.0086 120/598 Other Speech Disturbances Comorbidities 0.0287 32/93 Table 6. Key Indicators Associated with Decreased Risk of ASUD in Pa- tients with PTSD. neurocognitive impairments and behavioral vulnerabilities relevant to PTSD and SUD, with animal studies showing that iron deficiency alters ferritin, dopamine metabolism, and neuroproteins such as prion protein (PrPC) and α-synuclein. Maintaining adequate levels of both B12 and iron may therefore support neurotransmitter function and brain health, providing a nutritional avenue to indirectly reduce ASUD risk. Lidocaine, a sodium channel blocker, selectively attenuates cue-induced cocaine- seeking behavior in preclinical models by modulating amygdala circuits, though clinical studies have yet to show significant reductions in craving [36, 37]. Hydroxyzine, par- ticularly when combined with the 5-HT3 antagonist palonosetron, has been shown to alleviate opioid withdrawal severity, potentially lowering relapse risk [38]. Lamotrig- ine, a glutamate-modulating antiepileptic, reduces cue-induced alcohol seeking and co- caine use in both preclinical and clinical settings through modulation of glutamatergic, dopaminergic, and serotonergic neurotransmission [38, 40]. Escitalopram and duloxe- tine, selective serotonin and serotonin/norepinephrine reuptake inhibitors, respectively, have demonstrated reductions in alcohol consumption and craving, with duloxetine also mitigating anxiety-like behaviors that may trigger relapse [41, 42]. Vitamin D (ergo- calciferol) supplementation helps in modulating neurotransmitter systems, improving psychological symptoms, and reducing drug-seeking behaviors in at-risk populations [43, 44]. Finally, ceftriaxone restores glutamate homeostasis via upregulation of GLT-1, attenuating cocaine- and alcohol-related reinstatement and partially reversing alcohol- induced gut dysbiosis, thereby further reducing vulnerability to ASUD [45, 46, 47]. Certain comorbid conditions may confer a protective effect, through mechanisms related to increased healthcare engagement and structured medical supervision. Our analysis identified associations with reduced ASUD risk for disorders of bone continu- ity, other unspecified ear disorders, COVID-19, and unspecified speech disturbances. 17 These conditions typically necessitate regular medical monitoring, specialist care, or ongoing management, providing opportunities to reinforce adaptive coping strategies, identify early signs of substance misuse, and limit exposure to high-risk behaviors. Con- sequently, such comorbidities may indirectly mitigate ASUD risk in PTSD patients by promoting treatment adherence and engagement in structured care environments. Discussion In our study, we introduced BiPETE, a transformer-encoder classifier designed to pre- dict single disease using structured,',\n",
       "  'signs of substance misuse, and limit exposure to high-risk behaviors. Con- sequently, such comorbidities may indirectly mitigate ASUD risk in PTSD patients by promoting treatment adherence and engagement in structured care environments. Discussion In our study, we introduced BiPETE, a transformer-encoder classifier designed to pre- dict single disease using structured, longitudinal EHR data. The central innovation of BiPETE lies in its bi-positional encoding strategy, which combines RoPE for captur- ing relative time gaps and SPE for encoding absolute visit order. This dual encoding captures fine-grained temporal dependencies in patient EHR sequence, addressing one of the key challenges in EHR modeling. The AUPRC scores of the variant models con- figured with only SPE or only RoPE indicate that RoPE plays a key role in modeling interdependencies among EHR tokens within and between visits, thereby enhancing the attention mechanism’s ability to focus on tokens relevant for distinguishing cases from controls (Table 1). However, the training and validation metrics of the RoPE only model (Figure 2) indicate instability, likely due to the variability of the days-ago indices across instances, which hinders learning of consistent temporal patterns. In contrast, the SPE- only model shows stable training but lower performance. By integrating SPE, which encodes absolute visit order, with RoPE, which models relative token relationships, BiPETE outperforms the single positional embedding variant models whilst retaining stability in model performance. Moreover, incorporating the most recent and clinically relevant patient visit data provided sufficient contextual information for BiPETE to identify label-specific patterns. These results demonstrate that optimizing positional encoding and incorporating task-relevant data substantially enhance the classification performance of transformer-based models. BiPETE, which does not rely on pretraining, exhibited strong performance in distin- guishing cases and controls–achieving over 90% on both AUROC and AUPRC–across the depression and PTSD cohorts with sizes of 65,643 and 9,310, respectively. This sug- gests that the model identified distinct token patterns for the classes without learning the semantic meaning of the tokens. It is unlikely that the model had learned meaning- ful token embedding during training for the classification task alone. Model learning was facilitated by reducing diagnosis vocabulary size by over 90% in both depression and PTSD cohorts. Without this step, ICD10 codes would dominate the vocabulary, leading to issues such as attention skew toward diagnosis, reduced representation of other vocabularies, and overfitting to ICD10 codes, all of which could impair model learning and generalizability. This process removed redundancy in diagnosis codes, enabling the model to learn meaning token patterns while integrating multiple vocab- ularies. We believe that BiPETE’s disease prediction performance could be further improved through pretraining on large-scale EHR corpora using tasks such as masked 18 language modeling or disease prediction tasks to learn token context and semantics, as demonstrated in previous studies [6, 10, 12]; we leave this for the future work. Our IG analysis provided key information about the biological and clinical markers associated with increased or reduced risk of ASUD among patients with depression and PTSD. In PTSD, elevated inflammatory and metabolic markers such as mean platelet volume and hypoalbuminemia suggested systemic stress and inflammation as key path-',\n",
       "  'future work. Our IG analysis provided key information about the biological and clinical markers associated with increased or reduced risk of ASUD among patients with depression and PTSD. In PTSD, elevated inflammatory and metabolic markers such as mean platelet volume and hypoalbuminemia suggested systemic stress and inflammation as key path- ways linking trauma-related dysregulation to substance misuse. Concurrently, medica- tions such as hydrocodone and oxybutynin highlight the complex interaction between pain management, neuropsychiatric effects, and addiction risk. Conversely, protective features including adequate vitamin B12 and ferritin levels, and use of agents like lam- otrigine and ceftriaxone, point toward neurobiological stabilization and glutamatergic modulation as potential resilience mechanisms. Similar patterns emerged in the depres- sion cohort, where immune and coagulation markers (e.g., altered NLR, PT/INR) re- flected shared inflammatory and hepatic pathways underlying comorbid depression and SUD. Notably, β-lactam antibiotics and psychostimulant therapies appeared protective, consistent with emerging evidence for neuroimmune and dopaminergic modulation in addiction recovery. Collectively, these findings demonstrate how BiPETE can bridge data-driven prediction with mechanistic understanding, revealing clinically meaningful targets for early identification and personalized intervention in comorbid psychiatric and substance use disorders. There are several limitations in our current work that should be addressed in future evaluations of BiPETE. First, we tested BiPETE only on the ASUD risk prediction task, so its performance on other disease prediction tasks remains uncertain. Second, BiPETE has not yet been evaluated on datasets beyond the AoU data, and its gen- eralizability remains to be fully validated. Third, without pretraining to learn token contextual information, BiPETE is currently limited to single-disease prediction and requires retraining for each new disease prediction task. Fourth, we did not include any demographic information in our input. Incorporating such information could improve performance, especially given the diverse racial composition of our cohort (Supplemen- tary Table 1). In addition, there are methodological caveats in interpreting attributions from transformer models. The context-dependent nature of the attention mechanism can lead to varying attributions for the same code across patients or visits. Further- more, confounding by indication may cause treatment codes associated with more severe disease to appear linked to ASUD outcomes, even when they reflect disease progression rather than risk. In conclusion, we proposed a single-disease classifier, BiPETE, and evaluated it on ASUD risk prediction. BiPETE’s positional encoding configuration could be applied to other clinical relevant tasks by improving temporal modeling of EHR data. This con- figuration can be readily adapted to existing transformer-based models to potentially boost their performance. BiPETE is a high-performing classifier, making it particu- larly valuable in settings where pretrained models or large-scale pretraining data are unavailable. 19 Acknowledgments We gratefully acknowledge All of Us participants for their contributions, without whom this research would not have been possible. We also thank the National Institutes of Health’s All of Us Research Program for making available the participant data and cohorts examined in this study. 20 References [1] Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J., AI in health and medicine, Nature medicine, 28, 31-38, (2022). [2] Hama, T. et al., Enhancing patient outcome prediction through deep',\n",
       "  'of Us Research Program for making available the participant data and cohorts examined in this study. 20 References [1] Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J., AI in health and medicine, Nature medicine, 28, 31-38, (2022). [2] Hama, T. et al., Enhancing patient outcome prediction through deep learning with sequential diagnosis codes from structured electronic health record data: Systematic review, Journal of Medical Internet Research 27, e57358, (2025). [3] Rasmy, L. et al., Recurrent neural network models (CovRNN) for predicting out- comes of patients with COVID-19 on admission to hospital: model development and validation using electronic health record data, The Lancet Digital Health 4, e415–e425, (2022). [4] Atasoy, H., Greenwood, B. N. & McCullough, J. S., The digitization of patient care: a review of the effects of electronic health records on health care quality and utilization, Annual Review of Public Health 40, 487–500, (2019). [5] Yang, X. et al., A large language model for electronic health records, NPJ Digital Medicine 5, 194, (2022). [6] Rasmy, L., Xiang, Y., Xie, Z., Tao, C. & Zhi, D., Med-BERT: pretrained contex- tualized embeddings on large-scale structured electronic health records for disease prediction, NPJ Digital Medicine 4, 86, (2021). [7] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K., BERT: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), 4171–4186, (2019). [8] Xie, F. et al., Deep learning for temporal data representation in electronic health records: A systematic review of challenges and methodologies, Journal of Biomed- ical Informatics 126, 103980, (2022). [9] Holmes, J. H. et al., Why is the electronic health record so challenging for research and clinical care?, Methods of Information in Medicine 60, 032–048, (2021). [10] Li, Y. et al., BEHRT: transformer for electronic health records, Scientific Reports 10, 1–12, (2020). [11] Si, Y. et al., Deep representation learning of patient data from Electronic Health Records (EHR): A systematic review, Journal of Biomedical Informatics 115, 103671, (2021). [12] Lindhagen, L., Van Hemelrijck, M., Robinson, D., Stattin, P. & Garmo, H., How to model temporal changes in comorbidity for cancer patients using prospective cohort data, BMC Medical Informatics and Decision Making 15, 96, (2015). 21 [13] Valderas, J. M., Starfield, B., Sibbald, B., Salisbury, C. & Roland, M., Defining comorbidity: implications for understanding health and health services, The Annals of Family Medicine 7, 357–363, (2009). [14] Yang, Z., Mitra, A., Liu, W., Berlowitz, D. & Yu, H., TransformEHR: transformer- based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records, Nature Communications 14, 7857, (2023). [15] Su, J. et al., Roformer: Enhanced transformer with rotary position embedding, Neurocomputing 568, 127063, (2024). [16] Rabbani, N., Kim, G. Y., Suarez, C. J. & Chen, J. H., Applications of machine learning in routine laboratory medicine: Current state and future directions, Clin- ical Biochemistry 103, 1–7, (2022). [17] Bailly, A. et al., Effects of dataset size and interactions on the prediction per- formance of logistic regression and deep learning models, Computer Methods and Programs in',\n",
       "  'Chen, J. H., Applications of machine learning in routine laboratory medicine: Current state and future directions, Clin- ical Biochemistry 103, 1–7, (2022). [17] Bailly, A. et al., Effects of dataset size and interactions on the prediction per- formance of logistic regression and deep learning models, Computer Methods and Programs in Biomedicine 213, 106504, (2022). [18] Ramirez, A. H. et al., The All of Us Research Program: data quality, utility, and diversity, Patterns 3, (2022). [19] Sadeghi, Z. et al., A review of Explainable Artificial Intelligence in healthcare, Computers and Electrical Engineering 118, 109370, (2024). [20] Sundararajan, M., Taly, A. & Yan, Q., Axiomatic attribution for deep networks (Integrated Gradients) in International Conference on Machine Learning (ICML), 3319–3328 (PMLR), (2017). [21] Abgrall, G., Holder, A. L., Chelly Dagdia, Z., Zeitouni, K. & Monnet, X., Should AI models be explainable to clinicians?, Critical Care 28, 301, (2024). [22] Rodriguez JD, Perez A, & Lozano JA, Sensitivity analysis of k-fold cross validation in prediction error estimation, IEEE transactions on pattern analysis and machine intelligence 32, 569-575, (2009). [23] Hoffmann, J.et al., Training Compute-Optimal Large Language Models in NeurIPS 22: Proceedings of the 36th International Conference on Neural Information Pro- cessing Systems, 30016–30030, (2022). https://doi.org/10.48550/arXiv.2203.15556 [24] Mart´ın-Gonz´alez, C. et al., Mean platelet volume and mortality in patients with alcohol use disorder, Digestive and Liver Disease 55, 1236–1241, (2023). [25] Annoni, G., Weiner, F. R., Colombo, M., Czaja, M. J. & Zern, M. A., Albumin and collagen gene regulation in alcohol- and virus-induced human liver disease, Gastroenterology 98, 197–202, (1990). 22 [26] Darwish, M. et al., Abuse potential with oral route of administration of a hy- drocodone extended-release tablet formulated with abuse-deterrence technology in nondependent, recreational opioid users, Pain Medicine 18, 61–77, (2017). [27] Miller, N. S. & Greenfeld, A., Patient characteristics and risks factors for devel- opment of dependence on hydrocodone and oxycodone, American Journal of Ther- apeutics 11, 26–32, (2004). [28] Schwartz, A. C. et al., Pain medication use among patients with posttraumatic stress disorder, Psychosomatics 47, 136–142, (2006). [29] Gulsun, M., Pinar, M. & Sabanci, U., Psychotic disorder induced by oxybutynin: Presentation of two cases, Clinical Drug Investigation 26, 603–606, (2006). [30] Welk, B., Etaby, K., McArthur, E. & Chou, Q., The risk of delirium and falls or fractures with the use of overactive bladder anticholinergic medications, Neurourol- ogy and Urodynamics 41, 348–356, (2022). [31] Iqbal, K. M., Luke, P. K. & Ingram, M. T., Psychosis resulting from trimethoprim- sulfamethoxazole treatment for preseptal cellulitis, Taiwan Journal of Ophthalmol- ogy 12, 223–226, (2022). [32] Stuhec, M., Trimethoprim-sulfamethoxazole-related hallucinations, General Hospi- tal Psychiatry 36, 230.e237–230.e238, (2014). [33] Scholz, S. M., Thalmann, N. F., M¨uller, D., Trippolini, M. A. & Wertli, M. M., Factors influencing pain medication and opioid use in patients with musculoskeletal injuries: a retrospective insurance claims database study, Scientific Reports 14, 1978, (2024). [34] Pastore, G. P., Goulart, D. R., Pastore, P. R., Prati, A. J. & de Moraes, M., Self-medication among myofascial pain patients: a preliminary study, The Open Dentistry Journal 12, 347, (2018). [35] Berlt, M., de Souza, K. B., Zhang, L.,',\n",
       "  'insurance claims database study, Scientific Reports 14, 1978, (2024). [34] Pastore, G. P., Goulart, D. R., Pastore, P. R., Prati, A. J. & de Moraes, M., Self-medication among myofascial pain patients: a preliminary study, The Open Dentistry Journal 12, 347, (2018). [35] Berlt, M., de Souza, K. B., Zhang, L., Bock, P. M. & Hort, M. A., Prevalence of self-medication for dental issues in the general population: a systematic review and meta-analysis, Discover Public Health 22, 1–32, (2025). [36] Becker, J. E. et al., The efficacy of lidocaine in disrupting cocaine cue-induced memory reconsolidation, Drug and Alcohol Dependence 212, 108062, (2020). [37] Kantak, K. M., Black, Y., Valencia, E., Green-Jordan, K. & Eichenbaum, H. B., Dissociable effects of lidocaine inactivation of the rostral and caudal basolateral amygdala on the maintenance and reinstatement of cocaine-seeking behavior in rats, Journal of Neuroscience 22, 1126–1136, (2002). 23 [38] Erlendson, M. J. et al., Palonosetron and hydroxyzine pre-treatment reduces the objective signs of experimentally-induced acute opioid withdrawal in humans: a double-blinded, randomized, placebo-controlled crossover study, The American Journal of Drug and Alcohol Abuse 43, 78–86, (2017). [39] Vengeliene, V., Heidbreder, C. A. & Spanagel, R., The effects of lamotrigine on alcohol seeking and relapse, Neuropharmacology 53, 951–957, (2007). [40] Brown, E. S., Sunderajan, P., Hu, L. T., Sowell, S. M. & Carmody, T. J., A randomized, double-blind, placebo-controlled, trial of lamotrigine therapy in bipolar disorder, depressed or mixed phase and cocaine dependence, Neuropsychopharma- cology 37, 2347–2354, (2012). [41] Skelly, M. J. & Weiner, J. L., Chronic treatment with prazosin or duloxetine lessens concurrent anxiety-like behavior and alcohol intake: evidence of disrupted nora- drenergic signaling in anxiety-related alcohol use, Brain and Behavior 4, 468–483, (2014). [42] Mohammadi, N. et al., Preventive effects of duloxetine against methamphetamine induced neurodegeneration and motor activity disorder in rat: Possible role of CREB/BDNF signaling pathway, International Journal of Preventive Medicine 10, 195, (2019). [43] Ghaderi, A. et al., Exploring the effects of vitamin D supplementation on cogni- tive functions and mental health status in subjects under methadone maintenance treatment, Journal of Addiction Medicine 14, 18–25, (2020). [44] Jalilian-Khave, L. et al., Potential roles for vitamin D in preventing and treating impulse control disorders, behavioral addictions, and substance use disorders: A scoping review, Addiction Neuroscience, 100190, (2024). [45] Duclot, F., Wu, L., Wilkinson, C. S., Kabbaj, M. & Knackstedt, L. A., Ceftriax- one alters the gut microbiome composition and reduces alcohol intake in male and female Sprague–Dawley rats, Alcohol 120, 169–178, (2024). [46] Stennett, B. A., Frankowski, J. C., Peris, J. & Knackstedt, L. A., Ceftriaxone reduces alcohol intake in outbred rats while upregulating xCT in the nucleus ac- cumbens core, Pharmacology Biochemistry and Behavior 159, 18–23, (2017). [47] Rao, P. & Sari, Y., Effectiveness of ceftriaxone treatment in preventing relapse- like drinking behavior following long-term ethanol dependence in P rats, Journal of Addiction Research & Therapy 5, 1000183, (2014). [48] Karatoprak, S., Uzun, N., Akıncı, M. A. & D¨onmez, Y. E., Neutrophil-lymphocyte and platelet-lymphocyte ratios among adolescents with substance use disorder: A preliminary study, Clinical Psychopharmacology and Neuroscience 19, 669, (2021). 24',\n",
       "  'behavior following long-term ethanol dependence in P rats, Journal of Addiction Research & Therapy 5, 1000183, (2014). [48] Karatoprak, S., Uzun, N., Akıncı, M. A. & D¨onmez, Y. E., Neutrophil-lymphocyte and platelet-lymphocyte ratios among adolescents with substance use disorder: A preliminary study, Clinical Psychopharmacology and Neuroscience 19, 669, (2021). 24 [49] Gasparyan, A. Y., Ayvazyan, L., Mukanova, U., Yessirkepov, M. & Kitas, G. D., The platelet-to-lymphocyte ratio as an inflammatory marker in rheumatic diseases, Annals of Laboratory Medicine 39, 345, (2019). [50] Sharma, T., Kumar, M., Rizkallah, A., Cappelluti, E. & Padmanabhan, P., Cocaine-induced thrombosis: review of predisposing factors, potential mechanisms, and clinical consequences with a striking case report, Cureus 11, (2019). [51] Alonzo, M. M., Lewis, T. V. & Miller, J. L., Disulfiram-like reaction with metron- idazole: an unsuspected culprit, The Journal of Pediatric Pharmacology and Ther- apeutics 24, 445–449, (2019). [52] Martinez-Diaz, G. J. & Hsia, R., Altered mental status from acyclovir, The Journal of Emergency Medicine 41, 55–58, (2011). [53] Ng, M.-H. et al., Macrocytosis among patients with heroin use disorder, Neuropsy- chiatric Disease and Treatment, 2293–2298, (2019). [54] Li, D. et al., The relationship between mean corpuscular hemoglobin concentration and mortality in hypertensive individuals: A population-based cohort study, PLOS ONE 19, e0301903, (2024). [55] Bohler, R. M. et al., The policy landscape for naloxone distribution in four states highly impacted by fatal opioid overdoses, Drug and Alcohol Dependence Reports 6, 100126, (2023). [56] Bazazi, A. R., Zaller, N. D., Fu, J. J. & Rich, J. D., Preventing opiate overdose deaths: examining objections to take-home naloxone, Journal of Health Care for the Poor and Underserved 21, 1108–1113, (2010). [57] Fischer, L. S. et al., Effectiveness of naloxone distribution in community settings to reduce opioid overdose deaths among people who use drugs: a systematic review and meta-analysis, BMC Public Health 25, 1135, (2025). [58] Petrovitch, D. et al., State program enables the identification of factors associated with naloxone awareness, self-efficacy, and use for overdose reversal: A cross- sectional, observational study in an urban emergency department population, Jour- nal of Substance Use and Addiction Treatment 167, 209506, (2024). [59] Alasmari, F., Rao, P. & Sari, Y., Effects of cefazolin and cefoperazone on glutamate transporter 1 isoforms and cystine/glutamate exchanger as well as alcohol drinking behavior in male alcohol-preferring rats, Brain Research 1634, 150–157, (2016). [60] Rao, P. et al., Effects of ampicillin, cefazolin and cefoperazone treatments on GLT-1 expressions in the mesocorticolimbic system and ethanol intake in alcohol- preferring rats, Neuroscience 295, 164–174, (2015). 25 [61] Weiland, A., Garcia, S. & Knackstedt, L. A., Ceftriaxone and cefazolin attenuate the cue-primed reinstatement of alcohol-seeking, Frontiers in Pharmacology 6, 44, (2015). [62] Mergenhagen, K. A., Wattengel, B. A., Skelly, M. K., Clark, C. M. & Russo, T. A., Fact versus fiction: a review of the evidence behind alcohol and antibiotic in- teractions, Antimicrobial Agents and Chemotherapy 64, 10.1128/aac.02167-02119, (2020). [63] Hakami, A. Y., Hammad, A. M. & Sari, Y., Effects of amoxicillin and augmentin on cystine-glutamate exchanger and glutamate transporter 1 isoforms as well as ethanol intake in alcohol-preferring rats, Frontiers in Neuroscience',\n",
       "  'of the evidence behind alcohol and antibiotic in- teractions, Antimicrobial Agents and Chemotherapy 64, 10.1128/aac.02167-02119, (2020). [63] Hakami, A. Y., Hammad, A. M. & Sari, Y., Effects of amoxicillin and augmentin on cystine-glutamate exchanger and glutamate transporter 1 isoforms as well as ethanol intake in alcohol-preferring rats, Frontiers in Neuroscience 10, 171, (2016). [64] Callans, L. S. et al., Clavulanic acid decreases cocaine cue reactivity in addiction- related brain areas, a randomized fMRI pilot study, Psychopharmacology Bulletin 54, 8, (2024). [65] Philogene-Khalid, H. L. et al., The GLT-1 enhancer clavulanic acid suppresses cocaine place preference behavior and reduces GCPII activity and protein levels in the rat nucleus accumbens, Drug and Alcohol Dependence 232, 109306, (2022). [66] Maser, J. et al., Clavulanic Acid-Mediated Increases in Anterior Cingulate Glu- tamate Levels are Associated With Decreased Cocaine Craving and Brain Net- work Functional Connectivity Changes, Current Therapeutic Research 101, 100751, (2024). [67] Schroeder, J. A. et al., Clavulanic acid reduces rewarding, hyperthermic and locomotor-sensitizing effects of morphine in rats: a new indication for an old drug?, Drug and Alcohol Dependence 142, 41–45, (2014). [68] Mather, J. F., Seip, R. L. & McKay, R. G., Impact of famotidine use on clinical outcomes of hospitalized patients with COVID-19, Official journal of the American College of Gastroenterology — ACG 115, 1617–1623, (2020). [69] Aouizerate, B. et al., Glucocorticoid negative feedback in methadone-maintained former heroin addicts with ongoing cocaine dependence: dose–response to dexam- ethasone suppression, Addiction Biology 11, (2006). [70] Capasso, A., Pinto, A., Sorrentino, L. & Cirino, G., Dexamethasone inhibition of acute opioid physical dependence in vitro is reverted by anti-lipocortin-1 and mimicked by anti-type II extracellular PLA2 antibodies, Life Sciences 61, PL127– PL134, (1997). [71] Smith, M. A. et al., Treatment with dextroamphetamine decreases the reacquisi- tion of cocaine self-administration: Consistency across social contexts, Drug and Alcohol Dependence 260, 111328, (2024). 26 [72] Chang, Z. et al., Stimulant ADHD medication and risk for substance abuse, Journal of Child Psychology and Psychiatry 55, 878–885, (2014). [73] Mariani, J. J. & Levin, F. R., Treatment strategies for co-occurring ADHD and substance use disorders, The American Journal on Addictions 16, 45–56, (2007). [74] Musco, S. et al., Characteristics of patients experiencing extrapyramidal symptoms or other movement disorders related to dopamine receptor blocking agent therapy, Journal of Clinical Psychopharmacology 39, 336–343, (2019). 27 Supplementary Information Cohort Stats Depressive Disorder PTSD COHORT SIZE 84,163 15,334 CASES%:CTRL% 21.3% : 78.7% 43.9% : 56.1% GENDER RATIO M%:F% 26.5% : 73.5% 35.4% : 64.6% AGE DISTRIBUTION 18-44 45-66 ≥66 39.3% 43.6% 18.1% 18-44 45-66 ≥66 32.2% 39.9% 27.9% WHITE 54% 62% AFAM 2% 14% AI/AN 2% 1% ASIAN 1% 2% MENA <1% <1% NHPI <1% <1% OTHER 25% 21% Supplementary Table 1. Cohort Characteristics Preceding Data Preprocess- ing. Gender reflects sex assigned at birth. Age distribution is based on participants’ age at the time of consent to data collection for the AoU program. The ”Other” category in the race row includes individuals who did not answer, selected multiple popula- tions, or indicated a race not listed. Abbreviations of Race: AFAM–African American; AI/AN–American Indian or Alaska',\n",
       "  'distribution is based on participants’ age at the time of consent to data collection for the AoU program. The ”Other” category in the race row includes individuals who did not answer, selected multiple popula- tions, or indicated a race not listed. Abbreviations of Race: AFAM–African American; AI/AN–American Indian or Alaska Native; MENA–Middle Eastern or North African; NHPI–Native Hawaiian or Other Pacific Islander. 28 0.0 0.5 1.0 0.0 0.2 0.4 0.6 0.6 0.8 1.0 ROC Curve Both Emb AUROC=0.961 0.0 0.5 1.0 0.0 0.2 0.4 0.6 0.6 0.8 1.0 Only RoPE AUROC=0.942 0.0 0.5 1.0 0.0 0.2 0.4 0.6 0.6 0.8 1.0 Only SPE AUROC=0.882 0.0 0.5 1.0 0.2 0.4 0.6 0.6 0.8 1.0 Precision-Recall Curve AUPRC=0.932 0.0 0.5 1.0 0.2 0.4 0.6 0.6 0.8 1.0 AUPRC=0.907 0.0 0.5 1.0 0.2 0.4 0.6 0.6 0.8 1.0 AUPRC=0.773 Supplementary Figure 1. Test ROC and PR Curve Comparison of Classi- fiers with Different Positional Encoding Configurations. For receiver operating characteristic (ROC) curves, the x-axis shows the false positive rate, and the y-axis shows the true positive rate. For precision-recall curves, the x-axis shows recall, and the y-axis shows precision. Curves and metrics are computed using the mean values of the corresponding rates across the cross-validation folds. 29 Model T PPV CV Sensitivity CV Specificity CV Dep BiPETE 0.2 91.42 (±2.06) 1.41 82.59 (±1.41) 1.19 97.97 (±0.56) 0.35 0.5 93.28 (±2.04) 81.36 (±1.71) 98.46 (±0.53) 0.8 94.61 (±1.93) 80.22 (±1.85) 98.80 (±0.48) BiGRU 0.2 67.19 (±5.81) 4.34 55.23 (±5.56) 6.25 92.69 (±2.60) 1.35 0.5 71.05 (±5.81) 51.23 (±5.42) 94.32 (±2.14) 0.8 74.74 (±5.59) 47.38 (±5.30) 95.64 (±1.73) LR 0.2 45.42 (±0.22) 25.70 74.86 (±0.92) 35.28 76.71 (±0.24) 10.76 0.5 72.61 (±0.31) 49.80 (±0.56) 95.14 (±0.09) 0.8 88.17 (±1.21) 30.33 (±0.68) 98.94 (±0.14) PTSD BiPETE 0.2 88.48 (±3.47) 1.08 85.50 (±2.31) 0.52 96.28 (±1.34) 0.37 0.5 89.40 (±3.36) 85.02 (±2.28) 96.62 (±1.34) 0.8 90.83 (±3.24) 84.41 (±2.41) 97.15 (±1.14) BiGRU 0.2 60.60 (±4.68) 5.16 52.09 (±5.75) 9.80 88.52 (±3.22) 2.28 0.5 63.89 (±4.26) 45.99 (±6.09) 91.19 (±2.58) 0.8 68.69 (±4.73) 40.98 (±6.37) 93.61 (±2.22) LR 0.2 49.45 (±1.35) 20.37 78.40 (±1.23) 25.12 73.72 (±1.57) 11.34 0.5 67.65 (±1.46) 58.45 (±1.14) 90.83 (±0.72) 0.8 82.61 (±1.77) 41.81 (±2.83) 97.12 (±0.34) Supplementary Table 2. Performance Metrics for ASUD Risk Prediction us- ing BiPETE and Baseline Models Across Decision Thresholds in Depression and PTSD Cohorts. PPV (Precision), Sensitivity (Recall) and Specificity (True Neg- ative Rate) are reported for comparisons with baseline models. Metrics are calculated at three decision thresholds: 0.2, 0.5 and 0.8. ± indicates standard deviation across five-fold cross-validation. Coefficient of Variation (CV), expressed as a percentage, is reported for each metric to show its variability across thresholds. 30 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 1 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3)',\n",
       "  'ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 1 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.040 0.050 0.060 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 2 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.040 0.045 0.050 0.055 0.060 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 3 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.040 0.050 0.060 0.070 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 4 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.045 0.050 0.055 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 5 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.035 0.040 0.045 0.050 0.055 0.060 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 6 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.045 0.050 0.055 0.060 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER',\n",
       "  '(1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.045 0.050 0.055 0.060 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 7 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.040 0.045 0.050 0.055 0.060 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 8 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.040 0.050 0.060 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) Head 9 ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ICD10 (1) ER (1) ICD10 (2) ICD10 (2) ICD10 (2) ER (2) ICD10 (3) ICD10 (3) ICD10 (3) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) ICD10 (4) 0.040 0.045 0.050 0.055 0.060 Supplementary Figure 2. Attention Heads of Final Layer in BiPETE. The attention is calculated using a representative EHR code sequence of length 20. EHR codes in axes labels are replaced with the vocabulary type to preserve patient anonymity. Attention maps show tokens tend to receive higher or lower scores in groups of tokens from the same visit. Different heads focus on distinct visit-level token groups, seeking different patterns in the sequence. Depressive Disorder Concept Set Concept Id PTSD Concept Set Concept Id Depressive disorder 440383 Acute post-trauma stress state 4100536 Chronic post-traumatic stress disorder 443414 Complex posttraumatic stress disorder 40484109 Posttraumatic stress disorder 436676 Supplementary Table 3. All of Us Concept Sets Used to Define MHD Cohorts. 31 ASUD Concept Set Concept Id ASUD Concept Set (Continued) Concept Id Abuse of antidepressant drug 440992 Drug dependence in remission 43530680 Accidental poisoning by hallucinogens 438946 Drug withdrawal 441260 Accidental poisoning by heroin 440307 Drug-induced amnestic syndrome 373172 Acute alcoholic intoxication in alcoholism 433735 Drug-induced delirium 373449 Acute alcoholic intoxication in remission, in alcoholism 432609 Drug-induced delusional disorder 443559 Acute alcoholic liver disease 201343 Drug-induced dementia 376095 Alcohol abuse 433753 Drug-induced paranoia or hallucinatory states 4101142 Alcohol dependence 435243 Drug-induced psychosis 434900 Alcohol withdrawal syndrome 375519 Drug-induced sleep disorder 435792 Alcohol-induced anxiety disorder 4146660 Episodic acute alcoholic intoxication in alcoholism',\n",
       "  'in remission, in alcoholism 432609 Drug-induced delusional disorder 443559 Acute alcoholic liver disease 201343 Drug-induced dementia 376095 Alcohol abuse 433753 Drug-induced paranoia or hallucinatory states 4101142 Alcohol dependence 435243 Drug-induced psychosis 434900 Alcohol withdrawal syndrome 375519 Drug-induced sleep disorder 435792 Alcohol-induced anxiety disorder 4146660 Episodic acute alcoholic intoxication in alcoholism 441261 Alcohol-induced mood disorder 4205002 Gastric hemorrhage due to alcoholic gastritis 45757783 Alcohol-induced psychotic disorder with delusions 442582 Hallucinogen abuse 437245 Alcohol-induced sleep disorder 375794 Hallucinogen dependence 433180 Alcoholic fatty liver 193256 Hallucinogen dependence in remission 434921 Alcoholic gastritis 195300 Hallucinosis caused by drug 440987 Amphetamine abuse 432878 Hypnotic or anxiolytic abuse 439554 Amphetamine and amphetamine derivative drug dependence 3654785 Inhalant abuse 4290538 Amphetamine dependence 437533 Inhalant dependence 4176120 Amphetamine or psychostimulant dependence in remission 432884 Inhalant-induced mood disorder 4232492 Amphetamine or psychostimulant dependence, continuous 434916 Inhalant-induced organic mental disorder 4264889 Amphetamine or psychostimulant dependence, episodic 441262 Mood disorder caused by drug 436079 Cannabis abuse 434327 Nicotine dependence 4209423 Cannabis dependence 440387 Nicotine dependence in remission 3654548 Cannabis dependence in remission 440996 Nondependent antidepressant type drug abuse in remission 439313 Cannabis intoxication delirium 4220197 Nondependent cocaine abuse in remission 436098 Cannabis-induced anxiety disorder 4221077 Nondependent cocaine abuse, continuous 439796 Cannabis-induced psychotic disorder with hallucinations 4097389 Nondependent hallucinogen abuse 4150794 Cocaine abuse 432303 Nondependent hallucinogen abuse in remission 441272 Cocaine dependence 436389 Nondependent mixed drug abuse 439312 Cocaine dependence in remission 432302 Nondependent mixed drug abuse in remission 4103426 Cocaine-induced anxiety disorder 4198826 Nondependent opioid abuse 4099935 Cocaine-induced mood disorder 4012869 Nondependent opioid abuse in remission 436088 Cocaine-induced psychotic disorder with hallucinations 4272033 Opioid abuse 438130 Combined drug dependence, excluding opioid, in remission 433458 Opioid dependence 438120 Combined drug dependence, excluding opioids 436370 Opioid dependence in remission 432301 Combined opioid with other drug dependence 4099809 Poisoning by heroin 433919 Combined opioid with other drug dependence in remission 4100520 Poisoning by methadone 440919 Combined opioid with other drug dependence, continuous 4102817 Poisoning by opium alkaloid 439223 Combined opioid with other drug dependence, episodic 4103413 Psychoactive substance abuse 4239381 Continuous acute alcoholic intoxication in alcoholism 437257 Psychoactive substance dependence 4080762 Delirium due to sedative withdrawal 4262566 Psychoactive substance use disorder 4004672 Dementia associated with alcoholism 378726 Psychoactive substance-induced organic hallucinosis 4155336 Dilated cardiomyopathy secondary to alcohol 318773 Psychotic disorder caused by cocaine 37110437 Disorder caused by alcohol 36714559 Stimulant abuse 40479573 Drug dependence 440069 Therapeutic drug dependence 4319165 Drug dependence during pregnancy - baby delivered 442915 Drug dependence in mother complicating pregnancy, childbirth AND/OR puerperium 440787 Supplementary Table 4. All of Us Concept Set Used to Define ASUD. 32'],\n",
       " ['Learning Fourier shapes to probe the geometric world of deep neural networks Jian Wang1, Yixing Yong1, Haixia Bi1, Lijun He1, Fan Li1* 1Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi’an Jiaotong University, Xi’an, 710049, China. *Corresponding author(s). E-mail(s): lifan@mail.xjtu.edu.cn; Contributing authors: wj851329121@stu.xjtu.edu.cn; yongyx@stu.xjtu.edu.cn; haixia.bi@mail.xjtu.edu.cn; lijunhe@mail.xjtu.edu.cn; Abstract While both shape and texture are fundamental to visual recognition, research on deep neural networks (DNNs) has predominantly focused on the latter, leaving their geometric understanding poorly probed. Here, we show: first, that opti- mized shapes can act as potent semantic carriers, generating high-confidence classifications from inputs defined purely by their geometry; second, that they are high-fidelity interpretability tools that precisely isolate a model’s salient regions; and third, that they constitute a new, generalizable adversarial paradigm capa- ble of deceiving downstream visual tasks. This is achieved through an end-to-end differentiable framework that unifies a powerful Fourier series to parameterize arbitrary shapes, a winding number-based mapping to translate them into the pixel grid required by DNNs, and signal energy constraints that enhance opti- mization efficiency while ensuring physically plausible shapes. Our work provides a versatile framework for probing the geometric world of DNNs and opens new frontiers for challenging and understanding machine perception. Keywords: Visual understanding, Adversarial attack, Learnable Fourier shapes 1 arXiv:2511.04970v1 [cs.CV] 7 Nov 2025 Fig. 1 Conceptual overview of adversarial shape learning. a, Human and machine visual sys- tems rely on consistent shape and appearance attributes for robust object recognition. When these attributes are mismatched, such as an apple’s shape with a banana’s texture, perceptual conflict arises, illustrating that shape is an independently salient attribute. b, Prior work on adversarial attacks primarily targets the appearance domain. This involves either adding subtle, global pixel perturba- tions to misclassify an image (e.g., a panda recognized as a gibbon) or deploying localized adversarial patches to cause detection failures. These methods operate on pixel values without explicitly manipu- lating underlying geometry. c, Our framework enables end-to-end differentiable optimization of object shapes for adversarial machine learning. It addresses three key challenges: (1) Shape parameteriza- tion: Arbitrary closed contours are represented by a compact set of Fourier series coefficients. (2) Differentiable mapping: A module based on the winding number theorem translates these coefficients into a 2D grid image, creating a differentiable bridge to DNNs. (3) Effective optimization: Regular- ization, inspired by signal energy theory, guides the learning process to ensure physically plausible shapes by constraining high-frequency components. This integrated pipeline allows for the discovery and optimization of effective adversarial shapes. Images in b are from ref. [20] and [23] 1 Introduction The remarkable ability of the human visual system [1, 11] to recognize objects relies on a sophisticated synthesis of two fundamental attributes: geometric shape and surface texture. Shape provides the structural scaffold of an object, defining its boundaries and identity, while texture and colour furnish the finer details of its appearance. A significant distortion in either of these attributes can disrupt perception (Fig. 1a), sug- gesting that both are fundamental to robust recognition. A well-trained DNN should ideally mirror this',\n",
       "  'structural scaffold of an object, defining its boundaries and identity, while texture and colour furnish the finer details of its appearance. A significant distortion in either of these attributes can disrupt perception (Fig. 1a), sug- gesting that both are fundamental to robust recognition. A well-trained DNN should ideally mirror this biological duality [2], leveraging both shape and texture to make robust inferences. However, the vast body of research exploring model vulnerabili- ties through adversarial attacks [20, 21] has overwhelmingly focused on manipulating the texture domain [12, 15, 17]. By searching for subtle perturbations in the high- dimensional pixel space, these methods have revealed profound weaknesses in modern DNNs, yet they have largely overlooked the equally fundamental axis of shape. This intense focus on pixel-level manipulations (Fig. 1b), while fruitful for reveal- ing model weaknesses, carries inherent limitations. Adversarial perturbations [8–10], typically composed of high-frequency signals, are largely confined to the digital domain and lack direct physical-world applicability. While physically realizable meth- ods like adversarial patches [13, 14, 16, 22] have been developed, they still operate by manipulating texture within a predefined boundary rather than the object’s intrin- sic geometry. Furthermore, the explanatory power [3, 4] of such pixel-based methods is often limited, as the resulting patterns lack clear semantic meaning for human observers [5–7]. This raises a critical question: can we move beyond the pixel grid to 2 engage directly with a model’s understanding of geometry? Exploring the domain of shape offers a path to creating more physically robust and interpretable methods for analyzing and challenging machine perception. Directly optimizing an object’s shape, however, presents a formidable technical challenge. A shape is a continuous, geometric entity typically described by abstract parameters, unlike the discrete grid of pixels a DNN processes. Bridging this gap for gradient-based optimization requires two key components: a shape representation that is expressive enough to describe a diverse family of forms, and a differentiable mapping to translate those parameters into a pixel array. An effective representation must therefore combine expressive power with optimization efficiency, providing a rich search space in which to discover effective adversarial geometries. Existing approaches [28–34] have struggled to satisfy these requirements simultaneously. Methods that model shapes on a discrete grid [29–32], for instance, are differentiable but require complex, hand-crafted aggregation constraints to maintain coherence, which restricts the search space and scales poorly with resolution. Conversely, approaches using continuous spline-based representations [33, 34] often lack a differentiable mapping, forcing a reliance on inefficient black-box optimization that yields poor scalability and performance. Here, we propose a complete framework for learning adversarial shapes through a parametric Fourier series representation. Inspired by how complex signals can be decomposed into a sum of simple sinusoids, we model any arbitrary 2D closed contour using a compact set of Fourier coefficients. This representation allows us to generate a vast and intricate space of shapes by controlling the amplitude and phase of different frequency components. To bridge the gap between these abstract parameters and the pixel domain, we employ a differentiable mapping based on the winding number theorem from complex analysis, which analytically',\n",
       "  'representation allows us to generate a vast and intricate space of shapes by controlling the amplitude and phase of different frequency components. To bridge the gap between these abstract parameters and the pixel domain, we employ a differentiable mapping based on the winding number theorem from complex analysis, which analytically draws the shape defined by the Fourier coefficients onto a 2D grid, generating an image where each pixel’s value is a function of its location relative to the contour. The entire pipeline, from Fourier coefficients to a rasterized shape image, is fully end-to-end differentiable. Furthermore, by introducing regularization constraints based on signal energy principles, we guide the optimization towards generating shapes that are both physically plausible and adversarially potent. This framework allows us to explore the role of shape in machine perception with unprecedented control. Our experiments reveal three key findings. First, we demon- strate that shape alone is a powerful carrier of semantic information, capable of generating high-confidence classifications from a DNN even in the complete absence of texture; moreover, the strength of this semantic information gracefully scales with the shape’s complexity via the number of Fourier terms. Second, we repurpose our method as a high-fidelity interpretability tool. For a given input image, by opti- mizing a shape mask to be as small as possible while preserving correct classification, we can isolate a model’s region of interest with sharper, more interpretable bound- aries than existing methods like Grad-CAM [18]. Conversely, we show that occluding a small but critical region, while retaining the vast majority of the original image, is sufficient to guarantee misclassification. Finally, we establish adversarial shapes as a generalizable attack paradigm, analogous to colour-based adversarial patches 3 Fig. 2 Overview of the three experimental frameworks enabled by the differentiable shape learning pipeline. a, Experiment 1: Class-specific shape generation. A set of Fourier coef- ficients, c = {ck}K k=−K, is converted via the differentiable mapping into a gray-scale shape image. This image is fed directly into a classifier. The coefficients are optimized using gradient descent to maximize the classification confidence for a chosen target class, demonstrating the semantic repre- sentation capability of shape alone. b, Experiment 2: Shape as an interpretability tool. The Fourier coefficients are mapped to a gray-scale image, which is used as a mask on a given natural image. The masked input is fed into a classifier. The coefficients are optimized using two symmetric objectives: (1) to maximize the confidence for the true class while simultaneously minimizing the shape’s area, thereby isolating the model’s minimal salient region; or (2) to minimize the true class confidence while maximizing the shape’s area, identifying the minimal critical region whose occlusion causes misclas- sification. c, Experiment 3: Shape as a generalizable adversarial paradigm. The Fourier coefficients are mapped to a gray-scale image, which is then rendered as an occlusion patch onto a target (e.g., a person) in a natural image. The rendered input is fed into an object detector. The coefficients are optimized to minimize the detection confidence scores for the occluded target, causing the model to fail the detection task.',\n",
       "  'image, which is then rendered as an occlusion patch onto a target (e.g., a person) in a natural image. The rendered input is fed into an object detector. The coefficients are optimized to minimize the detection confidence scores for the occluded target, causing the model to fail the detection task. with fixed shapes. We show that by optimizing the shape of a patch while keeping its colour fixed, we can effectively cause a person covered by the shape to evade the state-of-the-art object detectors, demonstrating the method’s applicability to diverse downstream vision tasks. 2 Results Our experiments demonstrate that the proposed adversarial shape learning frame- work is a powerful and versatile tool. We systematically show that our method can: (1) generate shapes from scratch that carry sufficient semantic information to be clas- sified as any target category by state-of-the-art models [35–40]; (2) serve as a novel, high-fidelity visualization tool for interpreting a network’s decision-making process by identifying salient object regions; and (3) function as a generalizable adversarial paradigm, analogous to adversarial patches, that can be deployed in diverse down- stream tasks such as object detection. The workflow for these experiments is illustrated in Fig. 2. 4 Fig. 3 Adversarial shapes generated from scratch can embody class-specific semantics. a, Qualitative examples of generated shapes by the ResNet-50 model. Left, a shape generated to be classified as tench using a complexity of K = 10. Right, a more detailed shape generated for the golden retriever class using K = 25. The top-5 classification predictions and their confidence scores are listed for each shape, demonstrating high confidence for the target class and semantically logical subsequent predictions. b, The effect of shape complexity on classification confidence for the ice bear class. As K increases from 5 to 25, the shape incorporates more detail, and the target confidence monotonically increases from 1.14% to 98.86%. c, Generalization of the learnable Fourier shape across diverse model architectures and all ImageNet classes. The plot displays the top-1 classification success rate as a function of shape complexity. Each curve represents a different model architecture. The success rate for each point is the average across all 1,000 ImageNet classes. For all models tested, the success rate consistently exceeds 90% as K increases beyond 20. 2.1 Fourier shapes can embody class-specific semantic information To investigate whether shape, in complete isolation from colour and texture, can func- tion as an effective semantic carrier for DNNs, we designed an experiment to generate class-specific shapes from scratch. We employed a targeted optimization process where the Fourier coefficients defining a shape were iteratively updated to maximize the classification confidence score for a designated ImageNet [19] class on a pre-trained ResNet-50 model [35]. The input to the network was the grayscale image generated by our differentiable mapping pipeline, containing only the optimized shape against a black background. This setup allows us to directly probe the geometric priors learned by the network. Our findings reveal that this process can successfully generate highly specific and recognizable shapes that effectively trigger the desired classification (Fig. 3). For instance, when targeting the',\n",
       "  'containing only the optimized shape against a black background. This setup allows us to directly probe the geometric priors learned by the network. Our findings reveal that this process can successfully generate highly specific and recognizable shapes that effectively trigger the desired classification (Fig. 3). For instance, when targeting the tench class, our method generates a shape that is not only classified with the highest confidence as a tench but is also intuitively recog- nizable to a human observer as the silhouette of a fish (Fig. 3a, left). This result 5 provides strong initial evidence that the network’s learned features for this class are intrinsically linked to a distinct geometric form. Notably, the network’s subsequent predictions (top-5) correspond to other visually similar aquatic creatures, such as coho and hammerhead, suggesting that its confusion is semantically logical and rooted in shared shape characteristics, rather than being an arbitrary failure mode. The capability of our method extends to more challenging, fine-grained categories where shape cues are subtler. When tasked with generating a golden retriever, a cat- egory distinguished from other dog breeds by features that are often textural, the optimization required a higher shape complexity (K = 25). The resulting shape, while more abstract to the human eye, was classified as a golden retriever with an excep- tionally high confidence of 81.52% (Fig. 3a, right). Close inspection reveals that the shape evolved to capture characteristic local details, such as the contours of the ears and paws. Again, the model’s top-5 predictions were all other visually similar retriever and spaniel breeds, reinforcing the notion that our method uncovers a hierarchy of geometric features learned by the model. A key advantage of our Fourier representation is its parametric efficiency. A stan- dard 224×224 pixel-space attack requires optimizing over 50,000 parameters, whereas our shape, even with a high complexity of K = 25, is defined by only 2K + 1 = 51 learnable parameters. This compactness does not sacrifice effectiveness. We found a direct and graceful correlation between the shape’s complexity K and the adversarial success. As K increases, the shape can incorporate finer details, leading to a mono- tonic increase in the target class confidence score (Fig. 3b). This demonstrates that the model’s confidence is tied to the fidelity of the geometric details present in the shape. Finally, to confirm that this phenomenon is not specific to one model or a few object classes, we conducted a large-scale quantitative analysis. We system- atically generated shapes for all 1,000 ImageNet classes across a diverse suite of seven leading architectures, including convolutional networks (ResNet-50, ResNet-101, DenseNet121 [37], VGG16 [36], MobileNetv2 [38]) and vision transformers (ViT-B- 16 [39], SwinTransformer-B [40]). The results show a universally consistent trend: for all tested models, the success rate of generating a shape correctly classified as the target class increases monotonically with K. As the number of Fourier terms increases beyond 20, most architectures achieve high success rates (above 96%), while the Swin-Transformer also achieves a success rate of approximately 90% (Fig. 3c). This comprehensively demonstrates that our shape-learning framework is a general and',\n",
       "  'classified as the target class increases monotonically with K. As the number of Fourier terms increases beyond 20, most architectures achieve high success rates (above 96%), while the Swin-Transformer also achieves a success rate of approximately 90% (Fig. 3c). This comprehensively demonstrates that our shape-learning framework is a general and robust method for instantiating nearly any object category conceivable by modern deep learning models, using geometry as the sole information carrier. 2.2 Learnable shapes serve as a high-fidelity interpretability tool Building on the discovery that shapes can intrinsically carry class-specific semantics, we investigated whether our learnable shapes could be repurposed as a high-fidelity tool to interpret the inner workings of DNNs. A central challenge in AI interpretabil- ity is to precisely identify the minimal visual evidence a model uses to make a specific 6 classification. Existing methods, such as gradient-based attribution maps, often pro- duce coarse, diffuse heatmaps that highlight general areas of importance but lack precise boundaries. We therefore sought to determine if our framework could isolate these critical regions with geometric precision. 7 Fig. 4 Learnable Fourier shapes as a high-fidelity tool for model interpretability. a, The optimization process for identifying the salient region in an image of a racket. As the number of iterations increases from 50 to 1,200, the shape mask progressively contracts to focus on the racket head, while the retained area decreases from 54.03% to 3.95%. Throughout this process, the masked input image is consistently classified correctly as racket with high confidence. b, Comparison of our learnable shape method with Grad-CAM for visualizing the salient regions for four different images. For each example (b1-b4), our method isolates a small, precise region with sharp boundaries (for example, 4.28% for tiger shark and 1.68% for candle) that is sufficient for correct top-1 classification. In contrast, the Grad-CAM heatmaps, generated from the final convolutional layer of a ResNet-50, highlight a more diffuse area. c, The effect of shape complexity (number of Fourier terms, K) on localizing the salient region for an image of a tench. Increasing K from 1 to 8 allows the shape to identify a progressively smaller critical area, reducing the retained area from 18.78% to 4.21%. At higher complexities (K = 6 and K = 8), high-frequency tails can appear in non-salient regions, as indicated by the red dashed circles. d, Results from the symmetric experiment, where the objective is to induce misclassification by occluding the smallest possible critical region. For the great white shark (d1), masking only the teeth and dorsal fin (occluding 26.17% of the image) results in a high- confidence misclassification to hammerhead. For the boxer (d2), masking the face (occluding 11.49% of the image) leads to a misclassification as basenji. To achieve this, we designed a dual-objective optimization experiment. For a given input image from the ImageNet dataset, we optimize a Fourier shape mask that is element-wise multiplied with the image. The resulting masked image, which preserves only the visual information within the shape’s contour, is then fed into a ResNet- 50 model. The optimization is guided by two competing objectives: maximizing',\n",
       "  'input image from the ImageNet dataset, we optimize a Fourier shape mask that is element-wise multiplied with the image. The resulting masked image, which preserves only the visual information within the shape’s contour, is then fed into a ResNet- 50 model. The optimization is guided by two competing objectives: maximizing the classification confidence for the image’s true label while simultaneously minimizing the area of the shape mask. This process forces the shape to iteratively contract and converge upon the most compact and informative region essential for the model’s decision. Furthermore, we designed a symmetric experiment to validate these findings by inverting the objectives. Specifically, we minimized the classification confidence while maximizing the mask area to identify the smallest critical region that, when occluded, guarantees misclassification. Our findings demonstrate that the learnable shape dynamically and efficiently converges to the semantically salient regions of an object. The optimization process reveals a clear, iterative focusing effect, where an initially large and simple shape gradually refines its contour to tightly envelop the key features of the target object, such as the racket head in a tennis image (Fig. 4a). The final optimized shape often constitutes a remarkably small fraction of the original image area; for instance, the model can correctly classify the racket image with high confidence even when only 3.95% of the original pixels are retained. When compared with established visualization tools like Grad-CAM [18], our method offers substantially improved precision and interpretability (Fig. 4b). Grad- CAM, which relies on the gradients of the final convolutional feature maps, inherently produces low-resolution, diffuse heatmaps that are spatially coarse. In contrast, our shape-based approach generates masks with sharp, unambiguous boundaries derived directly from the winding number calculation. This allows for a much clearer delin- eation of the model’s focus. For a candle image, our method identified that a mere 1.68% of the image, precisely covering the flame and wick, was sufficient for cor- rect classification, providing a far more concentrated and interpretable result than 8 the corresponding heatmap (Fig. 4b, bottom right). This heightened precision stems from our explicit optimization objective to minimize area, a constraint not present in attribution-based methods. We also analyzed how the shape’s complexity K influences this localization task (Fig. 4c). We observed that shapes with higher complexity (larger K) are capable of identifying smaller and more intricate salient regions. This is because the increased flexibility allows the contour to carve out non-essential areas with greater precision. However, this flexibility comes with a trade-off. At higher K values (e.g., K = 8), the optimized shape can develop self-intersections or high-frequency tails that extend into non-salient background areas, albeit covering a negligible area. These artifacts may arise partly from the optimization dynamics, where all coefficients are updated jointly. More fundamentally, these high-frequency tails are a manifestation of the shape’s own strong semantic information, as demonstrated in our first experiment. This suggests that the shape’s own semantics could potentially interfere with the goal of purely isolating the image’s salient features. We therefore recommend using a moderate complexity (e.g., K = 6), as it strikes an',\n",
       "  'a manifestation of the shape’s own strong semantic information, as demonstrated in our first experiment. This suggests that the shape’s own semantics could potentially interfere with the goal of purely isolating the image’s salient features. We therefore recommend using a moderate complexity (e.g., K = 6), as it strikes an effective balance, enabling fine- grained localization while mitigating the introduction of confounding shape-based priors. Finally, the symmetric experiment, where we aimed to retain as much of the original image as possible while inducing misclassification, offers compelling insights into the model’s failure modes (Fig. 4d). For an image of a great white shark, the algorithm learned to precisely mask out the teeth and dorsal fin. Despite preserving 73.83% of the image, the model’s classification switched to hammerhead with high confidence. Similarly, occluding the face of a boxer dog was sufficient to cause a mis- classification as a basenji. These results starkly reveal the model’s heavy reliance on a few local, discriminative features. Unlike human perception, which often relies on a holistic understanding of the object, the model’s decision can be completely over- turned by the absence of these key features, revealing a potential vulnerability in their decision-making process. 2.3 Adversarial shapes as a generalizable attack paradigm for downstream tasks We next investigated whether the Fourier shape could be generalized to function as a new adversarial paradigm for complex, downstream vision tasks [41–44]. This positions our method as a conceptual analogue to colour-based adversarial patches [23–26], which have proven effective in the physical world [13, 14, 27]. While those methods optimize the texture within a fixed, simple shape (such as a square), we invert this concept: we optimize the shape itself while keeping its internal colour fixed (e.g., solid white), thereby isolating the adversarial power of pure geometry. To test this paradigm, we targeted the object detection, a basic task of real- world computer vision systems. We designed an experiment to make a target object, specifically a person, invisible to a pre-trained YOLOv3 detector [42]. For a given image containing a person, our Fourier shape was rendered as a solid white patch onto the target. The patch was scaled relative to the person’s bounding box (e.g., 0.6× the height and width) and centred on the object. The resulting image was then fed to the 9 Fig. 5 Adversarial shapes as a generalizable attack paradigm for object detection. a, Qualitative results of the shape attack against the YOLOv3 detector. In each pair, the left image shows the benign detection (person detected, green box) and the right image shows the attacked version. The optimized white Fourier shape (K = 10) causes the detector to fail, and the person is no longer detected (detection confidence ≤0.5). b, Comparison of the optimized Fourier shape against simple geometric occlusions of similar area. While simple shapes (rectangle, ellipse, triangle, star) have a negligible effect on detection confidence (e.g., 93.2% - 94.9%), the optimized shape reduces the confidence to 15.9%, successfully evading detection. c, Quantitative ablation on the effect of shape complexity across a set of 140 COCO images. The Attack',\n",
       "  'of similar area. While simple shapes (rectangle, ellipse, triangle, star) have a negligible effect on detection confidence (e.g., 93.2% - 94.9%), the optimized shape reduces the confidence to 15.9%, successfully evading detection. c, Quantitative ablation on the effect of shape complexity across a set of 140 COCO images. The Attack Success Rate (ASR) vs. Confidence plot (left) shows that ASR (higher is better) increases with higher K. The Precision-Recall (PR) curves (right) show that the Average Precision (AP, lower is better) for the person class decreases as K increases. d, Generalization of the shape attack (K = 10) across diverse detector architectures (YOLOv3, RetinaNet, and FCOS). The ASR-Confidence plot (left) shows the attack is effective against all models. The PR curves (right) show a significant performance degradation for all attacked models (solid lines) compared to their benign baselines (dashed lines). 10 detector. Our optimization objective was to minimize the objectness confidence scores for all detection proposals associated with the target, thereby causing the detector to miss the person entirely (a false negative). The qualitative results are striking (Fig. 5a). In benign images, the detector robustly identifies the person class with high confidence. After the optimized Fourier shape is applied, the person becomes invisible to the detector; the associated bound- ing box and confidence score disappear, even though the person remains partially visible to a human observer. This suggests the shape’s optimized geometry introduces adversarial information that effectively overrides the detector’s learned features for the person category. A critical question is whether this effect is due to the specific optimized geometry or simply to the act of occlusion. To answer this, we conducted a control experiment comparing our optimized shape to simple, non-optimized geometric shapes (e.g., a rectangle, ellipse, or star) of similar area (Fig. 5b). The simple shapes had a negligible impact on the detector’s confidence, which remained high (e.g., 93.2% to 94.9%). In contrast, our optimized Fourier shape decimated the confidence score to 15.9%, well below the typical detection threshold. This finding is crucial, as it demonstrates that the attack’s potency stems not from mere occlusion, but from the specific, learned geometric contours of the shape itself. We further quantified this effect by evaluating the attack on a set of 140 images from the COCO dataset [41], analyzing performance as a function of shape complexity. We used two metrics: the Attack Success Rate (ASR) at different confidence thresh- olds, and the degradation in the model’s Precision-Recall (PR) curve. A higher ASR curve indicates a more potent attack, as does a lower, more suppressed PR curve. The results clearly show that attack efficacy scales with shape complexity (Fig. 5c). As K increases, the shape becomes more intricate, the ASR curves shift upwards, and the PR curves are pressed further downwards, indicating a greater drop in the model’s Average Precision (AP). Finally, to demonstrate the generalizability of this paradigm, we deployed the attack against three representative detectors: YOLOv3, RetinaNet [43], and FCOS [44]. The shape attack proved universally effective, significantly degrading the perfor- mance of all three models (Fig. 5d). The consistent drop',\n",
       "  'greater drop in the model’s Average Precision (AP). Finally, to demonstrate the generalizability of this paradigm, we deployed the attack against three representative detectors: YOLOv3, RetinaNet [43], and FCOS [44]. The shape attack proved universally effective, significantly degrading the perfor- mance of all three models (Fig. 5d). The consistent drop in the PR curves (solid lines) compared to their benign baselines (dashed lines) confirms that adversarial shapes are a robust attack vector, capable of exploiting vulnerabilities in both anchor-based (YOLOv3, RetinaNet) and anchor-free (FCOS) detectors. These results position adversarial shapes as a viable new attack modality with significant implications for real-world robustness. Unlike texture-based patches, which are highly sensitive to colour distortion from lighting and camera sensors, a shape- based attack encodes its adversarial information in its geometry, an attribute that is more resilient to such physical-world variations. While we used a simple white patch for these experiments, this framework opens the door to hybrid attacks that could optimize both shape and colour simultaneously. 11 3 Discussion In this work, we have established a paradigm for understanding and interacting with DNNs through the direct, holistic optimization of an object’s geometry. We demonstrated the profound potential of this learnable shape framework through three distinct lines of inquiry. First, we showed that shape, in complete isolation from colour and texture, can act as a potent carrier of semantic information, capable of eliciting high-confidence, class-specific responses from well-trained models. Second, we repur- posed this framework as a high-fidelity interpretability tool, capable of isolating a model’s critical regions of interest with a precision and clarity that surpasses exist- ing attribution methods. Finally, our findings establish shape as a new, generalizable modality for adversarial attacks, conceptually analogous to adversarial patches but operating in a fundamentally different domain, with broad applicability to diverse visual tasks. The success of our approach hinges on a fully end-to-end differentiable pipeline. The use of a Fourier series provides a compact, yet powerful, parameteri- zation for arbitrary closed contours. This abstract representation is then analytically bridged to the pixel space required by DNNs via a differentiable mapping derived from the winding number theorem. The framework is further guided by regulariza- tion constraints, inspired by signal processing principles, that ensure the generation of plausible, naturalistic shapes. Together, these components create a novel and effi- cient methodology for exploring the vast space of geometric forms in the context of machine perception. The implications of this work extend beyond adversarial analysis, opening up several promising avenues for future research. First, our framework enables a more tar- geted data augmentation strategy. By precisely masking a model’s most relied-upon features, we can compel networks to learn from a more holistic global context rather than exploiting local shortcuts, potentially leading to substantial improvements in gen- eralization. Second, this work invites exploration into more sophisticated optimization strategies. Instead of a joint optimization of all Fourier coefficients, a staged approach could be employed. This might involve first locating a region of interest with low- frequency terms and then refining the details with high-frequency ones, perhaps using frequency-specific learning rates, which could',\n",
       "  'invites exploration into more sophisticated optimization strategies. Instead of a joint optimization of all Fourier coefficients, a staged approach could be employed. This might involve first locating a region of interest with low- frequency terms and then refining the details with high-frequency ones, perhaps using frequency-specific learning rates, which could mitigate artifacts and enhance efficiency. Finally, the principles established here can be extended to three-dimensional surfaces. By parameterizing a 3D mesh with a spherical Fourier series, one could directly opti- mize a 3D object to attack models operating on point clouds [45] or volumetric data [46]. This top-down generative process would bypass many of the complex smooth- ness constraints required by traditional mesh manipulation methods [47], offering a powerful new direction for investigating and challenging the frontiers of 3D machine perception. 4 Methods 4.1 Overview Our method introduces a novel framework for generating adversarial shapes by directly optimizing the geometric form of an object, fundamentally shifting the paradigm 12 of adversarial attacks from the colour domain to the shape domain. Conventional adversarial attacks manipulate the input by adding subtle, often imperceptible, pixel- level perturbations to an existing image. Similarly, adversarial patches introduce a localized but fixed-shape pattern onto an object. While effective, these methods do not alter the object’s intrinsic geometry. Our approach, in contrast, generates a holistic and physically realizable shape from scratch, defined by a continuous boundary, that is optimized to deceive a DNN. The core innovation lies in creating a fully differentiable pipeline that connects a parametric representation of a closed shape to the output of a target DNN. This is achieved through three key stages. First, we model an arbitrary 2D closed shape using a Fourier series representation, which provides a compact and powerful parameteriza- tion capable of describing a vast family of complex geometries. Second, we introduce a differentiable mapping module based on the winding number theorem from com- plex analysis. This module analytically transforms the Fourier coefficients into a 2D rasterized image of the shape, where each pixel’s value is determined by its position relative to the shape’s boundary. This step is crucial as it builds a differentiable bridge between the abstract shape parameters and the pixel space that DNNs operate on. Finally, with this end-to-end differentiable pipeline, we can feed the generated shape image into the target DNN and compute the adversarial loss. The gradient of this loss is then backpropagated all the way to the Fourier coefficients, allowing us to iter- atively update and grow a shape that maximally fools the network. To ensure the generated shapes are both physically plausible and effective for attack, we introduce a set of regularization constraints based on signal processing principles, which govern the energy distribution across different frequency components of the shape. 4.2 Shape Modeling To mathematically represent any arbitrary 2D closed contour in a continuous and differentiable manner, we employed a Fourier series representation. This powerful technique can approximate any periodic function as an infinite sum of sine and cosine functions. By treating the x and y coordinates of a shape’s boundary as functions of a',\n",
       "  'represent any arbitrary 2D closed contour in a continuous and differentiable manner, we employed a Fourier series representation. This powerful technique can approximate any periodic function as an infinite sum of sine and cosine functions. By treating the x and y coordinates of a shape’s boundary as functions of a parameter t that traverses the contour, we can define the shape in the complex plane. A shape F(t) is thus represented as: F(t) = f(t) + i · g(t) = K X k=−K ckeikt for t ∈[0, 2π] (1) where f(t) and g(t) are the Cartesian coordinates of the boundary, i is the imaginary unit, and t is the parameter that sweeps along the contour. The shape is defined by a set of complex Fourier coefficients, ck = ak + ibk, which are the parameters we aim to optimize. The integer K determines the complexity, or degrees of freedom, of the shape. Intuitively, each coefficient ck controls a specific geometric characteristic of the shape: 13 • DC Offset (c0): This zero-frequency term is a complex number representing the shape’s centre of mass. • Fundamental Frequencies (c1, c−1): These terms, corresponding to k = 1 and k = −1, define the fundamental elliptical or circular form of the shape. They dictate its overall scale, elongation, and orientation. A simple circle, for instance, can be defined by setting c1 to a real number and all other coefficients to zero. • Higher Harmonics (ck, |k| ≥2): These coefficients add progressively finer details and complexity to the base ellipse. For instance, c2 and c−2 might introduce a twofold symmetry (like a peanut shape), while c3 and c−3 could add a threefold symmetry (like a cloverleaf). By combining these harmonics, we can construct an immense variety of intricate shapes. The primary advantage of this representation is its compactness and differentia- bility. Instead of optimizing tens of thousands of pixels, we only need to optimize a small set of 2 ∗(2K + 1) real-valued parameters (ak and bk), making the optimization process highly efficient. Crucially, the shape’s coordinates f(t) and g(t) are analytic functions of these coefficients, which is a prerequisite for gradient-based optimization. 4.3 Differentiable Mapping A key challenge is to bridge the parametric shape representation with the grid-like input required by a DNN. We need a differentiable process that can \"draw\" the shape onto a 2D canvas. We achieve this using a robust method derived from the winding number theorem. The winding number, W, quantifies how many times a closed curve travels counter-clockwise around a given point (x0, y0). For a simple, non-self-intersecting closed curve, the winding number is 1 for any point inside the curve and 0 for any point outside. This binary property provides a perfect criterion for defining the interior of our shape. The winding number can be calculated via the following line integral along the curve C: W(x0, y0) = 1 2π I C (x −x0)dy −(y −y0)dx (x −x0)2 + (y −y0)2 (2) By substituting our parametric expressions x = f(t), y = g(t), dx = f ′(t)dt, and dy =',\n",
       "  'The winding number can be calculated via the following line integral along the curve C: W(x0, y0) = 1 2π I C (x −x0)dy −(y −y0)dx (x −x0)2 + (y −y0)2 (2) By substituting our parametric expressions x = f(t), y = g(t), dx = f ′(t)dt, and dy = g′(t)dt, we can express the winding number as an integral over the parameter t: W(x0, y0) = 1 2π Z 2π 0 (f(t) −x0)g′(t) −(g(t) −y0)f ′(t) (f(t) −x0)2 + (g(t) −y0)2 dt (3) This integral gives the winding number for a single point (x0, y0). To generate a full image, we evaluate this integral for every pixel coordinate (xp, yp) in our target image grid I. The value of each pixel I(p) is thus a function of the winding number at its location. In practice, we implement this integral in a discrete and differentiable form. By sampling N points tj = j · (2π/N) along the curve, the integral is approximated 14 by the following differentiable sum: W(x0, y0) ≈1 N N−1 X j=0 (f(tj) −x0)g′(tj) −(g(tj) −y0)f ′(tj) (f(tj) −x0)2 + (g(tj) −y0)2 (4) This entire process, from the Fourier coefficients ck to the final raw image I, is fully differentiable. The resulting image I contains pixel values that are floating-point approximations of the true winding number at each coordinate. For a simple, non- self-intersecting curve, these values will be close to 1 for the interior and close to 0 for the exterior. During optimization, the shape may self-intersect, resulting in regions where the calculated values approximate other integers (e.g., 2, -1). The key insight is that any region with a calculated value significantly deviating from zero corresponds to the shape’s interior. Therefore, to create a robust mask for the DNN, we process the raw image I by first taking its absolute value. This step ensures that regions approximating both 1 and -1 are treated as positive. We then clip the values to the range [0, 1]. This normalization effectively thresholds the continuous-valued winding number field, mapping all significant interior regions (where the approximate wind- ing number’s absolute value is high) towards a value of 1 and the exterior towards 0, creating a well-formed input for the network. The automatic differentiation engines in modern deep learning frameworks like PyTorch can therefore compute the exact gradients ∂I(p)/∂ck for every pixel. This allows the adversarial loss, computed from the DNN’s output, to flow back and directly inform the update of the shape’s defining parameters. 4.4 Regularization Constraints Unconstrained optimization of the Fourier coefficients can lead to shapes that are physically unrealistic or contain excessive high-frequency noise. Such shapes may be effective in simulation but are not meaningful as real-world adversarial objects. To guide the optimization towards plausible and robust shapes, we introduce two regularization terms into our loss function, based on signal energy principles. First, we enforce fundamental frequency dominance. The overall structure of a natural object is typically defined by its low-frequency components. We therefore constrain the energy of the fundamental frequencies (c1, c−1) to be dominant over the higher harmonics.',\n",
       "  'two regularization terms into our loss function, based on signal energy principles. First, we enforce fundamental frequency dominance. The overall structure of a natural object is typically defined by its low-frequency components. We therefore constrain the energy of the fundamental frequencies (c1, c−1) to be dominant over the higher harmonics. We define the sum of fundamental and harmonic amplitudes as Sfund = |c1|+|c−1| and Sharm = PK |k|=2 |ck|, respectively. The constraint is formulated as a penalty term added to the loss if the following condition is violated: Sfund ≥λ · Sharm (5) where λ > 1 is a hyperparameter that enforces the desired dominance (e.g., λ = 2). This encourages the optimization to first establish a stable, low-frequency base shape before adding details. Second, we impose an individual higher harmonic amplitude limit. While higher harmonics are essential for crafting the specific features that deceive the net- work, allowing any single harmonic to become excessively strong can create unrealistic, 15 spiky artifacts. We therefore limit the amplitude of each individual higher harmonic coefficient to be no more than a fraction, γ, of the total fundamental amplitude: |ck| ≤γ · Sfund for all |k| ≥2 (6) where γ is a hyperparameter (e.g., γ = 0.25). This constraint ensures that the high-frequency details serve to refine the shape rather than dominate its structure. Together, the overall regularization constraint can be formulated as: Lreg = ReLU(λSharm −Sfund) + K X |k|=2 ReLU(|ck| −γSfund) (7) The Lreg acts as a prior for plausible geometries, accelerating convergence and resulting in smoother, more robust adversarial shapes. 4.5 Optimization Objectives To formally describe the optimization process for our three main experiments, we define the following. Let c = {ck}K k=−K be the set of optimizable Fourier coefficients. Let I(c) be the normalized grayscale image generated from these coefficients. Let C(·) be a classification network that outputs a probability distribution over classes, and let D(·) be a detection network. Experiment 1: Class-Specific Shape Generation. To generate a shape that embodies a target class ytarget, we optimize the coefficients c by minimizing the negative log-probability of the target class, combined with the regularization loss: L1(c) = −log(C(I(c))ytarget) + λreg · Lreg (8) where λreg is a weighting hyperparameter for the regularization term. Experiment 2: Shape as an Interpretability Tool. For a given natural image xnat with true label ytrue, we optimize a shape mask I(c) that is element-wise mul- tiplied with the image. To identify the minimal salient region, we maximize the confidence for the true class while minimizing the mask area: L2a(c) = −log(C(xnat ⊙I(c))ytrue) + λarea · mean(I(c)) + λreg · Lreg (9) To identify the minimal region to occlude for misclassification, we minimize the confidence for the true class while maximizing the mask area: L2b(c) = log(C(xnat ⊙I(c))ytrue) −λarea · mean(I(c)) + λreg · Lreg (10) where λarea is a weighting hyperparameter for the area term. Experiment 3: Shape as an Adversarial Patch. For a given image xnat con- taining foreground objects specified by bounding boxes B, we render the shape as an occlusion patch. Let R(xnat, I(c),',\n",
       "  '−λarea · mean(I(c)) + λreg · Lreg (10) where λarea is a weighting hyperparameter for the area term. Experiment 3: Shape as an Adversarial Patch. For a given image xnat con- taining foreground objects specified by bounding boxes B, we render the shape as an occlusion patch. Let R(xnat, I(c), B) be the function that renders the shape onto the 16 image at the specified locations. The goal is to minimize the objectness scores of all detections associated with the foreground objects occluded by the shape. Let {oj} be the set of these corresponding object confidence scores output by the detector D. The loss is: L3(c) = X j −log(1 −oj) + λreg · Lreg where {oj} from D(R(xnat, I(c), B)) (11) References [1] Mahner, F. P. et al. Dimensions underlying the representational alignment of deep neural networks with humans. Nature Machine Intelligence, 7(6), 848-859, 2025. [2] Geirhos, R. et al. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In Proc. International Conference on Learning Representations, 2018. [3] Woods, W., Chen, J., and Teuscher, C. Adversarial explanations for understanding image classification decisions and improved neural network robustness. Nature Machine Intelligence, 1(11), 508-516, 2019. [4] Ignatiev, A., Narodytska, N., and Marques-Silva, J. On relating explanations and adversarial examples. In Proc. Advances in Neural Information Processing Systems, 32, 2019. [5] Ilyas, A. et al. Adversarial examples are not bugs, they are features. In Proc. Advances in Neural Information Processing Systems, 32, 2019. [6] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional net- works: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. [7] Zeiler, M. D., and Fergus, R. Visualizing and understanding convolutional networks. In Proc. European Conference on Computer Vision, 818-833, 2014. [8] Ghaffari Laleh, N. et al. Adversarial attacks and adversarial robustness in computational pathology. Nature Communications, 13(1), 5711, 2022. [9] Veerabadran, V. et al. Subtle adversarial image manipulations influence both human and machine perception. Nature Communications, 14(1), 4933, 2023. [10] Paniagua, T., Savadikar, C., and Wu, T. Adversarial perturbations are formed by iteratively learning linear combinations of the right singular vectors of the adversarial jacobian. In Proc. International Conference on Machine Learning, 2025. 17 [11] Doerig, A. et al. High-level visual representations in the human brain are aligned with large language models. Nature Machine Intelligence, 1-15, 2025. [12] Dutta, S. K., and Zhang, X. IAP: Invisible adversarial patch attack through perceptibility-aware localization and perturbation optimization. In Proc. IEEE/CVF International Conference on Computer Vision, 2025. [13] Wang, J. et al. Physically realizable adversarial creating attack against vision- based BEV space 3D object detection. IEEE Transactions on Image Processing, 34, 538-551, 2025 [14] Wang, J. et al. A unified framework for adversarial patch attacks against visual 3D object detection in autonomous driving. IEEE Transactions on Circuits and Systems for Video Technology, 35(5), 4949-4962, 2025. [15] Lee, C., Song, Y., and Son, J. Data-free universal adversarial perturbation with pseudo-semantic prior. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13907-13916, 2025. [16] Wang, T. et al. Exploring the adversarial vulnerabilities of vision-language-action models in',\n",
       "  'on Circuits and Systems for Video Technology, 35(5), 4949-4962, 2025. [15] Lee, C., Song, Y., and Son, J. Data-free universal adversarial perturbation with pseudo-semantic prior. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13907-13916, 2025. [16] Wang, T. et al. Exploring the adversarial vulnerabilities of vision-language-action models in robotics. In Proc. IEEE/CVF International Conference on Computer Vision, 2025. [17] Fang, H. et al. One perturbation is enough: On generating universal adversarial perturbations against vision-language pre-training models. In Proc. IEEE/CVF International Conference on Computer Vision, 2025. [18] Selvaraju, R. R. et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proc. IEEE International Conference on Computer Vision, 618-626, 2017. [19] Deng, J. et al. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 248-255, 2009. [20] Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adver- sarial examples. arXiv preprint arXiv:1412.6572, 2014. [21] Szegedy, C. et al. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. [22] Brown, T. B. et al. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017. [23] Thys, S., Van Ranst, W., and Goedemé, T. Fooling automated surveillance cameras: adversarial patches to attack person detection. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2019. [24] Guesmi, A. et al. Dap: A dynamic adversarial patch for evading person detectors. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 18 pp. 24595-24604, 2024. [25] Hu, Y. C. T. et al. Naturalistic physical adversarial patch for object detectors. In Proc. IEEE/CVF International Conference on Computer Vision, 7848-7857, 2021. [26] Lian, J. et al. Benchmarking adversarial patch against aerial detection. IEEE Transactions on Geoscience and Remote Sensing, 60, 1-16, 2022. [27] Cheng, Z. et al. Physical attack on monocular depth estimation with optimal adversarial patches. In Proc. European Conference on Computer Vision, 514-532, 2022. [28] Chen, Z. et al. Shape matters: deformable patch attack. In Proc. European Conference on Computer Vision, 529-548, 2022. [29] Wei, X., Yu, J., and Huang, Y. Infrared adversarial patches with learnable shapes and locations in the physical world. International Journal of Computer Vision, 132(6), 1928-1944, 2024. [30] Wei, X., Yu, J., and Huang, Y. Physically adversarial infrared patches with learn- able shapes and locations. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12334-12342, 2023. [31] Zhu, X. et al. Infrared invisible clothing: Hiding from infrared detectors at mul- tiple angles in real world. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13317-13326, 2022. [32] Zhu, X. et al. Fooling thermal infrared pedestrian detectors in real world using small bulbs. In Proc. AAAI Conference on Artificial Intelligence, 35(4), 3616-3624, 2021. [33] Wei, X. et al. Unified adversarial patch for visible-infrared cross-modal attacks in the physical world. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(4), 2348-2363, 2023. [34] Wei, X. et al. Unified adversarial patch for cross-modal attacks in the physical world. In Proc. IEEE/CVF International Conference on Computer Vision, 4445- 4454, 2023. [35] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recog- nition.',\n",
       "  'Analysis and Machine Intelligence, 46(4), 2348-2363, 2023. [34] Wei, X. et al. Unified adversarial patch for cross-modal attacks in the physical world. In Proc. IEEE/CVF International Conference on Computer Vision, 4445- 4454, 2023. [35] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recog- nition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 770-778, 2016. [36] Simonyan, K., and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [37] Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely con- nected convolutional networks. In Proc. IEEE Conference on Computer Vision 19 and Pattern Recognition, 4700-4708, 2017. [38] Sandler, M. et al. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proc. IEEE Conference on Computer Vision and Pattern Recognition, 4510-4520, 2018. [39] Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [40] Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proc. IEEE/CVF International Conference on Computer Vision, 10012-10022, 2021. [41] Lin, T. et al. Microsoft coco: Common objects in context. In Proc. European Conference on Computer Vision, 740-755, 2014. [42] Redmon, J., and Farhadi, A. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. [43] Lin, T. Y. et al. Focal loss for dense object detection. In Proc. IEEE International Conference on Computer Vision, 2980-2988, 2017. [44] Tian, Z. et al. Fcos: Fully convolutional one-stage object detection. In Proc. IEEE/CVF International Conference on Computer Vision, 9627-9636, 2019. [45] Wen, Y. et al. Geometry-aware generation of adversarial point clouds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(6), 2984-2999, 2020. [46] Tu, J. et al. Physically realizable adversarial examples for lidar object detection. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 13716-13725, 2020. [47] Lou, T. et al. Hide in thicket: Generating imperceptible and rational adversarial perturbations on 3d point clouds. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 24326-24335, 2024. 20'],\n",
       " ['Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration Aupendu Kar* Dolby Laboratories, Inc India Krishnendu Ghosh Indian Institute of Technology Kharagpur India Prabir Kumar Biswas Indian Institute of Technology Kharagpur India ABSTRACT Continual learning is an emerging topic in the field of deep learning, where a model is expected to learn continuously for new upcoming tasks without forgetting previous experiences. This field has wit- nessed numerous advancements, but few works have been attempted in the direction of image restoration. Handling large image sizes and the divergent nature of various degradation poses a unique challenge in the restoration domain. However, existing works require heavily engineered architectural modifications for new task adaptation, re- sulting in significant computational overhead. Regularization-based methods are unsuitable for restoration, as different restoration chal- lenges require different kinds of feature processing. In this direction, we propose a simple modification of the convolution layer to adapt the knowledge from previous restoration tasks without touching the main backbone architecture. Therefore, it can be seamlessly applied to any deep architecture without any structural modifications. Un- like other approaches, we demonstrate that our model can increase the number of trainable parameters without significantly increasing computational overhead or inference time. Experimental validation demonstrates that new restoration tasks can be introduced without compromising the performance of existing tasks. We also show that performance on new restoration tasks improves by adapting the knowledge from the knowledge base created by previous restoration tasks. The code is available at https://github.com/aupendu/continual- restore CCS CONCEPTS • Computing methodologies →Supervised learning. KEYWORDS Continual learning, deep learning, image restoration ACM Reference Format: Aupendu Kar, Krishnendu Ghosh, and Prabir Kumar Biswas. 2025. Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration. In Proceedings of 16th Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP’25). ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn *Work done while Aupendu Kar was at the Indian Institute of Technology Kharagpur Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICVGIP’25, December 2025, Mandi, India © 2025 Copyright held by the owner/author(s). ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. https://doi.org/10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION Due to various weather conditions and adverse natural phenomena, captured images often suffer from various types of degradation - the presence of dust and aerosols causes hazing, rain or water droplets cause poor image visibility, camera and object motion cause blurring, to name a few. These degradations diminish image quality and clarity, which in turn affect various downstream tasks, such as medical imaging [13] and surveillance applications [36]. Figure 1: The proposed convolutional filter estimation block. 𝑀𝑤is continual memory, 𝑇𝑤is restoration task-specific weight vector, and 𝐾𝑤is the estimated convolutional kernel. Since the advent of Deep Learning, several image restoration- specific deep learning techniques have',\n",
       "  'turn affect various downstream tasks, such as medical imaging [13] and surveillance applications [36]. Figure 1: The proposed convolutional filter estimation block. 𝑀𝑤is continual memory, 𝑇𝑤is restoration task-specific weight vector, and 𝐾𝑤is the estimated convolutional kernel. Since the advent of Deep Learning, several image restoration- specific deep learning techniques have been proposed by researchers to mitigate the effects of image degradation factors, such as image dehazing [10, 30], deblurring [37], and deraining [8, 11]. Instead of designing a specific degradation model, several architectures have been proposed to handle different types of restoration [7, 42]. Most of these proposed restoration methods depend entirely on the current training samples and drastically forget the learned parameters if a new restoration task is introduced, a phenomenon commonly known as catastrophic forgetting [28]. This severe drawback renders a deep neural network ineffective for a previously trained task, limiting its application to the current restoration task. Several approaches [15, 38] have been introduced to address the problem of catastrophic forgetting. Kirkpatrick et al. [20] first pro- posed a parameter regularization-based algorithm where the move- ment of weights important to previous tasks is restricted using a quadratic constraint. Memory-aware synapses (MAS) [3] also re- strict the change of weights critical to previously learned tasks. It determines the relative importance of weights by computing the sen- sitivity of the output function with respect to each weight parameter in the network. Learning without forgetting (LwF) [23] proposes to impose parameter regularization by using a distillation loss. A gradual pruning-based method [27] is employed to compress the parameter space for a specific task and reuse the previously fixed pa- rameters for an upcoming task. These regularization-based networks have a fixed capacity, and their performance gradually reduces as arXiv:2511.05421v1 [cs.CV] 7 Nov 2025 ICVGIP’25, December 2025, Mandi, India dissimilar tasks are added to the network [17]. These make apply- ing them directly for image restoration tasks difficult, as the image degradation factors vary significantly in real life. Networks with dynamic architectures, such as Progressive Neural Nets [34], aim to alleviate the problem of fixed capacity by adding new sub-networks for each new task. However, this comes with excessive computational overhead, limiting its application to edge devices with constrained computational resources. [17] extended the idea of PackNet [27] to allow the expansion of the network by increasing the dimension of the CNN filters to accommodate new tasks. However, the simple expansion of the filter dimension causes an increase in computation in an 𝑂(𝑛) manner per pixel. In image restoration tasks, the high-resolution input images are commonly used and generally forward-propagated throughout the network ar- chitecture without any downsampling operation. Therefore, the com- putational burden increases significantly, and with deeper networks, the problem compounds to an even higher degree. Methods like LIRA [24] handle multiple degradations in a single image, where they utilize different task-specific expert networks for each degra- dation task and a common base network shared among all tasks. Due to the introduction of a new sub-network for each new task, significant computation overhead is added, and the network size increases substantially with',\n",
       "  'degradations in a single image, where they utilize different task-specific expert networks for each degra- dation task and a common base network shared among all tasks. Due to the introduction of a new sub-network for each new task, significant computation overhead is added, and the network size increases substantially with the addition of new tasks. [44] proposes lifelong learning for image restoration, focusing on a single task of deraining by allowing the network to continually learn from different rain datasets. Distinct from these previous works, we aim to address the cat- astrophic forgetting problem for various image restoration tasks without incurring any significant computational burden. For this purpose, we propose a simple modification of the convolution layer, where the convolution layer in the network is factored into two parts: a task-dependent, learnable vector and a task-independent, learnable weight matrix. The task-independent weight matrix constructs a knowledge base for all restoration tasks and facilitates knowledge sharing from previous tasks by storing and reusing the earlier learned parameters for upcoming restoration tasks. In our method, the net- work is trained sequentially for each new restoration problem. For the first restoration task, a preassigned portion of the task-independent weight matrix is trained along with a task-dependent weight vector, the product of which generates a simple convolution filter. After completion of the training, the trained parameters from the task- independent matrix are frozen and saved. For the next task, another separate task-dependent vector is introduced, and a separate portion of the free parameters in the task-independent matrix is trained. Pre- viously learned knowledge is reused to enhance the performance of the task at hand. This way, a very low computational overhead is incurred through a task-dependent vector for each new restoration task. The task-dependent vector also introduces a degree of freedom to the kernel generation, providing the network with the flexibility to choose or reject a previously learned filter for a new restoration task. This simple modification of the convolution layer can be easily adapted to any complex network architecture and can serve as a knowledge base for implementing continual learning-based image restoration. If the knowledge bank’s parameters are exhausted, new filter kernels can be prompted by simply appending the dimension of the task-specific vector and the corresponding new dimension of the task-independent matrix. Even then, the kernel size remains the same, so no extra computational load gets added to the network. The main contributions of this paper are as follows. • We introduce a new approach to estimate the kernels of a con- volutional layer, which eventually facilitates lifelong learning in image restoration tasks. To the best of our knowledge, this is the first work to deal with completely different restoration tasks in continual learning. • We also demonstrate that the proposed module can be easily adapted to any other state-of-the-art network without requir- ing any architectural modifications. • We experimentally demonstrate that the knowledge base of the proposed module can be easily expanded without incur- ring any significant computational burden. • We experimentally validate the performance improvement in the present restoration task by using',\n",
       "  'easily adapted to any other state-of-the-art network without requir- ing any architectural modifications. • We experimentally demonstrate that the knowledge base of the proposed module can be easily expanded without incur- ring any significant computational burden. • We experimentally validate the performance improvement in the present restoration task by using the knowledge from the previous restoration task. We also show the superiority of our proposed module as compared to similar lifelong learning approaches. 2 RELATED WORK 2.1 Advancement in Continual Learning The problem of catastrophic forgetting has been addressed using various methods, namely the parameter regularization method, data replay-based methods, and the dynamic network-based approach. In regularization-based methods, Kirkpatrick et al. [20] proposed a parameter regularization technique in which the weights that are relatively critical to old tasks were imposed stricter restrictions while updating for new tasks. [44] followed a similar approach to update parameters based on their importance. [4] estimates the importance by calculating the sensitivity of the output function to the change of parameters in the network. [9, 23] employ regularization in terms of distillation loss. [27] uses iterative pruning in a trained network and fixes the previously learned critical weights using a binary mask. Other approaches, such as [5, 15, 29, 35], rely on data replay to emulate information from previous tasks. Among these, the rehearsal- based methods [26, 31, 38] address the problem of catastrophic for- getting by remembering representative samples from the previous tasks in memory and replaying them while learning a new task. A major drawback of these methods is that previous data may not always remain available for future use. Other methods, such as cite shin2017continual,wu2018memory,wu2018incremental, alleviate the problem by employing pseudo-rehearsal-based training, primar- ily by using generative models to generate mock samples during training for new tasks. Dynamic network-based approaches address the forgetting prob- lem by dedicating a portion of the network to a particular task and expanding the network as needed for new tasks. Rusu et al. [34] pio- neered this approach by proposing a Progressive Neural Network that prevents catastrophic forgetting by adding a sub-network for each new task and transferring previously learned features through lateral connections from the base network. [17] allows model expansion but maintains compactness by choosing selected learned weights by means of a learnable mask. [32] proposes a linear combination of existing filters to learn filters corresponding to a new task. [22] Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration ICVGIP’25, December 2025, Mandi, India use Neural architecture search where as [40] adopt reinforcement learning based approach for network expansion. 2.2 Image Restoration Perspective Deep learning architectures have been used extensively in various image restoration tasks like rain-streak removal, haze removal, im- age denoising, motion blur removal etc [6, 11, 12, 21]. Some recent works have also focused on designing a single network to perform multiple restoration tasks, rather than separate dedicated networks for domain-specific tasks [7, 18, 25, 42]. Recently, the inherent ad- vantage of not forgetting and reusing previously acquired knowledge in continual learning (CL) has garnered interest in the restoration',\n",
       "  'recent works have also focused on designing a single network to perform multiple restoration tasks, rather than separate dedicated networks for domain-specific tasks [7, 18, 25, 42]. Recently, the inherent ad- vantage of not forgetting and reusing previously acquired knowledge in continual learning (CL) has garnered interest in the restoration domain. [24] propose a fork-join model where a new expert network that is specific to a restoration task is joined to a base pre-trained net- work, and a generative adversarial network is leveraged to emulate the memory replay process by generating pseudo-random samples of the previous tasks. Zhou et al. [44] employ the CL mechanism for an image de-raining task by using parameter regularization based on parameters’ individual importance. 3 METHODOLOGY We propose a new formulation of the convolution layer that can effec- tively handle multiple restoration tasks by sharing learned knowledge from previous tasks to train a new task. In this section, we discuss the proposed module, its training methodology, and the procedure for adapting previous task knowledge to new upcoming restoration tasks. Figure 2: CMC layer during the first restoration task. 3.1 Proposed Formulation of Convolution Layer Conventional convolution layers contain learnable weights that are convolved with the input features. It can be mathematically ex- pressed as 𝐹𝑜𝑢𝑡= 𝐹𝑖𝑛⊛𝐾𝑤, where ⊛is the convolution operator, 𝐹𝑖𝑛 is the input feature, 𝐹𝑜𝑢𝑡is the corresponding output feature, and 𝐾𝑤 is the kernel weights. 𝐾𝑤contains trainable parameters, which are updated through a gradient back-propagation algorithm. Unlike the conventional method of directly determining the ker- nels, we estimate them indirectly by triggering a task-independent learnable weight matrix with a task-specific learnable weight vector, as shown in Figure 1. 𝑀𝑡×𝑚 𝑤 is the task-independent weight matrix that contains the trainable weights of all the tasks. It can also be re- ferred to as the main memory of the convolution layer, as it stores the optimized weights for various tasks. It is also expandable if the train- able parameters are exhausted. Therefore, we term it as Continual Memory in Convolution (CMC). 𝑇1×𝑡 𝑤 is the task-dependent weight vector. It is fixed for each task, and a new weight vector is introduced during the adaptation of a new upcoming task in 𝑀𝑡×𝑚 𝑤 . Here, 𝑡is the length of the task-dependent weight vector. This 𝑡decides the capacity of CMC. As 𝑡increases, we need to add more rows in 𝑀𝑡×𝑚 𝑤 to increase the capacity of CMC seamlessly. 𝑚is the total number of parameters in a convolution kernel. The value of 𝑚is mathemati- cally expressed as 𝑚= 𝑘𝑖𝑛.𝑘𝑜𝑢𝑡.𝑛.𝑛, where 𝑘𝑖𝑛is the number of input features, 𝑘𝑜𝑢𝑡is the number of output features, and 𝑛is the kernel dimension. 𝑚only depends on the network architecture properties. If the architecture properties are fixed, 𝑚will be the same during lifelong learning. We do not need to change 𝑚for any restoration task. Therefore, the main computational overhead due to convolution on input features remains unchanged for continual learning-based image restoration tasks. However, the computation may increase as we extend the dimension 𝑡to expand the CMC capacity, but it is negligible compared to kernel expansion. During',\n",
       "  'need to change 𝑚for any restoration task. Therefore, the main computational overhead due to convolution on input features remains unchanged for continual learning-based image restoration tasks. However, the computation may increase as we extend the dimension 𝑡to expand the CMC capacity, but it is negligible compared to kernel expansion. During each task, 𝑇1×𝑡 𝑤 is matrix multiplicated with 𝑀𝑡×𝑚 𝑤 to estimate the kernels 𝐾1×𝑚 𝑤 . Both 𝑇1×𝑡 𝑤 and 𝑀𝑡×𝑚 𝑤 contain trainable free parameters that can be trained through the gradient back-propagation algorithm. A fraction of the CMC, 𝑀𝑡×𝑚 𝑤 , is utilized in each task based on performance requirements. Figure 3: Operations in CMC layer for 𝑛𝑡ℎrestoration task. 3.2 Multi-task handling The Continual Memory in Convolution (CMC) 𝑀𝑡×𝑚 𝑤 is the main module whose parameters are trained in each task. In this section, the mechanism of lifelong training for restoration tasks is explained in two parts, one for the first restoration task and the other for the forthcoming restoration tasks. For the first restoration task, there is no previous knowledge to adapt. However, the forthcoming restora- tion tasks build upon the knowledge base established in the previous tasks. Figure 2 shows the operations involved during adaptation of the first restoration task, and Figure 3 shows a pictorial representa- tion of adopting the 𝑛𝑡ℎtask in the CMC module. 3.2.1 First restoration task. At the beginning of the first task, all the weights of the CMC module remain as free parameters. We select a random fraction of these free weights by applying a task-specific ICVGIP’25, December 2025, Mandi, India binary mask H𝑡×𝑚 𝑤1 to the CMC module to train the network based on the restoration task requirements. These selected weights 𝑀𝑤1 are then represented as 𝑀𝑡×𝑚 𝑤 ⊙H𝑡×𝑚 𝑤1 , where ⊙is point-wise multi- plication operator. Only these weights are expected to be updated during training for the first restoration task. In each convolution layer, restoration-specific vector𝑇1 and the selected fraction of CMC 𝑀𝑤1 are updated to estimate the respective convolution kernels 𝐾𝑤1, as shown in eq.1. Other weights are considered zero during this operation. After training, we get a trained 𝑇1 and 𝑀𝑤1. 𝐾1×𝑚 1 = 𝑇1×𝑡 1 .(𝑀𝑡×𝑚 𝑤 ⊙H𝑡×𝑚 𝑤1 ) (1) 3.2.2 Forthcoming restoration task. After the model is trained on the first restoration task, the forthcoming restoration tasks are trained sequentially and utilize all the trained parameters of the previous tasks, as shown in Figure 3. From the 2𝑛𝑑task onward, the task-specific binary mask is chosen such that there is no overlap between the current mask and previously chosen masks. H𝑡×𝑚 𝑤𝑛 ⊙H𝑡×𝑚 𝑤𝑖 = 𝑂, 𝑓𝑜𝑟𝑎𝑙𝑙𝑖= 1, ...,𝑛−1 (2) Here 𝑂𝑡×𝑚is a zero matrix. If H𝑤is the mask representing all the weights in the network, then the available free parameters in the CMC module for the 𝑛𝑡ℎtask can be mathematically expressed as, 𝑀𝑛= 𝑀𝑤⊙(H𝑤−H𝑡×𝑚 𝑤1 ∪H𝑡×𝑚 𝑤2 ∪... ∪H𝑡×𝑚 𝑤𝑛−1) (3) The 𝑛𝑡ℎtask utilizes all the filters learned from the previous tasks and learns a fraction of 𝑀𝑛for its kernel estimation as shown in eq.4. (4) 𝐾1×𝑚 𝑛 = (𝑇1×𝑡 1 .(𝑀𝑡×𝑚 𝑤 ⊙H𝑡×𝑚',\n",
       "  'for the 𝑛𝑡ℎtask can be mathematically expressed as, 𝑀𝑛= 𝑀𝑤⊙(H𝑤−H𝑡×𝑚 𝑤1 ∪H𝑡×𝑚 𝑤2 ∪... ∪H𝑡×𝑚 𝑤𝑛−1) (3) The 𝑛𝑡ℎtask utilizes all the filters learned from the previous tasks and learns a fraction of 𝑀𝑛for its kernel estimation as shown in eq.4. (4) 𝐾1×𝑚 𝑛 = (𝑇1×𝑡 1 .(𝑀𝑡×𝑚 𝑤 ⊙H𝑡×𝑚 𝑤1 ) + ... +𝑇1×𝑡 𝑛−1.(𝑀𝑡×𝑚 𝑤 ⊙H𝑡×𝑚 𝑤𝑛−1)) +𝑇1×𝑡 𝑛 .(𝑀𝑡×𝑚 𝑤 ⊙H𝑡×𝑚 𝑤𝑛) This way, the filters estimated for the 𝑛𝑡ℎtask become a linear com- bination of previously learned filters and the newly trained kernels for the current task. All previous weights 𝑀𝑤1, 𝑀𝑤2, ...𝑀𝑤𝑛−1 and task specific vectors 𝑇1,𝑇2, ...𝑇𝑛−1 remains fixed. After training, the 𝑛𝑡ℎtask occupy the fraction 𝑀𝑤𝑛of total weight box 𝑀𝑤. Algorithm 1 shows the algorithmic representation of training the proposed CMC module for the 𝑛𝑡ℎtask. At the first stage, a fraction of free parameters is allocated at random for the 𝑛𝑡ℎtask by using a mask H𝑤𝑛. 𝑀𝑤⊙H𝑤𝑛represents the fraction of CMC module 𝑀𝑤, which will be tuned to learn knowledge from the 𝑛𝑡ℎtask. During training, all previously learned weights are used to share the knowledge of past experiences as kernel parameters 𝐾𝑜𝑙𝑑with the current task. This knowledge-sharing mechanism does not contribute to any gradient update operations. Therefore, the knowledge gained from previous experience remains unchanged. After that, 𝐾𝑜𝑙𝑑is fused with task-specific kernels 𝐾𝑤𝑛to estimate the final kernel 𝐾𝑓𝑖𝑛𝑎𝑙to extract features F𝑜𝑢𝑡from the input features F𝑖𝑛. 3.2.3 Extension of Parameters in a Layer. The dimension 𝑚 is fixed as it is the total number of parameters that are required for a convolution. Therefore, we can increase the dimension 𝑡if we exhaust all the free trainable parameters. We can also take a bigger 𝑡 if any layer demands more trainable parameters. For example, the input layer in a restoration task extracts key image features, and the output layer reconstructs the image from this feature domain. It may hamper performance drastically if we allocate the same percentage of weights as other layers. Unlike classification, we can not afford to lose key image features in image restoration. Our proposed modified Algorithm 1: Training algorithm of CMC module for 𝑛𝑡ℎ task Input: 𝑇1×𝑡 𝑛 = task-specific vector, 𝑀𝑡×𝑚 𝑤 = fractionally trained CMC module Output: Fully trained 𝑇1×𝑡 𝑛 and 𝑀𝑡×𝑚 𝑤 with trained parameters of 𝑛𝑡ℎtask Allocation of Random % of free parameters: H𝑡×𝑚 𝑤1 , H𝑡×𝑚 𝑤2 , ...H𝑡×𝑚 𝑤𝑛−1 represents the mask of 𝑛−1 different tasks; select H𝑤𝑛, where H𝑤𝑛∩H𝑤𝑖= ∅, 𝑖= 1, ...𝑛−1; Parameters to train 𝑀𝑤𝑛= 𝑀𝑤⊙H𝑤𝑛; Training on 𝑛𝑡ℎtask forall CMC layers do Gradient Operation Paused: for 𝑖≤𝑛−1, 𝑖+ + do 𝐾𝑤𝑖= 𝑇𝑖.(𝑀𝑤⊙H𝑤𝑖); 𝐾𝑜𝑙𝑑+ = 𝐾𝑖; end Gradient Operation Resumed: 𝐾𝑤𝑛= 𝑇𝑛.(𝑀𝑤⊙H𝑤𝑛); 𝐾𝑓𝑖𝑛𝑎𝑙= 𝐾𝑤𝑛+ 𝐾𝑜𝑙𝑑; F𝑜𝑢𝑡= F𝑖𝑛⊛𝐾𝑓𝑖𝑛𝑎𝑙 end convolution provides the flexibility to increase the parameters in those key layers without increasing computational complexity. 4 EXPERIMENTAL ANALYSIS 4.1 Experimental Setting In this section, we discuss all the experimental settings and the details of the implementation. We utilize standard datasets for var- ious restoration tasks and employ a simple deep neural network architecture to validate our proposed idea. 4.1.1 Restoration Task Selection. We use four',\n",
       "  '4 EXPERIMENTAL ANALYSIS 4.1 Experimental Setting In this section, we discuss all the experimental settings and the details of the implementation. We utilize standard datasets for var- ious restoration tasks and employ a simple deep neural network architecture to validate our proposed idea. 4.1.1 Restoration Task Selection. We use four different restora- tion tasks for our experimental analysis. Those selected tasks are deraining, denoising, deblocking, and deblurring. These restoration tasks are chosen based on the nature of their degradation factors. De- raining is needed to alleviate the degradation caused by rain streaks. Denoising effectively reduces noise in captured images due to poor camera sensors. On the other hand, deblurring addresses the degra- dation caused by motion blur or poor resolution during the capture process, and deblocking mitigates blocking in an image that occurs when storing it on a disk. We explain handling these four restoration tasks throughout our paper. However, any other restoration task can be included continuously without any significant modifications. 4.1.2 Dataset. We use four different datasets for the four differ- ent restoration tasks in our experiment. For deraining, we utilize the standard Rain100L dataset [41], which comprises 200 training images and 100 testing images. In the case of image denoising, we randomly add Gaussian noise with a standard deviation (std) of 50 to the DIV2K [2] dataset, a high-resolution image dataset, for training and testing the trained model on the BSD68 [33] dataset. In both de- blocking and deblurring, we use the DIV2K dataset and degrade the image with random JPEG artifacts and blurring, respectively, during Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration ICVGIP’25, December 2025, Mandi, India training. We consider the quality range [10, 70] for introducing the JPEG artifact. For blurring, we take account of the Gaussian blur and take 15 × 15 blur kernel with a random standard deviation in the range [0.2, 3]. However, during testing, we only considered the Gaussian blur kernel with a standard deviation of 2.5 for deblurring and the insertion of JPEG artifacts using a quality factor of 20 for de- blocking. In both deblocking and deblurring, we utilize the DIV2K validation dataset for testing purposes. 4.1.3 Model Architecture. We use a simple consecutive residual block-based network architecture [14] for our experimental purposes. There are 6 residual blocks in our network, excluding the input and output convolution blocks. Each residual block consists of 64 input and output channels. The convolution blocks inside each residual block use 3 × 3 convolution with stride 1. In our experiment, we replace the conventional convolution blocks with our proposed mod- ified block. However, all the kernel parameters remain the same. Therefore, it can be seamlessly integrated into any deep architecture without requiring any architectural modifications. 4.1.4 Implementation details. We use the same experimental setup for all the experiments. The model is trained for 125 epochs, and each epoch consists of 1, 000 batch updates. There are 16 image patches of size 128×128 in each batch. All images are normalized to the range [0, 1] during both training and testing.',\n",
       "  'use the same experimental setup for all the experiments. The model is trained for 125 epochs, and each epoch consists of 1, 000 batch updates. There are 16 image patches of size 128×128 in each batch. All images are normalized to the range [0, 1] during both training and testing. The mean-squared error (MSE) is used as a loss function for gradient back-propagation. Adam optimizer [19] with learning rate 10−4 is used for updating the weights, and the learning rate is halved after every 25 epochs. We use the Peak signal-to-noise ratio (PSNR) metric throughout the paper for performance analysis. % Params Knowledge Sharing Derain Denoise Deblocking Deblur 20 ✗ 33.50 27.65 30.99 29.64 ✓ 33.50 27.68 31.11 29.74 10 ✗ 32.14 27.43 30.81 29.32 ✓ 32.14 27.53 30.96 29.64 5 ✗ 30.36 27.08 30.55 28.97 ✓ 30.36 27.23 30.78 29.33 2.5 ✗ 29.71 26.58 30.29 28.35 ✓ 29.71 27.01 30.64 29.02 1.25 ✗ 29.19 26.10 30.21 27.92 ✓ 29.19 26.49 30.42 28.52 Table 1: Performance of continual task adaptation on PSNR metric. ✓means knowledge of previous tasks is adapted during training. Derain is the first task. Denoise, deblocking, and deblur are the next tasks on which the model is trained, following that sequence. 4.2 Experiments on Continual Task Adaptation Table 1 shows the quantitative analysis of lifelong restoration task learning with knowledge sharing. In the first experiment, we allocate parameters for different tasks and train on restoration datasets with- out sharing knowledge from other tasks. In the second experiment, the knowledge of past restoration tasks is shared with the current tasks. This way, lifelong learning persists. There is no performance difference in single-image deraining as it is the first task. Denois- ing, deblocking, and deblurring are the next consecutive tasks. We observe from the table that performance on these three tasks consis- tently yields better results when knowledge is shared. We can also observe that knowledge sharing performs significantly better as the percentage of parameter allocation decreases. This happens because decreasing the allocated parameters hinders the learning process, and the model can not acquire sufficient knowledge for that particular task. Therefore, similar knowledge of previous restorations becomes more helpful in learning the current restoration task. Figure 4 shows how the performance metric PSNR changes in each training epoch. We chose the final restoration task, deblurring, for this analysis purpose. Figure 4(a) shows performance analysis with 5% of model parameters, and Figure 4(b) shows performance analysis with 1.25% of model parameters. We can clearly see the improvement by applying the knowledge gained from deraining, denoising, and deblocking, as the PSNR in the first epoch already yields an initial difference of approximately 3.5 dB for 5% parameters and approximately 6 dB for 1.25% parameters. Therefore, we can say that previous task knowledge gives better performance and results in faster convergence. (a) 5% model parameters used (b) 1.25% model parameters used Figure 4: PSNR in dB vs Each epoch of training Methods Derain Denoise Deblocking Deblur Together 25.44 15.66 29.26 29.80 Deform 30.90 27.26 30.67 29.29 Pruning [27] 29.56 27.45 31.00 29.50 MAS [3]',\n",
       "  'better performance and results in faster convergence. (a) 5% model parameters used (b) 1.25% model parameters used Figure 4: PSNR in dB vs Each epoch of training Methods Derain Denoise Deblocking Deblur Together 25.44 15.66 29.26 29.80 Deform 30.90 27.26 30.67 29.29 Pruning [27] 29.56 27.45 31.00 29.50 MAS [3] 23.80 18.99 28.39 25.98 CMC-5 32.14 27.53 30.96 29.64 Table 2: Quantitative analysis of our proposed CMC module with other continual learning mechanisms. 4.3 Comparative Analysis Most of the continual learning based frameworks are specifically designed for classification tasks. Therefore, it is not feasible to apply those to restoration models. However, for comparative analysis, we evaluate different baseline models and popular pruning-based con- tinual learning methods. Table 2 shows the quantitative evaluation of our proposed Continual Memory in Convolution (CMC) module with different baseline models. The pruning-based methods directly prune the filters of the convolution layers and use those pruned weights for upcoming tasks [27]. The ‘Deform’ baseline utilizes a deformable convolution-based architecture to reduce the model’s parameters. This baseline aims to compare the advantage of a model with shared knowledge with that of smaller, distinct models for various tasks. ICVGIP’25, December 2025, Mandi, India In the deform baseline, separate models are used for different tasks. The ‘Together’ baseline model incorporates all tasks into a single model. This baseline uses the whole ResNet architecture to train the model for all four tasks. The ‘Deform’ baseline utilizes around 13% more model parameters compared to a plain convolution-based architecture. The ‘Pruning’ baseline uses 12.5% of model parameters, and our proposed CMC-5 takes 10% of the overall model parameters. If we consider the total number of parameters, our model has more parameters as compared to pruning-based methods. However, the kernel parameters and throughput speed remain the same. MAS [3] is a regularization-based continual learning method that can be easily applied to restoration tasks. For a fair comparison, we use the same residual block-based architecture to perform the experiments using MAS. MAS failed to maintain its performance in past restoration tasks. After training on all four restoration tasks sequentially, the PSNR drops significantly from 34.36 dB to 23.80 dB for deraining, from 25.42 dB to 18.99 dB for denoising, and from 30.11 dB to 28.39 dB for deblocking. 4.4 Additional Analysis 4.4.1 Subjective Comparison. Figure 5 shows the subjective comparison of restoration performance. We use the model which allocates 1.25% of trainable parameters for each restoration operation. We present the qualitative analysis of a method that does not utilize knowledge from previous tasks, alongside a method that leverages knowledge from past restoration tasks. The results of the method that does not share the knowledge are termed ’Image-1’ and the results of the knowledge sharing are termed ’Image-2’, as shown in Figure 5. In the case of knowledge sharing, we use the trained model of a task that has been trained at the end. Therefore, we can observe significant visual differences. In the case of blurring and deblocking, the red boxes highlight the performance improvement in both Figure 5p and Figure 5l as compared to Figure 5o',\n",
       "  'knowledge sharing, we use the trained model of a task that has been trained at the end. Therefore, we can observe significant visual differences. In the case of blurring and deblocking, the red boxes highlight the performance improvement in both Figure 5p and Figure 5l as compared to Figure 5o and Figure 5k, respectively. In the denoising task, Figure 5h shows better results with fewer artifacts as compared to Figure 5g. We also observe a lesser rain streak effect in Figure 5d as compared to Figure 5c. 4.4.2 Shuffling the Training order. Previously, we demonstrated only one sequence of different restoration tasks, which are trained continually, and found that the performance of a restoration task im- proves if we share the knowledge from past tasks. Here, we shuffle the sequence of the four restoration tasks in such a manner that each task occupies every available position in the sequence and perform the experiments. Table 3 shows the quantitative evaluation of four different experiments where each task is in a different training se- quence position in each experiment. For example, deraining is the first task in Experiment 1, while it is the last task in Experiment 2, and it is the second and third tasks in Experiment 3 and Experi- ment 4, respectively. Figure 6 depicts the graphical representation of Table 3. We observe that the performance of the task improves as the training sequence number increases. The training sequence number of a restoration task is the position at which it appears in training order while training the tasks continually. If a particular task’s training sequence position is 4, it means the model has al- ready been trained on three different restoration tasks and utilizes the shared knowledge from those three previous tasks for the current task. We can conclude two things from these experiments. Firstly, the knowledge of previously learned tasks plays a crucial role in lifelong learning, and our method successfully helps future tasks to adopt the knowledge of the past tasks. Secondly, the training sequence plays a crucial role. The performance of the first task always degrades due to the non-availability of previous knowledge. Therefore, backward Continual Learning, in which past-trained tasks can fine-tune their knowledge from future tasks, will be able to mitigate the effect of the training sequence. We plan to explore it in future work. Experiment 1 Experiment 2 Experiment 3 Experiment 4 Training Sequence PSNR in dB Training Sequence PSNR in dB Training Sequence PSNR in dB Training Sequence PSNR in dB Deraining 1 29.19 4 29.95 3 30.02 2 29.69 Denoising 2 26.49 1 26.10 4 26.66 3 26.61 Deblocking 3 30.42 2 30.29 1 30.21 4 30.46 Debluring 4 28.52 3 28.52 2 28.35 1 27.92 Table 3: Quantitative evaluation of the effect of training se- quence. In each experiment, the training sequences are shuffled. Knowledge Sharing Noise 10 Noise 20 Noise 30 Noise 40 Real Noise ✗ 33.75 30.44 28.22 27.12 36.90 ✓ 33.75 30.75 29.01 27.82 37.63 Table 4: Performance of continual task adaptation for 5 separate denoising tasks. 4.4.3 Similar restoration',\n",
       "  'training se- quence. In each experiment, the training sequences are shuffled. Knowledge Sharing Noise 10 Noise 20 Noise 30 Noise 40 Real Noise ✗ 33.75 30.44 28.22 27.12 36.90 ✓ 33.75 30.75 29.01 27.82 37.63 Table 4: Performance of continual task adaptation for 5 separate denoising tasks. 4.4.3 Similar restoration tasks. Till now, we have only consid- ered completely different restoration tasks. To analyze the continual restoration task adaptation in similar kinds of tasks, we performed experiments sequentially on four Gaussian noise levels: 10, 20, 30, and 40, followed by real noise. For training on real-world noise, we use the popular SIDD dataset [1], and clean images from the DIV2K dataset [2] are used to generate Gaussian noisy images. Ta- ble 4 shows the performance of our CMC module-based continual learning framework in those tasks. The experimental setup is the same as the previous one, as performed in Table 1. 20% of model parameters are allocated for each task. We can observe from Table 4 that adopting knowledge from previous easy denoising tasks (i.e., less noisy images) significantly improves the performance on com- plex denoising tasks (i.e., heavily noisy images) and real noise. The experimental results further validate the feasibility of our proposed method. Parameter Expansion Derain Denoise Deblocking Deblur ✗ 29.19 26.49 30.42 28.13 ✓ 29.51 26.65 30.51 28.72 Table 5: Quantitative performance analysis when the parameter of some key layer of restoration is increased without hampering the computational performance. Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration ICVGIP’25, December 2025, Mandi, India (a) Grouth-Truth (b) Rainy Image (c) Deraining (Image-1) (d) Deraining (Image-2) (e) Grouth-Truth (f) Noisy Image (g) Denoising (Image-1) (h) Denoising (Image-2) (i) Grouth-Truth (j) Blury Image (k) Deblurring (Image-1) (l) Deblurring (Image-2) (m) Grouth-Truth (n) JPEG Artifact Image (o) Deblocking (Image-1) (p) Deblocking (Image-2) Figure 5: Qualitative evaluation of the effect of the knowledge sharing in our continual learning framework. ’Image 1’ represents the outputs of the model without any knowledge sharing. ’Image 2’ depicts the results of those models where the respective tasks are trained at last using the knowledge of all the previous tasks. (Zoom for the best view.) 4.4.4 Parameter expansion. In a deep learning network, some layers can demand more parameters to learn key essential features. Allocating fewer parameters in those layers may result in the loss of crucial features, ultimately leading to poor performance. In the case of restoration tasks, the input and output layers play a crucial role, as the first one extracts valuable features from the image, and the second one reconstructs the image. The number of kernel parameters is generally less in those layers as compared to other layers. This is because the input layer maps from 3 −𝐷RGB image and the output layer maps to 3 −𝐷RGB image. Therefore, if we allocate the same percentage of free parameters in those layers similar to other layers, it will lead to poor performance. However, allocating a higher percentage in those layers will lead to early parameter exhaustion in those layers. Using our method, we can',\n",
       "  'maps to 3 −𝐷RGB image. Therefore, if we allocate the same percentage of free parameters in those layers similar to other layers, it will lead to poor performance. However, allocating a higher percentage in those layers will lead to early parameter exhaustion in those layers. Using our method, we can easily expand the parameter space of the input and output layers without any computational overhead. Table 5 shows the performance of parameter expansion in those two layers. All models in that table use only 1.25% of parameters. All the layers use the CMC-5 module, except for the Method Trainable Parameters Kernel Parameters Memory Flops (GMac) Inference Time Type-1 1× 1× 2083 MB 36.86 4.220 ms 1.8× 1.8× 2099 MB 65.4 9.080 ms 4× 4× 2099 MB 146.57 20.39 ms Type-2 1× 1× 2083 MB 36.86 4.220 ms 2× 2× 2571 MB 73.73 8.49 ms 4× 4× 3547 MB 147.46 16.42 ms CMC-n (Ours) 1× 1× 2093 MB 36.864 4.193 ms 2× 1× 2111 MB 36.87 4.237 ms 4× 1× 2111 MB 36.88 4.255 ms Table 6: Computational complexity analysis of different funda- mental continual learning mechanisms under the premises of parameter expansion first and final layers, which use the CMC-10 module. We can observe from the table that the performance increases significantly across all restoration tasks with the flexible and straightforward modifications that our module offers. ICVGIP’25, December 2025, Mandi, India Figure 6: Graphical representation of the effect of training se- quence on each task. As the training sequence number of a task is increased, the performance of the task increases due to better adoption of previous tasks’ knowledge. 4.4.5 Computational Complexity Analysis. The primary ad- vantage of this proposed method is its reduction of computational overhead for lifelong learning, as there are inherent limitations in hardware and computing power. In Table 6, we compare the com- putational overhead of our proposed CMC module and different continual learning ideas. In this experiment, we consider a convolu- tion layer that consists of the same kernel parameters. We consider that as a base. Now, we assume that all the parameters of the layer have already been occupied by different tasks. Therefore, we need to increase the number of parameters. We calculate and compare the computational overhead when increasing the number of parameters by a factor of 2× and 4×. We take two fundamental ways to increase the parameters in the literature. The first one is termed Type-1 in Table 6, where the kernel size is increased to accommodate a larger number of free parameters [17]. The second one is termed Type-2, where the number of kernels is increased or a new layer is introduced for adding new parameters [24, 34]. In this experimental setup, we use a convolutional layer with a 3 × 3 kernel size as the base layer. It has 64 input and 64 output channels. Now, this layer processes 64 input features, which have spatial dimensions of 1000 × 1000. In Type-1, we increase the kernel size to 4 × 4 and 6 × 6 from 3 × 3, thereby increasing',\n",
       "  'kernel size as the base layer. It has 64 input and 64 output channels. Now, this layer processes 64 input features, which have spatial dimensions of 1000 × 1000. In Type-1, we increase the kernel size to 4 × 4 and 6 × 6 from 3 × 3, thereby increasing the parameters by 1.8× and 4×, respectively. A new convolution layer is introduced to increase the parameters in Type-2. In our case, we use CMC-5 as a base layer and use CMC-10 and CMC-20 to double and quadruple the number of parameters. It can be clearly seen from Table 6 that the CMC module does not sig- nificantly burden the memory requirements, FLOPs, and inference time. However, Type-1 increases the inference time and Flops signif- icantly as we increase the parameters. This is because it increases the kernel size, which exponentially increases the computational burden. Type-2 drastically increases both the memory required dur- ing processing and inference time. As the CMC module does not significantly increase the inference time and memory requirement, it can serve the purpose of lifelong training. Model Knowledge Sharing denoise derain deblock deblur RDN ✗ 29.43/ 0.902 26.36/ 0.731 30.20/ 0.861 27.37/ 0.782 ✓ 29.43/ 0.902 26.47/ 0.734 30.38/ 0.866 27.90/ 0.799 Dense ✗ 28.84/ 0.889 25.98/ 0.709 30.16/ 0.860 27.90/ 0.799 ✓ 28.84/ 0.889 26.39/ 0.728 30.36/ 0.865 28.60/ 0.815 Table 7: Performance of continual task adaptation on two differ- ent model architectures. 4.4.6 Adopting CMC in different architectures. To prove the ex- tendibility of our proposed CMC in different deep architectures, we consider two popular network topologies for experiment purposes, namely dense block [16, 39] and residual dense network (RDN) [43]. Table 7 shows the performance of our proposed continual learning framework on two different network architectures. The experimental setup is the same as the previous one, as performed in Table 1. We consider 6 blocks for both dense and RDN block-based architecture. We only consider 1.25% of model parameters for each task. We can observe from Table 7 that both architectures follow a similar trend, as we witness in the residual architecture. We provide both PSNR and SSIM values, and our experimental results show that SSIM follows a similar trend to PSNR. 5 LIMITATIONS, IMPACT AND FUTURE WORK Our proposed Continual Memory in Convolution (CMC) module serves the purpose of lifelong learning, as it allows us to add knowl- edge without forgetting, and it does not impose an extra computa- tional burden on the compact system. However, the main drawback of this approach is the number of parameters. More parameters are required in the CMC module to produce the same performance as compared to a conventional convolution layer. But nowadays, the system memory in a compact system is easily extendable. Therefore, our module can easily work in those systems. Currently, our model can only reuse the knowledge from past tasks. However, we believe that with a simple modification, this module has tremendous poten- tial to learn backwards, i.e., to improve past restoration performance by utilizing knowledge from future tasks. Handling multiple known degradations in an',\n",
       "  'work in those systems. Currently, our model can only reuse the knowledge from past tasks. However, we believe that with a simple modification, this module has tremendous poten- tial to learn backwards, i.e., to improve past restoration performance by utilizing knowledge from future tasks. Handling multiple known degradations in an image by leveraging knowledge of individual degradations can be explored through knowledge sharing by fusing the knowledge of individual tasks. These ideas can be the future scope of this work. 6 CONCLUSION In this paper, we propose a modification of the conventional convolu- tion layer. By making this simple modification, we can continuously adapt the learned experience from previous tasks and share those experiences with the current task to improve its performance. We address the shortcomings of lifelong learning for image restoration tasks, and our module serves as a prospective solution. This is the first-of-a-kind work where diverse restoration tasks have been han- dled through continual learning. The proposed mechanism shares knowledge across tasks without changing the backbone architecture. The knowledge base can be continuously expanded with a minimal computational burden. We experimentally observe the benefits of knowledge sharing between completely different restoration tasks, as it helps to improve the performance by a significant margin. Sharing the Learned Knowledge-base to Estimate Convolutional Filter Parameters for Continual Image Restoration ICVGIP’25, December 2025, Mandi, India REFERENCES [1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S. Brown. 2018. A High-Quality Denoising Dataset for Smartphone Cameras. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1692–1700. https: //doi.org/10.1109/CVPR.2018.00182 [2] Eirikur Agustsson and Radu Timofte. 2017. Ntire 2017 challenge on single image super-resolution: Dataset and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 126–135. [3] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018. Memory Aware Synapses: Learning what (not) to forget. In The European Conference on Computer Vision (ECCV). [4] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV). 139–154. [5] Pratik Prabhanjan Brahma and Adrienne Othon. 2018. Subset replay based continual learning for scalable improvement of autonomous systems. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE, 1179–11798. [6] Meng Chang, Qi Li, Huajun Feng, and Zhihai Xu. 2020. Spatial-adaptive network for single image denoising. In European Conference on Computer Vision. Springer, 171–187. [7] Liangyu Chen, Xin Lu, Jie Zhang, Xiaojie Chu, and Chengpeng Chen. 2021. HINet: Half instance normalization network for image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 182– 192. [8] Sen Deng, Mingqiang Wei, Jun Wang, Yidan Feng, Luming Liang, Haoran Xie, Fu Lee Wang, and Meng Wang. 2020. Detail-recovery image deraining via context aggregation networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 14560–14569. [9] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. 2019. Learning without memorizing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5138–5146. [10] Hang Dong,',\n",
       "  'via context aggregation networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 14560–14569. [9] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. 2019. Learning without memorizing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5138–5146. [10] Hang Dong, Jinshan Pan, Lei Xiang, Zhe Hu, Xinyi Zhang, Fei Wang, and Ming- Hsuan Yang. 2020. Multi-scale boosted dehazing network with dense feature fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2157–2167. [11] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xinghao Ding, and John Paisley. 2017. Removing Rain From Single Images via a Deep Detail Network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [12] Yosef Gandelsman, Assaf Shocher, and Michal Irani. 2019. \"Double-DIP\": Unsu- pervised Image Decomposition via Coupled Deep-Image-Priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). [13] Lovedeep Gondara. 2016. Medical image denoising using convolutional denois- ing autoencoders. In 2016 IEEE 16th international conference on data mining workshops (ICDMW). IEEE, 241–246. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep resid- ual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778. [15] Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, Zhengwei Tao, Jinwen Ma, Dongyan Zhao, and Rui Yan. 2018. Overcoming catastrophic forgetting for continual learning via model adaptation. In International Conference on Learning Representations. [16] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. 2017. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4700–4708. [17] Steven CY Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. 2019. Compacting, picking and growing for unforget- ting continual learning. arXiv preprint arXiv:1910.06562 (2019). [18] Aupendu Kar, Sobhan Kanti Dhara, Debashis Sen, and Prabir Kumar Biswas. 2021. Zero-Shot Single Image Restoration Through Controlled Perturbation of Koschmieder’s Model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16205–16215. [19] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014). [20] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114, 13 (2017), 3521–3526. [21] Boyun Li, Yuanbiao Gou, Jerry Zitao Liu, Hongyuan Zhu, Joey Tianyi Zhou, and Xi Peng. 2020. Zero-shot image dehazing. IEEE Transactions on Image Processing 29 (2020), 8457–8466. [22] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. 2019. Learn to grow: A continual structure learning framework for overcoming cata- strophic forgetting. In International Conference on Machine Learning. PMLR, 3925–3934. [23] Zhizhong Li and Derek Hoiem. 2017. Learning without forgetting. IEEE transac- tions on pattern analysis and machine intelligence 40, 12 (2017), 2935–2947. [24] Jianzhao Liu, Jianxin Lin, Xin Li, Wei Zhou, Sen Liu, and Zhibo Chen. 2020. LIRA: Lifelong Image Restoration from Unknown Blended Distortions. In Com- puter',\n",
       "  '3925–3934. [23] Zhizhong Li and Derek Hoiem. 2017. Learning without forgetting. IEEE transac- tions on pattern analysis and machine intelligence 40, 12 (2017), 2935–2947. [24] Jianzhao Liu, Jianxin Lin, Xin Li, Wei Zhou, Sen Liu, and Zhibo Chen. 2020. LIRA: Lifelong Image Restoration from Unknown Blended Distortions. In Com- puter Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVIII 16. Springer, 616–632. [25] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki Okatani. 2019. Dual residual networks leveraging the potential of paired operations for image restora- tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7007–7016. [26] David Lopez-Paz and Marc’Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. Advances in neural information processing systems 30 (2017), 6467–6476. [27] Arun Mallya and Svetlana Lazebnik. 2018. Packnet: Adding multiple tasks to a single network by iterative pruning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 7765–7773. [28] Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in connec- tionist networks: The sequential learning problem. In Psychology of learning and motivation. Vol. 24. Elsevier, 109–165. [29] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. 2019. Learning to remember: A synaptic plasticity driven framework for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11321–11329. [30] Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie. 2019. Enhanced pix2pix dehazing network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8160–8168. [31] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. 2017. icarl: Incremental classifier and representation learning. In Pro- ceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2001–2010. [32] Amir Rosenfeld and John K Tsotsos. 2018. Incremental learning through deep adaptation. IEEE transactions on pattern analysis and machine intelligence 42, 3 (2018), 651–663. [33] Stefan Roth and Michael J Black. 2009. Fields of experts. International Journal of Computer Vision 82, 2 (2009), 205. [34] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. 2016. Pro- gressive neural networks. arXiv preprint arXiv:1606.04671 (2016). [35] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. 2017. Continual learning with deep generative replay. arXiv preprint arXiv:1705.08690 (2017). [36] Pavel Svoboda, Michal Hradiš, Lukáš Maršík, and Pavel Zemcík. 2016. CNN for license plate motion deblurring. In 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 3832–3836. [37] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya Jia. 2018. Scale- recurrent network for deep image deblurring. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition. 8174–8182. [38] Michalis K Titsias, Jonathan Schwarz, Alexander G de G Matthews, Razvan Pascanu, and Yee Whye Teh. 2020. Functional Regularisation for Continual Learning with Gaussian Processes. In ICLR. [39] Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao. 2017. Image super-resolution using dense skip connections. In Proceedings of the IEEE international conference on computer vision. 4799–4807. [40] Ju Xu and Zhanxing Zhu. 2018. Reinforced Continual Learning. In NeurIPS. [41] Wenhan Yang, Robby T',\n",
       "  'with Gaussian Processes. In ICLR. [39] Tong Tong, Gen Li, Xiejie Liu, and Qinquan Gao. 2017. Image super-resolution using dense skip connections. In Proceedings of the IEEE international conference on computer vision. 4799–4807. [40] Ju Xu and Zhanxing Zhu. 2018. Reinforced Continual Learning. In NeurIPS. [41] Wenhan Yang, Robby T Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. 2017. Deep joint rain detection and removal from a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1357–1366. [42] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. 2021. Multi-stage progressive image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14821–14831. [43] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. 2020. Residual dense network for image restoration. IEEE Transactions on Pattern Analysis and Machine Intelligence 43, 7 (2020), 2480–2495. [44] Man Zhou, Jie Xiao, Yifan Chang, Xueyang Fu, Aiping Liu, Jinshan Pan, and Zheng-Jun Zha. 2021. Image De-Raining via Continual Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 4907–4916.'],\n",
       " ['Accepted to AAAI 2026 BETA DISTRIBUTION LEARNING FOR RELIABLE ROADWAY CRASH RISK ASSESSMENT Ahmad Elallaf1 Nathan Jacobs2 Xinyue Ye3 Mei Chen4 Gongbo Liang1 1Texas A&M University-San Antonio 2Washington University in St. Louis 3University of Alabama 4University of Kentucky {aelallaf, gliang}@tamusa.edu jacobsn@wustl.edu xye10@ua.edu mei.chen@uky.edu https://www.gb-liang.com/projects/betarisk ABSTRACT Roadway traffic accidents represent a global health crisis, responsible for over a million deaths an- nually and costing many countries up to 3% of their GDP. Traditional traffic safety studies often examine risk factors in isolation, overlooking the spatial complexity and contextual interactions in- herent in the built environment. Furthermore, conventional Neural Network-based risk estimators typically generate point estimates without conveying model uncertainty, limiting their utility in crit- ical decision-making. To address these shortcomings, we introduce a novel geospatial deep learning framework that leverages satellite imagery as a comprehensive spatial input. This approach enables the model to capture the nuanced spatial patterns and embedded environmental risk factors that con- tribute to fatal crash risks. Rather than producing a single deterministic output, our model estimates a full Beta probability distribution over fatal crash risk, yielding accurate and uncertainty-aware predictions—a critical feature for trustworthy AI in safety-critical applications. Our model outper- forms baselines by achieving a 17-23% improvement in recall, a key metric for flagging potential dangers, while delivering superior calibration. By providing reliable and interpretable risk assess- ments from satellite imagery alone, our method enables safer autonomous navigation and offers a highly scalable tool for urban planners and policymakers to enhance roadway safety equitably and cost-effectively. 1 Introduction Roadway traffic accidents claim over 1.3 million lives annually [1] and impose economic burdens of 3% of the GDP in many countries [2]. As a critical infrastructure sector [3], transportation safety has garnered significant research [4–6], yet accurately estimating crash risk remains a challenge due to its inherent uncertainties and the sparse nature of crash events. Conventional safety research often analyzes individual factors separately, such as driver behavior [7], road infrastruc- ture [8], traffic patterns [9], and weather [10], overlooking the complex interplay between these elements [11]. Since crash occurrences frequently result from intricate multi-factor interactions, methods that analyze these factors in isola- tion struggle to predict risk holistically [12]. Furthermore, data limitations have constrained the scope of most studies to highways [13–17], leaving comprehensive crash risk analysis for local roads, where data is often less available, relatively unexplored. To overcome these limitations, we introduce a novel deep learning framework that learns a full Beta probability distribution, moving beyond simple point-estimates of fatal crash risk. Our primary contributions are threefold: • A holistic, vision-based model that captures the complex interplay of risk factors embedded in the visual data, in contrast to methods that study variables in isolation. • A probabilistic formulation that yields well-calibrated, uncertainty-aware predictions, a critical feature for trustworthy AI in high-stakes, safety-critical domains. arXiv:2511.04886v1 [cs.CV] 7 Nov 2025 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) • A highly scalable and equitable methodology that uses near-globally available satellite imagery, enabling risk assessment for both highways and previously under-assessed local roads. The proposed probabilistic model',\n",
       "  'trustworthy AI in high-stakes, safety-critical domains. arXiv:2511.04886v1 [cs.CV] 7 Nov 2025 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) • A highly scalable and equitable methodology that uses near-globally available satellite imagery, enabling risk assessment for both highways and previously under-assessed local roads. The proposed probabilistic model is evaluated through extensive experiments conducted over four major metropolitan areas, which have a population of ≈20 million. Our model achieves a 17-23% improvement in recall over baselines, a crucial metric for any safety-critical task, while also delivering superior model calibration and F1 scores. By producing reliable and interpretable risk assessments from satellite imagery alone, this work provides a foundational tool for enhancing traffic safety, from enabling safer route selection for drivers and autonomous vehicles to empowering urban planners and policymakers to mitigate high-risk areas. 2 Background 2.1 Estimate Roadway Crash Risk A primary challenge in data-driven roadway safety is formulating the risk estimation task. Existing methods often frame it as classifications, such as predicting a crash occurrence within a short time frame [9]. While valuable, these approaches do not estimate the inherent, continuous crash risk of a given road segment. A more nuanced approach is to directly estimate a crash probability, such as using Monte Carlo simulations [18–20]. However, this is funda- mentally challenged by the extreme sparsity of crash data. For instance, the average annual accident rate for a 25m2 road segment in the United States is just 0.1% [21]. This level of sparsity renders traditional estimation techniques unreliable, as they can obscure high-risk areas while falsely flagging safe ones [22], leading to false negatives that are dangerous in any safety-critical application. Furthermore, such simulation methods are often ill-suited for large-scale applications due to high computational costs and the need for carefully tuned parameters. Deep Neural Networks (DNNs) offer a powerful alternative, as they can learn complex, task-specific features di- rectly from data and provide near-instantaneous inference. However, supervised DNNs typically rely on large, man- ually labeled datasets, such as manually assigned risk levels (e.g., low, neutral, high) [23]. Creating these datasets is prohibitively expensive, and the manual labels can suffer from human bias, potentially misrepresenting the true risk [24, 25]. These challenges motivate the need for a new approach that can learn a continuous risk score from objective crash data while effectively handling the probabilistic nature of the task. 2.2 Deep Neural Network Miscalibration Over the recent years, DNNs have shown promising performance on various domains, such as medical imaging [26, 27], cybersecurity [28], transportation [29], and astrophysics [30]. However, for a predictive model to be trustworthy in high-stakes applications, its predicted confidence must accurately reflect its probability of being correct. However, modern DNNs are often miscalibrated, tending to produce overconfident predictions [31,32]. Mathematically, a model is perfectly calibrated if, for any given confidence level p, the long-run accuracy of predic- tions with that confidence is indeed p. For DNNs, the calibration error, the difference between a model’s predicted confidence and its actual accuracy, is often significantly greater than zero [33]. This miscalibration is a critical failure point in',\n",
       "  'if, for any given confidence level p, the long-run accuracy of predic- tions with that confidence is indeed p. For DNNs, the calibration error, the difference between a model’s predicted confidence and its actual accuracy, is often significantly greater than zero [33]. This miscalibration is a critical failure point in high-stakes applications where decisions depend on the model’s self-assessed certainty. While various techniques can mitigate this issue, they often have limitations. Post-processing methods like temperature scaling [32] adjust model outputs without altering the learned features, while in-training regularization [34,35] requires careful tuning for the weight scaler. Given that model complexity is a key contributor to miscalibration [36], we argue that an effective solution must be deeply integrated into the learning process. Our work achieves this by reformulating the risk estimation task as learning a full probability distribution, a method that inherently encourages better-calibrated and more reliable predictions. 3 Method 3.1 Probabilistic Modeling Framework Our method recasts roadway crash risk estimation from a standard classification task into a probabilistic learning problem, motivated by the limitations of conventional models that provide a single point-estimation. Consider a fatal crash, a stochastic occurrence, at a specific point in spacetime, C = (x, y, t, d), where (x, y) is the geolocation and (t, d) is the time and date. While any single crash is a random event, its location provides the strongest available evidence for a local maximum in the underlying, continuous risk field, R(·). Therefore, it is intuitive that the inferred risk should be higher at or near the crash site and should decay smoothly as one moves away in space or time. For 2 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Algorithm 1 Target Beta Distribution Generation Require: Original image x, binary label l ∈{0, 1}, base concentration Kbase, minimum positive risk mean µmin, minimum positive concentration kmin, distance weight wdist, size weight wsize, and ϵ = 1e−5 if l = 0 then ▷For negative samples, create a low-risk, ▷high-certainty distribution αt ←ϵ βb ←Kbase else ▷For positive samples (l=1), generate labels ▷based on crop geometry x′ ←random crop of x dnorm ←normalized distance of x′ from center of x snorm ←size(x′) size(x) influence ←wdist · (1 −dnorm) + wsize · snorm µt ←µmin + (1 −µmin) · influence kt ←kmin + (Kbase −kmin) · influence αt ←µt · kt βt ←ϵ end if return (αt, βt) ▷Return the target Beta distribution nearby points, such as a spatially displaced point C′ = (x −δ, y, t, d), the risk should be lower, i.e., R(C′) < R(C). Standard point-estimate classifiers fail to capture this continuous field, as they are trained to predict a binary outcome for each location independently. While a complete model would account for both spatial and temporal decay, this work focuses on the challenging and foundational task of estimating the static, inherent risk of a location based on its geographic and structural features. Our goal is to model the spatial component of this uncertainty by learning a distribution over possible risk values, capturing the intuition that for a',\n",
       "  'this work focuses on the challenging and foundational task of estimating the static, inherent risk of a location based on its geographic and structural features. Our goal is to model the spatial component of this uncertainty by learning a distribution over possible risk values, capturing the intuition that for a nearby point C′, the risk is attenuated but non-zero: 0 < R(C′) < R(C). We specifically employ the Beta distribution for this task due to its natural support on the [0, 1] interval and its flexi- bility in representing diverse risk profiles. Instead of a single value, our model h(x) maps an input image x to the two positive scalar parameters, (α, β), which define a Beta distribution, Pp ∼Beta(α, β). This formulation allows the model to express its uncertainty through the shape of the distribution: a sharp peak indicates high confidence, while a wide distribution signifies high uncertainty. The final risk score R is the mean of this predicted distribution: R = E[Pp] = α α + β . (1) To achieve this, our framework integrates three key technical contributions: 1) a novel procedural labeling technique that generates the targeting Beta distributions from data augmentation, 2) a multi-scale deep neural network architec- ture, and 3) a compound loss function for joint optimization. 3.2 Target Beta Distributions Generation A key innovation of our framework is the procedural generation of supervisory signals in the form of target Beta distributions. Instead of using static labels, we dynamically create a target Beta distribution, Pt ∼Beta(αt, βt), for each training sample based on the properties of the random crop augmentation. Specifically, given an input image, we first apply a random crop. The target Beta distribution is, then, generated using Algorithm 1. This process acts as a sophisticated form of structured label smoothing, transforming data augmentation from a simple regularizer into a rich source of continuous supervision for risk and uncertainty. For negative samples (no crash), the objective is to predict low risk with high confidence. The target distribution is therefore constant: αt is set to a small positive value ϵ and βt is set to a large value representing high certainty Kbase, creating a distribution sharply peaked at zero. For positive samples (crash), the target distribution reflects the quality of the visual evidence in the random crop. This is quantified by an influence score, which modulates the target distribution’s mean and concentration to generate a supervision signal that is proportional to the information content of the augmented image. The score is a weighted combination of two geometric properties of the crop: its centrality relative to the crash location and its size. 3 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Figure 1: Training Architecture with Joint Optimization We set the weights to 0.7 for centrality (wdist) and 0.3 for relative size (wsize). This weighting scheme is based on the strong intuition that the visual features most critical to understanding risk–such as specific road geometry, lane markings, the presence of an intersection, or the surrounding environments–are spatially concentrated around the event’s location. A crop',\n",
       "  'for centrality (wdist) and 0.3 for relative size (wsize). This weighting scheme is based on the strong intuition that the visual features most critical to understanding risk–such as specific road geometry, lane markings, the presence of an intersection, or the surrounding environments–are spatially concentrated around the event’s location. A crop that is well-centered on the crash point provides the clearest and most relevant evidence, thus deserving a higher influence score and a more confident target distribution. The relative size of the crop provides useful, but secondary, broader context about the surrounding environment. This principled approach transforms data augmentation into a rich source of supervision, teaching the model to dynamically associate higher risk and confidence with visual samples that contain the most informative evidence. This influence score then modulates the target mean µmin and the target concentration kt, which in turn define the final Beta parameters. For positive samples, the βt is set to the small constant ϵ, ensuring the distribution is always skewed towards high risk, with the influence score controlling the precise shape and confidence (see the supplement materials for the hyperparameters used in this study). 3.3 Model Architecture The architecture of our model, illustrated in Figure 1, is designed to process multi-scale satellite imagery. During training, a random crop is sampled from the input, which consists of image slices of the same location at different resolutions. The cropped images are, then, passed through a shared feature extractor backbone to produce multiple corresponding feature maps. These maps are concatenated along the channel dimension to form a unified feature representation, serving as the input for two parallel prediction heads: • A Distribution Learning Head, which outputs the two Beta parameters (α, β). • An auxiliary Classification Head, which outputs a single logit for the binary crash/no-crash task. 3.4 Training and Optimization The model is trained end-to-end by jointly optimizing the two parallel heads with a compound loss function. The primary distribution learning head is supervised by the a mean-variance loss that inspired by the squared Wasserstein-2 (W 2 2 ) distance [37], which measures the dissimilarity between the predicted (Pp) and the target (Pt) Beta distributions: LW 2 2 (Pp, Pt) = (µp −µt)2 + (σp −σt)2, (2) where the µ and σ are the mean and standard deviation. We empirically selected this W 2 2 surrogate over true W 2 2 distance and other distribution divergence metrics, including KL-Divergence [38] and the Cramér-von Mises criterion [39]. As a true metric, our W 2 2 surrogate loss provides a 4 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Figure 2: Streamlined Architecture for Inference more stable gradient than KL-Divergence, especially when the predicted and target distributions have little overlap. Most importantly, for one-dimensional distributions like the Beta, the W 2 2 surrogate loss directly optimize of the risk score (the mean) and confidence level (the standard deviation) simultaneously. Our experimental analysis also shows this surrogate is a close approximation of the true W 2 2 (errors on the order of 10−3 to 10−2), deviating only in extreme cases (see',\n",
       "  '2 2 surrogate loss directly optimize of the risk score (the mean) and confidence level (the standard deviation) simultaneously. Our experimental analysis also shows this surrogate is a close approximation of the true W 2 2 (errors on the order of 10−3 to 10−2), deviating only in extreme cases (see the supplementary materials). The auxiliary classification head is supervised by a Binary Cross-Entropy loss, which encourages the shared backbone to learn discriminative features relevant to the safety task: LBCE = −1 N N X i=1 [yi log(pi) + (1 −yi) log(1 −pi)] , (3) where yi and pi are the label and predicted probability. The overall objective function is a weighted combination of the two losses, balanced by hyperparameters, λ1 and λ2: L = λ1 · LBCE + λ2 · LW 2 2 . (4) 3.5 Inference Process The inference process, illustrated in Figure 2, is direct and computationally efficient. The random crop augmentation and the auxiliary classification head are removed. The full, uncropped multi-scale image is passed through the feature extractor backbone and the distribution head. The risk score R is calculated as the mean of the distribution, per Equation 1. This feed-forward process allows for rapid and scalable risk assessment of any location. 4 Experiment Setup This study utilizes the MSCM dataset [40], a large-scale collection of multi-scale satellite images from Texas, USA, with 16,451 locations labeled with historical fatal crashes. All models use a ResNet-50 [41] backbone, λ1 = 5, λ2 = 1, and were trained on NVIDIA A100 GPUs. See the supplementary materials for more information about the dataset, implementation details, and hyperparameter analysis for the selection of λ1 and λ2. 4.1 Evaluation Methodology Quantitative Metrics We first evaluate our model’s practical effectiveness by framing the risk estimation as a binary classification task to identify historical crash locations. The model’s predicted risk score R, derived from Equation 1 is thresholded at 0.5 to yield a binary prediction. We then assess the model’s predictive performance using standard metrics: F1-Score, Precision, Recall, AUC (Area Under the Receiver Operating Characteristic curve), and PRC (area under the precision-recall curve); and assess model’s calibration using Expected Calibration Error (ECE) and Brier score. Due to safety-oriented, we consider Recall to be the most critical metric that answers the question: “Of all crash locations, what fraction did our model successfully identify?\" 5 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Table 1: Main Quantitative Results (bold: best performance; underlined: second best performance) Methods Pre-Train Probabilistic Multi-Scale Performance (↑) Uncertainty (↓) F1 Precision Recall AUC PRC ECE Brier ImageNet ImageNet ✗ ✗ 0.4753 0.4968 0.4555 0.7980 0.4862 0.1281 0.1600 MSCM-SS MSCM ✗ ✗ 0.4966 0.4981 0.4950 0.8165 0.5185 0.1006 0.1458 MSCM-MS MSCM ✗ ✓ 0.5409 0.6731 0.4521 0.8572 0.6269 0.1067 0.1296 Prob-SS (Ours) MSCM ✓ ✗ 0.5001 0.4252 0.6070 0.7749 0.4409 0.1731 0.1922 Prob-MS (Ours) MSCM ✓ ✓ 0.5762 0.6296 0.5311 0.8663 0.6489 0.0881 0.1211 We also evaluate our method against a Deep Ensemble (DE) of the strongest baseline, constructed from three inde- pendent training runs. The final predicted risk score of a',\n",
       "  'MSCM ✓ ✗ 0.5001 0.4252 0.6070 0.7749 0.4409 0.1731 0.1922 Prob-MS (Ours) MSCM ✓ ✓ 0.5762 0.6296 0.5311 0.8663 0.6489 0.0881 0.1211 We also evaluate our method against a Deep Ensemble (DE) of the strongest baseline, constructed from three inde- pendent training runs. The final predicted risk score of a DE model, RDE, is calculated as the mean of the predictions from each individual model in the ensemble. This single score for each sample is then used to compute all the afore- mentioned performance and calibration metrics. The ensemble’s predictive uncertainty is quantified in two ways: the variance of the risk scores and the disagreement rate among the final binary predictions. A higher value in either metric reflects greater disagreement among the models and thus higher uncertainty in the final prediction. Qualitative Analysis To intuitively understand the value of our probabilistic approach, we conduct a qualitative analysis of the model’s outputs from two perspectives. First, we analyze the aggregate behavior of the model’s outputs by comparing the overall distribution of predicted probabilities from our model against the baselines. By plotting a histogram of all risk scores, we can visually assess model confidence. A well-calibrated, uncertainty-aware model is expected to utilize the full [0, 1] probability range, whereas overconfident models will show predictions heavily clus- tered at the extremes (near 0 and 1). Second, we visualize the predicted Beta distributions for four distinct scenarios: true positives (TP), true negatives (TN), false negatives (FN), and false positives (FP). The goal of this analysis is to provide an intuitive understanding of the model’s behavior by interpreting its successes and failures. Case Study: San Antonio River Walk To demonstrate the model’s utility, we conduct a case study of the San Antonio River Walk, providing practical insight into the model’s performance in a challenging, safety-critical area. 4.2 Baseline Models We evaluate our method against three baselines to isolate our framework’s contributions. Our primary benchmark is the Multi-Scale Cross-Matching (MSCM) model [40], the current state-of-the-art for fatal crash risk estimation using only satellite imagery. ImageNet Baseline: A standard model pre-trained on ImageNet [42] that takes single-scale satellite images as input, providing us the performance of a generic, non-domain-specific feature extractor on our task. MSCM-SS (Single-Scale): The same single-scale architecture but using weights generated by the self-supervised pre- training through cross-matching, proposed in the MSCM paper, to test the value of domain-specific features. MSCM-MS (Multi-Scale): The full MSCM model, which uses both its domain-specific pre-training and multi-scale imagery as input, represents the strongest available baseline, allowing us to compare against the current state-of-the-art classification approach directly. The best checkpoint for each model was selected based on the model accuracy on the validation set. 5 Results 5.1 Quantitative Analysis Table 1 summarizes the quantitative results. The single-scale baselines (ImageNet and MSCM-SS) achieve a < 0.5 precision and recall scores, indicating their predictions for positive cases are close to random and exhibit little ability to identify high-risk areas. While the MSCM-MS model achieves high precision (0.6731), its poor recall (0.4521) means it fails to identify over half of all crash locations,',\n",
       "  'MSCM-SS) achieve a < 0.5 precision and recall scores, indicating their predictions for positive cases are close to random and exhibit little ability to identify high-risk areas. While the MSCM-MS model achieves high precision (0.6731), its poor recall (0.4521) means it fails to identify over half of all crash locations, rendering it unreliable for safety-critical applications. 6 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Table 2: Deep Ensemble Results over Three Training Trails (bold: best performance) Methods Performance (↑) Uncertainty (↓) Disagreement (↓) F1 Precision Recall AUC PRC ECE Brier Variance Disagr. Rate Ensemble MSCM-MS 0.5966 0.7062 0.5165 0.8839 0.6890 0.0787 0.1112 0.0925 16.93% Ensemble Prob-MS (Ours) 0.5976 0.6750 0.5361 0.8761 0.6886 0.0605 0.1075 0.0822 15.14% Figure 3: Uncertainty Quantification and Interpretability In contrast, our models demonstrate a significant improvement in identifying potential dangers. Our multi-scale model, Prob-MS, achieves the best overall balance of performance, attaining the highest F1-score. Its most significant contri- bution is boosting the recall to 0.5311, a 17% relative improvement over MSCM-MS, drastically reducing the number of hazardous sites that would be missed. Our single-scale model, Prob-SS (0.6070 recall score), significantly improves the metric by 23% over the best baseline (0.4950). Crucially, Prob-MS is also the most trustworthy model, achieving the lowest (best) ECE of 0.881 and Brier of 0.1211. This confirms that our model’s probabilistic outputs are more statistically sound and reliable for real-world decision- making. We also evaluate our method against a Deep Ensemble of the strongest baseline (Table 2). When comparing our single Prob-MS model against the baseline Ensemble MSCM-MS, we find that our single model achieves competitive performance, including a 3% higher recall, better calibration, and lower uncertainty at only 1/3 the computational cost at both training and inference times. This highlights the efficiency and practical advantage of our approach. In an apples-to-apples comparison between ensembled methods, our Ensemble Prob-MS demonstrates the clear su- periority of our probabilistic framework. It outperforms the baseline ensemble on the most critical metrics for this task, achieving a higher F1-score and recall. Most importantly, it is significantly better calibrated and exhibits lower uncertainty, as evidenced by its superior (lower) ECE, Brier, Variance, and Disagreement Rate scores. 5.2 Qualitative Analysis Our qualitative analysis highlights the superior interpretability and trustworthiness of our probabilistic framework. As shown in Figure 3, our model provides a comprehensive and practical understanding of risk that standard classifiers cannot offer. The “Beta Model Uncertainty\" plot (left) confirms the model’s rational behavior, showing that prediction uncertainty is lowest for highly confident predictions and highest for ambiguous ones around a 0.5 risk score. The “Confidence Intervals\" plot (center) demonstrates that every prediction is accompanied by a 95% confidence interval, with the interval’s width directly communicating the model’s certainty on a per-prediction basis. Finally, the “Predic- tion Interpretability\" table (right) crystallizes this key advantage, showing how our Beta model resolves the ambiguity of a baseline’s “Risk: 0.50\" output by distinguishing between a low-confidence prediction (e.g., with α = 10, β = 10) and a very uncertain one (e.g., with α = 2, β = 2).',\n",
       "  '“Predic- tion Interpretability\" table (right) crystallizes this key advantage, showing how our Beta model resolves the ambiguity of a baseline’s “Risk: 0.50\" output by distinguishing between a low-confidence prediction (e.g., with α = 10, β = 10) and a very uncertain one (e.g., with α = 2, β = 2). This additional context is invaluable for any safety-critical application. This nuanced, per-prediction behavior leads to a more rational distribution of predictions in aggregate (Figure 4). While baseline models behave like overconfident black boxes with predictions heavily clustered at the extremes of 0 and 1, our model utilizes the full probability spectrum to express varying degrees of certainty. This ability to be “less confident\" is not a weakness but a hallmark of a more honest and trustworthy risk assessment tool. 7 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Figure 4: Analysis of Predicted Probability Distributions (a) (b) Figure 5: Qualitative Results for Unambiguous (“Easy\") Cases (a: True Negatives, b: True Positives) 5.3 Visualizing Model Uncertainty To be a trustworthy tool for risk assessment, a model must not only make accurate predictions but also provide a reliable measure of its own uncertainty. We visualize this uncertainty using a Beta distribution for each prediction. As shown in Figure 5, our model demonstrates well-calibrated confidence across a spectrum of cases, a crucial feature for real-world deployment. For visually unambiguous locations, the model produces predictions with high confidence. For example, in a simple suburban neighborhood (Figure 5a, left), it predicts a low risk (0.051) with a correspondingly low uncertainty score (0.006), reflected in a sharp Beta distribution. Likewise, for a coastal road with high traffic density and high potential of distractions (Figure 5b, left), it correctly predicts a high risk (0.924) with high confidence (uncertainty of 0.010). The model’s utility is further demonstrated in more complex scenarios where it appropriately reduces its confidence. For a visually complex but safe highway overpass (Figure 5a, right), the model still correctly predicts low risk, but the wider Beta distribution indicates higher uncertainty. This nuanced confidence is also evident when assessing a complex highway interchange (Figure 5b, right); the model correctly predicts a high risk of 0.717, but acknowledges the significant uncertainty due to the challenging visual features. 8 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) (a) (b) Figure 6: Interpreting Model Behavior on Ambiguous (“Hard\") Cases (a: False Negative, b: False Positive) Crucially, the model’s rational expression of uncertainty extends to its failures, a characteristic vital for establishing trust. For false negatives (Figure 6a), where the model misses a crash, the low-risk predictions are consistently paired with wider, higher-uncertainty distributions. This correctly signals that the visual evidence was ambiguous, contain- ing conflicting features (e.g., Figure 6a Left: an arterial road with many intersections within an otherwise low-risk residential area). Similarly, for false positives (Figure 6b), the model flags locations as high-risk despite no recorded crashes, but again with reduced confidence. This behavior is highly interpretable, as the model correctly identifies latent risk factors, such as sharp (L-shape) turns or high-density highways. The',\n",
       "  'intersections within an otherwise low-risk residential area). Similarly, for false positives (Figure 6b), the model flags locations as high-risk despite no recorded crashes, but again with reduced confidence. This behavior is highly interpretable, as the model correctly identifies latent risk factors, such as sharp (L-shape) turns or high-density highways. The prediction thus reflects a successful identification of hazardous features, while the increased uncertainty correctly marks them as borderline cases. This ability to temper certainty in response to visual complexity, especially when incorrect, distinguishes our model as a more reliable and interpretable system for risk assessment. 5.4 Case Study To demonstrate the practical utility of our model, we conducted a case study of the San Antonio River Walk, a major tourist destination that presents a challenging environment with a complex mix of vehicular, pedestrian, and cyclist traffic. We generated risk predictions for over 140 locations in this area using Prob-MS and MSCM-MS. The results, shown in Figure 7, highlight the superior performance of our approach. The baseline MSCM-MS model (middle panel) fails to identify close to half of the historical fatal crash locations (red diamonds), assigning them erro- neously low risk scores. The baseline’s predictions also lack spatial coherence, exhibiting sharp, unrealistic gradients between adjacent points and producing polarized risk scores with few intermediate values. In contrast, our Prob-MS model (right panel) correctly assigns elevated risk scores (yellow and orange) to a greater number of the known crash sites. This is exemplified at the intersection near Navarro St and Villita St, a known fatal crash location at the bottom-right in the map. While the baseline model misses this site, ours correctly assigns the area a high-risk score. An analysis of the satellite and ground-level imagery reveals a confluence of latent risk factors not apparent from an overhead view alone. The location, a major entry point to the River Walk, is surrounded by numerous parking facilities. Ground-level images (Figure S1) show that these structures, combined with dense trees and building columns, create significant visual obstructions and blind spots for both drivers and pedestrians. This environment forces complex interactions: vehicles constantly enter and exit parking garages across wide pedestrian walkways as tourists navigate narrow sidewalks. Our model likely learned to associate this specific combination of visual clutter, unpredictable vehicle maneuvers, and high pedestrian-vehicle conflict with an elevated risk of a fatal crash. Furthermore, our model generates a more nuanced and spatially coherent risk map where predictions transition smoothly across locations. This case study demonstrates that our model’s strong quantitative performance translates into more reliable, interpretable, and actionable safety assessments for complex urban environments. 9 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Figure 7: A case study of crash risk assessment for the San Antonio River Walk. Historical fatal crashes (red diamonds) serve as ground truth. (Left) The baseline MSCM-MS model exhibits low recall and spatially inconsistent predictions, with abrupt risk changes between adjacent points. (Right) Our Prob-MS model demonstrates superior recall by cor- rectly identifying more crash sites and generates a more nuanced and spatially coherent risk field, providing a more',\n",
       "  'serve as ground truth. (Left) The baseline MSCM-MS model exhibits low recall and spatially inconsistent predictions, with abrupt risk changes between adjacent points. (Right) Our Prob-MS model demonstrates superior recall by cor- rectly identifying more crash sites and generates a more nuanced and spatially coherent risk field, providing a more realistic safety assessment. See the Result section for more. 6 Discussion and Conclusion Our evaluation demonstrates that the proposed probabilistic framework yields a risk assessment model that is not only more effective but also more reliable and interpretable than deterministic baselines. By predicting a full Beta probability distribution instead of a single point-estimate, our model learns a more nuanced and less overconfident representation of risk. This trustworthiness is reinforced by its interpretable behavior; the model’s “mistakes\" are often rational, such as flagging visually complex but historically safe highway interchanges as high-risk. This capac- ity to reason about visual factors and express nuanced confidence is highly valuable for practical applications, from enabling more sophisticated path planning for autonomous vehicles to allowing urban planners to confidently priori- tize infrastructure improvements. Furthermore, by relying solely on publicly available satellite imagery, our method circumvents the significant privacy concerns associated with other data sources 6.1 Ethical Considerations and Responsible Deployment The ethical implications of deploying an AI tool for public safety are significant. As historical crash data may contain undiscovered biases, such as under-reporting in certain socioeconomic or geographic areas, a model used without critical oversight could perpetuate inequities. We therefore emphasize that this model is designed as a decision-support tool to augment, not replace, human expertise. A key feature for responsible, human-in-the-loop deployment is the model’s ability to signal its own uncertainty, which can serve as a bias and fairness mitigation tool. High uncertainty in any prediction (whether high-risk or low- risk) can flag regions with potential data disparities or under-reporting. For instance, a visually complex area with high 10 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) uncertainty and a low-risk prediction may indicate a dangerous false negative due to a lack of historical crash data. These uncertain predictions should act as a flag for human experts to conduct a more detailed investigation, enabling a more equitable allocation of safety resources. 6.2 Limitations This study has several limitations that open avenues for future research. Our model estimates static risk and does not account for dynamic variables like real-time traffic or weather; future work should focus on integrating these data streams. Our study is also geographically constrained to Texas, and validation on diverse international datasets is a critical next step to ensure generalizability. Furthermore, this work can be extended by exploring a learned weighting mechanism for the centrality and size components of our procedural labeling scheme. Finally, while our model identifies strong correlations, future work could explore methods for moving toward causal inference. In conclusion, this work demonstrates that moving from deterministic point-estimates to a full probabilistic framework is a crucial step toward creating more reliable and trustworthy AI for public safety. By learning to predict a Beta probability distribution from satellite imagery,',\n",
       "  'future work could explore methods for moving toward causal inference. In conclusion, this work demonstrates that moving from deterministic point-estimates to a full probabilistic framework is a crucial step toward creating more reliable and trustworthy AI for public safety. By learning to predict a Beta probability distribution from satellite imagery, our model not only outperforms existing baselines in identifying high- risk locations but also provides the well-calibrated uncertainty estimates that are vital for interpretable, human-in-the- loop decision-making in applications from urban planning to autonomous navigation. 7 Conclusion This work presents a deep learning framework for reliable roadway risk assessment that quantifies uncertainty. Instead of a single risk score, our model predicts a full Beta probability distribution to provide a more comprehensive hazard assessment. This is achieved using a procedural labeling technique with data augmentation to supervise uncertainty, and a compound loss function that jointly optimizes for classification accuracy and probabilistic calibration. Our model significantly outperforms existing baselines, with a 17-23% relative improvement in recall on high-risk locations, up to 17% in ECE on calibration, and about 11% more stable. More importantly, it yields interpretable predictions, reliably signaling its uncertainty in ambiguous cases. By delivering a more robust and trustworthy as- sessment of roadway risk, our work represents a crucial step toward the responsible deployment of AI in high-stakes applications, such as public safety, urban planning, and autonomous navigation. Acknowledgement This material is partially based upon work supported by the National Science Foundation under 2401860 and 2526487. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author and the funders have no role in the study design, data collection, analysis, or preparation of this article. Portions of this research were conducted with the advanced computing resources provided by the High Performance Computing Research Center at Texas A&M University-San Antonio. References [1] WHO, “Road traffic injuries,” World Health Organization, 2023, accessed: 2025-05-22. [Online]. Available: https://www.who.int/news-room/fact-sheets/detail/road-traffic-injuries [2] ——, “Global status report on road safety,” World Health Organization, 2018, accessed: 2025-05-22. [Online]. Available: https://www.who.int/publications/i/item/9789241565684 [3] CISA, “Critical infrastructure sectors,” Cybersecurity and Infrastructure Security Agency, 2024, accessed: 2024-03-22. [Online]. Available: https://www.cisa.gov/topics/critical-infrastructure-security-and-resilience/ critical-infrastructure-sectors [4] C. Caliendo et al., Accident Analysis & Prevention, vol. 39, no. 4, pp. 657–670, 2007. [5] J. Tamerius, X. Zhou, R. Mantilla, and T. Greenfield-Huitt, “Precipitation effects on motor vehicle crashes vary by space, time, and environmental conditions,” Weather, Climate, and Society, vol. 8, no. 4, pp. 399–407, 2016. [6] C. Zhu, B. Dadashova, C. Lee, X. Ye, and C. T. Brown, “Equity in non-motorist safety: Exploring two pathways in houston,” Transportation research part D: transport and environment, vol. 132, p. 104239, 2024. [7] B. G. Simons-Morton, F. Guo, S. G. Klauer, J. P. Ehsani, and A. K. Pradhan, “Keep your eyes on the road: Young driver crash risk increases according to duration of distraction,” Journal of Adolescent Health, vol. 54, no. 5, pp. S61–S67, 2014. 11 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) [8] A. Pembuain et al., “The effect of road infrastructure on traffic accidents,” in 11th Asia Pacific Transportation and the',\n",
       "  'risk increases according to duration of distraction,” Journal of Adolescent Health, vol. 54, no. 5, pp. S61–S67, 2014. 11 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) [8] A. Pembuain et al., “The effect of road infrastructure on traffic accidents,” in 11th Asia Pacific Transportation and the Environment Conference (APTE 2018). Atlantis Press, 2019, pp. 176–182. [9] T. Huang et al., “Highway crash detection and risk estimation using deep learning,” Accident Analysis & Preven- tion, vol. 135, p. 105392, 2020. [10] D. Jaroszweski and T. McNamara, “The influence of rainfall on road accidents in urban areas: A weather radar approach,” Travel behaviour and society, vol. 1, no. 1, pp. 15–21, 2014. [11] C. Gu, J. Xu, C. Gao, M. Mu, G. E, and Y. Ma, “Multivariate analysis of roadway multi-fatality crashes using association rules mining and rules graph structures: A case study in china,” Plos one, vol. 17, no. 10, p. e0276817, 2022. [12] C. Carrodano, “Data-driven risk analysis of nonlinear factor interactions in road safety using bayesian networks,” Scientific Reports, vol. 14, no. 1, p. 18948, 2024. [13] I. Ahmed, “Road infrastructure and road safety,” Transport and Communications Bulletin for Asia and the Pacific, vol. 83, pp. 19–25, 2013. [14] W. Song, S. Workman, A. Hadzic, X. Zhang, E. Green, M. Chen, R. Souleyrette, and N. Jacobs, “Farsa: Fully automated roadway safety assessment,” in 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 2018, pp. 521–529. [15] G. Cheng, R. Cheng, S. Zhang, and X. Sun, “Risk evaluation method for highway roadside accidents,” Advances in Mechanical Engineering, vol. 11, no. 1, p. 1687814018821743, 2019. [16] Q. Ma, H. Yang, Z. Wang, K. Xie, and D. Yang, “Modeling crash risk of horizontal curves using large-scale auto-extracted roadway geometry data,” Accident Analysis & Prevention, vol. 144, p. 105669, 2020. [17] Y.-J. Joo et al., “A generalized driving risk assessment on high-speed highways using field theory,” Analytic Methods in Accident Research, vol. 40, p. 100303, 2023. [18] V. de Almeida Guimarães et al., “Evaluating the sustainability of urban passenger transportation by monte carlo simulation,” Renewable and Sustainable Energy Reviews, vol. 93, pp. 732–752, 2018. [19] L. Al-Sharif et al., “The use of monte carlo simulation in evaluating the elevator round trip time under up-peak traffic conditions and conventional group control,” Building Services Engineering Research and Technology, vol. 33, no. 3, pp. 319–338, 2012. [20] S. Jeon and B. Hong, “Monte carlo simulation-based traffic speed forecasting using historical big data,” Future generation computer systems, vol. 65, pp. 182–195, 2016. [21] S. Moosavi, M. H. Samavatian, S. Parthasarathy, R. Teodorescu, and R. Ramnath, “Accident risk prediction based on heterogeneous sparse data: New dataset and insights,” in Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, 2019, pp. 33–42. [22] S. He, M. A. Sadeghi, S. Chawla, M. Alizadeh, H. Balakrishnan, and S. Madden, “Inferring high-resolution traffic accident risk maps based on satellite imagery and gps trajectories,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11 977–11 985. [23] A. Najjar et al., “Combining',\n",
       "  '33–42. [22] S. He, M. A. Sadeghi, S. Chawla, M. Alizadeh, H. Balakrishnan, and S. Madden, “Inferring high-resolution traffic accident risk maps based on satellite imagery and gps trajectories,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 11 977–11 985. [23] A. Najjar et al., “Combining satellite imagery and open data to map road safety,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no. 1, 2017. [24] Y. Li et al., “Label bias: A pervasive and invisibilized problem,” Notices of the American Mathematical Society, vol. 71, no. 8, pp. 1069–1077, 2024. [25] C. Chen and S. S. Sundar, “Is this ai trained on credible data? the effects of labeling quality and performance bias on user trust,” in Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 2023, pp. 1–11. [26] X. Xing, G. Liang, C. Wang, N. Jacobs, and A.-L. Lin, “Self-supervised learning application on covid-19 chest x-ray image classification using masked autoencoder,” Bioengineering, vol. 10, no. 8, p. 901, 2023. [27] L. Liu, Y. Wang, J. Chang, P. Zhang, G. Liang, and H. Zhang, “Llrhnet: multiple lesions segmentation using local-long range features,” Frontiers in Neuroinformatics, vol. 16, p. 859973, 2022. [28] J. Zulu, B. Han, I. Alsmadi, and G. Liang, “Enhancing machine learning based sql injection detection using contextualized word embedding,” in Proceedings of the 2024 ACM Southeast Conference, 2024, pp. 211–216. [29] R. Jonnala, G. Liang, J. Yang, and I. Alsmadi, “Exploring the potential of large language models in public transportation: San antonio case study,” 2025. 12 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) [30] S.-C. Lin, Y. Su, G. Liang, Y. Zhang, N. Jacobs, and Y. Zhang, “Estimating cluster masses from sdss multiband images with transfer learning,” Monthly Notices of the Royal Astronomical Society, vol. 512, no. 3, pp. 3885– 3894, 2022. [31] G. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hinton, “Regularizing neural networks by penalizing confident output distributions,” in International Conference on Learning Representations, 2017. [32] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, “On calibration of modern neural networks,” in International conference on machine learning. PMLR, 2017, pp. 1321–1330. [33] G. E. Hinton et al., “Distilling the knowledge in a neural network,” ArXiv, vol. abs/1503.02531, 2015. [34] A. Kumar et al., “Trainable calibration measures for neural networks from kernel mean embeddings,” in Inter- national Conference on Machine Learning, 2018, pp. 2805–2814. [35] G. Liang, Y. Zhang, X. Wang, and N. Jacobs, “Improved trainable calibration method for neural networks on medical imaging classification,” in British Machine Vision Conference (BMVC), 2020. [36] M. Chidambaram and R. Ge, “On the limitations of temperature scaling for distributions with overlaps,” in International Conference on Learning Representations, 2023. [37] L. N. Vaserstein, “Markov processes over denumerable products of spaces, describing large systems of automata,” Problemy Peredachi Informatsii, vol. 5, no. 3, pp. 64–72, 1969. [38] I. Csiszár, “I-divergence geometry of probability distributions and minimization problems,” The annals of prob- ability, pp. 146–158, 1975. [39] H. Cramér, “On the composition of elementary errors,” Scandinavian Actuarial',\n",
       "  'over denumerable products of spaces, describing large systems of automata,” Problemy Peredachi Informatsii, vol. 5, no. 3, pp. 64–72, 1969. [38] I. Csiszár, “I-divergence geometry of probability distributions and minimization problems,” The annals of prob- ability, pp. 146–158, 1975. [39] H. Cramér, “On the composition of elementary errors,” Scandinavian Actuarial Journal, vol. 1928, no. 1, pp. 13–74, 1928. [40] G. Liang, J. Zulu, X. Xing, and N. Jacobs, “Unveiling roadway hazards: Enhancing fatal crash risk estimation through multiscale satellite imagery and self-supervised cross-matching,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 17, pp. 535–546, 2024. [41] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778. [42] A. Krizhevsky et al., “Imagenet classification with deep convolutional neural networks,” Advances in neural information processing systems, vol. 25, 2012. [43] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” arXiv preprint arXiv:1711.05101, 2017. [44] ——, “Sgdr: Stochastic gradient descent with warm restarts,” arXiv preprint arXiv:1608.03983, 2016. 13 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Supplementary Materials San Antonio River Walk Ground-Level Imagery Figure S1 shows the ground-level image at 146-Navarro-St, San Antonio, TX, USA, one main entry point to the San Antonio River Walk area. A previous fatal crash also occurred at this location (i.e., the bottom-right one in Figure 7). (a) North (b) West (c) South (d) East Figure S1: Ground-level images for the four directions at 146-Navarro-St, San Antonio, TX, USA, one main entry point to the San Antonio River Walk area. Dataset This study utilizes the comprehensive, multi-scale satellite imagery dataset provided by MSCM [40]. The dataset covers diverse regions in Texas, USA, including the Gulf Coast, Hill Country, and Prairies and Lakes regions. The dataset contains a total of 240,828 satellite images. The images for each location are provided at three distinct levels of detail, each with a resolution of 768 × 768 pixels. The images for each location are provided at three distinct levels of detail: 1.1943 m/pixel, 0.5972 m/pixel, and 0.2986 m/pixel. Examples of this multi-scale imagery are shown in Figure S2. The data is sampled from 80,276 distinct locations, which are categorized into positive and negative classes. The positive class consists of 16,451 locations where at least one fatal crash occurred between 2010 and 2020; of these, 1,185 locations experienced multiple fatal crashes within a 50-meter radius. The remaining locations serve as the negative class, having no recorded fatal crashes through the end of 2020. These negative samples were selected using specific criteria, ensuring they were within 1250 meters of a fatal crash location but at least 250 meters away from any such site. To create a challenging learning environment, approximately 70% of the negative samples were designated as hard negatives by sampling them along primary and secondary roads. The other 30% were sampled randomly to represent a broader variety of environments, including open spaces. 14 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Figure',\n",
       "  'learning environment, approximately 70% of the negative samples were designated as hard negatives by sampling them along primary and secondary roads. The other 30% were sampled randomly to represent a broader variety of environments, including open spaces. 14 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Figure S2: Multi-Scale Satellite Imagery Inputs with Three Detail Levels. From left to right: 1.1943 m/pixel, 0.5972 m/pixel, and 0.2986 m/pixel. Figure S3: Comparison between true and surrogate Wasserstein-2 distance for Beta distributions. Left: Absolute Error. Right: Relative Error. Wasserstein-2 Surrogate Analysis To evaluate the accuracy of our surrogate Wasserstein-2 loss, we computed the true squared Wasserstein-2 distance between a fixed target distribution Beta(2, 5) and a range of predicted Beta distributions with α, β ∈[0.5, 10]. For each pair of predicted parameters, the true distance was estimated via numerical integration of the quantile functions, while the surrogate distance was computed using the closed-form mean–variance expression defined in our loss. The resulting absolute and relative differences are visualized in Figure S3. Both plots demonstrate that the surrogate closely matches the true distance, with errors typically on the order of 10−3 to 10−2 and only slightly increasing in extreme parameter regions. 15 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Implementation Details All models are built upon a ResNet-50 backbone and trained for 75 epochs using the AdamW [43] optimizer with a CosineAnnealingWarmRestarts [44] learning rate schedule. The initial learning rate for the backbone and classifier was set to 1e−4, while the distribution learning head used a rate of 0.02. The batch size was 128 for single-scale models and 48 for multi-scale models. Based on our hyperparameter analysis, the final weights for our compound loss function were set to λ1 = 5 and λ2 = 1 to prioritize recall for this safety-critical task. The same data augmentation pipeline are used for all models. The random seeds are set to 0. All experiments were conducted on two NVIDIA A100 GPUs. The rest of this section, provided the complete list of hyperparameters and data augmentation settings. Model Architecture • Backbone: All models use a ResNet-50 architecture with weights pre-trained on ImageNet. • Modification: A 1x1 convolutional layer was inserted before the final Global Average Pooling . MSCM Pre-training and Pre-trained Checkpoint Selection The MSCM weights, used to initialize our proposed model and the corresponding baselines, were generated by fol- lowing the pre-training procedure described in the original work. • Setup: The pre-training used a contrastive learning approach with an InfoNCE and classification loss. It was run for 25 epochs with a batch size of 64, using the AdamW optimizer with a learning rate of 10−3 and a CosineAnnealingWarmRestarts schedule (T_0=10, T_mult=2). • Checkpoint Selection: A model checkpoint was saved after each pre-training epoch. To select the optimal checkpoint, each of the 25 checkpoints was used to initialize a single-scale classification model, which was then fine-tuned for 5 epochs on the downstream task (batch size 128, AdamW, learning rate 10−4, CosineAn- nealingWarmRestarts schedule). The checkpoint that yielded the best performance after this short fine-tuning',\n",
       "  'To select the optimal checkpoint, each of the 25 checkpoints was used to initialize a single-scale classification model, which was then fine-tuned for 5 epochs on the downstream task (batch size 128, AdamW, learning rate 10−4, CosineAn- nealingWarmRestarts schedule). The checkpoint that yielded the best performance after this short fine-tuning process was selected for all main experiments. Main Training for Proposed Probabilistic Models • Optimizer: AdamW. • Learning Rates (LR): We used different learning rates for distinct parts of the model: – Feature Extractor Backbone: 10−4 – Beta Distribution Learning Head: 0.02 – Auxiliary Classification Head: 10−4 • LR Schedule: CosineAnnealingWarmRestarts with scheduler parameters T_0=10 and T_mult=2. • Epochs: All models were trained for 75 epochs. The epoch with the highest accuracy on the test set is chosen as the best model. • Batch Size: 128 for single-scale models; 48 for multi-scale models. Loss Function and Hyperparameters • Compound Loss Weights: The reported results use λ1 = 5 (for BCE loss) and λ2 = 1 (for Wasserstein loss). • BCE Loss: Class imbalance was handled using inverse frequency weights applied to the BCE loss: [1.25948, 4.85382]. • Procedural Beta Distribution Parameters: – base_K: 22.0 – ϵ: 0.08 – min_positive_risk_mean: 0.18 – min_concentration_positives: 18.0 – Influence Score Weights: weight_distance=0.7, weight_crop_size=0.3. 16 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) Figure S4: Hyperparameter Analysis: Precision-Recall Trade-off Table S1: Ablation Study on Loss Weights λ1 λ2 F1 Score (↑) Precision (↑) Recall (↑) 10 1 0.5880 0.5358 0.6514 5 1 0.5981 0.5607 0.6409 1 1 0.5905 0.5406 0.6505 1 5 0.5979 0.5932 0.6026 1 10 0.5855 0.5577 0.6163 Data Augmentation The augmentation pipelines were used for our proposed models versus the baselines. • For Proposed Probabilistic Models: – Random Crop Ratio: (0.5, 1.0) – Random horizontal flip (p=0.5) – Random vertical flip (p=0.5) – Random rotations (from -90 to 90 degrees) – ColorJitter (brightness/contrast/saturation: [0.6, 1.4], hue: [0.0, 0.1]) • For Baseline Models: – Random Crop Ratio: (0.3, 1.0) – Random horizontal flip (p=0.5) – Random vertical flip (p=0.5) – Random rotations (from -90 to 90 degrees) – ColorJitter (brightness/contrast/saturation: [0.6, 1.4], hue: [0.0, 0.1]) Hardware • All experiments were conducted on two NVIDIA A100 GPUs, each with 40GB of memory. Hyperparameter Analysis: Effect of Loss Weights To understand how the components of our compound loss function influence model behavior, we conducted a hyper- parameter analysis on the loss weights, λ1 (for the classification loss, LBCE) and λ2 (for the Beta distribution loss, LW 2 2 ). Table S1 demonstrates that these weights serve as a practical lever to tune the model’s predictive trade-offs for different application needs. See the supplementary materials for a detailed analysis. Our analysis confirms two key trends. First, increasing the weight of the classification loss (λ1) makes the model prioritize Recall. As shown in Table S1, increasing λ1 to 10 yields the highest recall score of 0.6514. This indicates that a stronger emphasis on the classification task pushes the model to more aggressively identify all potential high-risk locations, which is critical for safety applications. Conversely, increasing the',\n",
       "  'the model prioritize Recall. As shown in Table S1, increasing λ1 to 10 yields the highest recall score of 0.6514. This indicates that a stronger emphasis on the classification task pushes the model to more aggressively identify all potential high-risk locations, which is critical for safety applications. Conversely, increasing the weight of the Beta distribution loss (λ2) encourages a more balanced and precise model. The box plots in Figure S4, which show the performance distribution over 25 epochs, provide additional insight. They visually confirm that as λ2 increases, the median precision rises while recall moderately decreases, bringing the two metrics into closer alignment. This is exemplified by the λ1 = 1, λ2 = 5 configuration, which achieves the highest precision of all tested settings (0.5932) while maintaining a strong recall (0.6026), as detailed in Table S1. This 17 Elallaf, Beta Distribution Learning for Reliable Roadway Crash Risk Assessment (AAAI’26) demonstrates that a stronger emphasis on the distribution-matching loss component encourages a more conservative model that makes fewer, but more accurate, high-risk predictions. This analysis provides clear guidance for hyperparameter selection based on the desired outcome. For a safety-critical system where failing to identify a hazard is the worst-case scenario, a higher λ1 is optimal. For applications requiring high confidence in positive predictions to efficiently allocate resources, a higher λ2 would be chosen. For the main results reported in this paper, we selected the λ1 = 5, λ2 = 1 configuration, as it achieved the highest F1-score and maintained a strong recall, offering an excellent balance for our primary task. 18'],\n",
       " ['INTERNAL Example Journal, XXXX, XX: 1–42 LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation Vasileios Magoulianitis1, Catherine A. Alexander1, Jiaxin Yang1 and C.-C. Jay Kuo1 1Minh Hsieh Department of Electrical and Computer Engineering, University of Southern California (USC), Los Angeles, CA, USA ABSTRACT Nuclei segmentation is the cornerstone task in histology im- age reading, shedding light on the underlying molecular pat- terns and leading to disease or cancer diagnosis. Yet, it is a laborious task that requires expertise from trained physi- cians. The large nuclei variability across different organ tissues and acquisition processes challenges the automation of this task. On the other hand, data annotations are ex- pensive to obtain, and thus, Deep Learning (DL) models are challenged to generalize to unseen organs or different domains. This work proposes Local-to-Global NuSegHop (LG-NuSegHop), a self-supervised pipeline developed on prior knowledge of the problem and molecular biology. There are three distinct modules: (1) a set of local processing op- erations to generate a pseudolabel, (2) NuSegHop a novel data-driven feature extraction model and (3) a set of global operations to post-process the predictions of NuSegHop. Notably, even though the proposed pipeline uses no manu- ally annotated training data or domain adaptation, it main- tains a good generalization performance on other datasets. Experiments in three publicly available datasets show that our method outperforms other self-supervised and weakly ISSN XXXX-XXXX; DOI XXXXXXXX XXXX XXXXXXXX 2 Vasileios Magoulianitis et al. INTERNAL supervised methods while having a competitive standing among fully supervised methods. Remarkably, every mod- ule within LG-NuSegHop is transparent and explainable to physicians. Keywords: Histopathology images, Nucleus segmentation, Self-supervision, Data-driven feature extraction 1 Introduction Cancer diagnosis from biopsy tissue specimens has been the standard way to tumor detection and grading. Cancerous and healthy cells have distinct molecular profiles which can provide important visual cues to pathologists. Nuclei segmentation is a fundamental task within this diagnosis pipeline, since the nuclei cell topology, size and shape play a crucial role to cancer grade reading. Hematoxylin and Eosin (H&E)- stained images give rise to this molecular profile by highlighting the nuclei cells and it has been the cornerstone process for histolopatho- logical slides preparation [40]. Undoubtedly, histopathological image reading is a painstaking task. It relies on very subtle visual cues, requiring also highly expertise. On top of this, digitized slides are captured usually under a high magnifi- cation level, typically ranging from 20x-40x. That results in very high resolution images which pathologists need to examine thoroughly to recognize potentially cancerous regions. Given that multiple cores usu- ally sampled out of each patient, one can realize that analyzing histol- ogy slides is a fairly time consuming and laborious task [10]. Computer- aided diagnosis (CAD) tools are meant to automate certain physicians’ tasks, offering also a more objective decision making process. Auto- mated nuclei segmentation can expedite the slide reading process by highlighting the molecular patterns and enhance pathologist’s reading. Moreover, it can be used as the intermediate step toward whole-slide classification (WSI) for models aiming to learn the pattern of clusters that nuclei form and map that to a',\n",
       "  'process. Auto- mated nuclei segmentation can expedite the slide reading process by highlighting the molecular patterns and enhance pathologist’s reading. Moreover, it can be used as the intermediate step toward whole-slide classification (WSI) for models aiming to learn the pattern of clusters that nuclei form and map that to a grade group of cancer [34, 4]. Nuclei segmentation poses several challenges to models and algo- rithms. At first, the H&E staining process [35] involves many steps LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 3 INTERNAL carried out manually from humans and thus it is far from stable and noise-free. Staining artifacts can also increase the intra-class distance, while during the image acquisition process, the type of scanner and its parameterization can also affect the nuclei appearance [54]. Another challenge that modern Deep Learning (DL) models are faced with is the lack of large annotated datasets, since it is a labor intensive task that only expert pathologists can perform. Therefore, data annotation is expensive and also subject to high inter-observer variability [6], which regards this problem as learning from noisy labels. There is a plethora of works in existing literature which approach the problem from different angles. Prior to the DL-based methods, most of the works focused on unsupervised methodologies. For in- stance, different variants of thresholding operations [59, 41], active contours [1] and level sets [7], watershed algorithm [17, 32], Graph cuts [16] and K-means clustering [58]. Those approaches were mostly relied on biological priors of the problem, particularly about the nuclei appearance, shape and size. It has been almost a decade since the advent of DL in the medical imaging field. For segmentation tasks, fully convolutional pipelines, such as U-Net [47] are popular choices among the researchers for se- mantic segmentation. Fully supervised methods use U-Net as backbone architecture [8, 14], also coupled with attention mechanisms tailored to focusing the learning on the error-prone regions (i.e. nuclei bound- aries) [70, 29]. Since fully supervised methods are heavily challenged from the lack of large annotated datasets, weakly supervised meth- ods[67, 27] attempt to learn either using less labels or point-wise an- notations [26, 44]. Furthermore, unsupervised learning methods use self-supervision and specifically employ domain adaptation [13] and predictive learning [49] to transfer the nuclei appearance from other domains. Nevertheless, they fail to achieve a competitive performance. Despite their success in other computer vision problems, DL mod- els are challenged in the medical imaging tasks, mainly due to the lack of large datasets. More importantly, DL models are often criticized as “black-boxes\" from physicians [42], since inherently their feature learning process is intricate. Moreover, to achieve a good performance, backbone models require pretraining on the ImageNet. As such, it is unclear how the representations can be adapted from a natural imag- ing domain to the biological one. Furthermore, those models fail to 4 Vasileios Magoulianitis et al. INTERNAL explicitly incorporate human’s prior knowledge which is important for a transparent decision making from the tools. All the mentioned reasons motivate this work to attempt a fully un- supervised pipeline and also decouple',\n",
       "  'ing domain to the biological one. Furthermore, those models fail to 4 Vasileios Magoulianitis et al. INTERNAL explicitly incorporate human’s prior knowledge which is important for a transparent decision making from the tools. All the mentioned reasons motivate this work to attempt a fully un- supervised pipeline and also decouple from the DL paradigm. Instead, a novel data-driven feature extraction model for histology images is introduced, namely NuSegHop. It is a linear, feedfoward and multi- scale model to learn the local texture from the histology images. Our approach is based on the Green Learning (GL) [20] paradigm which of- fers a framework for feature learning at a significantly lower complexity, where the features can be seamlessly interpreted [21]. The proposed pipeline consists of three major modules, starting with a set of local processing operations using priors of the task to generate a pseudola- bel. Then, NuSegHop is used in a self-supervised manner to predict a heatmap for nuclei presence. Finally, a set of global processing oper- ations takes place as a post-processing to decrease the false negative and positive rates, also in a self-supervised manner. Overall, the full pipeline incorporates self-supervised learning and priors insights, rang- ing from local areas (i.e. patches), up to global image decisions. There- fore, the overall proposed pipeline is named Local-to-Global NuSegHop (LG-NuSegHop). The main contributions of this work are: • NuSegHop as a data-driven feature extraction model to learn the texture in histology images in an unsupervised way. • LG-NuSegHop self-supervised pipeline which combines local and global image processing techniques, along with the NuSegHop model for nuclei segmentation. • Competitive performance in three diverse datasets among other DL-based supervised and weakly supervised models. Also, ex- tensive quantitative and qualitative comparisons and discussions are offered to help realizing in which cases supervision makes a difference in this task. • Cross-domain experiments showing high generalization perfor- mance across multi-organ datasets, with different staining meth- ods. LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 5 INTERNAL 2 Related Work In this section we provide an overview of methods about nuclei seg- mentation across different categories, beginning with the traditional pipelines that our work has elements from, and further including the DL state-of-the-art methods. For a more comprehensive and detailed overview of nuclei segmentation, a recent survey [35] conducted that provides detailed explanations and comparisons. 2.1 Łraditional Methods Earlier works relied mostly on priors from the nuclei appearance and certain assumptions to solve the problem. Threshodling was funda- mental in early segmentation works, where different methods propose mechanisms for calculating the appropriate threshold to binarize the input image. A popular algorithm in many works is Otsu’s threshold- ing [41, 2], trying to minimize the intra-class variance or maximizing the inter-class one and automatically discover the best threshold. Win et.al. [59] apply a median filter on each color component and then perform Otsu’s thresholding on the grayscaled image, followed by mor- phological operations to refine the output. A locally adaptive thresh- olding mechanism on linear color projections has been also proposed in [38]. Watershed algorithm [46] is another popular approach',\n",
       "  '[59] apply a median filter on each color component and then perform Otsu’s thresholding on the grayscaled image, followed by mor- phological operations to refine the output. A locally adaptive thresh- olding mechanism on linear color projections has been also proposed in [38]. Watershed algorithm [46] is another popular approach that uses topological information to segment an image into regions called catch- ment basins. This algorithm requires initial markers which are the seeds of the catchment basis. It is a popular choice in many works a post-processing step to find the nuclei boundaries [32, 60]. 2.2 Learning-based Methods 2.2.1 Full Supervision The initial DL-based works [62] fully relied on pathologists’ annota- tions to learn the nuclei color and texture variations. Region-proposal works employ Mask-RCNN to detect the nuclei [24, 48]. One of the most popular architectures for medical image segmentation used as a backbone in several works is the U-Net [47]. Kumar et al. [19] pro- posed a 3-way CNN model trained to supervise the nuclei boundaries. 6 Vasileios Magoulianitis et al. INTERNAL Instead of a binary map, it produces a ternary one which helps dis- tinguishing better the nuclei from background. Essentially, this study shows that attention on the nuclei boundary improves the accuracy of the detection and segmentation. CIA-Net [70] leverages the mutual dependencies between nuclei and their boundaries across different scales, and combines two distinct decoders, one for nuclei and one for their contours. It also proposes the Truncated Loss for diminishing the influence of outlier regions and mitigate the noisy labels effect. Moreover, to enhance the multi- scale learning capabilities, it introduces lateral connections between the encoder and decoder in each layer. In this way, the texture information learned in the early layers can be combined with the semantic features from the deeper ones. Graham et al. [8] introduce Hover-Net, a multi- branch network that is trained on segmentation, classification and pixel distance from the nuclear mass targets. In the loss function, the mean squared error (MSE) is calculated between the ground truth and the distance map, as well as the MSE of the distance gradients. By including the gradients into the loss function, it was found that it helped to delineate the nuclei boundaries more accurately. As emphasized, full label collection in this task is expensive and not in abundance. To this end, point-wise labels [44, 66] can be used to learn the appearance of nuclei from partial point annotations. Further- more, it is possible to combine point annotations and a limited number of full nuclei masks to enhance the learning process and improve the results [45]. 2.2.2 Self-Supervision Several methods have used self-supervision, to learn from a different task domain and transfer the knowledge into nuclei segmentation. Do- main adaptation is a popular self-supervised choice since it exploits the large volumes of labeled data from other domains, and then apply it to the target domain. Domain Adaptive Region-based CNN (DARCNN) is proposed in [13] which learns definition of objects from a generic natural object detection dataset [25] and adapts it on the biomedical datasets. This is',\n",
       "  'it exploits the large volumes of labeled data from other domains, and then apply it to the target domain. Domain Adaptive Region-based CNN (DARCNN) is proposed in [13] which learns definition of objects from a generic natural object detection dataset [25] and adapts it on the biomedical datasets. This is possible through a domain separation module that learns domain specific and domain invariant features. Liu et al. [28] propose the Cycle Consistent Panoptic Domain Adaptive Mask RCNN LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 7 INTERNAL (CyC-PDAM) that learns from fluorescence microscopy images and synthesizes H&E stained images using the CycleGAN [71]. Contrastive learning is another way for applying self-supervision. Xie et al. [61] propose an instance aware self-supervised method which involves scale- wise triplet learning and count ranking to implicitly learn nuclei from the different magnification levels. Predictive learning is another alternative to learn representations implicitly from the data. Sahasrabudhe et al. [49] have proposed a method on the assumption that the image magnification level can be determined by the texture and size of nuclei. In turn, this can be used as a self-supervised signal to detect nuclei locations and seed the watershed algorithm. Zheng et al. [68] have proposed a method that generates pseudo labels obtained from an unsupervised module using k-means clustering on the Hue-Saturation-Intensity (HSI) colorspace. Then, an SVM classifier is trained on a feature vector with color and texture, as well as topological attributes. Our work is conceptually sim- ilar to that work, since it creates a pseudolabel from local thresholding operations and then uses self-supervision at a global level. 2.3 Green Learning Green Learning (GL), has been recently introduced in[20], aiming to provide a more transparent feed-forward feature extraction process, at a small complexity and model size [21, 64]. The proposed feature ex- traction model create a multi-scale feature extraction and creates a rich spatial-spectral representation of the input image [5]. Instead of con- volutional filters trained with backpropagation, principal component analysis (PCA) is used to learn the local subspace across different lay- ers, where each feature has larger receptive field along deeper stages. Following GL’s terminology, each layer is called “Hop\", in which fea- tures are learned in an unsupervised and data-driven way. Within the medical imaging field, Liu et. al. [31, 30] have proposed the first works on segmentation and classification tasks. Also, GL has recently achieved competitive results in prostate cancer detection from Magnetic Resonance Images [37]. From the same framework, a U-Net inspired model, namely GUSL, has been introduced for medical image analysis segmentation problems, with applications to prostate gland segmentation. It is a fully supervised method, meant to offer multi- 8 Vasileios Magoulianitis et al. INTERNAL scale feature extraction and semantic segmentation at different scales, introducing a novel coarse-to-fine scale regression model. Our core proposed module, NuSegHop, uses the channel-wise Saab transform from GL for pixel-wise feature extraction and classification in a self- supervised manner. To the best of our knowledge, this is the first GL-based work in digital histology. 3 Materials And Methods Although the existing DL-based methods',\n",
       "  'scale regression model. Our core proposed module, NuSegHop, uses the channel-wise Saab transform from GL for pixel-wise feature extraction and classification in a self- supervised manner. To the best of our knowledge, this is the first GL-based work in digital histology. 3 Materials And Methods Although the existing DL-based methods usually comprise one model that is trained in an end-to-end manner using backpropagation, LG- NuSegHop has distinct modules, and each of those has a discrete task within the pipeline. This approach decouples from the DL paradigm, and one of its key benefits is the transparency of every step, since ev- ery module has a specific task and role within the overall pipeline. The proposed pipeline comprises three distinct modules that operate suc- cessively. In Section 3.1, we describe the image preprocessing steps, meant to enhance the input image towards the subsequent operations. In Section3.2, the local pixel-wise operations are described to predict a pseudolabel for NuSegHop, where its architecture and process is de- tailed in Section 3.3. The global processing modules are presented in 3.4. An overview of the entire proposed LG-NuSegHop pipeline is il- lustrated in Fig. 1. 3.1 Preprocessing The preprocessing modules aim at preparing the input image tile for the subsequent local processing module which is mainly based on thresh- olding. Thus, the key goals are: (1) highlight the nuclei over the back- ground and (2) convert the color image into grayscale. Prior to thresholding, the goal is to make nuclei more distinct over the background tissue. From the theory of the H&E staining pro- cess, hematoxylin principally colors the nuclei cells to a darker color (e.g. blue or dark-purple), while eosin mainly stains the cytoplasm and other structures in the background area. There are several meth- ods in literature for carrying out this color conversion. We choose the work of Salvi et al. [50]. It is an Singular Value Decomposition (SVD)- geodesic based method for stain separation, after converting the input LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 9 INTERNAL Pseudolabel Heatmap Candidate ROIs FP Instances Figure 1: An overview of LG-NuSegHop pipeline. Pre-processing applies image enhancement to prepare the image for the local processing modules. NuSegHop receives as input the pseudolabel and predicts a heatmap. In the last step, global processing modules increase the nuclei detection rate using information across the entire input image. image from RGB to the optical density space, where SVD can be more effective. Another benefit from stain separation, it helps mitigating the large stain variability across different images which is one of the challenges in this task. After separating the stain colors using the orthogonal spaces from SVD, we project all pixels on the Hematoxylin’s subspace -eigenvector from SVD corresponding to nuclei- to create image H. To further enhance this separation and make also nuclei boundary more distinct – especially for images that suffer from blurry artifacts due to the staining or acquisition process– we apply histogram equalization. The last preprocessing step to prepare the input tile for the thresh- olding operation is convert it to grayscale. Although other colorspaces',\n",
       "  'separation and make also nuclei boundary more distinct – especially for images that suffer from blurry artifacts due to the staining or acquisition process– we apply histogram equalization. The last preprocessing step to prepare the input tile for the thresh- olding operation is convert it to grayscale. Although other colorspaces (e.g. LAB) could be an option, in our earlier work [36] we have shown a more optimal way to convert the image into grayscale. Transforma- tion across different colorspaces use certain formula to map pixel values Preprocessing Hematoxylin Extraction Input Predicted Mask Contrast Enhancement Local Processing Global Processing Intermediate Outputs Patch Splitting Adaptive Thresholding Instance Refinement Anomalous Instance Removal NuSegHop Heatmap Filtering Local Maxima Detection Watershed + Thresholding Self-Supervised Classification 10 Vasileios Magoulianitis et al. INTERNAL Figure 2: An illustration of the main preprocessing steps, involving stain separation and the PQR method to convert color into grayscale. from one domain to the other. PQR, named after the three channels of principal component analysis, is a data-driven way for color conver- sion, adapted to the input content. It is based on SVD to calculate the color subspace direction that maximizes the data variance. One ad- vantage is the better energy compaction in one channel, comparing to a fixed colorspace conversion. Moreover, finding the color conversion that maximizes the variance is particularly important for the subse- quent thresholding operation, since we assume that along the direction of that subspace, the separation between nuclei and background is max- imized. Therefore, after SVD, we linearly project patches Hp from the H image on its first principal component P (p-value) to convert from color to grayscale. Since, different areas of the tile may have different statistics, we perform PQR independently after the image is split in local patches which are subject to thresholding. An illustrative exam- ple of PQR is shown in Fig. 2. The color conversion formulae are as follows: LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 11 INTERNAL Hp = U · S · V ⊺ (1) P ≜ V1,1:, Q ≜ V2,1:, R ≜ V3,1: (2) 3.2 Local Processing The main purpose of this module is to create a pseudolabel for training NuSegHop. This module uses simple, yet effective and intuitive image processing techniques at a local level. It employs prior knowledge of the problem and self-supervision locally, to filter out error predictions from the unsupervised local processing. To this end, certain assumptions are made, to overcome the lack of supervision: 1. The bi-modal distribution according to which at a local area histogram there are two peaks, where the lower intensity corre- sponds to nucleus and the brighter to background. Preprocessing is meant to accentuate this assumption 2. Local similarity where adjacent nuclei tend to have less color or texture variations 3. Larger low intensity components are less likely to be false posi- tives than the smaller instances 3.2.1 Adaptive Thresholding The thresholding method we propose is adaptive in two ways: (1) scale- wise and (2) intensity-wise. The input image is split into patches of size P 50 × 50 and the',\n",
       "  'Larger low intensity components are less likely to be false posi- tives than the smaller instances 3.2.1 Adaptive Thresholding The thresholding method we propose is adaptive in two ways: (1) scale- wise and (2) intensity-wise. The input image is split into patches of size P 50 × 50 and the process starts out with estimating the local distribution. If the bi-modal criteria are not met, the process tries also patches of 25 × 25 and 100 × 100. This is to adapt on different nuclei sizes or magnification levels. On the other hand, the threshold at each local patch is automatically adjusted based on the local area statistics. One choice for threshold calculation is to simply pick the intermediate value between the two peaks. Yet, we opt for a more adaptive way to calculate the optimal threshold [38], thus reducing the under or over segmentation effects and eventually create a less 12 Vasileios Magoulianitis et al. INTERNAL 2 Figure 3: Demonstration of the P-value distribution in a local patch where the bimodal assumption holds. The auxiliary lines to calculate the adapted threshold Tˆ are also depicted. noisy pseudolabel for self-supervision. Given the histogram and the two main peaks T1 and T2 under the bi-modal assumption, we define as L12 the line crosses through T1, T2, the intermediate point To where the threshold correction is about To = T1+T2 . Also, Tc is defined as the intercept point of the intensity value axis of the histogram and the perpendicular line of L12, passing from To (see Fig. 3). By using this heuristic and simple method to correct the intermediate p-value from (P)QR pre-processing, the local thresholding is more resilient to false positive and negative pixels. More details can be found in [38]. A λ hyperparameter is used to control the amount of correction about To. The adjusted threshold Tˆ formula is calculated using Eq. 3. Tˆ = To + λ(To − Tc) (3) 3.2.2 Morphological Instance Refinement Although adaptive thresholding works well in areas with relatively low variation, there are patches where the color variance is higher, thereby causing over or under segmentation effects. Morphological processing has been widely used in literature for processing binarized images. To refine the thresholding operation, we apply a set of morphological oper- ations, such as hole filling, small instance removal and nuclei splitting. LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 13 INTERNAL Priors about the nuclei size and shape are incorporated to apply sim- ple morphological processing and filter out noisy instances. For nuclei splitting, the convex hull algorithm is employed to detect highly deep curvatures that are not indicative of nuclei shape. This step is signifi- cant since subsequent operations operate on a per-instance base. 3.2.3 Locally Anomalous Instance Removal (LAIR) To further filter out larger instances that are possible to be false pos- itives, we carry out a simple local comparison among the detected in- stances. For this operation we need a larger patch, in order to in- clude more nuclei instances and make the comparison effective. Hence. for this submodule a 200',\n",
       "  'out larger instances that are possible to be false pos- itives, we carry out a simple local comparison among the detected in- stances. For this operation we need a larger patch, in order to in- clude more nuclei instances and make the comparison effective. Hence. for this submodule a 200 × 200 patch is used. In each large patch, the first criterion for query instances q is the size. That is, if an in- stance has a small to medium size, it will be compared against the rest larger instances r that provide reference, according to assumption 3 (see Section 3.2). We create a reference representation by forming the ensemble from the non-query instances. This can be viewed as a first step of introducing self-supervision locally in our pipeline, coupled with priors from the task. Intuitively, abnormally looking instances at a certain feature space can be regarded as anomalies and in turn eliminated. We define, Q = {q1, q2, . . . , qN } the instances being tested and R = {r1, r2, . . . , rM } the reference instances. Regarding the feature representation, we use the HSI colorspace from the H image, along with the channel-wise contrast value. For similarity comparison, a Gaussian kernel is used to measure the dis- tance between each query instance xj and the ensemble reference xR (see Eq. 4), and determine the anomaly instances that are subject to removal obtaining a similarity score S (see Eq. 5). Instances q that have a lower similarity with their local reference class R from a predefined threshold Ts are removed from the foreground. 1 xR = |R| Σ xi, (4) i∈R S(x , x ) = e−γ||xj −xR||2 R j 2 , ∀j ∈ Q, (5) 14 Vasileios Magoulianitis et al. INTERNAL Figure 4: Graphical overview of the proposed NuSegHop for feature extraction. It consists of two layers that operate in two different scales. From both layers two types of features are extracted: (1) spatial and (2) spectral. With red we depict the extracted feature maps that have low energy and will be discarded. All the spatial feature maps (green color) are concatenated to extract the spectral ones (gray color). 3.3 NuSegHop After the local processing module operations, we have obtained an ini- tial segmentation output using no labels or training data. This output can be used as a pseudolabel to a classifier. Aiming at obtaining a probability heatmap for nuclei segmentation, we propose a novel and unsupervised feature extraction method, named NuSegHop, to learn the local texture of nuclei for pixel-wise classification. Other methods in the past [68] have used hand-crafted features or pure color-based methods which lack robustness. A data-driven and multi-scale feature extraction is proposed in this paper for pixel-wise nuclei segmentation using the GL paradigm [21]. This method has as a key advantage the low complexity and small model size, which is essential for fast infer- ence. Moreover, the GL-based feature extraction module is linear and thereby more transparent and interpretable [64]. This is another major distinction from the DL models that',\n",
       "  'the GL paradigm [21]. This method has as a key advantage the low complexity and small model size, which is essential for fast infer- ence. Moreover, the GL-based feature extraction module is linear and thereby more transparent and interpretable [64]. This is another major distinction from the DL models that use backprogagation for learning feature representations. NuSegHop learns features in a feed-forward manner and does not require large datasets in order to obtain robust feature representations. Therefore, GL-based methods have certain LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 15 INTERNAL advantages in specific tasks and this is why we opt for such a feature extraction method. A detailed architecture of NuSegHop is illustrated in Fig. 4. For NuSegHop, the H image is converted into the HSI colorspace before feature extraction. Originally, a window area A of size S(1)×S(1) is considered to characterize the area about a pixel. For this problem, we choose S(1) = 9, since too small windows may not be able to learn the local texture, while larger ones may induce more noise into fea- ture learning. NuSegHop learns the texture within the window area in two scales S1 and S2, to give multi-scale properties in the feature space. The core operation in NuSegHop for texture learning is the Saab transform [21], which is based on the Principal Component Anal- ysis (PCA), applied in two ways: (1) spatial and (2) spectral. The full feature extraction diagram of NuSegHop is shown in Figure 4. 3.3.1 Feature learning - Spatial Saab To learn the texture across different local areas within a window, a neighborhood construction with filters of spatial size of f (1) × f (1) is applied with stride 1 and equal padding. Since the input image has three color channels, each local neighborhood defines a cuboid C(1) with size K(1) = f (1)×f (1)×3 which contains the local HSI color information. As a consequence, K(1) is the maximum number of extracted subspaces from the Saab transform in layer 1. L(1) = S(1) × S(1) such cuboids can be extracted from a window at layer-1 using padding on A. By sampling across windows centered on the pixels of the original image, one can create a tensor T (1) for training with size N × L(1) × K(1), where N the number of sampled pixels from the input image to training NuSegHop. In turn, T (1) can be used for training layer-1 and calculate the subspaces (Eq. 6) used for feature extraction. After SVD matrix decomposition, the rows of V are the eigenvectors correspond to the orthogonal subspaces of the signal (see Eq. 7). T (1) = U · S · V ⊺ (6) W(1) = V1:M,1: (7) 16 Vasileios Magoulianitis et al. INTERNAL In training, Saab transform is applied on C(1) to extract K(1) or- thogonal subspaces (i.e. principal components). Moreover, because many principal components may carry no significant energy –as it is dictated from their corresponding eigenvalues–, they can be discarded to remove unnecessary complexity and noise. That is, the first M prin- cipal components are retained, with',\n",
       "  'on C(1) to extract K(1) or- thogonal subspaces (i.e. principal components). Moreover, because many principal components may carry no significant energy –as it is dictated from their corresponding eigenvalues–, they can be discarded to remove unnecessary complexity and noise. That is, the first M prin- cipal components are retained, with M < K(1). We define the weight matrix W (1) ∈ RM×K(1) which contains all the information to decomposing the cuboids into their spectral represen- tations by projecting onto the extracted principal components. After training, W incudes the weights for feature extraction. For instance, to perform spatial feature extraction in layer 1, one needs to multiply L(1) cuboids extracted within A window, and project them along the M principal components. To formulate this operation, we construct matrix C with size L(1) × K(1), where its rows contain the cuboids. By multiplying them, we calculate matrix F of size L(1) × M which includes all the spatial features of layer-1 (see Eq. 8). Spectral maps F (1) in layer 1 are obtained from the matrix F , by reshaping it back to size S(1) × S(1) × M . We view each principal component as a different spectral local representation of A. F = C · W ⊺ (8) We also concatenate as additional feature (one more column in F ) the mean of each local cuboid in both layers (Eq. 9), since apart from the texture, the local color is also important to differentiate nuclei from background. If we are to draw a parallel with circuit theory, the DC component is the mean color and AC are the textures derived from the Saab transform. Thus, F is now of size L(1) × (M + 1). F = FDC ⊕ FAC (9) In layer-2, feature map F (1) is fed as input after a max-pooling layer. The spatial feature extraction process in layer-2 is similar to layer-1, with one difference, the Saab transform is applied independently on each of the M +1 feature maps of layer-1 [5]. Therefore, after neighbor- hood construction each cuboid C(2) has a shape of K(2) = f (2)×f (2)×1. Also, the spatial size of L(2) = S(2) × S(2) = ⌈(S(1)/2)⌉ × ⌈(S(1)/2)⌉ after max-pooling. The filter sizes and output shapes of each layer are show in in Table 1. LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 17 INTERNAL s s Table 1: Architecture of the proposed NuSegHop. F Resolution f Size Stride Layer 1 (9 × 9) × 3 (3 × 3) × 1 (1 × 1) × 1 Max-pool 1 (5 × 5) × 1 (2 × 2) × 1 (2 × 2) × 1 Layer 2 (5 × 5) × 1 (3 × 3) × 1 (1 × 1) × 1 Independent Saab transforms as many as M + 1 are applied on tensors with size of N × L(2) × K(2). After channel-wise Saab transform in layer-2, each spectral map originally has a size of S(2) × S(2) × (M + 1) × K(2), after concatenating all the feature maps from the',\n",
       "  'transforms as many as M + 1 are applied on tensors with size of N × L(2) × K(2). After channel-wise Saab transform in layer-2, each spectral map originally has a size of S(2) × S(2) × (M + 1) × K(2), after concatenating all the feature maps from the channel- wise Saab transforms. Energy-based spectral truncation is also applied in layer-2. Supposing that Q principal components are kept from each channel-wise Saab transform (Q < C(2)), then the final layer-2 spectral maps will have a shape of S(2) × S(2) × (M + 1) × (Q + 1), adding also the DC channels in the same way as in layer 1. The energy threshold for both layers is set at Te = 1e − 03. 3.3.2 Feature learning - Spectral Saab Spatial-wise Saab provides a spectral analysis of A across all its spatial regions at the scales of S(1) and S(2) A. Therefore, each feature has a spatial correspondence. Yet, it is required to extract features that have a global reference to A as well. Those features are unassociated from the spatial domain and meant to capture different patterns within A area, such as boundary transitions from nuclei to background. To this end, on each spectral map F (1) and F (2) from layers 1 and 2, we apply a PCA using the spectral maps’ spatial components as features. By doing so, the transformed signal will have no spatial correspondence anymore. This is performed independently for every M + 1 and (M + 1) ×(Q + 1) spectral maps for layers 1 and 2, respectively. The spectral features G(l) from each layer is simply the union of all PCA transformed spatial features F (1). (see Eq. 10 and 11.) The same Te is used to filter out the principal components and reduce the dimensionality. G(1) = ∪M+1{PCA(F (1))T } (10) s s=1 s e 18 Vasileios Magoulianitis et al. INTERNAL G(2) = ∪M+Q+2{PCA(F (2))T } (11) s s=1 s e The last step in NuSegHop is to concatenate all the spatial and spectral features from both layers to form the final feature X that characterizes A. After concatenation the top 100 discriminant features are selected [65]. This provides a rich spatial-spectral feature represen- tation about the color and texture of the local neighborhood under A. Besides, in the Saab feature space, the spectral dimensions are uncor- related because the principal components are orthogonal by definition. NuSegHop enables for fast pixel-wise predictions, requiring no su- pervision for its feature extraction part. Having extracted the features on each W , one can train a classifier using the pseudolabels from the lo- cal processing module. We train an Xtreme Gradient Boosting (XGB) classifier and use its probability predictions to generate a heatmap Pˆ . Each pixel contains the probability of belonging to the foreground. In the LG-NuSegHop pipeline each module has a clear scope for im- proving certain aspects of the nuclei segmentation task. Therefore, each module’s purpose and output are transparent. The linearity property in NuSegHop features enables the visualization of the local',\n",
       "  'Each pixel contains the probability of belonging to the foreground. In the LG-NuSegHop pipeline each module has a clear scope for im- proving certain aspects of the nuclei segmentation task. Therefore, each module’s purpose and output are transparent. The linearity property in NuSegHop features enables the visualization of the local patterns that weigh in classifying a pixel. Concretely, during inference, one can trace back the more “informative\" NuSegHop feature dimensions of the classifier. In turn, those features can be mapped back to the input layer –using the inverse Saab transform–, to elucidate the texture and color patterns they correspond to. In other words, NuSegHop enables the pathologist to review the visual elements that classify a certain region as nuclei or background. This seamless decision interpretation provides great advantages in clinical applications since pathologists can under- stand the decision-making process of the tool, thereby making it more trustworthy for clinical deployment. 3.4 Global Processing This module aims at integrating the locally made decisions, based on color and texture, and perform a global post-processing. Most opera- tions from the local processing group are pixel-wise and carried out in local patches of the original image to reduce variability, whereas global processing has the entire information about the image. LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 19 INTERNAL Figure 5: An overview of the global processing pipeline. The Laplacian of Gaus- sian filter detects local minima from the NuSegHop heatmap to decrease the false negative rate. Watershed and probability thresholding binarize the image and de- lineate nuclei boundary from the heatmap. Candidate instances are classified in a self-supervised manner to detect any false positives. As mentioned, locally-based decisions may miss faintly stained nu- clei or misclassify background areas as nuclei. The goal of moving from local to global decisions is to decrease the false negative (FN) rate by including more instances as candidates, based on the probability ar- eas from the local decisions. Since, it is inevitable for this process to give rise to false positives (FP), self-supervision is also employed at the end of the global processing modules to help discern potentially FP instances. A diagram of the global processing pipeline is shown in Fig. 5. 20 Vasileios Magoulianitis et al. INTERNAL 3.4.1 Heatmap Filtering Out of our observations on the obtained probability heatmap P , most of the nuclei are predicted with high confidence from NuSegHop mod- ule. This refers to solid stained instances that can be easily recognized from their color and texture. As we want to decrease the complexity of the global processing unit, large instances are retained and we con- sider only the less confident ones for the subsequent module (i.e. local maxima detection). This helps both the complexity and efficiency. To do so, the heatmap and the predicted mask from NuSegHop are con- sidered to calculate the per instance confidence, by taking the average from all pixels belong to the instance. Highly confident instances with average probability more than 0.95 are removed from the local max- ima detection submodule. After this submodule we obtain the filtered heatmap P',\n",
       "  'mask from NuSegHop are con- sidered to calculate the per instance confidence, by taking the average from all pixels belong to the instance. Highly confident instances with average probability more than 0.95 are removed from the local max- ima detection submodule. After this submodule we obtain the filtered heatmap P ′ . 3.4.2 Local Maxima Detection This submodule aims mainly at increasing the recall ratio of nuclei, on the remaining areas after instance filtering, where the NuSegHop unit is not confident. Texture and color variations or faintly stained nuclei from the local processing module result in scattered high probability ar- eas that during binarization become isolated small instances. The goal is to detect those areas and create candidate ROIs as foreground. Given the filtered heatmap P ′ this task boils down to a local maxima detec- tion (LMD). We apply a Laplacian of Gaussian filter (LoG) to detect “blob-like\" regions which corresponds to potential nuclei instances. The Gaussian filter is meant to smooth and unify the pixel-wise heatmap estimation, thus to mitigating the color and texture variance. 3.4.3 Watershed Post-processing For the highly confident instances, their boundary is typically distinct and can be determined from the local processing modules and in turn from the heatmap. However, for the low confidence instances that are hard to be accurately classified based on the heatmap. We can detect the position of the nuclei but it is hard to estimate accurately their boundaries, since those areas are outliers when training the classifier. LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 21 INTERNAL As long as the rough locations of candidates are obtained from the pre- vious module, we their centroids to seed the watershed algorithm, and find the adjacent nuclei boundary lines. This helps areas with multiple less confident nuclei located therein, where their boundaries estimation is more challenging. As a last step, we binarize the filtered heatmap, ap- pending back also the confident instances using a probability threshold to create instances for the subsequent classification and false positive reduction. As it is desired to include many candidates, so to increase the recall rate, we choose a lower probability threshold Tp = 0.35. This typically increases the false positive rate, but the subsequent module is devised to mitigate that. 3.4.4 Self-Supervised Instance Classification The final step of the proposed pipeline is a self-supervised instance- based classification in order to detect nuclei that their representation falls out of the normal nuclei and their appearance is closer to back- ground. This ROI instance based classification is similar to FLAIR module but with two main differences: (1) it is performed in a global level and (2) there are no size-based criteria to select instances. The hypothesis here is the following: so long as the majority of instances is correctly classified, the minority of instances that are false positives do not affect the ensemble learning, as they are statistically less significant. Moreover, if their representation is closer to the back- ground, rather than the foreground then they are simply classified as false positives and are removed from the final',\n",
       "  'correctly classified, the minority of instances that are false positives do not affect the ensemble learning, as they are statistically less significant. Moreover, if their representation is closer to the back- ground, rather than the foreground then they are simply classified as false positives and are removed from the final segmentation output. For feature extraction and classification, we use the H image and convert it to the HSI colorspace (as in NuSegHop). We apply feature extraction on each channel separately and concatenate them before the classifier’s input. For features we opt for the first order statistics to learn the color characteristics, as well as the gray-level zone matrix which includes several features that capture the rough texture of the nuclei [33]. Here, we choose an SVM classifier with a radial basis kernel, to predict for each instance the probability of being a true positive. 22 Vasileios Magoulianitis et al. INTERNAL Table 2: Summary of hyperparameters configuration in LG-NuSegHop, finetuned on a small subset of training images from MoNuSeg. Hyperparameter Value Local Processing λ (Adapt. Thresh.) 0.2 S (LAIR) 0.7 γ (LAIR) 0.1 NuSegHop Energy Te 10e − 4 # Spectral Dimensions 10 XGB – # trees 100 XGB – Tree depth 4 XGB – Learning rate 0.075 Global Processing Blob Threshold (LoG) 0.05 Tp 0.35 4 Experimental Results This section includes details of the experiments conducted in this work to validate the efficiency of our work, as well as potential areas of im- provement. The datasets used for experiments and comparisons are briefly introduced in subsection 4.1, while the metrics for the quanti- tative analysis in 4.2. Additionally, our method is compared against other state-of-the-art methods, from unsupervised to weakly and fully supervised methods in 4.3. Furthermore, an ablation study is carried out in 4.4 to evaluate how different modules affect the performance, accompanied with visualization examples (4.4.1). In subsection 4.5 we discuss on the findings from the comparisons with the state-of-the-art. 4.1 Datasets Three publicly available datasets are chosen to compare our proposed methodology. MoNuSeg [18] The Multi-Organ Nuclei Segmentation (MoNuSeg) dataset includes 30 training image tiles of size 1000x1000 and magnifica- tion level 40x, comprising 21, 623 nuclei. Also, 14 testing imag tiles are available for benhmarking. The extracted tiles come from histological slides of breast, liver, kidney, prostate, bladder, colon, and stomach col- LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 23 INTERNAL lected from The Cancer Genome Atlas(TCGA) [53]. Also, the samples collected come from different hospitals and patients. Therefore, it is a fairly diverse dataset across different aspects that challenges the gen- eralization ability of the segmentation model. The Aperio ImageScope was used for digitizing the slides. CryoNuSeg [39] This is the first H&E multi-organ dataset from frozen samples. Slides digitization from frozen samples involves a dif- ferent process and therefore the nuclei appearance is different. This technique pertains intra-operative surgical sessions and its major ben- efit is that it can be performed rapidly. Yet, the requirement for a quick slide preparation, staining and digitization comes at the expense of the image quality. The dataset provides',\n",
       "  'dif- ferent process and therefore the nuclei appearance is different. This technique pertains intra-operative surgical sessions and its major ben- efit is that it can be performed rapidly. Yet, the requirement for a quick slide preparation, staining and digitization comes at the expense of the image quality. The dataset provides 30 digitized images of size 512 × 512, acquired at a 40× magnification level. The slides come from 10 different organs (larynx gland, adrenal, lymph nodes, pan- creas, skin, pleura, mediastinum, thyroid gland, thymus, testes) and there exist 7, 596 annotated nuclei. CoNSeP [9] This dataset includes 41 image tiles from 16 slides of patients with colorectal adenocarcinoma. 27 tiles are used for training and 14 for testing. The extracted tiles are of size 1000 × 1000. The Omnyx VL120 scanner was used for the slides digitization at a 40× magnification level. Overall, 24, 319 nuclei are annotated. 4.2 Evaluation Metrics For performance evaluation, we use three different metrics, that have been commonly used in the literature. It is worth noting that nuclei segmentation is an instance-level segmentation problem. That is, a nucleus instance needs to be detected and then segmented. F1 score is the harmonic mean of the precision and recall. The F1 score regards nuclei segmentation as an instance detection problem, without taking into account the segmentation aspect. To complete our metrics, we also include the Aggregated Jaccard Index (AJI) metric [19] and the Panoptic Quality (PQ) [15]. These two metrics are more suitable for instance-level segmentation problems as they take into account both aspects. In particular, PQ calculates the detection quality (DQ), as well as the segmentation quality as a similarity measure with the ground truth. Dice similarity coefficient is also included in our comparisons to measure the segmentation performance. 24 Vasileios Magoulianitis et al. INTERNAL 4.3 Results & Comparisons 4.3.1 Experimental Setup To have a thorough understanding on the advantages and weaknesses of our work, we compare it against several state-of-the-art works with different levels of supervision. At first, we compare our method with other self-supervised methods on the MoNuSeg dataset, which use no labels from the target datasets. Another category is the weakly super- vised methods that either use less training samples from the annotation masks or point annotations. Moreover, we include in our analysis a few popular fully supervised methods, so as to provide a thorough compar- ison of our work. Before we delve into the comparisons, one aspect we would like to stress is that our method does not use any training data for pa- rameters learning. Yet, since there are several hyperparameters (see Table 2) need adjustment, we use 6 out of the 30 training images ran- domly from the MoNuSeg dataset to finetune the LG-NuSegHop. This can be viewed as the validation set of our experiments. Hence, the hyperparameters are empirically finetuned using this validation set. At first, we adjust the local processing module, using the AJI and F1 score calculated from the intermediate (i.e. pseudolabel) output. Then, NuSegHop and global processing modules are adjusted together. For all the modules we tune the',\n",
       "  'of our experiments. Hence, the hyperparameters are empirically finetuned using this validation set. At first, we adjust the local processing module, using the AJI and F1 score calculated from the intermediate (i.e. pseudolabel) output. Then, NuSegHop and global processing modules are adjusted together. For all the modules we tune the hyperparameters to maximize the AJI metric, whereas in the last module of the global processing set (self-supervised instance classification) we try to maximize the F1 score, since it is an ROI-wise classification task. After the model is fixed, we test it out on the three testing datasets. This aims at testing how well our model generalizes to data with in- herent discrepancies. However, we individually finetune LG-NuSegHop hyperparameters on each datasets using their a subset of their train- ing data, in order to also test the performance when LG-NuSegHop is adapted to a certain domain. 4.3.2 Performance benchmarking At first glance, LG-NuSegHop has a competitive standing in compari- son with other works, including also the fully supervised ones (Tables 3, 4 and 5). In the MoNuSeg dataset, it outperforms by large margins LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 25 INTERNAL Table 3: Performance benchmarking with self, weakly and fully supervised methods in the MoNuSeg dataset. AJI F1 Dice Self Supervised DARCNN [13] 0.446 0.5410 - Self-Attention [49] 0.535 - 0.747 CyC-PDAM [28] 0.561 0.748 - Nucleus-Aware [51] 0.593 0.759 - Weakly Supervised Partial Points [43] 0.543 0.776 0.732 Point Annotations [26] 0.562 0.776 0.744 BoNuS [27] 0.607 0.780 0.767 Cyclic Learning [69] 0.636 0.774 0.774 Fully Supervised U-Net [47] 0.543 0.779 - RCSAU-Net [56] 0.619 0.82 - HoVer-Net [8] 0.618 0.826 - CDNet [11] 0.633 0.831 - TopoSeg [12] 0.643 - - GUSL [63] 0.673 0.886 0.803 NucleiSegNet [22] 0.688 0.813 - CIA-Net [70] 0.691 0.901 - LG-NuSegHop (Baseline) 0.651 0.887 0.778 LG-NuSegHop (Dom. Adapted) 0.658 0.892 0.791 the self and weakly supervised works in terms of all the reported met- rics, with a 0.651 AJI. It also has an impressive performance stand- ing among the fully supervised works, including sophisticated models for nuclei segmentation, such as HoVer-Net [8] and CDNet [11] (see Table 3). However, other models such as NucleiSegNet [22] and CIA- Net [70] achieve higher performance in AJI, yet LG-NuSegHop achieves a competitive standing in detecting nuclei based on the F1 score (0.892 vs. 0.901). Comparing our method with GUSL model from the GL methodology, it achieves an AJI score of 0.673. The full supervision of GUSL helps to learn the more challenging distinctions between nu- clei and background, which are hard to capture by a self-supervised method. Yet, in terms of F1 score, the detection performance is sim- ilar. Also, when fine-tuned using all the MoNuSeg training images, the performance increases slightly to the 0.658 AJI score and 0.892 F1 score. In terms of the Dice score our method also achieves the best per- 26 Vasileios Magoulianitis et al. INTERNAL Table 4: Performance benchmarking with weakly and fully supervised methods in the CryoNuSeg dataset. AJI Dice PQ Weakly Supervised BoNuS [27] 0.431 0.693 0.399 Partial',\n",
       "  'AJI score and 0.892 F1 score. In terms of the Dice score our method also achieves the best per- 26 Vasileios Magoulianitis et al. INTERNAL Table 4: Performance benchmarking with weakly and fully supervised methods in the CryoNuSeg dataset. AJI Dice PQ Weakly Supervised BoNuS [27] 0.431 0.693 0.399 Partial Points [43] 0.410 0.682 0.357 DAWN [67] 0.508 0.804 0.476 Pseudoedgenet [66] 0.321 0.620 0.306 DoNuSeg [57] 0.441 0.672 0.306 Fully Supervised U-Net [47] 0.469 0.697 0.403 HoVer-Net [8] 0.526 0.804 0.495 Swin-unet [3] 0.524 0.849 0.498 CDNet [11] 0.539 0.776 0.499 LG-NuSegHop (Baseline) 0.545 0.703 0.419 LG-NuSegHop (Dom. Adapted) 0.567 0.723 0.479 formance among the weakly and self supervised methods by significant difference from the second leading performance (0.791 vs. 0.774). In the CryoNuSeg, our method surpasses all the weakly supervised methods by large margins, achieving an AJI of 0.545 (Table 4). Com- paring with the Dice coefficient and PQ, only the recently proposed DAWN [67] has a better performance. Besides, from the fully super- vised category LG-NuSegHop achieves a higher AJI score from the state-of-the-art. Remarkably, even when it is not finetuned on the CryoNuSeg data (i.e. baseline model), LG-NuSegHop achieves a com- petitive performance in this dataset, where the acquisition process is considerably different than the standard H&E staining process. When finetuned on the same domain, the AJI performance increases further to 0.567. Overall, our method achieves a high AJI and PQ perfor- mance comparing to the other supervised methods. Full supervision in this dataset seems to boost more the segmentation performance, as one can see from the higher Dice score, comparing to the weakly or unsupervised methods. In the third dataset under comparison, CoNSeP, all methods achieve a lower performance –compared to the other datasets–, since it has a large intra-class variance, where nuclei have quite different textures. Hence, this dataset is challenging for most methods. LG-NuSeg achieves LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 27 INTERNAL Table 5: Performance benchmarking with weakly and fully supervised methods in the CoNSeP dataset. AJI Dice PQ Weakly Supervised Pseudoedgenet [66] 0.221 0.331 0.153 BoNuS [27] 0.354 0.651 0.380 Partial Points [43] 0.366 0.646 0.391 Point Annotations [52] 0.464 0.749 0.398 DAWN [67] 0.509 0.805 0.477 Fully Supervised U-Net [47] 0.499 0.761 0.434 HoVer-Net [8] 0.513 0.837 0.492 CDNet [11] 0.541 0.835 0.514 Mulvernet [55] 0.515 0.833 0.482 LG-NuSegHop (Baseline) 0.422 0.654 0.407 LG-NuSegHop (Dom. Adapted) 0.461 0.691 0.427 an AJI of 0.422 without any finetuning (Table 5). Comparing to the weakly supervised methods it has a competitive performance across all metrics. Only the point annotations [52] and DAWN [67] methods per- form better. Compared to the fully supervised works, the performance gap is larger. Yet, if we finetune the hyperparameters on the training images, the AJI score increases significantly to 0.461, surpassing most of the weakly supervised works and narrowing the gap with the fully supervised ones, such as U-Net. Considering all the metrics, in this dataset fully supervised methods achieve a higher performance in both segmentation and detection metrics. Therefore, one can infer that when the nuclei',\n",
       "  'significantly to 0.461, surpassing most of the weakly supervised works and narrowing the gap with the fully supervised ones, such as U-Net. Considering all the metrics, in this dataset fully supervised methods achieve a higher performance in both segmentation and detection metrics. Therefore, one can infer that when the nuclei appearance variance is higher, supervision is crucial for both detecting and delineating the nuclei cells. This can be also observed in Fig. 6, where our method achieves a good segmentation result in MoNuSeg and CryoNuSeg, while in CoNSeP, for certain nuclei types it is more challenging to detect and segment them accurately without supervision or any domain adaptation. 4.4 Ablation Study As LG-NuSegHop is a pipeline consisting multiple processing steps and modules, we conduct an ablation study with different modules to 28 Vasileios Magoulianitis et al. INTERNAL Input GT U-Net HoVer-Net LG-NuSegHop Figure 6: Visual comparisons of LG-NuSegHop with HoVer-Net and U-Net on patch examples from MoNuSeg (first row), CryoNuSeg (second row) and ConSeP (third row) datasets. demonstrate their efficacy and importance within the overall pipeline. Table 6 shows a few comparisons between the PQR and L,AB color con- version as a pre-processing step, as well as the contribution of the local processing modules. Table 7 at first compares a handcrafted feature extraction approach against NuSegHop. In turn, we progressively add the global processing modules to test the performance improvement. First of all, one can observe that our PQR pre-processing conversion helps both the detection and segmentation metrics. Also, the adaptive thresholding improves significantly the AJI over the non-adaptive one. Morphological post-processing is important to remove noisy instances and split nuclei and it is reflected from the large improvement of the F1 score (see Table 6). LAIR module also provides a small improvement in F1 score, by filtering some false positives in images, wherever it is more likely to have a high false positive rate. On the other hand, by replacing a handcrafted feature extraction in lieu of NuSegHop, all metrics drop significantly. Moreover, local maxima detection on NuSegHop’s heatmap improves mainly the detection performance by recalling areas that indicate nuclei existence. It is also evident that both watershed and self-supervised instance classification help to improve mainly the detection aspect of the task, by reducing the false positive rate and delineate the nuclei more precisely (see Table 7). LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 29 INTERNAL Morph. Refin. 0.605 0.763 0.732 0.611 0.779 0.738 0.583 0.745 0.705 0.595 0.749 0.721 0.608 0.774 0.728 0.622 0.813 0.740 0.641 0.836 0.763 ✓ ✓ 0.647 0.875 0.769 ✓ ✓ ✓ 0.651 0.883 0.772 ✓ ✓ ✓ ✓ 0.658 0.892 0.778 Table 6: Ablation study on the MoNuSeg dataset with combinations of preprocessing and local processing modules. Table 7: Ablation study on the MoNuSeg dataset with different global processing modules. NuSegHop data-driven features are also compared with a set of hand- crafted features for nuclei segmentation. All the pre-processing and local processing operations are kept to their best configuration. 4.4.1 Qualitative Analysis In the last part of our analysis, two visualization comparisons are',\n",
       "  'MoNuSeg dataset with different global processing modules. NuSegHop data-driven features are also compared with a set of hand- crafted features for nuclei segmentation. All the pre-processing and local processing operations are kept to their best configuration. 4.4.1 Qualitative Analysis In the last part of our analysis, two visualization comparisons are pro- vided. In Fig. 7, one can observe how the adaptive thresholding can help in segmenting local patches, where the bi-modal assumption is not very pronounced. The binarization performance is compared over a more conservative thresholding by choosing the intermediate point. It is demonstrated that the adjustment of the threshold about the in- termediate point between the histogram peaks, helps to reduce the false positive areas upfront in LG-NuSegHop and thereby provides a less noisy pseudolabel to NuSegHop. Moreover, in Fig. 8, we illustrate a few examples across the three datasets and compare the instance self-supervised classification module operation in removing false pos- itive instances as the last post-processing step of LG-NuSegHop. In MoNuSeg, our method achieves better results and in turn we can see that the false positive and negative rate is lower. CryoNuSeg yields a higher rate of false positives, which is mitigated from the global pro- cessing module. The ConSeP image has a higher false negative rate and hence the instance classification module does not have much effect. 30 Vasileios Magoulianitis et al. INTERNAL Figure 7: Illustrative examples of the adaptive filtering from the local processing module. To is the intermediate point between the two peaks of the bi-modal distri- bution and the Tˆ the adapted threshold about To. The input patch is shown after the staining normalization. 4.5 Limitations & Discussion From the quantitative and qualitative analysis, it is evident the impor- tance and role of each individual module. In LG-NuSegHop pipeline other modules (e.g. instance classification) are meant to increase the detection accuracy and other the segmentation (i.e. NuSegHop). Over- all, our proposed methodology achieves a very competitive performance among other self-supervised or weakly and fully supervised methods. It is worth noting that even without any finetuning on the target datasets, LG-NuSegHop outperforms most of the weakly supervised methods and can be compared to the state-of-the-art fully supervised ones. This seamless generalization ability provides a huge advantage of our method, since there is no need to form a training dataset or have a pathologists to annotate data. It can be deployed in a plug-n-play man- ner for nuclei segmentation and achieve a competitive performance in certain datasets (e.g. CryoNuSeg) or to provide pseudolabel to another self-supervised model. As already mentioned, LG-NuSegHop is a pipeline that relies on hu- LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 31 INTERNAL man’s prior knowledge in solving the problem and requires certain as- sumptions to hold. From experimenting with the three diverse datasets, we can observe that our method shows a relatively lower performance on the CoNSeP dataset. This can be attributed to the fact that the local similarity assumption is very weak in this dataset. The nuclei instances have a large distance in shape and color, thus challenging for',\n",
       "  'three diverse datasets, we can observe that our method shows a relatively lower performance on the CoNSeP dataset. This can be attributed to the fact that the local similarity assumption is very weak in this dataset. The nuclei instances have a large distance in shape and color, thus challenging for the local processing module to predict a less noisy pseudolabel for NuSegHop. In turn, it is also hard for the self-supervision to improve significantly over the pseudolabel prediction, resulting in a higher false negative rate. From the experiments, fully supervised methods on the CoNSep dataset have a higher gap in performance both from the weakly su- pervised methods and the LG-NuSegHop one. Our method fits better into MoNuSeg and CryoNuSeg assumptions where the intra-class dis- tance is relatively smaller and can yield a competitive performance even without any finetuning. One higher level conclusion that can be drawn from this comparison is that the nuclei segmentation problem on cer- tain histology images can be solved solely by relying on clinical and biology prior knowledge using no or little supervision (Tables 3 and 4). Yet, when certain assumptions are not well met, full supervision is needed to learn the nuclei variability and achieve a higher perfor- mance (see Table 5). Another observation from the results and the comparison with different metrics is that our method is more effective on the detection aspect of the problem over the segmentation one, es- pecially on the CryoNuSeg. The main reason is the appearance of the nuclei –faintly stained–, sourcing from the staining process. The lack of explicit full supervision makes harder for our method to accurately learn the boundaries distinctions for this type of staining. Thus, our method achieves a better Dice score, compared to the weakly super- vised methods, but HoVer-Net and CDNet achieve better results due to their fully supervised training, which is crucial for this dataset in the accurate delineation of nuclei. LG-NuSegHop can offer a high nuclei segmentation performance and generalize well with no specific domain adaptation. For example, although our method does not require any training from MoNuSeg, we use 6 images for hyperparameter finetuning. So, LG-NuSegHop is inherently adapted to this domain to a certain extend. Two recent works that have carried out domain adaptation from MoNuSeg (train) 32 Vasileios Magoulianitis et al. INTERNAL to CryoNuSeg (test), have achieved an AJI of 0.452 [23] and 0.484 [67]. For the same domain shift, LG-NuSegHop achieves an AJI of 0.545. Therefore, one can infer that supervision may not always be in favor of the cross domain generalization. We contend that for this task, biology priors and human insights play a pivotal role to mitigate the domain adaptation requirement. From a complexity standpoint, LG-NuSegHop uses simple image processing operations pre and post the NuSegHop module with very low complexity. NuSegHop has a total number of parameters equal to 40K for feature extraction. Notably, the local processing pipeline and NuSegHop can be implemented in parallel to achieve a very short inference time. On the other hand, DL state-of-the-art solutions require several million of parameters and',\n",
       "  'module with very low complexity. NuSegHop has a total number of parameters equal to 40K for feature extraction. Notably, the local processing pipeline and NuSegHop can be implemented in parallel to achieve a very short inference time. On the other hand, DL state-of-the-art solutions require several million of parameters and special equipment (e.g. large GPUs) for model deployment. For instance, as a reference point, HoVer-Net requires about 11.04 seconds to predict an image of size 1000 × 1000 using a GPU with 12GB memory [8]. On the same comparison, LG- NuSegHop requires about 9.38 seconds on average to predict a histology image of the same size, adopting a multi-thread implementation and deployed on a Intel Xeon CPU E5-2620 v3 at 2.40GHz. As a final remark, it is also important to emphasize once more that within our pipeline every module is intuitive and transparent, including the feature extraction process in NuSegHop module. We underline this advantage as it is crucial for medical image solutions to be explainable to physicians, so that they can be effectively utilized in real clinical settings. 5 Conclusion This work proposes the LG-NuSegHop pipeline for unsupervised nuclei segmentation from histology images. A novel feature extraction model named NuSegHop is introduced to learn the local texture. Regarding NuSegHop, several custom-made image processing modules are pro- posed to preprocess the input image, provide a pseudo label, and post- process the predicted heatmap to increase the nuclei detection rate. Key advantages of our method are the generalization ability to unseen domains with inherent discrepancies and the small number of parame- ters. Every proposed module is intuitive and transparent, based on spe- LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 33 INTERNAL cific biological priors of the problem. In future work, we will investigate ways to focus NuSegHop feature extraction on the nuclei boundaries, aiming to improve its segmentation performance. Moreover, another research direction is the incorporation of supervision into our current pipeline, in the challenging areas where the unsupervised approach has certain limitations, thus benefiting from the expert’s knowledge. Figure 8: Visualization examples of the nuclei segmentation performance in three datasets. It is also compared the performance of the instance ROI classification within the global processing module. The true positive areas are marked with white, the false positives with yellow and the false negatives with blue. Red boxes highlight areas where the false positive removal is successful. 34 Vasileios Magoulianitis et al. INTERNAL References [1] Sabeena Beevi, Madhu S Nair, and GR Bindu, “Automatic seg- mentation of cell nuclei using Krill Herd optimization based multi- thresholding and localized active contour model”, Biocybernetics and Biomedical Engineering, 36(4), 584–96. [2] Hongmin Cai, Zhong Yang, Xinhua Cao, Weiming Xia, and Xi- aoyin Xu, “A new iterative triclass thresholding technique in im- age segmentation”, IEEE transactions on image processing, 23(3), 1038–46. [3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang, “Swin-unet: Unet-like pure transformer for medical image segmentation”, in European con- ference on computer vision, Springer, 2022, 205–18. [4] Tsai Hor Chan, Fernando Julio Cendra, Lan Ma, Guosheng',\n",
       "  'transactions on image processing, 23(3), 1038–46. [3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang, “Swin-unet: Unet-like pure transformer for medical image segmentation”, in European con- ference on computer vision, Springer, 2022, 205–18. [4] Tsai Hor Chan, Fernando Julio Cendra, Lan Ma, Guosheng Yin, and Lequan Yu, “Histopathology whole slide image analysis with heterogeneous graph representation learning”, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, 2023, 15661–70. [5] Yueru Chen, Mozhdeh Rouhsedaghat, Suya You, Raghuveer Rao, and C-C Jay Kuo, “Pixelhop++: A small successive-subspace- learning-based (ssl-based) model for image classification”, in 2020 IEEE International Conference on Image Processing (ICIP), IEEE, 2020, 3294–8. [6] Joann G Elmore, Heidi D Nelson, Margaret S Pepe, Gary M Longton, Anna NA Tosteson, Berta Geller, Tracy Onega, Patricia A Carney, Sara L Jackson, Kimberly H Allison, et al., “Variability in pathologists’ interpretations of individual breast biopsy slides: a population perspective”, Annals of internal medicine, 164(10), 649–55. [7] Pegah Faridi, Habibollah Danyali, Mohammad Sadegh Helfroush, and Mojgan Akbarzadeh Jahromi, “An automatic system for cell nuclei pleomorphism segmentation in histopathological images of breast cancer”, in 2016 IEEE Signal Processing in Medicine and Biology Symposium (SPMB), IEEE, 2016, 1–5. [8] S. Graham, Q. D. Vu, S. E A. Raza, A. Azam, Y. W. Tsang, J. T. Kwak, and N. Rajpoot, “Hover-net: Simultaneous segmentation LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 35 INTERNAL and classification of nuclei in multi-tissue histology images”, Med- ical image analysis, 58, 101563. [9] Simon Graham, Mostafa Jahanifar, Ayesha Azam, Mohammed Nimir, Yee-Wah Tsang, Katherine Dodd, Emily Hero, Harvir Sa- hota, Atisha Tank, Ksenija Benes, et al., “Lizard: A large-scale dataset for colonic nuclear instance segmentation and classifica- tion”, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, 684–93. [10] Metin N Gurcan, Laura E Boucheron, Ali Can, Anant Madab- hushi, Nasir M Rajpoot, and Bulent Yener, “Histopathological image analysis: A review”, IEEE reviews in biomedical engineer- ing, 2, 147–71. [11] Hongliang He, Zhongyi Huang, Yao Ding, Guoli Song, Lin Wang, Qian Ren, Pengxu Wei, Zhiqiang Gao, and Jie Chen, “Cdnet: Centripetal direction network for nuclear instance segmentation”, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, 4026–35. [12] Hongliang He, Jun Wang, Pengxu Wei, Fan Xu, Xiangyang Ji, Chang Liu, and Jie Chen, “Toposeg: Topology-aware nuclear in- stance segmentation”, in Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, 2023, 21307–16. [13] Joy Hsu, Wah Chiu, and Serena Yeung, “Darcnn: Domain adap- tive region-based convolutional neural network for unsupervised instance segmentation in biomedical images”, in Proceedings of the IEEE/CVF conference on computer vision and pattern recog- nition, 2021, 1003–12. [14] Iqra Kiran, Basit Raza, Areesha Ijaz, and Muazzam A Khan, “DenseRes-Unet: Segmentation of overlapped/clustered nuclei from multi organ histopathology images”, Computers in Biology and Medicine, 143, 105267. [15] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár, “Panoptic segmentation”, in Proceedings of the IEEE/CVF conference on computer vision and pattern recogni- tion, 2019, 9404–13. [16] Yousef Al-Kofahi, Wiem Lassoued, William Lee, and Badrinath Roysam, “Improved',\n",
       "  'organ histopathology images”, Computers in Biology and Medicine, 143, 105267. [15] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár, “Panoptic segmentation”, in Proceedings of the IEEE/CVF conference on computer vision and pattern recogni- tion, 2019, 9404–13. [16] Yousef Al-Kofahi, Wiem Lassoued, William Lee, and Badrinath Roysam, “Improved automatic detection and segmentation of cell nuclei in histopathology images”, IEEE Transactions on Biomed- ical Engineering, 57(4), 841–52. 36 Vasileios Magoulianitis et al. INTERNAL [17] Can Fahrettin Koyuncu, Ece Akhan, Tulin Ersahin, Rengul Cetin- Atalay, and Cigdem Gunduz-Demir, “Iterative h-minima-based marker-controlled watershed for cell nucleus segmentation”, Cy- tometry Part A, 89(4), 338–49. [18] Neeraj Kumar, Ruchika Verma, Deepak Anand, Yanning Zhou, Omer Fahri Onder, Efstratios Tsougenis, Hao Chen, Pheng-Ann Heng, Jiahui Li, Zhiqiang Hu, et al., “A multi-organ nucleus segmentation challenge”, IEEE transactions on medical imaging, 39(5), 1380–91. [19] Neeraj Kumar, Ruchika Verma, Sanuj Sharma, Surabhi Bhar- gava, Abhishek Vahadane, and Amit Sethi, “A dataset and a technique for generalized nuclear segmentation for computational pathology”, IEEE transactions on medical imaging, 36(7), 1550– 60. [20] C-C Jay Kuo and Azad M Madni, “Green learning: Introduction, examples and outlook”, Journal of Visual Communication and Image Representation, 103685. [21] C-C Jay Kuo, Min Zhang, Siyang Li, Jiali Duan, and Yueru Chen, “Interpretable convolutional neural networks via feedforward de- sign”, Journal of Visual Communication and Image Representa- tion, 60, 346–59. [22] Shyam Lal, Devikalyan Das, Kumar Alabhya, Anirudh Kanfade, Aman Kumar, and Jyoti Kini, “NucleiSegNet: Robust deep learn- ing architecture for the nuclei segmentation of liver cancer histopathol- ogy images”, Computers in Biology and Medicine, 128, 104075. [23] Zhongyu Li, Chaoqun Li, Xiangde Luo, Yitian Zhou, Jihua Zhu, Cunbao Xu, Meng Yang, Yenan Wu, and Yifeng Chen, “Toward source-free cross tissues histopathological cell segmentation via target-specific finetuning”, IEEE Transactions on Medical Imag- ing, 42(9), 2666–77. [24] H. Liang, Zh. Cheng, H. Zhong, A. Qu, and L. Chen, “A region- based convolutional network for nuclei detection and segmen- tation in microscopy images”, Biomedical Signal Processing and Control, 71, 103276. [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick, “Microsoft coco: Common objects in context”, in Computer Vision– LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 37 INTERNAL ECCV 2014: 13th European Conference, Zurich, Switzerland, Septem- ber 6-12, 2014, Proceedings, Part V 13, Springer, 2014, 740–55. [26] Yi Lin, Zhiyong Qu, Hao Chen, Zhongke Gao, Yuexiang Li, Lili Xia, Kai Ma, Yefeng Zheng, and Kwang-Ting Cheng, “Nuclei segmentation with point annotations from pathology images via self-supervised learning and co-training”, Medical Image Analysis, 89, 102933. [27] Yi Lin, Zeyu Wang, Dong Zhang, Kwang-Ting Cheng, and Hao Chen, “BoNuS: Boundary Mining for Nuclei Segmentation with Partial Point Labels”, IEEE Transactions on Medical Imaging. [28] Dongnan Liu, Donghao Zhang, Yang Song, Fan Zhang, Lauren O’Donnell, Heng Huang, Mei Chen, and Weidong Cai, “Unsuper- vised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting”, in Proceedings of the IEEE/CVF conference on computer vision and pattern recogni- tion, 2020, 4243–52. [29] Wenxi Liu, Qing Zhang, Qi Li, and Shu',\n",
       "  'Song, Fan Zhang, Lauren O’Donnell, Heng Huang, Mei Chen, and Weidong Cai, “Unsuper- vised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting”, in Proceedings of the IEEE/CVF conference on computer vision and pattern recogni- tion, 2020, 4243–52. [29] Wenxi Liu, Qing Zhang, Qi Li, and Shu Wang, “Contrastive and uncertainty-aware nuclei segmentation and classification”, Com- puters in Biology and Medicine, 108667. [30] Xiaofeng Liu, Fangxu Xing, Hanna K Gaggin, Weichung Wang, C-C Jay Kuo, Georges El Fakhri, and Jonghye Woo, “Segmenta- tion of cardiac structures via successive subspace learning with saab transform from cine mri”, in 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology So- ciety (EMBC), IEEE, 2021, 3535–8. [31] Xiaofeng Liu, Fangxu Xing, Chao Yang, Chung-Chieh Jay Kuo, Suma Babu, Georges El Fakhri, Thomas Jenkins, and Jonghye Woo, “Voxelhop: Successive subspace learning for als disease clas- sification using structural mri”, IEEE journal of biomedical and health informatics, 26(3), 1128–39. [32] Xiaoming Liu, Zhengsheng Guo, Jun Cao, and Jinshan Tang, “MDC-net: A new convolutional neural network for nucleus seg- mentation in histopathology images with distance maps and con- tour information”, Computers in Biology and Medicine, 135, 104543. [33] Tommy Löfstedt, Patrik Brynolfsson, Thomas Asklund, Tufve Nyholm, and Anders Garpebring, “Gray-level invariant Haralick texture features”, PloS one, 14(2), e0212110. 38 Vasileios Magoulianitis et al. INTERNAL [34] Wei Lou, Xiang Wan, Guanbin Li, Xiaoying Lou, Chenghang Li, Feng Gao, and Haofeng Li, “Structure embedded nucleus classifi- cation for histopathology images”, IEEE Transactions on Medical Imaging. [35] Vasileios Magoulianitis, Catherine A Alexander, and C-C Jay Kuo, “A Comprehensive Overview of Computational Nuclei Seg- mentation Methods in Digital Pathology”, arXiv preprint arXiv:2308.08112. [36] Vasileios Magoulianitis, Peida Han, Yijing Yang, and C-C Jay Kuo, “An Unsupervised Parameter-Free Nuclei Segmentation Method for Histology Images”, in 2022 IEEE International Conference on Image Processing (ICIP), IEEE, 2022, 226–30. [37] Vasileios Magoulianitis, Jiaxin Yang, Yijing Yang, Jintang Xue, Masatomo Kaneko, Giovanni Cacciamani, Andre Abreu, Vinay Duddalwar, C-C Jay Kuo, Inderbir S Gill, et al., “PCa-RadHop: A transparent and lightweight feed-forward method for clinically significant prostate cancer segmentation”, Computerized Medical Imaging and Graphics, 102408. [38] Vasileios Magoulianitis, Yijing Yang, and C-C Jay Kuo, “HUNIS: High-Performance Unsupervised Nuclei Instance Segmentation”, in 2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP), IEEE, 2022, 1–5. [39] Amirreza Mahbod, Gerald Schaefer, Benjamin Bancher, Chris- tine Löw, Georg Dorffner, Rupert Ecker, and Isabella Ellinger, “CryoNuSeg: A dataset for nuclei instance segmentation of cryosec- tioned H&E-stained histological images”, Computers in biology and medicine, 132, 104349. [40] Esha Sadia Nasir, Arshi Parvaiz, and Muhammad Moazam Fraz, “Nuclei and glands instance segmentation in histology images: a narrative review”, Artificial Intelligence Review, 56(8), 7909–64. [41] Hady Ahmady Phoulady, Dmitry B Goldgof, Lawrence O Hall, and Peter R Mouton, “Nucleus segmentation in histology im- ages with hierarchical multilevel thresholding”, in Medical Imag- ing 2016: Digital Pathology, Vol. 9791, SPIE, 2016, 280–5. [42] Markus Plass, Michaela Kargl, Tim-Rasmus Kiehl, Peter Regit- nig, Christian Geißler, Theodore Evans, Norman Zerbe, Rita Car- valho, Andreas Holzinger, and Heimo Müller, “Explainability and causability in digital',\n",
       "  'segmentation in histology im- ages with hierarchical multilevel thresholding”, in Medical Imag- ing 2016: Digital Pathology, Vol. 9791, SPIE, 2016, 280–5. [42] Markus Plass, Michaela Kargl, Tim-Rasmus Kiehl, Peter Regit- nig, Christian Geißler, Theodore Evans, Norman Zerbe, Rita Car- valho, Andreas Holzinger, and Heimo Müller, “Explainability and causability in digital pathology”, The Journal of Pathology: Clin- ical Research, 9(4), 251–60. LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 39 INTERNAL [43] H. Qu, P. Wu, Q. Huang, J. Yi, G. M Riedlinger, S. De, and D. N Metaxas, “Weakly supervised deep nuclei segmentation us- ing points annotation in histopathology images”, in International Conference on Medical Imaging with Deep Learning, PMLR, 2019, 390–400. [44] H. Qu, P. Wu, Q. Huang, J. Yi, Z. Yan, K. Li, G. M Riedlinger, S. De, S. Zhang, and D. N Metaxas, “Weakly supervised deep nuclei segmentation using partial points annotation in histopathology images”, IEEE transactions on medical imaging, 39(11), 3655– 66. [45] H. Qu, J. Yi, Q. Huang, Pengxiang Wu, and D. Metaxas, “Nuclei segmentation using mixed points and masks selected from uncer- tainty”, in 2020 IEEE 17th International Symposium on Biomed- ical Imaging (ISBI), IEEE, 2020, 973–6. [46] Jos BTM Roerdink and Arnold Meijster, “The watershed trans- form: Definitions, algorithms and parallelization strategies”, Fun- damenta informaticae, 41(1-2), 187–228. [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox, “U-net: Convolutional networks for biomedical image segmentation”, in Medical Image Computing and Computer-Assisted Intervention– MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, Springer, 2015, 234– 41. [48] Kaushiki Roy, Subhadeep Saha, Debapriya Banik, and Debotosh Bhattacharjee, “Nuclei-Net: A multi-stage fusion model for nuclei segmentation in microscopy images”. [49] Mihir Sahasrabudhe, Stergios Christodoulidis, Roberto Salgado, Stefan Michiels, Sherene Loi, Fabrice André, Nikos Paragios, and Maria Vakalopoulou, “Self-supervised nuclei segmentation in histopatho- logical images using attention”, in Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd Interna- tional Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V 23, Springer, 2020, 393–402. [50] Massimo Salvi, Nicola Michielli, and Filippo Molinari, “Stain Color Adaptive Normalization (SCAN) algorithm: Separation and standardization of histological stains in digital pathology”, Com- puter methods and programs in biomedicine, 193, 105506. 40 Vasileios Magoulianitis et al. INTERNAL [51] Zhiyun Song, Penghui Du, Junpeng Yan, Kailu Li, Jianzhong Shou, Maode Lai, Yubo Fan, and Yan Xu, “Nucleus-aware self- supervised pretraining using unpaired image-to-image translation for histopathology images”, IEEE Transactions on Medical Imag- ing. [52] Kuan Tian, Jun Zhang, Haocheng Shen, Kezhou Yan, Pei Dong, Jianhua Yao, Shannon Che, Pifu Luo, and Xiao Han, “Weakly- supervised nucleus segmentation based on point annotations: A coarse-to-fine self-stimulated learning strategy”, in Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part V 23, Springer, 2020, 299–308. [53] Katarzyna Tomczak, Patrycja Czerwińska, and Maciej Wiznerow- icz, “Review The Cancer Genome Atlas (TCGA): an immeasur- able source of knowledge”, Contemporary Oncology/Współczesna Onkologia, 2015(1), 68–77. [54] Thaína A Azevedo Tosta, Paulo Rogério de Faria, Leandro Alves Neves, and Marcelo Zanchetta do Nascimento, “Computational normalization of H&E-stained histological images: Progress, chal- lenges and future',\n",
       "  'Czerwińska, and Maciej Wiznerow- icz, “Review The Cancer Genome Atlas (TCGA): an immeasur- able source of knowledge”, Contemporary Oncology/Współczesna Onkologia, 2015(1), 68–77. [54] Thaína A Azevedo Tosta, Paulo Rogério de Faria, Leandro Alves Neves, and Marcelo Zanchetta do Nascimento, “Computational normalization of H&E-stained histological images: Progress, chal- lenges and future potential”, Artificial intelligence in medicine, 95, 118–32. [55] Vi Thi-Tuong Vo and Soo-Hyung Kim, “Mulvernet: nucleus seg- mentation and classification of pathology images using the HoVer- Net and multiple filter units”, Electronics, 12(2), 355. [56] Huadeng Wang, Guang Xu, Xipeng Pan, Zhenbing Liu, Rushi Lan, and Xiaonan Luo, “Multi-task generative adversarial learn- ing for nuclei segmentation with dual attention and recurrent con- volution”, Biomedical Signal Processing and Control, 75, 103558. [57] Ziyue Wang, Ye Zhang, Yifeng Wang, Linghan Cai, and Yongbing Zhang, “Dynamic Pseudo Label Optimization in Point-Supervised Nuclei Segmentation”, arXiv preprint arXiv:2406.16427. [58] K. Y. Win, S. Choomchuay, and K. Hamamoto, “K mean clus- tering based automated segmentation of overlapping cell nuclei in pleural effusion cytology images”, in 2017 International Con- ference on Advanced Technologies for Communications (ATC), IEEE, 2017, 265–9. [59] Khin Yadanar Win and Somsak Choomchuay, “Automated seg- mentation of cell nuclei in cytology pleural fluid images using LG-NuSegHop: A Local-to-Global Self-Supervised Pipeline For Nuclei Instance Segmentation 41 INTERNAL OTSU thresholding”, in 2017 International Conference on Digi- tal Arts, Media and Technology (ICDAMT), IEEE, 2017, 14–8. [60] Lipeng Xie, Jin Qi, Lili Pan, and Samad Wali, “Integrating deep convolutional neural networks with marker-controlled watershed for overlapping nuclei segmentation in histopathology images”, Neurocomputing, 376, 166–79. [61] Xinpeng Xie, Jiawei Chen, Yuexiang Li, Linlin Shen, Kai Ma, and Yefeng Zheng, “Instance-aware self-supervised learning for nuclei segmentation”, in Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Confer- ence, Lima, Peru, October 4–8, 2020, Proceedings, Part V 23, Springer, 2020, 341–50. [62] Fuyong Xing, Yuanpu Xie, and Lin Yang, “An automatic learning- based framework for robust nucleus segmentation”, IEEE trans- actions on medical imaging, 35(2), 550–66. [63] Jiaxin Yang, Vasileios Magoulianitis, Catherine Aurelia Christie Alexander, Jintang Xue, Masatomo Kaneko, Giovanni Caccia- mani, Andre Abreu, Vinay Duddalwar, C-C Jay Kuo, Inderbir S Gill, et al., “GUSL: A Novel and Efficient Machine Learn- ing Model for Prostate Segmentation on MRI”, arXiv preprint arXiv:2506.23688. [64] Yijing Yang, Vasileios Magoulianitis, and C-C Jay Kuo, “E-pixelhop: An enhanced pixelhop method for object classification”, in 2021 Asia-Pacific Signal and Information Processing Association An- nual Summit and Conference (APSIPA ASC), IEEE, 2021, 1475– 82. [65] Yijing Yang, Wei Wang, Hongyu Fu, C-C Jay Kuo, et al., “On su- pervised feature selection from high dimensional feature spaces”, APSIPA Transactions on Signal and Information Processing, 11(1). [66] Inwan Yoo, Donggeun Yoo, and Kyunghyun Paeng, “Pseudoed- genet: Nuclei segmentation only with point annotations”, in Med- ical Image Computing and Computer Assisted Intervention– MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part I 22, Springer, 2019, 731– 9. [67] Ye Zhang, Yifeng Wang, Zijie Fang, Hao Bian, Linghan Cai, Ziyue Wang, and Yongbing Zhang, “DAWN: Domain-Adaptive 42 Vasileios Magoulianitis et al. INTERNAL Weakly Supervised Nuclei Segmentation via Cross-Task Interac-',\n",
       "  'Intervention– MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part I 22, Springer, 2019, 731– 9. [67] Ye Zhang, Yifeng Wang, Zijie Fang, Hao Bian, Linghan Cai, Ziyue Wang, and Yongbing Zhang, “DAWN: Domain-Adaptive 42 Vasileios Magoulianitis et al. INTERNAL Weakly Supervised Nuclei Segmentation via Cross-Task Interac- tions”, arXiv preprint arXiv:2404.14956. [68] Xin Zheng, Yong Wang, Guoyou Wang, and Jianguo Liu, “Fast and robust segmentation of white blood cell images by self-supervised learning”, Micron, 107, 55–71. [69] Yang Zhou, Yongjian Wu, Zihua Wang, Bingzheng Wei, Maode Lai, Jianzhong Shou, Yubo Fan, and Yan Xu, “Cyclic learning: Bridging image-level labels and nuclei instance segmentation”, IEEE Transactions on Medical Imaging, 42(10), 3104–16. [70] Yanning Zhou, Omer Fahri Onder, Qi Dou, Efstratios Tsouge- nis, Hao Chen, and Pheng-Ann Heng, “Cia-net: Robust nuclei instance segmentation with contour-aware information aggrega- tion”, in Information Processing in Medical Imaging: 26th Inter- national Conference, IPMI 2019, Hong Kong, China, June 2–7, 2019, Proceedings 26, Springer, 2019, 682–93. [71] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros, “Unpaired image-to-image translation using cycle-consistent ad- versarial networks”, in Proceedings of the IEEE international con- ference on computer vision, 2017, 2223–32.'],\n",
       " ['SurgiATM: A Physics-Guided Plug-and-Play Model for Deep Learning-Based Smoke Removal in Laparoscopic Surgery Mingyu Sheng, Jianan Fan, Dongnan Liu, Guoyan Zheng, Ron Kikinis, Weidong Cai Abstract— During laparoscopic surgery, smoke gener- ated by tissue cauterization can significantly degrade the visual quality of endoscopic frames, increasing the risk of surgical errors and hindering both clinical decision- making and computer-assisted visual analysis. Conse- quently, removing surgical smoke is critical to ensuring patient safety and maintaining operative efficiency. In this study, we propose the Surgical Atmospheric Model (Sur- giATM) for surgical smoke removal. SurgiATM statistically bridges a physics-based atmospheric model and data- driven deep learning models, combining the superior gen- eralizability of the former with the high accuracy of the latter. Furthermore, SurgiATM is designed as a lightweight, plug-and-play module that can be seamlessly integrated into diverse surgical desmoking architectures to enhance their accuracy and stability, better meeting clinical require- ments. It introduces only two hyperparameters and no additional trainable weights, preserving the original net- work architecture with minimal computational and modi- fication overhead. We conduct extensive experiments on three public surgical datasets with ten desmoking meth- ods, involving multiple network architectures and cover- ing diverse procedures, including cholecystectomy, par- tial nephrectomy, and diaphragm dissection. The results demonstrate that incorporating SurgiATM commonly re- duces the restoration errors of existing models and rela- tively enhances their generalizability, without adding any trainable layers or weights. This highlights the conve- nience, low cost, effectiveness, and generalizability of the proposed method. The code for SurgiATM is released at https://github.com/MingyuShengSMY/SurgiATM. Index Terms— Endoscopy, laparoscopic surgery, smoke removal, plug-and-play, parameter-efficient I. INTRODUCTION M INIMALLY invasive surgery (MIS) has noticeably ben- efited patients by reducing risk and enabling faster recovery [1], [2]. However, surgical smoke, commonly gen- erated by high-energy instruments (e.g., electrosurgical units, laser surgical tools, and high-frequency ultrasonic scalpels), Mingyu Sheng, Jianan Fan, Dongnan Liu, and Weidong Cai are with the School of Computer Science, the University of Sydney, Aus- tralia (e-mail: {mshe0136, jfan6480}@uni.sydney.edu.au, {dongnan.liu, tom.cai}@sydney.edu.au). Guoyan Zheng is with the School of Biomedical Engi- neering, the Shanghai Jiao Tong University, China (e-mail: guoyan.zheng@sjtu.edu.cn). Ron Kikinis is with the Department of Radiology, Harvard Medical School, USA (e-mail: kikinis@bwh.harvard.edu). Weidong Cai is the corresponding author. can severely impair endoscopic visibility, posing a notable challenge. The presence of dense smoke obstructs the sur- geon’s field of view, increases the risk of surgical errors, and reduces procedural efficiency. Moreover, smoke-induced image degradation adversely affects vision-based downstream surgical tasks, such as phase recognition [3]–[8], tool detec- tion, semantic segmentation [9]–[11], depth estimation [12]– [14], 3D surface reconstruction [15], [16], etc., which require both high frame quality and real-time processing speed [17]– [21] . At present, mechanical smoke evacuation or filtration is commonly achieved through hardware-based systems, which often require additional effort and impose extra workload during surgery, disrupting the surgical workflow and causing surgeon fatigue [22], [23]. Therefore, ensuring a clear visual field in real time is critical [24]–[26]. To this end, real-time image processing has gained increas- ing attention as a means of digitally removing smoke during intraoperative visualization in a more streamlined',\n",
       "  'workload during surgery, disrupting the surgical workflow and causing surgeon fatigue [22], [23]. Therefore, ensuring a clear visual field in real time is critical [24]–[26]. To this end, real-time image processing has gained increas- ing attention as a means of digitally removing smoke during intraoperative visualization in a more streamlined and auto- mated way by leveraging computer vision and deep learning techniques. It offers a cost-effective, low-risk, and efficient alternative for surgical smoke removal [27]–[31]. However, traditional models often struggle with this task, as they were originally designed for outdoor hazy environments. Unlike nat- ural hazy images, in vivo surgical scenes present unique chal- lenges, including complex physics parameter estimation due to non-uniform distribution of smoke, spatially heterogeneous illumination caused by radial light attenuation, inaccurate color restoration on non-smoke objects (e.g., surgical instruments), severe information loss resulting from highly dense smoke ob- scuration, and temporal incoherence of smoke occurrence due to the intermittent use of energy devices [32]–[37]. To address these limitations, deep learning-based approaches have been increasingly adopted for their powerful non-linear mapping capabilities and ease of deployment in laparoscopic systems [38], [39]. These methods show considerable promise for surgical desmoking, offering more consistent color restoration and improved visual clarity. Nevertheless, due to the scarcity of paired desmoking benchmarks, most deep learning-based models face the challenge of generalizability, as they can produce high-quality predictions on a dataset with synthetic smoke while suffering from performance degradation on an- other dataset with real smoke. This is an important concern for real-world clinical applications [40]. To overcome these limitations, we propose the Surgical arXiv:2511.05059v1 [cs.CV] 7 Nov 2025 Atmospheric Model (SurgiATM), a physics-guided model tailored to the laparoscopic desmoking task. SurgiATM can be seamlessly embedded into existing neural networks with minimal modifications to their original architectures, while enhancing both performance and generalizability. Notably, SurgiATM introduces no additional trainable parameters, indicating that the observed improvements arise from the integration of physical guidance rather than merely increasing model trainable weights and complexity. Specifically, the main contributions of this study are: 1) We propose SurgiATM, a parameter-efficient, plug-and- play, physics-guided model, that improves performance and generalizability of surgical desmoking approaches to better meet clinical requirements. 2) The proposed model introduces zero trainable weights and can be seamlessly integrated into state-of-the-art (SOTA) methods built upon various backbone architec- tures (e.g., U-Net, Swin Transformer). 3) We validate SurgiATM on three public surgical datasets, demonstrating improvements in both performance and generalizability across multiple SOTA desmoking meth- ods and various surgical scenes, thereby confirming its effectiveness, stability, and generalizability. II. RELATED WORK A. Physics-Based Methods The Atmospheric Scattering Model (ASM) is a widely used physical model that describes the formation of hazy natural images by modeling the absorption, scattering, and reflection of light by objects and particles in the atmospheric medium [41], [42]. Based on the ASM, clear images can be restored from degraded observations by estimating the required phys- ical parameters, such as medium transmission, atmospheric light, and scene depth, which has motivated extensive research in the field. In early studies, the Gaussian distribution and Markov Random Field (MRF) model',\n",
       "  '[41], [42]. Based on the ASM, clear images can be restored from degraded observations by estimating the required phys- ical parameters, such as medium transmission, atmospheric light, and scene depth, which has motivated extensive research in the field. In early studies, the Gaussian distribution and Markov Random Field (MRF) model were classic and well- known techniques for estimating the parameters [43], [44]. Then, [45] proposed the Dark Channel Prior (DCP) model for natural image dehazing, in which the medium transmission and global atmospheric light were statistically estimated according to their physical properties. The DCP assumes that haze in an image is uniformly distributed and that, in hazeless images, at least one color channel in most non-sky patches or pixels has very low intensity. In contrast, haze tends to raise pixel intensities across all channels, causing the dark channel to become brighter and the image to appear more whitish. Since 2012, the field of surgical desmoking has gained increasing attention, with many valuable studies published over the past decade. [46] developed a digital defogging system that uses the DCP to enhance image quality in laparoscopic surgery. [47] combined the desmoking and denoising tasks of laparo- scopic images and transformed them into a Bayesian inference problem by proposing a novel probabilistic graphical model. [48] extends [47] by introducing additional image priors of color and texture derived from sparse dictionary models. In [49], histogram equalization is combined with an adaptive DCP to remove surgical smoke. [27] rearranged the original atmospheric model by introducing an atmospheric veil to represent the transmission map and then solved the parameter estimation problem via the Poisson equation in the frequency domain. [50] first developed an energy function to optimize and estimate a smoke veil to recover smokeless images based on [37]. Combining the hardware camera and the software algorithm, [51] proposed a method based on multiple-exposure fusion, which enhances local contrast and color saturation, thereby improving image quality. These works offer valuable insights into the feasibility of physics-based approaches for surgical desmoking, encouraging further research. B. Deep Learning-Based Methods To achieve higher restoration quality, the powerful fitting and generative capabilities of Deep Neural Networks (DNNs) offer an alternative solution [52]–[57]. Current deep learning- based desmoking methods can be broadly categorized into two frameworks: non-adversarial learning and adversarial learning. 1) Non-adversarial Learning Methods: This framework indi- cates that the model is tasked with learning a mapping from smoky or hazy images to their corresponding clean versions. [58] trained a U-Net model [59] on laparoscopic videos with synthetic smoke using Blender1. [56] attempted to address surgical smoke removal via AODNet [53], a Convolutional Neural Network (CNN) designed to estimate global light illuminance and the transmission map simultaneously. Similar to [58], [56] trained the model on synthetically generated smoky in vivo frames by blending Perlin noise with clean frames. Furthermore, instead of training on synthetic smoky frames, [60] attempted to use adjacent frames, along with the presence or absence of smoke, as training inputs and corresponding ground truth. Similarly, in the most recent study [40], they implemented a real-world self-supervised video desmoking approach',\n",
       "  'blending Perlin noise with clean frames. Furthermore, instead of training on synthetic smoky frames, [60] attempted to use adjacent frames, along with the presence or absence of smoke, as training inputs and corresponding ground truth. Similarly, in the most recent study [40], they implemented a real-world self-supervised video desmoking approach by capturing the informative features from frames recorded prior to the activation of high-energy surgical instruments. Based on the above training data strategy, considerable efforts have been made to improve performance further. [61] designed a U-Net-like network and proposed Laplacian image pyramid decomposition to capture multi-scale features. [28] presented a novel generative-collaborative learn- ing framework (De-smokeGCN), dividing the desmoking into two sub-tasks: smoke detection and smoke removal. A smoke detector predicted the smoke mask, which was leveraged as a prior for the smoke removal network and motivated several subsequent studies [62], [63]. [64] improved the decoder part of the U-Net by appending a convolutional block attention module to generate a guidance mask for feature maps. [65] introduced a workflow to simultaneously remove smoke and restore colors, using U-Net as the backbone. [66] leveraged deformable convolution incorporated with a mutual attention mechanism to model temporal features and combined Local Binary Patterns with input frames as a texture prior. [67] devel- oped a method consisting of three U-Nets to produce distinct outputs, each serving as an element in the energy function derived from their previous study [50]. [30] developed a sur- gical desmoking method based on the Diffusion Model [68], 1https://www.blender.org/ [69], incorporating a multi-level frequency analysis module to integrate features across different frequency bands. In addition to convolutional network architectures, the Swin Transformer (SwinT) [70] has recently gained increasing attention in this field because of its hierarchical representation and efficient local-to-global attention mechanism. [71] proposed a network that utilized several convolutional layers to extract low-level features and then used SwinT to analyze deep and global information from smoky frames. [31] developed a U-shaped transformer model based on U-Net and SwinT to enhance feature representation by combining local detail extraction with global context modeling. 2) Adversarial Learning Methods: Apart from conventional image-to-image translation, converting laparoscopic images from the smoky domain to the smokeless domain offers an alternative paradigm for addressing the surgical desmoking problem. By regarding desmoking as an image style transfer task, the Generative Adversarial Network (GAN) [72] and its variants have emerged as effective solutions. [57] modeled a mapping between smoky and smoke-free domains using a conventional GAN as the backbone and leveraging the perceptual image quality score as a loss function to improve the result quality. [29] developed a method based on a Con- ditional Generative Adversarial Network (cGAN) [73] and used an embedded dark channel as a prior. Compared to the above GANs, the Cycle-Consistent Adversarial Network (CycleGAN) [74] is well-suited for image style transfer tasks and is therefore more commonly used for surgical smoke removal [75]–[77]. [78] enhanced CycleGAN by incorporating multi-scale feature extraction and introducing an upsampling loss to improve the contrast of the desmoked frames. In their subsequent studies, they further included structure-consistency loss and designed a',\n",
       "  'well-suited for image style transfer tasks and is therefore more commonly used for surgical smoke removal [75]–[77]. [78] enhanced CycleGAN by incorporating multi-scale feature extraction and introducing an upsampling loss to improve the contrast of the desmoked frames. In their subsequent studies, they further included structure-consistency loss and designed a refinement module to improve restoration quality [79], [80]. [81] further designed internal-channel and dark-channel loss functions based on the characteristics of smoky pixels to improve model performance. Similar to [28], [63] added an extra detection network to the CycleGAN archi- tecture to estimate the smoke mask as a prior for subsequent desmoking. [62] adopted a multilevel strategy to adaptively learn non-homogeneous smoke features, using the predicted smoke distribution as a prior. To date, existing studies have made significant contribu- tions; however, some limitations remain, as the feasibility and practicality of physics-based methods are primarily restricted by accuracy, while those of deep learning-based methods are mainly hindered by robustness and generalizability. In this paper, we conduct a statistical analysis of their respective advantages and subsequently derive a plug-and-play module to augment the existing end-to-end deep learning desmoking methods, further enhancing their performance and generaliz- ability across different benchmarks. III. METHODS The proposed method is motivated by an interesting obser- vation in laparoscopic surgical video smoke removal: predic- tions from most desmoking methods yield a Laplacian-like error distribution (see Fig. 1). Building upon this finding, we introduce a new restoration formula that minimizes prediction errors and enhances model robustness by blending physics- based and deep learning-based approaches from the perspec- tive of probability distributions. A. Background 1) Physics-Based Modeling: The atmospheric scattering model [41]–[44] is typically formulated as: I(x, c) = J(x, c) · t(x) + A(c) · (1 −t(x)), (1) where c denotes a color channel; x represents the spatial location of a pixel; I ∈RH×W ×C is the observed image, where C, W, and H are the number of color channels, width and height, respectively; J indicates the corresponding haze- free image; A ∈R1×1×C denotes the global atmospheric light; and t ∈RH×W ×1 is the medium transmission, which can alternatively be expressed as t = 1−S with smoke density S. In particular, t and J can be further formulated as: t(x) = e−β(λ)·d(x), (2) J(x, c) = A(c) · ρ(x, c), (3) where β(λ) ∈R is the scattering coefficient for a specific wavelength λ of light; d ∈RH×W ×1 represents the scene depth; and ρ ∈RH×W ×C is the normalized radiance [42] or reflectance [44] of a scene point. [45] proposed the Dark Channel Prior (DCP), which estimates the transmission map t using the dark channel: t(x) = 1 −D(x) = 1 −min c∈C \\x12 min u∈Ω(x;z) \\x12I(u, c) A(c) \\x13\\x13 , (4) where D denotes the dark channel and Ω(x) represents a square window of size z centered at location x. Accordingly, the restoration formula is given by: JDCP(x, c) = I(x, c) −A(c) 1 −D(x) + A(c), (5) where JDCP indicates that the image J is estimated using DCP; 1 −D(x) is empirically clipped by a lower bound t0 in',\n",
       "  'a square window of size z centered at location x. Accordingly, the restoration formula is given by: JDCP(x, c) = I(x, c) −A(c) 1 −D(x) + A(c), (5) where JDCP indicates that the image J is estimated using DCP; 1 −D(x) is empirically clipped by a lower bound t0 in practical implementations to prevent division by zero, while this application-based adjustment is omitted for simplicity and clarity of the subsequent mathematical derivation. 2) Deep Learning-Based Modeling: Clearly, in pixels where the smoke density is high and white or gray surgical instru- ments are present (i.e., D →1), the conventional physics- based model (5) may fail. Moreover, radial light attenuation can violate the assumption that A ∈R1×1×C; under such conditions, A is more appropriately represented as A ∈ RH×W ×C. Adhering to the original assumption may intro- duce more errors, whereas adopting the latter formulation significantly increases the degrees of freedom of the solution space. Therefore, Deep Neural Network (DNN), a powerful technique for non-linear mapping, is widely employed for Surgical Smoky Video Natural Hazy Image 0.00 0.01 0.01 0.02 0.02 0.03 0.03 0.04 0.04 -255 -204 -153 -102 -51 0 51 102 153 204 255 Natural Surgical Laplacian-Like Gaussian-Like Error Distribution Fig. 1. The left images are from a natural dehazing dataset, O-HAZE [82]; the middle group is from a real-world surgical desmoking benchmark VASST-desmoke [23]; and the line chart (right) shows the difference in error distributions between surgical desmoking and natural dehazing, computed from the entire dataset. The 1st row displays the hazy or smoky images; the 2nd row shows the smoke or haze mask estimated from the ground truth; and the 3rd row presents the error magnitude map of the DCP prediction. smoke removal. Its formulation and training objective are typically expressed as: JDNN(x, c) =Φθ \\x10 I(x, c), ˜Z \\x11 , (6) θ∗= arg min θ L(J, JDNN; X, θ), (7) where Φ indicates a non-linear mapping parameterized by trainable weights θ; θ∗represents the optimized weights obtained through training on the dataset X; ˜Z and L denote priors and loss function, respectively, as determined by the specific method design. Nevertheless, its performance and generalizability may be limited when deployed in unseen surgical scenarios. B. Mixture-of-Experts and Optimization Inspired by the Mixture-of-Experts (MoE), an effective mechanism for complementary advantages [83], [84], we begin with an ideal gating model: JMoE(x, c) = ˜W(x) · JDCP(x, c) + (1 −˜W(x)) · JDNN(x, c), (8) where JMoE is the restored result of the gating model, and ˜W ∈{0, 1}H×W ×1 is a pixel-wise indicator specifying which method is activated for smoke removal. Specifically, ˜W(x) = 1 if JDCP(x, c) is deemed more reliable than JDNN(x, c); oth- erwise, ˜W(x) = 0. Optimizing ˜W facilitates the most accurate prediction. For simplification and tractability, ˜W is relaxed to a continuous domain, yielding W ∈[0, 1]H×W ×1, and the restorations are formulated using additive error models: JDCP(x, c) = J(x, c) + εDCP(x, c), (9) JDNN(x, c) = J(x, c) + εDNN(x, c), (10) where the predicted image is represented as the ground truth augmented by',\n",
       "  'is relaxed to a continuous domain, yielding W ∈[0, 1]H×W ×1, and the restorations are formulated using additive error models: JDCP(x, c) = J(x, c) + εDCP(x, c), (9) JDNN(x, c) = J(x, c) + εDNN(x, c), (10) where the predicted image is represented as the ground truth augmented by an independent and identically distributed (i.i.d.) error term ε ∈RH×W ×C. The error term is commonly assumed to follow a zero-mean Gaussian distribution with variance σ2, denoted as ε(x, c) i.i.d. ∼ N(µ = 0, σ2), which is a classic assumption in statistical learning. Then, we have: JMoE(x, c) = J(x, c) + εMoE(x, c), (11) εMoE(x, c) = W(x) · εDCP(x, c) + (1 −W(x)) · εDNN(x, c). (12) TABLE I COMPARISON OF PREDICTION ERROR DISTRIBUTIONS WITH GAUSSIAN AND LAPLACIAN DISTRIBUTIONS VIA JENSEN-SHANNON (JS) DIVERGENCE IN VASST-DESMOKE WITH REAL-WORLD SMOKE AND CHOLEC80 WITH SYNTHETIC SMOKE Method VASST-desmoke Cholec80 Gaussian Laplacian Gaussian Laplacian DCP [45] 0.044989 0.013664 0.002782 0.010886 AODNet [53] 0.077348 0.056151 0.006405 0.012263 CGAN-DC [29] 0.033480 0.033894 0.007939 0.005278 De-smokeGCN [28] 0.027138 0.001988 0.007484 0.005802 GCANet [85] 0.009448 0.003621 0.003164 0.008474 LGUTransformer [31] 0.010483 0.003129 0.016931 0.003623 MARS-GAN [62] 0.024441 0.064570 0.009878 0.005369 MSBDN [54] 0.020062 0.007690 0.005995 0.005536 RSTN [71] 0.026959 0.006900 0.012523 0.004193 SSIM-PAN [57] 0.018315 0.011999 0.005900 0.006517 SVPNet [67] 0.022240 0.000675 0.007876 0.006492 Avg. 0.028628 0.013795 0.008734 0.006734 Clearly, optimizing JMoE is equivalent to minimizing the error term εMoE: min W (x) E[ε2 MoE], s.t. 0 ≤W(x) ≤1. (13) Interestingly, in the context of laparoscopic surgical video smoke removal, we observe that, for real-world surgical desmoking, the error terms do not follow a Gaussian-like distribution; rather, they follow a Laplacian-like distribu- tion, as shown in Fig. 1 and reported in Table I, where most methods yield lower Jenson-Shannon (JS) divergence to Laplacian distribution. We attribute this phenomenon to the non-uniform and temporally incoherent nature of real-world surgical smoke, which, compared to natural haze, results in more zero-error regions in smokeless areas and frames, with notable errors at smoky pixels. Based on this finding, we first model the error terms in surgical desmoking as zero-mean Laplacian, that is, ε i.i.d. ∼Laplace (µ = 0, b), where µ and b are the location and scale parameters, respectively. Furthermore, our analysis indicates that µ and b are potentially correlated with the dark channel D (see Fig. 2). Hence, in the proposed approach, dark channel D is incorporated as a conditioning factor for the non-zero-mean Laplacian distribution: εM(x, c) i.i.d. ∼Laplace (µM(D(x)), bM(D(x))) , (14) Fig. 2. The Laplacian parameters (i.e., µ and b) estimated from the specific dataset, and indicator W ∗calculated with (19), are presented across different methods with the Confidence Interval (CI). The upper and lower rows correspond to Cholec80 and VASTT-desmoke, respectively. where M represents a method, either DCP or DNN. Collecting (12), and (14), we have: Var[εMoE] = 2 · \\x00W 2(x) · b2 DCP(D(x)) + (1 −W(x))2 · b2 DNN(D(x)) \\x01 , (15) E[εMoE] = W(x) · µDCP(D(x)) + (1 −W(x)) · µDNN(D(x)). (16) Then, the original optimization problem (13) is rewritten as: min',\n",
       "  'method, either DCP or DNN. Collecting (12), and (14), we have: Var[εMoE] = 2 · \\x00W 2(x) · b2 DCP(D(x)) + (1 −W(x))2 · b2 DNN(D(x)) \\x01 , (15) E[εMoE] = W(x) · µDCP(D(x)) + (1 −W(x)) · µDNN(D(x)). (16) Then, the original optimization problem (13) is rewritten as: min W (x) Var(εMoE) + (E(εMoE))2 , (17) s.t. 0 ≤W(x) ≤1. (18) We solve (17) and obtain the unconstrained solution: W ∗(x; D) = 2b2 2,x −µ2,x · (µ1,x −µ2,x) 2b2 1,x + 2b2 2,x + (µ1,x −µ2,x)2 , (19) where W ∗(x; D) is the optimized W(x) conditioned on the dark channel D without constraints. For clarity, bDCP(D(x)) and µDNN(D(x)) are written as b1,x and µ2,x, respectively. Subsequently, W ∗(x) is clipped to the interval [0, 1] to satisfy the constraint (18). C. A New Restoration Formula Following (19), Fig. 2 presents the trends of W ∗given D, based on statistics across all methods in two datasets: Cholec80 [86] with synthetic smoke, and the benchmark VASST-desmoke [23], [87] containing real-world smoke. Deep learning models display varied trends, while a consistent neg- ative correlation between W ∗and D is observed. Therefore, in this work, W is estimated by: ˆW(x) = 1 −D(x), (20) where ˆW represents the approximated W given D. The motivations are as follows: 1) Developing a general plug-and-play mechanism for the existing DNN methods is one of the objectives of this study, and this function captures the overall correlation between W and D; 2) Equation (20) is intuitive: a high intensity of dark chan- nel (i.e., D(x) →1) statistically indicates dense smoke [23], [45], which challenges DCP due to information loss and division by zero or very small values, thereby making DNN the dominant model (i.e., W(x) →0); 3) It encourages consistent predictions in smokeless regions (i.e., D(x) →0); 4) The denominator 1 −D and the unknown parameter A in (5) are eliminated in the subsequent derivation, stabilizing the computation, simplifying the modeling, and precluding hard value clipping. By combining (5), (8), and (20), we obtain: JMoE(x, c) = I(x, c) −A(c) · D(x) + D(x) · JDNN(x, c), (21) where A remains an unknown parameter that requires an additional model for its estimation. To address this issue, we first use (3) to further decompose JDNN: JMoE(x, c) = I(x, c) −A(c) · D(x) + D(x) · A(c) · ρDNN(x, c), (22) where ρDNN(x, c) = JDNN(x, c)/A(c) represents the re- flectance calculated from the normalized DNN prediction. Equation (22) can be reformulated as: JMoE(x, c) = I(x, c) −D(x) · A(c) · (1 −ρDNN(x, c)) = I(x, c) −D(x) · (1 −ρDNN(x, c)), (23) where D denotes the denormalized dark channel D. Recalling (4), D is defined as: D(x) = D(x) · A(c) = min c∈C \\x12 min u∈Ω(x;z) (I(u, c)) \\x13 , (24) where A is eliminated, thereby avoiding the challenging es- timation of global light illuminance in laparoscopic surgical scenes. In this case, the original DNN modeling in (6) and (7) can be improved as: ρDNN(x, c) =σ \\x10 Φθ \\x10 I(x, c), ˜Z \\x11\\x11 ,',\n",
       "  'u∈Ω(x;z) (I(u, c)) \\x13 , (24) where A is eliminated, thereby avoiding the challenging es- timation of global light illuminance in laparoscopic surgical scenes. In this case, the original DNN modeling in (6) and (7) can be improved as: ρDNN(x, c) =σ \\x10 Φθ \\x10 I(x, c), ˜Z \\x11\\x11 , (25) θ∗= arg min θ L (J, (I −D · (1 −ρDNN)); X, θ) , (26) where σ is a differentiable function (typically a sigmoid) that maps the DNN output to the interval [0, 1]. It can be ignored if the output is already constrained within this range, depend- ing on the specific model design. The proposed formulation introduces only minimal modifications to the original DNN model Φθ, without adding any additional trainable parameters. Notably, SurgiATM leverages the denormalized dark channel as a key element, in line with recent methods [28], [62], [85], suggesting that the dark channel prior plays a useful role in surgical desmoking [23]. D. Gradient Computation and Refinement In this section, we compute the gradient with respect to the model prediction ϱDNN for two commonly used loss functions: Mean Absolute Error (MAE or L1) loss and Mean Squared Error (MSE or L2) loss. Their gradients are denoted as ∇ϱL1 and ∇ϱL2, respectively: ∇ϱL1 = \\uf8f1 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f3 −D, J > 1 −D · (1 −ρ); D, J < 1 −D · (1 −ρ); 0, J = 1 −D · (1 −ρ), (27) ∇ϱL2 = −D · (J −(1 −D · (1 −ρ))), (28) where, for simplicity, ρDNN is written as ρ. A potential issue arises when D(x) = 0, as this results in a zero gradient, preventing the DNN model from updating its weights at those pixels. To address this, we refine D as: ˆD = D + η 1 + η , η ∈[0, +∞), (29) where η is a smoothing factor; η = 0 corresponds to no refinement; and η →+∞implies ˆD →1, causing (26) to degenerate into conventional residual prediction. Ablation results for η and z are reported in Section IV-E. IV. EXPERIMENTS AND RESULTS A. Implementation Details By default, we set the smoothing factor η = 0.1 and the window size z = 15 [45]. All experiments are conducted on a 24GB NVIDIA RTX 3090 GPU. To ensure fair comparisons, training configurations are largely standardized across all methods, employing the Adam optimizer with a learning rate of 0.0002 and epochs of 50. Due to GPU memory constraints, the batch size depends on their respective publications. B. Datasets and Metric 1) Datasets: In this study, we utilize three datasets: Cholec80 [86], VASST-desmoke [23], [87], and the Hamlyn Centre Laparoscopic and Endoscopic Dataset Videos (referred to as the Hamlyn Dataset for simplicity) [88], [89]. The Hamlyn Dataset is specifically reserved for external evaluation. For Cholec80, 500 smokeless and 500 smoky frames are randomly sampled from each video [90], with the smoke-free frames then blended with synthetic smoke [56] for training. VASST-desmoke is a recently released paired benchmark for laparoscopic surgical smoke removal. It provides both real smoky frames and corresponding ground-truth smoke- free annotations,',\n",
       "  'Cholec80, 500 smokeless and 500 smoky frames are randomly sampled from each video [90], with the smoke-free frames then blended with synthetic smoke [56] for training. VASST-desmoke is a recently released paired benchmark for laparoscopic surgical smoke removal. It provides both real smoky frames and corresponding ground-truth smoke- free annotations, thereby enabling evaluation on real surgi- cal smoke with intuitive full-reference metrics. Potentially, VASST-desmoke also allows models to learn from real smoke. The Hamlyn Dataset is excluded from all training and sta- tistical analysis procedures in this work, and its frames are sampled from the videos at 25 frames per second (FPS). We perform five-fold cross-evaluation, ensuring no video overlap between the training and evaluation sets. The three datasets record various surgical procedures with inconsistent camera devices and lighting environments in different hospitals, signif- icantly challenging the stability and generalizability of surgical desmoking methods. 2) Metrics: For the evaluation of paired datasets, we lever- age full-reference metrics: CIEDE2000, PSNR, RMSE, and SSIM, to evaluate perceptual color difference, reconstruction quality, prediction error, and structure similarity, respectively. For the unpaired datasets, we evaluate model performance using non-reference metrics, including BRISQUE and NIQE to measure how close an image is to natural scenes, and FADE [91], which specifically measures fog-related degeneration. C. Comparison of Quantitative Results We train five folds of ten existing methods and their corre- sponding SurgiATM modified versions, using Cholec80 with synthetic smoke and VASST-desmoke with real-world smoke, respectively. We then compare their five-fold average perfor- mance on the real-world surgical smoke datasets: VASST- desmoke, Cholec80 (real smoke), and the Hamlyn Dataset. In total, we train 200 models and conduct 500 evaluations. The quantitative results on VASST-desmoke are presented in Tables II and III. They demonstrate that incorporating SurgiATM effectively improves model performance, regardless of the backbone architecture or loss function. However, for SVPNet in Table III, SurgiATM yields no improvement in SSIM. We argue that, compared to other desmoking methods, SVPNet [67] employs an energy function [50] for restoration. This is a physics-guided approach similar to ours, and its three physics-related parameters are predicted by three independent U-Nets. As a result, incorporating an extra physics-guided component (our SurgiATM) into SVPNet may slightly disrupt the internal physical consistency, leading to the observed deterioration in SSIM. For Cholec80, we report the evaluation results in Table IV. Our SurgiATM effectively improves most model performances in terms of the fog-related metric (i.e., FADE score). In terms of the external evaluation on the Hamlyn Dataset, as shown in Tables VI and V, for the defogging metric FADE, employing our SurgiATM enhances most methods, except for the traditional dehazing method AODNet in Table VI. We attribute this to two factors: 1) the simple architecture and insufficient trainable weights of AODNet (i.e., only five convolutional layers) compared to other methods, and 2) the non-Laplacian error distribution of AODNet when trained on Cholec80 with synthetic smoke, as reported in Table I. In contrast, although GCANet and SSIM-PAN also exhibit non-Laplacian error distributions in Cholec80, and CGAN- DC and MARS-GAN in VASST-desmoke, their performances TABLE II TRAINING AND EVALUATION ON',\n",
       "  'other methods, and 2) the non-Laplacian error distribution of AODNet when trained on Cholec80 with synthetic smoke, as reported in Table I. In contrast, although GCANet and SSIM-PAN also exhibit non-Laplacian error distributions in Cholec80, and CGAN- DC and MARS-GAN in VASST-desmoke, their performances TABLE II TRAINING AND EVALUATION ON VASST-DESMOKE (BASELINE / BASELINE + SURGIATM) Method Year Backbone Loss Function CIEDE2000 ↓ PSNR ↑ RMSE ↓ SSIM ↑ AODNet [53] 2017 CNN MSE 10.349 / 6.327 17.378 / 21.994 0.147 / 0.090 0.693 / 0.789 GCANet [85] 2019 CNN MSE 7.893 / 5.369 21.091 / 23.907 0.095 / 0.073 0.729 / 0.806 De-smokeGCN [28] 2020 U-Net MAE + Others 9.577 / 7.348 19.779 / 22.127 0.115 / 0.094 0.550 / 0.717 MSBDN [54] 2020 U-Net MSE 6.679 / 5.164 21.604 / 23.881 0.088 / 0.073 0.651 / 0.813 SVPNet [67] 2024 U-Net MSE + Others 5.574 / 5.141 23.464 / 24.362 0.073 / 0.070 0.789 / 0.820 CGAN-DC [85] 2019 cGAN Adversarial + Others 6.735 / 5.812 22.453 / 23.809 0.083 / 0.079 0.676 / 0.782 MARS-GAN [62] 2023 CycleGAN Adversarial + Others 6.389 / 5.899 20.121 / 23.488 0.113 / 0.076 0.602 / 0.819 SSIM-PAN [57] 2020 GAN Adversarial + Others 7.046 / 5.811 21.465 / 23.477 0.090 / 0.075 0.655 / 0.750 RSTN [71] 2023 SwinT Charbonnier + Others 5.385 / 4.976 23.263 / 24.306 0.074 / 0.071 0.750 / 0.824 LGUTransformer [31] 2024 SwinT + U-Net Charbonnier 9.927 / 5.990 18.238 / 23.376 0.121 / 0.080 0.510 / 0.761 TABLE III TRAINING ON CHOLEC80 WITH SYNTHETIC SMOKE AND EVALUATION ON VASST-DESMOKE Method CIEDE2000 ↓ PSNR ↑ RMSE ↓ SSIM ↑ AODNet [53] 9.609 / 7.774 18.621 / 21.526 0.132 / 0.109 0.729 / 0.798 GCANet [85] 9.803 / 7.042 19.338 / 22.220 0.119 / 0.097 0.747 / 0.801 De-smokeGCN [28] 8.370 / 7.585 20.654 / 22.088 0.109 / 0.102 0.716 / 0.788 MSBDN [54] 8.277 / 7.327 20.360 / 22.153 0.114 / 0.103 0.769 / 0.810 SVPNet [67] 8.105 / 7.797 21.196 / 21.712 0.110 / 0.109 0.806 / 0.793 CGAN-DC [85] 8.177 / 7.525 20.663 / 21.988 0.113 / 0.105 0.770 / 0.805 MARS-GAN [62] 7.808 / 7.444 21.444 / 22.110 0.107 / 0.104 0.792 / 0.811 SSIM-PAN [57] 8.350 / 7.600 20.338 / 21.805 0.115 / 0.106 0.761 / 0.806 RSTN [71] 7.963 / 7.533 21.034 / 22.065 0.111 / 0.107 0.763 / 0.813 LGUTransformer [31] 7.357 / 7.352 22.108 / 22.111 0.105 / 0.104 0.812 / 0.818 TABLE IV TRAINING WITH SYNTHETIC SMOKE AND EVALUATION WITH REAL SMOKE ON CHOLEC80 Method BRISQUE ↓ FADE ↓ NIQE ↓ AODNet [53] 12.551 / 14.814 0.440 / 0.484 10.449 / 9.434 GCANet [85] 7.795 / 11.566 0.407 / 0.403 9.866 / 9.331 De-smokeGCN [28] 4.857 / 5.575 0.384 / 0.376 10.885 / 9.569 MSBDN [54] 11.838 / 12.176 0.421 / 0.411 10.299 / 9.026 SVPNet [67] 12.812 / 12.134 0.416 / 0.402 10.579 / 9.387 CGAN-DC [85] 10.258 / 9.945 0.413 / 0.404 9.517 / 9.039 MARS-GAN [62] 13.999 / 11.680 0.416 / 0.413 13.336 / 13.884 SSIM-PAN',\n",
       "  '/ 0.376 10.885 / 9.569 MSBDN [54] 11.838 / 12.176 0.421 / 0.411 10.299 / 9.026 SVPNet [67] 12.812 / 12.134 0.416 / 0.402 10.579 / 9.387 CGAN-DC [85] 10.258 / 9.945 0.413 / 0.404 9.517 / 9.039 MARS-GAN [62] 13.999 / 11.680 0.416 / 0.413 13.336 / 13.884 SSIM-PAN [57] 10.539 / 11.093 0.411 / 0.403 9.811 / 11.999 RSTN [71] 13.868 / 12.652 0.420 / 0.414 12.245 / 16.910 LGUTransformer [31] 9.126 / 11.236 0.417 / 0.416 13.734 / 12.471 are effectively enhanced owing to both their well-designed architectures and the strong generalizability of SurgiATM. On the other hand, SurgiATM demonstrates limited effec- tiveness in terms of BRISQUE and NIQE across the ten existing methods in Tables IV, V, and VI. The two possible reasons are: 1) these two metrics are designed to measure similarity to natural images, which may not generalize well to surgical endoscopic frames due to the substantial domain gap between them [31]; and 2) the Laplacian-guided error minimization of SurgiATM may contribute to dissimilarity from natural images (Gaussian-like error). D. Comparison of Qualitative Results In this section, we present qualitative comparisons across several representative baselines: De-smokeGCN, GCANet, MARS-GAN, and SVPNet, each selected for different rea- sons. Specifically, De-smokeGCN [28] is a classic DNN- based desmoking method introduced in 2020; GCANet [85] is designed for natural dehazing and deraining; MARS-GAN TABLE V TRAINING ON VASST-DESMOKE AND EVALUATION ON HAMLYN DATASET Method BRISQUE ↓ FADE ↓ NIQE ↓ AODNet [53] 36.549 / 30.515 0.331 / 0.322 12.418 / 11.851 GCANet [85] 13.354 / 26.555 0.324 / 0.294 11.512 / 10.983 De-smokeGCN [28] 33.236 / 28.007 0.329 / 0.325 29.448 / 18.802 MSBDN [54] 26.992 / 25.198 0.384 / 0.322 14.224 / 10.944 SVPNet [67] 39.862 / 30.325 0.401 / 0.317 16.245 / 13.306 CGAN-DC [85] 14.052 / 16.699 0.352 / 0.336 16.616 / 13.216 MARS-GAN [62] 33.722 / 30.601 0.377 / 0.369 15.011 / 16.381 SSIM-PAN [57] 11.936 / 16.047 0.357 / 0.298 16.613 / 20.297 RSTN [71] 35.629 / 30.232 0.412 / 0.330 16.442 / 12.645 LGUTransformer [31] 35.260 / 29.706 0.295 / 0.294 15.300 / 12.135 TABLE VI TRAINING ON CHOLEC80 WITH SYNTHETIC SMOKE AND EVALUATION ON HAMLYN DATASET Method BRISQUE ↓ FADE ↓ NIQE ↓ AODNet [53] 30.292 / 31.168 0.426 / 0.432 11.074 / 11.834 GCANet [85] 19.366 / 26.102 0.385 / 0.293 11.628 / 11.083 De-smokeGCN [28] 13.920 / 17.647 0.330 / 0.315 11.628 / 11.467 MSBDN [54] 26.630 / 27.559 0.409 / 0.332 12.152 / 11.882 SVPNet [67] 29.065 / 28.375 0.406 / 0.267 14.358 / 12.292 CGAN-DC [85] 25.175 / 24.127 0.408 / 0.315 10.445 / 10.741 MARS-GAN [62] 29.811 / 27.287 0.409 / 0.311 16.443 / 15.136 SSIM-PAN [57] 24.605 / 26.001 0.395 / 0.312 10.852 / 19.337 RSTN [71] 26.305 / 28.347 0.414 / 0.328 13.875 / 14.217 LGUTransformer [31] 23.974 / 26.497 0.383 / 0.346 16.786 / 17.448 represents an adversarial-learning-based approach; and SVP- Net [67], published in 2024, uses a physics-based loss function tailored for surgical desmoking. We train the models on Cholec80 with synthetic smoke and',\n",
       "  '26.305 / 28.347 0.414 / 0.328 13.875 / 14.217 LGUTransformer [31] 23.974 / 26.497 0.383 / 0.346 16.786 / 17.448 represents an adversarial-learning-based approach; and SVP- Net [67], published in 2024, uses a physics-based loss function tailored for surgical desmoking. We train the models on Cholec80 with synthetic smoke and evaluate them on VASST- desmoke, Cholec80, and the Hamlyn Dataset with real surgical smoke, challenging their stability and generalizability. As shown in the error maps in Fig. 3, SurgiATM effectively reduces desmoking errors in a computationally efficient and interpretable manner, making it well-suited for deployment on edge devices and for stable surgical desmoking. For the natural dehazing method GCANet, it causes color distoration in Fig. 3(a), which is corrected to a certain degree by using SurgiATM. In addition, the proposed approach can mitigate the grid artifact (see the first two rows in Fig. 3), demonstrating promising stability. As shown in Fig. 4, the desmoking results of the baselines are improved to varying degrees. For instance, De-smokeGCN and GCANet can produce more stable results with SurgiATM by reducing the grid artifacts (see Fig. 4(a)-(d) Fig. 3. Error comparison in VASST-desmoke. “+” indicates the base- line integrated with our SurgiATM. ”GT” represents ground truth. The methods are trained in Cholec80 with synthetic smoke. Orange boxes mark the errors declined by SurgiATM. Overall, “Baseline + SurgiATM” exhibits more accurate restoration. of De-smokeGCN) and correcting the restoration colors (see Fig. 4(d) of GCANet). Moreover, SurgiATM helps baselines eliminate more smoke, as shown in Fig. 4(a) for MARS-GAN and Fig. 4(c) for SVPNet, GCANet, and MARS-GAN. E. Ablation Study We conduct ablation studies to investigate the influence of two hyperparameters (i.e., smoothing factor η and shift- ing window size z) on model performance. Experiments are carried out on the real-world benchmark VASST-desmoke using three representative baselines (i.e., AODNet, RSTN, and SSIM-PAN), chosen for their minimal reliance on auxiliary modules and loss functions, thereby enabling a clear assess- ment of the impact of the only two hyperparameters, smoother η and window size z, with minimal confounding factors. The RMSE metrics are averaged from five-fold cross-validation and reported in Fig. 5. We observed that for most methods, η = 0.1 usually leads to better performance. No obvious trend is observed for the window size z: a larger window size is preferred for AODNet and SSIM-PAN, while a smaller one is better for RSTN. Therefore, for the use of SurgiATM, η = 0.1 is a recommended configuration, whereas the optimal window size should be determined for each specific method. V. CONCLUSION AND FUTURE WORK In this work, we propose SurgiATM for laparoscopic surgi- cal smoke removal, which can be seamlessly incorporated into existing DNN-based desmoking methods without additional trainable parameters. By using SurgiATM, the performance and generalizability of these methods are enhanced, enabling the generation of cleaner frames for both surgeons and down- stream tasks. We analyze the discrepancy between natural dehazing and surgical desmoking and design SurgiATM by first formulating a Mixture-of-Experts model. We then conduct a comprehensive statistical analysis of existing desmoking methods and derive a new restoration',\n",
       "  'are enhanced, enabling the generation of cleaner frames for both surgeons and down- stream tasks. We analyze the discrepancy between natural dehazing and surgical desmoking and design SurgiATM by first formulating a Mixture-of-Experts model. We then conduct a comprehensive statistical analysis of existing desmoking methods and derive a new restoration formula tailored for Fig. 4. Comparison of SurgiATM with baselines on real datasets: Cholec80 (left two columns) and the Hamlyn Dataset (right two columns). Blue boxes highlight the comparatively accurate and stable restoration achieved by SurgiATM. Window Size AODNet RSTN SSIM-PAN RMSE Smoother Fig. 5. Each heatmap illustrates the ablation study for a specific baseline model incorporated with SurgiATM, showing the impact of different hyperparameter settings in terms of the RMSE metric (lower is better). Each heatmap uses an independent color scale. surgical smoke removal. Extensive experiments across three distinct datasets validate the effectiveness of SurgiATM. Nevertheless, several limitations accompany its advantages. First, the indicator W is currently estimated as a linear func- tion of the dark channel D, which captures the general trends across methods, and thus leaves some room for improvement when focusing on a particular method; this could poten- tially be addressed by deriving method-specific formulations. Second, the denormalized dark channel obtained from DCP remains relatively coarse-grained. Developing an appropriate refinement strategy may further improve its performance and result quality; for example, using guided image filtering or attention mechanisms could generate a comparatively fine- grained denormalized dark channel. REFERENCES [1] L. Maier-Hein et al., “Surgical data science for next-generation inter- ventions,” Nature Biomedical Engineering, vol. 1, no. 9, pp. 691–696, 2017. [2] L. Maier-Hein et al., “Surgical data science – from concepts toward clinical translation,” Medical Image Analysis, vol. 76, p. 102306, 2022. [3] S. Yang, L. Luo, Q. Wang, and H. Chen, “Surgformer: Surgical trans- former with hierarchical temporal attention for surgical phase recogni- tion,” in Medical Image Computing and Computer Assisted Intervention – MICCAI, 2024, pp. 606–616. [4] X. Zou, D. Yu, and G. Zheng, “Capturing action triplet correlations for accurate surgical activity recognition,” Computerized Medical Imaging and Graphics, vol. 124, p. 102604, 2025. [5] M. Wagner et al., “Comparative validation of machine learning al- gorithms for surgical workflow and skill analysis with the heichole benchmark,” Medical Image Analysis, vol. 86, p. 102770, 2023. [6] W. Yue, H. Liao, Y. Xia, V. Lam, J. Luo, and Z. Wang, “Cascade multi-level transformer network for surgical workflow analysis,” IEEE Transactions on Medical Imaging, vol. 42, no. 10, pp. 2817–2831, 2023. [7] J. Guan, X. Zou, R. Tao, and G. Zheng, “Label-guided teacher for surgical phase recognition via knowledge distillation,” in Medical Image Computing and Computer Assisted Intervention – MICCAI, 2024, pp. 349–358. [8] J. Zhang, M. Xu, Y. Wang, and Q. Dou, “Csap-assist: Instrument-agent dialogue empowered vision-language models for collaborative surgical action planning,” in Medical Image Computing and Computer Assisted Intervention – MICCAI, 2025, pp. 139–148. [9] M. Sheng, J. Fan, D. Liu, R. Kikinis, and W. Cai, “Amncutter: Affinity- attention-guided multi-view normalized cutter for unsupervised surgical instrument segmentation,” in 2025 IEEE/CVF Winter Conference on Applications of Computer Vision',\n",
       "  'collaborative surgical action planning,” in Medical Image Computing and Computer Assisted Intervention – MICCAI, 2025, pp. 139–148. [9] M. Sheng, J. Fan, D. Liu, R. Kikinis, and W. Cai, “Amncutter: Affinity- attention-guided multi-view normalized cutter for unsupervised surgical instrument segmentation,” in 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025, pp. 4533–4544. [10] M. Sheng, J. Fan, D. Liu, R. Kikinis, and W. Cai, “Revisiting surgical in- strument segmentation without human intervention: a graph partitioning view,” in Proceedings of the 1st International Workshop on Multimedia Computing for Health and Medicine, 2024, p. 16–25. [11] W. Yue, J. Zhang, K. Hu, Y. Xia, J. Luo, and Z. Wang, “Surgicalsam: Efficient class promptable surgical instrument segmentation,” Proceed- ings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 7, pp. 6890–6898, 2024. [12] X. Zou et al., “Ssifnet: Spatial–temporal stereo information fusion network for self-supervised surgical video inpainting,” Computerized Medical Imaging and Graphics, vol. 125, p. 102622, 2025. [13] R. Tao, B. Huang, X. Zou, and G. Zheng, “Svt-sde: Spatiotemporal vi- sion transformers-based self-supervised depth estimation in stereoscopic surgical videos,” IEEE Transactions on Medical Robotics and Bionics, vol. 5, no. 1, pp. 42–53, 2023. [14] X. Cheng, Y. Zhong, M. Harandi, T. Drummond, Z. Wang, and Z. Ge, “Deep laparoscopic stereo matching with transformers,” in Medical Image Computing and Computer Assisted Intervention – MICCAI, 2022, pp. 464–474. [15] S. Gong et al., “Self-supervised cyclic diffeomorphic mapping for soft tissue deformation recovery in robotic surgery scenes,” IEEE Transac- tions on Medical Imaging, vol. 43, no. 12, pp. 4356–4367, 2024. [16] L. Maier-Hein et al., “Comparative validation of single-shot optical tech- niques for laparoscopic 3-d surface reconstruction,” IEEE Transactions on Medical Imaging, vol. 33, no. 10, pp. 1913–1930, 2014. [17] Z. Han, J. Zhou, J. Pei, J. Qin, Y. Fan, and Q. Dou, “Towards reliable ar-guided surgical navigation: Interactive deformation modeling with data-driven biomechanics and prompts,” IEEE Transactions on Medical Imaging, pp. 1–1, 2025. [18] Y. Long et al., “Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery,” Science Robotics, vol. 10, no. 104, p. eadt3093, 2025. [19] H. Wang et al., “Learning dissection trajectories from expert surgical videos via imitation learning with equivariant diffusion,” Medical Image Analysis, vol. 103, p. 103599, 2025. [20] M. Xu, Z. Huang, J. Zhang, X. Zhang, and Q. Dou, “Surgical action planning with large language models,” in Medical Image Computing and Computer Assisted Intervention – MICCAI, 2026, pp. 563–572. [21] Q. Dou, K. Nyangoh-Timoh, P. Jannin, and Y. Shen, “Artificial intel- ligence in gynecology surgery: Current status, challenges, and future opportunities,” Chinese Medical Journal, vol. 138, no. 6, 2025. [22] T. G. Manning et al., “Laparoscopic lens fogging: Solving a common surgical problem in standard and robotic laparoscopes via a scientific model,” Surgical Endoscopy, vol. 32, no. 3, pp. 1600–1606, 2018. [23] W. Xia, T. M. Peters, V. Fan, H. Sthanunathan, O. Qi, and E. C. S. Chen, “In vivo laparoscopic image de-smoking dataset, evaluation, and beyond,” IEEE Transactions on Medical Imaging, pp. 1–1, 2025. [24] B. C. Ulmer, “The hazards of surgical smoke,” AORN Journal,',\n",
       "  'no. 3, pp. 1600–1606, 2018. [23] W. Xia, T. M. Peters, V. Fan, H. Sthanunathan, O. Qi, and E. C. S. Chen, “In vivo laparoscopic image de-smoking dataset, evaluation, and beyond,” IEEE Transactions on Medical Imaging, pp. 1–1, 2025. [24] B. C. Ulmer, “The hazards of surgical smoke,” AORN Journal, vol. 87, no. 4, pp. 721–738, 2008. [25] H. Carbajo-Rodr´ıguez, J. L. Aguayo-Albasini, V. Soria-Aledo, and C. Garc´ıa-L´opez, “Surgical smoke: Risks and preventive measures,” Cirug´ıa Espa˜nola (English Edition), vol. 85, no. 5, pp. 274–279, 2009. [26] K. J. Weld et al., “Analysis of surgical smoke produced by various energy-based instruments and effect on laparoscopic visibility,” Journal of Endourology, vol. 21, no. 3, pp. 347–351, 2007. [27] X. Luo, A. J. McLeod, S. E. Pautler, C. M. Schlachta, and T. M. Peters, “Vision-based surgical field defogging,” IEEE Transactions on Medical Imaging, vol. 36, no. 10, pp. 2021–2030, 2017. [28] L. Chen, W. Tang, N. W. John, T. R. Wan, and J. J. Zhang, “De- smokegcn: Generative cooperative networks for joint surgical smoke detection and removal,” IEEE Transactions on Medical Imaging, vol. 39, no. 5, pp. 1615–1625, 2020. [29] S. Salazar-Colores, H. M. Jim´enez, C. J. Ortiz-Echeverri, and G. Flo- res, “Desmoking laparoscopy surgery images using an image-to-image translation guided by an embedded dark channel,” IEEE Access, vol. 8, pp. 208 898–208 909, 2020. [30] H. Li et al., “Multi-frequency and smoke attention-aware learning based diffusion model for removing surgical smoke,” in Medical Image Computing and Computer Assisted Intervention – MICCAI, 2024, pp. 47–56. [31] W. Wang, F. Liu, J. Hao, X. Yu, B. Zhang, and C. Shi, “Desmoking of the endoscopic surgery images based on a local-global u-shaped transformer model,” IEEE Transactions on Medical Robotics and Bionics, vol. 7, no. 1, pp. 254–265, 2024. [32] R. Modrzejewski, T. Collins, A. Hostettler, J. Marescaux, and A. Bartoli, “Light modelling and calibration in laparoscopy,” International Journal of Computer Assisted Radiology and Surgery, vol. 15, no. 5, pp. 859– 866, 2020. [33] P. Azagra et al., “Endomapper dataset of complete calibrated endoscopy procedures,” Scientific Data, vol. 10, no. 1, p. 671, 2023. [34] J. Rodr´ıguez-Puigvert et al., “Lightdepth: Single-view depth self- supervision from illumination decline,” in 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 21 216–21 226. [35] V. Stelzer et al., “Generation and distribution of surgical smoke during high frequency electrocauterization,” in New Results in Numerical and Experimental Fluid Mechanics XIV, 2024, pp. 559–568. [36] S. Kumar, C. Crowley, M. F. Khan, M. D. Bustamante, R. A. Cahill, and K. Nolan, “Understanding surgical smoke in laparoscopy through la- grangian coherent structures,” PLOS ONE, vol. 18, no. 11, p. e0293287, 2023. [37] H. Tian, W. Li, P. Ogunbona, and L. Wang, “Single image smoke detection,” in Computer Vision – ACCV 2014, 2015, pp. 87–101. [38] W.-T. Chen et al., “Desmokenet: a two-stage smoke removal pipeline based on self-attentive feature consensus and multi-level contrastive regularization,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 6, pp. 3346–3359, 2022. [39] W. Li et al., “Endoscopy image enhancement method by generalized imaging defect models',\n",
       "  '[38] W.-T. Chen et al., “Desmokenet: a two-stage smoke removal pipeline based on self-attentive feature consensus and multi-level contrastive regularization,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 6, pp. 3346–3359, 2022. [39] W. Li et al., “Endoscopy image enhancement method by generalized imaging defect models based adversarial training,” Physics in Medicine and Biology, vol. 67, no. 9, p. 095016, 2022. [40] R. Wu et al., “Self-supervised video desmoking for laparoscopic surgery,” in Computer Vision – ECCV 2024, 2025, pp. 307–324. [41] S. G. Narasimhan and S. K. Nayar, “Vision and the atmosphere,” International Journal of Computer Vision, vol. 48, no. 3, pp. 233–254, 2002. [42] S. G. Narasimhan and S. K. Nayar, “Contrast restoration of weather degraded images,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 6, pp. 713–724, 2003. [43] R. Fattal, “Single image dehazing,” ACM transactions on graphics (TOG), vol. 27, no. 3, pp. 1–9, 2008. [44] R. T. Tan, “Visibility in bad weather from a single image,” in 2008 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008, pp. 1–8. [45] H. Kaiming, S. Jian, and T. Xiaoou, “Single image haze removal using dark channel prior,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 12, pp. 2341–2353, 2011. [46] L. Gu, P. Liu, C. Jiang, M. Luo, and Q. Xu, “Virtual digital defogging technology improves laparoscopic imaging quality,” Surgical Innovation, vol. 22, no. 2, pp. 171–176, 2015. [47] A. Kotwal, R. Bhalodia, and S. P. Awate, “Joint desmoking and denois- ing of laparoscopy images,” in 2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI), 2016, pp. 1050–1054. [48] A. Baid, A. Kotwal, R. Bhalodia, S. N. Merchant, and S. P. Awate, “Joint desmoking, specularity removal, and denoising of laparoscopy images via graphical models and bayesian inference,” in Proceedings - International Symposium on Biomedical Imaging, 2017, pp. 732–736. [49] K. Tchaka, V. M. Pawar, and D. Stoyanov, “Chromaticity based smoke removal in endoscopic images,” in Medical Imaging 2017: Image Processing, vol. 10133, 2017, pp. 463–470. [50] C. Wang, F. Alaya Cheikh, M. Kaaniche, A. Beghdadi, and O. J. Elle, “Variational based smoke removal in laparoscopic images,” BioMedical Engineering OnLine, vol. 17, no. 1, p. 139, 2018. [51] M. A. Azam, K. B. Khan, E. Rehman, and S. U. Khan, “Smoke removal and image enhancement of laparoscopic images by an artificial multi- exposure image fusion method,” Soft Computing, vol. 26, no. 16, pp. 8003–8015, 2022. [52] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “Dehazenet: an end-to-end system for single image haze removal,” IEEE Transactions on Image Processing, vol. 25, no. 11, pp. 5187–5198, 2016. [53] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng, “Aod-net: All-in-one dehazing network,” in Proceedings of the IEEE international conference on computer vision (ICCV), 2017, pp. 4780–4788. [54] H. Dong et al., “Multi-scale boosted dehazing network with dense feature fusion,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 2154–2164. [55] X. Qin, Z. Wang, Y. Bai, X. Xie, and H. Jia, “Ffa-net: Feature',\n",
       "  'international conference on computer vision (ICCV), 2017, pp. 4780–4788. [54] H. Dong et al., “Multi-scale boosted dehazing network with dense feature fusion,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 2154–2164. [55] X. Qin, Z. Wang, Y. Bai, X. Xie, and H. Jia, “Ffa-net: Feature fusion attention network for single image dehazing,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, pp. 11 908–11 915, 2020. [56] S. Bolkar, C. Wang, F. A. Cheikh, and S. Yildirim, “Deep smoke removal from minimally invasive surgery videos,” in 2018 25th IEEE International Conference on Image Processing (ICIP), 2018, pp. 3403– 3407. [57] O. Sidorov, C. Wang, and F. A. Cheikh, “Generative smoke removal,” in Proceedings of the Machine Learning for Health NeurIPS Workshop, vol. 116, 2020, pp. 81–92. [58] L. Chen, W. Tang, and W. John, “Unsupervised learning of surgical smoke removal from simulation,” in The 11th Hamlyn Symposium on Medical Robotics, 2018. [59] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in Medical Image Computing and Computer-Assisted Intervention – MICCAI, 2015, pp. 234–241. [60] L. Ma, H. Song, X. Zhang, and H. Liao, “A smoke removal method based on combined data and modified u-net for endoscopic images,” in 2021 43rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2021, pp. 3783–3786. [61] C. Wang, A. K. Mohammed, F. A. Cheikh, A. Beghdadi, and O. J. Elle, Multiscale Deep Desmoking for Laparoscopic Surgery, ser. SPIE Medical Imaging, 2019, vol. 10949. [62] T. Hong et al., “Mars-gan: Multilevel-feature-learning attention-aware based generative adversarial network for removing surgical smoke,” IEEE Transactions on Medical Imaging, vol. 42, no. 8, pp. 2299–2312, 2023. [63] Y. Zhou, Z. Hu, Z. Xuan, Y. Wang, and X. Hu, “Synchronizing detection and removal of smoke in endoscopic images with cyclic consistency adversarial nets,” IEEE/ACM Transactions on Computational Biology and Bioinformatics, vol. 21, no. 4, pp. 670–680, 2024. [64] J. Lin et al., “A desmoking algorithm for endoscopic images based on improved u-net model,” Concurrency and Computation: Practice and Experience, vol. 33, no. 22, p. e6320, 2021. [65] A. Kanakatte, K. Seemakurthy, J. Gubbi, J. Saha, A. Ghose, and B. Purushothaman, “Surgical smoke dehazing and color reconstruction,” in 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), 2021, pp. 280–284. [66] C. Ma, C. Wang, and M. Zhao, “Laparoscopic video desmoking with mutually attention-guided deformable convolutional networks and lbp prior,” in 2024 International Joint Conference on Neural Networks (IJCNN), 2024, pp. 1–8. [67] C. Wang et al., “Smoke veil prior regularized surgical field desmoking without paired in-vivo data,” Computers in Biology and Medicine, vol. 168, p. 107761, 2024. [68] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in Advances in Neural Information Processing Systems 33 (NeurIPS 2020), vol. 33, 2020, pp. 6840–6851. [69] S. Jiaming, M. Chenlin, and E. Stefano, “Denoising diffusion im- plicit models,” in International Conference on Learning Representations (ICLR), 2021. [70] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using shifted windows,” in 2021 IEEE/CVF International Conference on',\n",
       "  'Processing Systems 33 (NeurIPS 2020), vol. 33, 2020, pp. 6840–6851. [69] S. Jiaming, M. Chenlin, and E. Stefano, “Denoising diffusion im- plicit models,” in International Conference on Learning Representations (ICLR), 2021. [70] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using shifted windows,” in 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 9992–10 002. [71] F. Wang, X. Sun, and J. Li, “Surgical smoke removal via residual swin transformer network,” International Journal of Computer Assisted Radiology and Surgery, vol. 18, no. 8, pp. 1417–1427, 2023. [72] I. Goodfellow et al., “Generative adversarial networks,” Commun. ACM, vol. 63, no. 11, p. 139–144, 2020. [73] M. Mirza and S. Osindero, “Conditional generative adversarial nets,” arXiv preprint arXiv:1411.1784, 2014. [74] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent adversarial networks,” in 2017 IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2242– 2251. [75] X. Su and Q. Wu, “Multi-stages de-smoking model based on cyclegan for surgical de-smoking,” International Journal of Machine Learning and Cybernetics, vol. 14, no. 11, pp. 3965–3978, 2023. [76] W. Wang, Q. Yuan, P. Huang, X. Wang, and H. Song, “Desmoke-vcu: Improved unpaired image-to-image translation for removing smoke from laparoscopic images,” Digital Signal Processing, vol. 162, p. 105177, 2025. [77] A. J. Islam, S. Salehin, S. U. Alam, K. Islam, S. Paul, and P. Paul, “Lvqe: Laparoscopic video quality enhancement using gan-based smoke elimination guided by an embedded dark channel,” in 2024 IEEE Inter- national Conference on Signal Processing, Information, Communication and Systems (SPICSCON), 2024, pp. 01–06. [78] V. Vishal, N. Sharma, and M. Singh, “Guided unsupervised desmoking of laparoscopic images using cycle-desmoke,” in OR 2.0 Context-Aware Operating Theaters and Machine Learning in Clinical Neuroimaging, 2019, pp. 21–28. [79] V. Vishal, V. Venkatesh, K. Lochan, N. Sharma, and M. Singh, “Unsuper- vised desmoking of laparoscopy images using multi-scale desmokenet,” in Advanced Concepts for Intelligent Vision Systems, 2020, pp. 421–432. [80] V. Venkatesh, N. Sharma, V. Srivastava, and M. Singh, “Unsupervised smoke to desmoked laparoscopic surgery images using contrast driven cyclic-desmokegan,” Computers in Biology and Medicine, vol. 123, p. 103873, 2020. [81] Y. Pan, S. Bano, F. Vasconcelos, H. Park, T. T. Jeong, and D. Stoy- anov, “Desmoke-lap: Improved unpaired image-to-image translation for desmoking in laparoscopic surgery,” International Journal of Computer Assisted Radiology and Surgery, vol. 17, no. 5, pp. 885–893, 2022. [82] C. O. Ancuti, C. Ancuti, R. Timofte, and C. D. Vleeschouwer, “O- haze: a dehazing benchmark with real hazy and haze-free outdoor images,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2018, pp. 867–8678. [83] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, “Adaptive mixtures of local experts,” Neural Computation, vol. 3, no. 1, pp. 79–87, 1991. [84] R. Ding, H. Lu, and M. Liu, “Denseformer-moe: a dense transformer foundation model with mixture of experts for multi-task brain image analysis,” IEEE Transactions on Medical Imaging, pp. 1–1, 2025. [85] D. Chen et al., “Gated context aggregation network for image dehazing and deraining,” in 2019 IEEE Winter Conference on Applications of',\n",
       "  'H. Lu, and M. Liu, “Denseformer-moe: a dense transformer foundation model with mixture of experts for multi-task brain image analysis,” IEEE Transactions on Medical Imaging, pp. 1–1, 2025. [85] D. Chen et al., “Gated context aggregation network for image dehazing and deraining,” in 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), 2019, pp. 1375–1383. [86] A. P. Twinanda, S. Shehata, D. Mutter, J. Marescaux, M. d. Mathelin, and N. Padoy, “Endonet: a deep architecture for recognition tasks on laparoscopic videos,” IEEE Transactions on Medical Imaging, vol. 36, no. 1, pp. 86–97, 2017. [87] W. Xia, V. Fan, T. Peters, and E. C. S. Chen, “A new benchmark in vivo paired dataset for laparoscopic image de-smoking,” in Medical Image Computing and Computer Assisted Intervention – MICCAI, 2024, pp. 3–13. [88] S. Giannarou, M. Visentini-Scarzanella, and G. Z. Yang, “Probabilistic tracking of affine-invariant anisotropic regions,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 130–143, 2013. [89] M. Ye, E. Johns, A. Handa, L. Zhang, P. Pratt, and G.-Z. Yang, “Self- supervised siamese learning on stereo image pairs for depth estimation in robotic surgery,” in The Hamlyn Symposium on Medical Robotics, 2017, p. 27. [90] A. Leibetseder, M. J. Primus, S. Petscharnig, and K. Schoeffmann, “Real-time image-based smoke detection in endoscopic videos,” in Proceedings of the on Thematic Workshops of ACM Multimedia 2017, 2017, p. 296–304. [91] L. K. Choi, J. You, and A. C. Bovik, “Referenceless prediction of per- ceptual fog density and perceptual image defogging,” IEEE Transactions on Image Processing, vol. 24, no. 11, pp. 3888–3901, 2015.'],\n",
       " ['DEEP LEARNING MODELS ARE VULNERABLE, BUT ADVERSARIAL EXAMPLES ARE EVEN MORE VULNERABLE A PREPRINT Jun Li†1,2, Yanwei Xu*,†1, Keran Li1, and Xiaoli Zhang3 1School of Management Science and Information Engineering, Jilin University of Finance and Economics, Jingyue Street, Changchun 130117, China 2Center for Artificial Intelligence, Jilin University of Finance and Economics, Jingyue Street, Changchun 130117, China 3College of Computer Science and Technology, Jilin University, Qianjin Street, Changchun 130012, China November 10, 2025 ABSTRACT Understanding intrinsic differences between adversarial examples and clean samples is key to en- hancing DNN robustness and detection against adversarial attacks. This study first empirically finds that image-based adversarial examples are notably sensitive to occlusion. Controlled experiments on CIFAR-10 used nine canonical attacks (e.g., FGSM, PGD) to generate adversarial examples, paired with original samples for evaluation. We introduce Sliding Mask Confidence Entropy (SMCE) to quantify model confidence fluctuation under occlusion. Using 1800+ test images, SMCE cal- culations—supported by Mask Entropy Field Maps and statistical distributions—show adversarial examples have significantly higher confidence volatility under occlusion than originals. Based on this, we propose Sliding Window Mask-based Adversarial Example Detection (SWM-AED), which avoids catastrophic overfitting of conventional adversarial training. Evaluations across classifiers and attacks on CIFAR-10 demonstrate robust performance, with accuracy over 62% in most cases and up to 96.5%. Keywords Adversarial Examples · Adversarial Detection · Deep Learning · Computer Vision 1 Introduction In the era of rapid digital transformation, artificial intelligence (AI), particularly deep learning, has become deeply integrated into diverse domains, including image recognition, speech processing, and natural language understanding, thereby significantly enhancing daily life and work. Deep learning models, renowned for their exceptional feature extraction and learning capabilities, have demonstrated outstanding performance in complex tasks. Critical applications, such as facial recognition in security systems, medical image diagnostics, and road condition monitoring in autonomous vehicles, heavily depend on the accurate decision-making of these models. However, as AI technology evolves, emerging security concerns surrounding deep learning systems have gained attention (Chakraborty et al., 2021). (Szegedy et al., 2013) first introduced the concept of adversarial examples, wherein subtle, nearly imperceptible perturbations are added to input data, leading to drastic mispredictions by deep neural networks. This phenomenon underscores a fundamental divergence in how deep learning models and human cognition interpret data, while also posing significant threats to systems relying on AI. For instance, in the context of autonomous driving, adversarial attacks could mislead a vehicle into misidentifying traffic signs, potentially resulting in catastrophic accidents. In response, researchers have developed a range of adversarial example generation techniques, including gradient-based attacks, optimization-based methods, and meta-learning-based approaches, etc. Notable methods include the Fast *Corresponding author: xuyanwei@s.jlufe.edu.cn †These authors contributed equally to this work. arXiv:2511.05073v1 [cs.CV] 7 Nov 2025 arXiv Template A PREPRINT Gradient Sign Method (FGSM), Projected Gradient Descent (PGD) (Madry et al., 2017), DeepFool (Moosavi-Dezfooli et al., 2016), and You Only Attack Once (YOAO) (Li et al., 2025b), among others. These adversarial attacks are not confined to image classification tasks but have been extended to domains such as object detection, speech recognition, and real-world scenarios. To counteract the threats posed by adversarial examples, researchers',\n",
       "  '(Moosavi-Dezfooli et al., 2016), and You Only Attack Once (YOAO) (Li et al., 2025b), among others. These adversarial attacks are not confined to image classification tasks but have been extended to domains such as object detection, speech recognition, and real-world scenarios. To counteract the threats posed by adversarial examples, researchers have proposed various defense and detection strategies. Early defense mechanisms focused on adversarial training, where adversarial examples were incorporated into the model’s training process. However, studies have shown that this approach often faces challenges, such as reduced detection accuracy and the risk of catastrophic overfitting during training. As research advanced, more sophisticated defense strategies emerged, including model ensembles that combine predictions from multiple models to enhance robustness. However, most defense methods still center on improving model robustness, which typically requires significant computational resources and increases model complexity. To tackle the growing challenges of adversarial attacks, increasing attention has been directed toward adversarial example detection technologies. Unlike traditional defense methods, which focus on enhancing model robustness, adversarial example detection targets the identification and rejection of adversarial inputs before they can compromise the model, thereby bolstering the security of AI systems. These detection approaches typically offer lower computational complexity and reduced resource demands compared to robust training strategies, allowing for better cost control while effectively mitigating the impact of adversarial examples. The capacity to detect and filter adversarial examples prior to their integration holds considerable practical value, ensuring the reliability of deep learning models in real-world applications. Thus, establishing an efficient and accurate detection mechanism is paramount to preserving the integrity of AI-driven systems. Despite the significant advancements in adversarial example detection, numerous challenges remain. As adversarial example generation techniques continuously evolve, attackers develop increasingly sophisticated perturbation strategies, which raise the difficulty of detection. New attack algorithms are adept at bypassing existing detection methods and exhibit greater concealment. Moreover, deep learning models themselves are highly complex and heterogeneous, with each model responding differently to adversarial inputs, complicating the development of universal detection models. To address these issues, this study delves into the causes of adversarial examples and the inherent vulnerabilities within deep learning models. While adversarial examples are often attributed to the weaknesses of these models, a more pressing issue lies in their inherent instability, whereby slight changes can lead to misclassifications across different categories. This phenomenon, termed the “vulnerable of adversarial examples\" represents a critical aspect that requires further investigation. Building upon this finding, the study introduces the concept of Sliding Mask Confidence Entropy (SMCE) to quantify the vulnerable of adversarial examples and the stability of deep learning models. By applying a sliding window to mask local areas of an image and calculating the average of confidence entropy values for each window, the method assesses not only the stability of the image itself but also the robustness of the classifier when confronted with partial occlusion or perturbation. Through extensive experimentation, the study reveals a striking finding: as model detection accuracy improves, the use of the SMCE strategy results in enhanced robustness of the deep learning model, significantly boosting security. This approach effectively avoids',\n",
       "  'but also the robustness of the classifier when confronted with partial occlusion or perturbation. Through extensive experimentation, the study reveals a striking finding: as model detection accuracy improves, the use of the SMCE strategy results in enhanced robustness of the deep learning model, significantly boosting security. This approach effectively avoids the catastrophic overfitting associated with traditional adversarial training methods, thereby providing a novel research avenue for subsequent work in this domain. Incorporating SMCE, the study proposes the Sliding Window Masking-Adversarial Example Detection (SWM-AED) algorithm, an innovative solution designed to improve model robustness for adversarial example detection. The core of this algorithm lies in leveraging SMCE values to detect adversarial examples. Specifically, the SWM-AED algorithm calculates the Sliding Mask Confidence Entropy of an image, identifying samples with abnormal entropy values indicative of adversarial perturbations. Notably, the SWM-AED algorithm is non-specific to any particular type of adversarial example, making it resilient to a broad range of adversarial attacks generated by various adversarial generation algorithms, demonstrating superior performance. The study further highlights a key observation: as the model’s detection accuracy improves, the SWM-AED strategy adeptly mitigates catastrophic overfitting, enhancing both robustness and security. From a theoretical perspective, this study establishes a positive correlation between the accuracy and robustness of deep learning models after the integration of the SWM-AED algorithm. This work fills a critical gap in the measurement of adversarial example vulnerable by introducing the concept of SMCE. Based on this metric, the SWM-AED detection algorithm demonstrates outstanding detection performance, achieving over 80% accuracy in detecting adversarial examples generated by multiple attack algorithms. Moreover, by proactively identifying and filtering adversarial examples, the SWM-AED algorithm significantly mitigates their impact on deep learning models, thereby substantially improving the overall security of AI systems. 2 arXiv Template A PREPRINT 2 Background and Related Work Adversarial examples are generated by introducing subtle perturbations to an original image, resulting in misclas- sifications by deep neural networks (DNNs). Despite these perturbations being imperceptible to human observers, they cause the model to misidentify the original input—incorrectly assigning it a label B instead of the true label A. The vulnerability is particularly concerning in safety-critical applications, such as autonomous driving, where an adversarially altered “Stop\" sign could be misinterpreted as “Go\", potentially leading to catastrophic outcomes (Xu et al., 2023; Li et al., 2025c; West et al., 2023). Adversarial Defense and Detection: To enhance the security and robustness of deep learning models against adversarial examples, (Goodfellow et al., 2014) first introduced adversarial examples into the training process, pioneering the research into model robustness. This approach was later extended by studies showing that training models with adversarial examples generated via PGD attacks could significantly improve robustness (Madry et al., 2017). Further advancements led to ensemble adversarial training, which incorporated transferable perturbations, further enhancing model robustness (Tramèr et al., 2017). The concept of Smooth Adversarial Training introduced a different perspective, achieving notable robustness improvements (Xie et al., 2020). Other methods, including self-supervised adversarial training (Naseer et al., 2020) and causal parameter estimation (Lee et al., 2023), further diversified the strategies to improve robustness. Additionally, researchers',\n",
       "  'robustness (Tramèr et al., 2017). The concept of Smooth Adversarial Training introduced a different perspective, achieving notable robustness improvements (Xie et al., 2020). Other methods, including self-supervised adversarial training (Naseer et al., 2020) and causal parameter estimation (Lee et al., 2023), further diversified the strategies to improve robustness. Additionally, researchers efforts extended to 3D point cloud recognition, highlighting the broad applicability of adversarial defense techniques (Ji et al., 2023). These studies have cumulatively enriched the adversarial training landscape. As research progressed, the focus shifted toward optimizing the finer details of adversarial training. The analysis of smoothness enhancement revealed its specific role in domain adversarial training for improved target domain generalization (Rangwani et al., 2022). Later, techniques to constrain loss variations between epochs and introduce convergence strides were developed to enhance adversarial training efficiency (Zhao et al., 2023). Feature denoising methods emerged as a way to increase robustness by removing noise from input features during inference, although they faced limitations with resistance to white-box attacks and increased computational costs Xie et al. (2019). In response, selective feature regeneration was proposed as an alternative (Borkar et al., 2020), while image restoration-based denoising was explored, though it was prone to content distortion and vulnerability to EOT attacks (Mustafa et al., 2020). As adversarial attack strategies diversified, targeted defenses were developed. Image-semantic dual adversarial training (ISDAT) addressed the limited diversity of adversarial examples (Sui et al., 2025). For high-intensity perturbations, non-robust loss adjustment methods proved effective in improving model resilience (Le et al., 2025). Advances in bidirectional mapping and self-attention feature alignment further bolstered resistance to attacks (Zhang et al., 2025), while universal attention mechanisms successfully countered diverse attention-based adversarial attacks (Zhao et al., 2025). These targeted strategies have significantly improved defenses across various attack scenarios. In parallel, adversarial sample detection has become an increasingly critical area of research. Early methods focused on gradient masking to reduce the model’s sensitivity to small input perturbations (Papernot et al., 2017). Subsequently, denoising-based reconstruction methods were developed to detect adversarial examples by identifying reconstruction errors and prediction discrepancies (Meng and Chen, 2017). Projection-based methods measured the prediction probability distance for sample detection (Xu et al., 2017), while local intrinsic dimensionality analysis emerged as an effective technique (Ma et al., 2018). Other approaches, including Mahalanobis distance (Lee et al., 2018), natural scene statistics (Kherchouche et al., 2020), and autoencoders (Sotgiu et al., 2020), collectively established a robust adversarial detection framework. Novel perspectives have further propelled advancements in the field. Key feature modulation frameworks solved adversarial patch detection (Wu et al., 2024), while clustering effect analysis explained adversarial robustness and introduced regularization techniques (Jin et al., 2023). Lightweight ensemble attacks (LEA2) presented new detection strategies for adversarial examples (Qian et al., 2023), and dual-function defense frameworks provided comprehensive mitigation for adversarial instances (Yang et al., 2025). Models such as Adversarial Surgery and Regeneration (ASR) significantly improved generalization and robustness (Fu et al., 2025). Masking Techniques and Entropy-Based Adversarial Attacks: Masking techniques have emerged as essential tools in adversarial defense. Instance-binding augmentation techniques, for example, reconstructed perturbation distributions via masking branches (Zhang',\n",
       "  'instances (Yang et al., 2025). Models such as Adversarial Surgery and Regeneration (ASR) significantly improved generalization and robustness (Fu et al., 2025). Masking Techniques and Entropy-Based Adversarial Attacks: Masking techniques have emerged as essential tools in adversarial defense. Instance-binding augmentation techniques, for example, reconstructed perturbation distributions via masking branches (Zhang et al., 2024b). Gradient-based masking was shown to enhance attack transferability by perturbing sensitive regions (Zhang et al., 2024a). Information masking and region intersection strategies were developed to purify adversarial examples (Liu et al., 2025), while in audio attacks, random masking optimized adversarial training processes (Bui et al., 2025). Variance-based masking (RAPID) was another approach for detecting candidate regions (Kim et al., 2024). These techniques underscore the growing importance of masking strategies in adversarial research. 3 arXiv Template A PREPRINT Figure 1: Seeking the vulnerability of adversarial examples From an information-theoretic perspective, high entropy in adversarial patches facilitates their localization and removal (Tarchoun et al., 2024). Entropy-based detectors have successfully identified adversarial examples by analyzing entropy differences before and after bit-depth reduction (Ryu and Choi, 2024). Advanced entropy analysis further refined patch localization methods (Tarchoun et al., 2023), providing novel detection techniques for the field. 3 Method The fundamental principle of adversarial example detection lies in identifying and leveraging discriminative features to effectively distinguish adversarial examples from clean ones. This section first establishes the theoretical foundation and the motivation for this study, followed by a visualization-based analysis of the distinct vulnerability characteristics exhibited by adversarial examples in contrast to clean samples. Building on these observations, we introduce a quantitative metric, termed Sliding Mask Confidence Entropy (SMCE), to systematically assess the degree of adversarial vulnerability. To further elucidate the effectiveness of this metric, we visualize the quantified vulnerability distributions, thereby establishing a robust framework for distinguishing adversarial examples from their clean counterparts. 3.1 Research Objective The central objective of this study is to enhance the security of artificial intelligence systems by enabling them to effectively resist adversarial attacks. Specifically, this work aims to uncover the inherent characteristics of adversarial examples from a novel perspective, and to leverage these characteristics to accurately classification between adversarial examples and clean samples, thereby mitigating the impact of adversarial attacks. 3.2 Motivation and Intuition The goal of adversarial attacks is to induce misclassification in DNN models with minimal perturbations, which can be formally expressed as the following optimization problem: 4 arXiv Template A PREPRINT f(x) ̸= f(x + r) subject to r ≤ϵ (1) Where f(x) denotes the predicted class for the original image, f(x+r) represents the predicted class for the adversarial example, and ϵ is a small, predefined perturbation bound. Based on the preceding formulation, it follows that an adversarial example x + r arises from the superposition of a small perturbation r onto the original image x. Although the magnitude of this perturbation is minimal, it is sufficient to induce misclassification in deep learning models. Mathematically, the adversarial example x + r can be viewed as a linear combination of the original image and the perturbation, underscoring the fundamental mechanism of adversarial attacks—where imperceptible perturbations in',\n",
       "  'x. Although the magnitude of this perturbation is minimal, it is sufficient to induce misclassification in deep learning models. Mathematically, the adversarial example x + r can be viewed as a linear combination of the original image and the perturbation, underscoring the fundamental mechanism of adversarial attacks—where imperceptible perturbations in the input space lead to substantial alterations in the model’s output. Further analysis suggests that the perturbation r can be interpreted as a specialized form of noise. Despite its constrained numerical range, its influence on the model’s classification outcomes is nontrivial. Related study reveals that the introduction of noise to an image not only perturbs its pixel value distribution but also compromises its stability (Tian et al., 2020; Buades et al., 2005). We posit that adversarial examples exhibit increased vulnerability relative to their unperturbed counterparts, characterized by more ambiguous class boundaries that are inherently more prone to misclassification. This heightened vulnerability arises from the proximity of adversarial examples to decision boundaries in feature space, rendering them particularly sensitive to small input variations. Adversarial examples exhibit greater susceptibility to perturbations compared to clean samples. Building upon this foundation, we further investigate the underlying factors contributing to this vulnerability. Notably, the human visual system can reliably recognize objects even when images are partially occluded. Motivated by this observation, we examine how classification outcomes for clean and adversarial examples evolve when their occluded counterparts are fed into a deep neural network. 3.3 Vulnerability Exploration through Mask Box Drawing inspiration from human perception, we systematically apply an m × m dark occlusion block that traverses local regions of both clean and adversarial examples. By analyzing the resulting shifts in classification decisions, we aim to uncover the fundamental basis of adversarial vulnerability. Figure 1 presents a detailed analysis of the vulnerability of adversarial examples in the context of image classification. In this experiment, we introduced a occlusion block, referred to as the “mask box\", and used it to occlude various parts of both clean images and adversarial examples. The occlusion was applied systematically from left to right and top to bottom. We then utilized a ResNet-18 classifier to make predictions on the occluded images, with the aim of investigating the stability of clean images and adversarial examples under occlusion and their corresponding classification results. The 16 sub-figures on the left side of Figure 1 illustrate the sliding of a 7 × 7 green mask box across the original sample. Each sub-figure presents the classification results, including the predicted class label and the confidence score for that label, as determined by the ResNet-18 classifier. Notably, when occluding the clean sample with the sliding mask, the model’s confidence and predicted class label remained largely unaffected. Even with partial occlusions, the clean image was consistently classified correctly, demonstrating the robustness of the model to such perturbations. In contrast, the 16 sub-figures on the right side of Figure 1 reveal a striking difference in behavior when adversarial examples are occluded. In these cases, the classification results exhibited significant instability, characterized by considerable drops in confidence scores. This observation underscores the inherent',\n",
       "  'of the model to such perturbations. In contrast, the 16 sub-figures on the right side of Figure 1 reveal a striking difference in behavior when adversarial examples are occluded. In these cases, the classification results exhibited significant instability, characterized by considerable drops in confidence scores. This observation underscores the inherent vulnerable of adversarial examples, which are highly susceptible to perturbations. This instability can be exploited as a key characteristic for detecting adversarial examples, offering a potential avenue for enhancing defenses against adversarial attacks. Adversarial examples have been shown to exhibit significant instability, necessitating the development of a robust metric to quantify this instability. This paper introduces a novel formula to measure the variations in confidence scores and class labels resulting from occlusions in an image. By analyzing the differences in instability between clean samples and adversarial examples, we provide a clear methodology for distinguishing between these two categories. The key innovation lies in the introduction of a precise metric that enables effective differentiation between adversarial and clean samples, providing new insights into the behavior of adversarial attacks and advancing adversarial examples detection techniques. 3.4 Sliding Mask Confidence Entropy This study addresses a fundamental challenge in adversarial example detection: quantifying how an image’s classification confidence varies under partial occlusion. To systematically evaluate confidence stability, we propose Sliding Mask Confidence Entropy (SMCE), a novel metric that measures the uncertainty in classification confidence as an image 5 arXiv Template A PREPRINT Figure 2: The process of calculating confidence entropy for a single occluded image. undergoes successive occlusions via a sliding window mask. One of the key innovation of this work is the entropy-based formulation of SMCE, which enables precise quantification of confidence stability during occlusion. The formula for Sliding Mask Confidence Entropy (SMCE) is as follows: HSMCE(I) = 1 n n X i=1 \\uf8eb \\uf8ed− m X j=1 pij log2(pij) \\uf8f6 \\uf8f8 (2) where HSMCE(I) represents the Sliding Mask Confidence Entropy of image I. pij is the confidence that image I is classified as the j-th class after being occluded by the i-th sliding window mask position. n is the number of sliding positions of the window mask, i.e., the number of different positions the mask moves across the image. m is the total number of classes that the classifier can identify, i.e., the number of possible categories for the image. The formula 2 calculates the average of confidence entropy across all sliding window mask positions to assess the stability of the image’s confidence during the occlusion process. Specifically, for each sliding position i, we first compute the confidence entropy at that position, i.e., −Pm j=1 pij log2(pij). Then, we sum up the confidence entropies for all positions and take the average to obtain the final Sliding Mask Confidence Entropy HSMCE(I). A higher value of this metric indicates greater uncertainty in confidence changes under different occlusion conditions, implying poorer stability of the confidence; conversely, a lower value indicates more stable confidence changes, suggesting that the image’s classification results are more reliable under various occlusion conditions. 3.4.1 Property about SMCE Property 1. Non-negativity . HSMCE(I) ≥0',\n",
       "  'this metric indicates greater uncertainty in confidence changes under different occlusion conditions, implying poorer stability of the confidence; conversely, a lower value indicates more stable confidence changes, suggesting that the image’s classification results are more reliable under various occlusion conditions. 3.4.1 Property about SMCE Property 1. Non-negativity . HSMCE(I) ≥0 (3) The Sliding Mask Confidence Entropy (SMCE) metric is inherently non-negative, as established in formula 3. When the SMCE value approaches zero, it indicates that the predicted labels remain consistent across diverse occlusion conditions, with model confidence converging towards one. This behaviour reflects high resilience to localized perturbations and suggests strong intrinsic image stability. Property 2. Maximum Value of the SMCE . HSMCE(I) ≤log2 m (4) Furthermore, formula A.1 demonstrates that the maximum value of SMCE is log2 m. As the image undergoes successive occlusions through sliding masks, greater variations in confidence scores and class labels lead to higher SMCE values. These elevated values signify reduced stability, emphasizing the image’s vulnerability to adversarial examples. One of the key innovation of this work lies in quantitatively linking SMCE to the degree of stability. 3.4.2 The Calculation Process of SMCE In the previous section, we introduced the Sliding Mask Confidence Entropy (SMCE) as a quantitative measure of image stability about adversarial or clean samples. This section provides a detailed exposition of the SMCE computation process. When an image is classified through a deep learning model, the model outputs a classification vector and a classification label. As shown in the probability distribution diagram in Figure 2, the classification vector represents the probability 6 arXiv Template A PREPRINT Figure 3: Mask Entropy Field Map. distribution output by the classifier, reflecting the confidence levels of the input image belonging to each predefined category. The classification vector is a numerical vector, with each element corresponding to a category, having a value range of [0, 1], and the sum of all elements equals 1. The classification label is the final classification result generated by the classifier based on the classification vector, indicating the most likely category to which the image belongs. Figure 2 illustrates the confidence entropy computation process for a single occluded image. Specifically, the image is first partially occluded using a mask block. The occluded image is then fed into the deep learning model for classification to obtain the classification vector and label. Subsequently, the classification vector is substituted into the confidence entropy calculation formula to obtain the confidence entropy of the image under the occluded state. By sliding the mask block from left to right and top to bottom in sequence, and repeatedly computing the confidence entropy, the SMCE value of the image can be obtained by averaging all the computed results. 3.4.3 Mask Entropy Field Map The Sliding Mask Confidence Entropy (SMCE) is an indicator used to evaluate the stability of images under occlusion conditions. To more intuitively display the differences in stability between adversarial and clean samples, a visualization method called the Mask Entropy Field Map (MEFM) is proposed. As shown in Figure 3, this method involves sliding a mask over the image to',\n",
       "  'used to evaluate the stability of images under occlusion conditions. To more intuitively display the differences in stability between adversarial and clean samples, a visualization method called the Mask Entropy Field Map (MEFM) is proposed. As shown in Figure 3, this method involves sliding a mask over the image to progressively occlude different regions and calculating the output confidence entropy of the classifier for each occluded image. Specifically, the entropy values at each position are mapped onto the image, where regions with higher entropy values (i.e., lower classifier confidence) are represented by warm colors (such as red and black), and regions with lower entropy values (i.e., higher classifier confidence) are represented by cool colors (such as yellow and white). The Mask Entropy Field Map is not only clearly demonstrates the changes in stability under different occlusion conditions but also intuitively reveals the significant differences in stability between adversarial and clean samples. To further elucidate the stability differences between adversarial examples and clean samples, a subset of clean samples from the CIFAR-10 dataset was randomly selected, and their Mask Entropy Field Map (MEFM) were generated, as shown in the “Clear\" subfigure of Figure 4. Adversarial examples were then crafted from these clean samples using the Fast Gradient Sign Method (FGSM), and the corresponding MEFM were plotted, as illustrated in the “Adversarial\" subfigure of Figure 4. The color intensity within the maps intuitively reflects image stability: darker regions correspond to higher Sliding Mask Confidence Entropy (SMCE) values, indicating reduced stability. A direct comparison between the two subfigures reveals that adversarial examples consistently exhibit darker MEFM, reflecting significantly lower stability compared to clean samples. This finding substantiates the inherent vulnerability of adversarial examples. The presence of elevated entropy values in certain clean samples is likely due to underfitting in deep neural networks. While neural network classifiers achieve high performance on training datasets, attaining perfect classification accuracy remains challenging. Consequently, some clean samples are misclassified, thereby affecting the probability distribution in the SMCE computation and leading to higher entropy values. Conversely, the lower entropy values observed in some adversarial examples can be attributed to the loss of essential original features and the introduction of new category-related features induced by adversarial attack algorithms. This transformation enhances the robustness of the 7 arXiv Template A PREPRINT Figure 4: Mask Entropy Field Map for clean samples and adversarial examples. Figure 5: The Sliding Window Masking-Adversarial Example Detection classifier against adversarial examples in the presence of occlusion, thereby yielding lower SMCE values. Experimental results reveal that adversarial examples consistently exhibit higher Sliding Mask Confidence Entropy (SMCE) values than clean samples. This distinct characteristic enables effective detection of adversarial examples through SMCE-based threshold. 3.4.4 Adversarial Example Detection Algorithm In the preceding analysis, the vulnerability of adversarial examples was systematically examined, revealing a marked decline in classification confidence and shifts in predicted class labels when subjected to occlusion via a sliding window. 8 arXiv Template A PREPRINT To quantitatively capture this behavior, we propose the Sliding Mask Confidence Entropy (SMCE) metric, which measures fluctuations in confidence scores and the frequency of class',\n",
       "  'a marked decline in classification confidence and shifts in predicted class labels when subjected to occlusion via a sliding window. 8 arXiv Template A PREPRINT To quantitatively capture this behavior, we propose the Sliding Mask Confidence Entropy (SMCE) metric, which measures fluctuations in confidence scores and the frequency of class transitions under localized occlusion, thereby providing a precise quantitative assessment. Comparative analysis demonstrates that adversarial examples consistently yield significantly higher SMCE values than their clean counterparts, establishing a robust criterion for distinguishing adversarial examples from benign data. Building on these findings, we propose an SMCE threshold-based approach for adversarial example detection. Given an input image, its SMCE value is first computed, if the value exceeds a predefined threshold, the image is identified as an adversarial example, prompting appropriate defensive measures, such as flagging or removing it from the dataset to mitigate its impact on model misclassification. Conversely, if the SMCE value falls below the threshold, the image is classified as a clean sample and can be retained for model training or inference. The proposed method for detecting adversarial examples, as depicted in Figure 5, Upon inputting an image into the detection system, it is inherently uncertain whether the image is a pristine sample or one subjected to adversarial perturbations. To resolve this ambiguity, a novel strategy is employed: sliding masks are sequentially applied to occlude portions of the image, moving from the top-left corner to the bottom-right corner. Each occluded version of the image is then processed through a deep learning model, yielding a key metric—Sliding Mask Confidence Entropy (SMCE). The SMCE quantifies the stability of the image’s classification under occlusion, with a low value indicating high confidence that the image is a clean sample. Conversely, a high SMCE value signals instability in classification, suggesting the presence of adversarial manipulations. By setting a predefined threshold for SMCE, the method effectively classifies images as either unaltered or adversarial. Algorithm 1 introduces a crucial innovation in adversarial detection, as it enables precise identification of adversarial examples by leveraging occlusion-induced uncertainty, providing a robust approach to safeguard against adversarial attacks in real-world applications. Algorithm 1 SWM-AED: The Sliding Window Masking-Adversarial Example Detection 1: procedure SWM-AED(x, f, m, threshold) 2: Input: Image x, classifier f, mask size m, threshold 0.1 (default). 3: Output: Boolean indicating if x is an adversarial example. 4: Initialize: HSMCE ←0, n ←number of windows. 5: for each sliding window mask Mi in image x do 6: pij = fj(x ⊙Mi) 7: H = −Pm j=1 pij log2(pij) 8: HSMCE ←HSMCE + H 9: end for 10: HSMCE ←HSMCE n 11: if HSMCE > threshold then 12: return True ▷Adversarial example 13: else 14: return False ▷Not an adversarial example 15: end if 16: end procedure where x ⊙Mi denotes the region-wise occlusion of the image x by the i-th sliding window mask Mi of size m × m, and pij denotes the probability score assigned by the classifier f when predicting that the image x belongs to class j, and x is occluded by the i-th sliding mask of size m',\n",
       "  'occlusion of the image x by the i-th sliding window mask Mi of size m × m, and pij denotes the probability score assigned by the classifier f when predicting that the image x belongs to class j, and x is occluded by the i-th sliding mask of size m × m. 4 Experimental 4.1 Datasets and Classifiers All experiments are conducted on the widely adopted CIFAR-10 (Krizhevsky et al., 2009), which comprises 60,000 colour images (32 × 32 pixels) evenly distributed across ten object categories. Its balanced class distribution and moderate image resolution make it a standard benchmark in adversarial examples research. In the comparative experiments that reveal the sensitivity of adversarial examples to occlusion (e.g., Figure 4 and Figure 6), we randomly selected 1,800 images from the 10,000-image CIFAR-10 test set to form the control group. For the experimental group, we generated corresponding adversarial examples from these clean images using nine different adversarial attack algorithms, with each algorithm producing 200 adversarial samples. In the evaluation of the detection algorithm, 9 arXiv Template A PREPRINT we combined the experimental group (adversarial examples) and the control group (clean images), and applied the SWM-AED detection method to assess the accuracy of adversarial example detection. To comprehensively assess adversarial susceptibility, multiple deep neural network architectures with varying levels of accuracy—including ResNet-18, ResNet-50 and VGG-11—are trained on CIFAR-10. These models serve both as baselines and as entropy calculators for the proposed Sliding Window Masking-Adversarial Example Detection (SWM- AED) framework, which introduces the Sliding Mask Confidence Entropy (SMCE) as a discriminative feature for adversarial detection. Through extensive comparative experiments, SWM-AED consistently outperforms conventional defense methods across multiple evaluation metrics. These results highlight its robustness, scalability and potential as a generalizable solution to adversarial vulnerability in deep learning models. 4.2 Evaluation Metrics The SWM-AED algorithm we proposed can be regarded as a binary classifier, used to distinguish between adversarial examples and clean samples. In this context, the positive class represents adversarial examples, while the negative class represents clean samples that have not been attacked. We use Precision, Recall and F1 score as Three important metrics to evaluate the performance of this adversarial example detection algorithm. Confusion Matrix: A confusion matrix is a table used to evaluate the performance of a classification model. It contains four key components, TP (True Positive): Correctly predicted as adversarial. FP (False Positive): Incorrectly predicted as adversarial. FN (False Negative): Incorrectly predicted as clean. TN (True Negative): Correctly predicted as clean. For binary classification problem, the confusion matrix can be visualized as follows: Table 1: Confusion Matrix Actual Predicted Adversarial (Positive) Clean (Negative) Adversarial TP FN Clean FP TN Precision: Precision is the proportion of samples that are actually adversarial among those predicted as adversarial. The formula for calculating Precision is: Precision = TP TP + FP Recall: Recall is the proportion of samples that are correctly predicted as adversarial among all actual adversarial examples. It reflects the extent to which the algorithm can detect adversarial examples. The formula for calculating Recall is: Recall = TP TP + FN Accuracy: Accuracy denoted as',\n",
       "  'TP TP + FP Recall: Recall is the proportion of samples that are correctly predicted as adversarial among all actual adversarial examples. It reflects the extent to which the algorithm can detect adversarial examples. The formula for calculating Recall is: Recall = TP TP + FN Accuracy: Accuracy denoted as Acc, is a widely used and intuitive evaluation metric. It represents the proportion of correctly predicted samples out of the total number of samples. This metric provides a comprehensive reflection of the overall prediction accuracy of a model. Acc = TP + TN TP + TN + FP + FN F1 Score: The F1 score is the harmonic mean of Precision and Recall, providing a single metric that balances both. The formula for calculating the F1 score is: F1 = 2 × Precision × Recall Precision + Recall This score is particularly useful when the class distribution is unbalanced, as it gives a more balanced view of the model’s performance. 10 arXiv Template A PREPRINT Figure 6: The figure presents a comparative analysis of the Sliding Mask Confidence Entropy distributions for adversarial examples generated by nine distinct attack algorithms and their original, unperturbed counterparts. Empirical distributions are visualized, while fitted Gaussian distributions are overlaid with line graphs. Specifically, yellow bars and lines represent adversarial examples, whereas blue bars and lines denote clean samples. The x-axis corresponds to the range of Sliding Mask Confidence Entropy values, which reflect the uncertainty in classification outcomes for image samples under occlusion. The y-axis indicates the statistical frequency of Sliding Mask Confidence Entropy, providing insight into the density distribution across varying entropy levels. 4.3 Experimental Results In this experiment, the vulnerability of adversarial examples is comprehensively investigated, with particular emphasis on analyzing the distribution of SMCE values for adversarial examples generated using various attack methods. These distributions are systematically compared to those of clean samples. To further enhance the generalizability of the SWM-AED algorithm, we explore the impact of threshold settings on its performance, ensuring its efficacy across a wide range of adversarial attacks. Additionally, the influence of key factors (including mask size, model accuracy, model depth, and architecture) on the detection performance of the SWM-AED algorithm is thoroughly examined. Finally, comparative experiments with existing state-of-the-art adversarial defense algorithms demonstrate the superior detection accuracy of the proposed approach, validating its significant advantages. 4.3.1 The Empirical Distribution of the Sliding Mask Confidence Entropy This study systematically evaluates nine widely used adversarial attack algorithms—JSMA(Papernot et al., 2016), PGD(Madry et al., 2017), DeepFool(Moosavi-Dezfooli et al., 2016), FGSM(Goodfellow et al., 2014), BIM(Kurakin et al., 2018), FFGSM(Wong et al., 2020), APGD(Croce and Hein, 2020), Pixel(Pomponi et al., 2022), and PIFGSMPP(Gao et al., 2020)—by analyzing the Gaussian empirical distributions of Sliding Mask Confidence Entropy 11 arXiv Template A PREPRINT Figure 7: The figure illustrates the detection performance of the proposed method against adversarial examples generated by multiple attack algorithms. To ensure statistical significance, each algorithm generates a large-scale adversarial example dataset. The x-axis represents the range of classification decision thresholds, while the y-axis depicts the accuracy of the SWM-AED. for adversarial examples and',\n",
       "  'illustrates the detection performance of the proposed method against adversarial examples generated by multiple attack algorithms. To ensure statistical significance, each algorithm generates a large-scale adversarial example dataset. The x-axis represents the range of classification decision thresholds, while the y-axis depicts the accuracy of the SWM-AED. for adversarial examples and their original, unperturbed counterparts. The results reveal notable differences in the confidence entropy distributions between adversarial and clean samples, which are visually distinguishable. As can be seen from Figure 6, yellow distributions correspond to adversarial examples, while blue distributions represent clean samples. Differences in both the position and shape of these distributions serve as indicators of adversarial robustness. The findings demonstrate a significant correlation between the magnitude of distributional differences and the robustness of adversarial examples. When the entropy distribution of adversarial examples deviates substantially from that of clean samples, the adversarial examples tend to be more fragile, exhibiting higher susceptibility to perturbations and greater instability in classification outcomes. Conversely, smaller distributional discrepancies indicate that the generated adversarial examples yield more stable classification results. As illustrated in Figure 6, among the nine tested algorithms, adversarial examples generated by BIM, PGD, APGD, and PIFGSMPP exhibit entropy distributions that closely resemble those of clean samples, suggesting enhanced stability. This similarity in entropy profiles indicates that adversarial examples produced by these four algorithms share characteristics more akin to normal, non-adversarial data. Consequently, these adversarial examples are more stable, as they are better able to evade detection by the SWM-AED algorithm when compared to samples generated by other attack methods in the evaluation. Notably, the study finds that complete overlap between the distributions of adversarial and clean samples is exceedingly rare. This observation aligns with the fundamental nature of adversarial example generation: adversarial perturbations introduce deliberate noise into the original samples, inevitably disrupting their statistical properties and shifting their distributions. The entropy distribution plots provide direct insight into the vulnerability of adversarial examples. By examining these distributions, researchers can systematically assess the susceptibility of adversarial examples generated by different attack algorithms and further explore the impact of various adversarial attacks on model confidence. 12 arXiv Template A PREPRINT 4.3.2 The Relationship between Threshold and Model Performance Since the proposed detection algorithm is inherently a binary classification task, the selection of an appropriate decision threshold is crucial for effectively distinguishing between adversarial and clean samples. The choice of this threshold directly influences detection performance. To systematically assess the optimal detection accuracy across different threshold settings, the experiment evaluates ten representative adversarial attack algorithms, including FGSM, PGD, and DeepFool, among others. This comprehensive analysis aims to rigorously examine the effectiveness of the detection method against a wide range of adversarial attacks. Figure 7 depicts the variation in accuracy across different threshold conditions. The Sliding Mask Confidence Entropy (SMCE) values are computed using the ResNet-18 model to systematically assess the classification-based detection performance of the proposed SWM-AED algorithm across a range of threshold settings. Experimental findings reveal that SWM-AED consistently delivers strong detection performance against a wide spectrum of adversarial attacks. Notably, when the detection threshold is set to',\n",
       "  'values are computed using the ResNet-18 model to systematically assess the classification-based detection performance of the proposed SWM-AED algorithm across a range of threshold settings. Experimental findings reveal that SWM-AED consistently delivers strong detection performance against a wide spectrum of adversarial attacks. Notably, when the detection threshold is set to 0.1, the algorithm achieves robust generalisation, attaining detection accuracies exceeding 75% for the majority of adversarial examples—excluding only the BIM and OnePixel attacks. In particular, the detector demonstrates exceptional resilience against targeted attacks such as JSMA and DeepFool, achieving detection accuracies above 90%. Although performance against BIM-like attacks—characterised by iterative gradient-based perturbation—is comparatively lower, the algorithm still maintains detection rates above 60%, underscoring its robustness even in challenging scenarios. A comprehensive analysis of the detection results reveals that SWM-AED achieves an average detection accuracy exceeding 80% across randomly sampled adversarial examples. Among the nine evaluated attack methods, two yield detection rates above 90% and three exceed 80%, highlighting the algorithm’s strong generalisation capability and reliable performance across diverse adversarial threat models. 4.3.3 Investigation of Key Factors Influencing Algorithm Performance Table 2: Based on the ResNet-18 model with a classification accuracy rate of 96%, the mask detection method based on a 3 × 3 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack algorithms. Attack Method Precision (%) Recall (%) F1 Score Accuracy (%) JSMA 96.04 97.00 0.96 96.50 PGD 71.43 70.00 0.70 71.00 DeepFool 93.68 0.89 0.91 91.50 FGSM 76.56 98.00 0.85 84.00 BIM 61.54 56.00 0.58 60.50 FFGSM 79.05 83.00 0.80 80.50 APGD 64.66 75.00 0.69 67.00 OnePixel 77.27 68.00 0.72 74.00 Pixle 87.04 94.00 0.90 90.00 PIFGSMPP 67.97 87.00 0.76 73.00 Table 3: Based on the ResNet-18 model with a classification accuracy rate of 96%, the mask detection method based on a 7 × 7 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack algorithms. Attack Method Precision (%) Recall (%) F1 Score Accuracy (%) JSMA 95.10 97.00 0.96 96.00 PGD 76.53 75.00 0.75 76.00 DeepFool 86.36 95.00 0.90 90.00 FGSM 80.87 93.00 0.86 85.50 BIM 59.12 81.00 0.68 62.50 FFGSM 79.63 86.00 0.82 82.00 APGD 71.93 82.00 0.76 75.00 OnePixel 71.96 77.00 0.74 73.50 Pixle 84.21 96.00 0.89 89.00 PIFGSMPP 76.85 83.00 0.79 79.00 13 arXiv Template A PREPRINT Table 4: Based on the ResNet-18 model with a classification accuracy rate of 96%, the mask detection method based on a 9 × 9 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack algorithms. Attack Method Precision (%) Recall (%) F1 Score Accuracy (%) JSMA 88.39 99.00 0.93 93.00 PGD 71.30 82.00 0.76 74.50 DeepFool 82.76 96.00 0.88 88.00 FGSM 79.13 91.00 0.84 83.50 BIM 60.19 62.00 0.61 60.50 FFGSM 72.87 94.00 0.82 79.50 APGD 67.65 92.00 0.77 74.00 OnePixel 68.18 75.00 0.71 70.00 Pixle 83.18 89.00 0.85 85.50 PIFGSMPP 69.85 95.00 0.80 77.00 Table 5: Based on the ResNet-18 model with a classification accuracy rate of 80.8%, the mask detection method based on a 7',\n",
       "  '62.00 0.61 60.50 FFGSM 72.87 94.00 0.82 79.50 APGD 67.65 92.00 0.77 74.00 OnePixel 68.18 75.00 0.71 70.00 Pixle 83.18 89.00 0.85 85.50 PIFGSMPP 69.85 95.00 0.80 77.00 Table 5: Based on the ResNet-18 model with a classification accuracy rate of 80.8%, the mask detection method based on a 7 × 7 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack algorithms. Attack Method Precision (%) Recall (%) F1 Score Accuracy (%) JSMA 73.33 88.00 0.80 78.00 PGD 58.70 81.00 0.68 62.00 DeepFool 67.94 89.00 0.77 73.50 FGSM 60.87 84.00 0.70 65.00 BIM 59.09 13.00 0.21 52.00 FFGSM 59.40 79.00 0.67 62.50 APGD 57.96 91.00 0.70 62.50 OnePixel 53.05 87.00 0.65 55.00 Pixle 68.09 96.00 0.79 75.50 PIFGSMPP 60.31 79.00 0.68 63.50 Table 6: Based on the ResNet-50 model with a classification accuracy rate of 79.1%, the mask detection method based on a 7 × 7 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack algorithms. Attack Method Precision (%) Recall (%) F1 Score Accuracy (%) JSMA 78.07 89.00 0.83 82.00 PGD 67.16 90.00 0.76 73.00 DeepFool 70.87 90.00 0.79 76.50 FGSM 72.36 89.00 0.79 77.50 BIM 50.81 94.00 0.65 51.50 FFGSM 69.23 90.00 0.78 75.00 APGD 67.54 77.00 0.71 70.00 OnePixel 55.00 11.00 0.18 51.00 Pixle 61.74 92.00 0.73 67.50 PIFGSMPP 70.25 85.00 0.76 74.50 From the analysis of formula 2, it is evident that the SWM-AED algorithm requires a well-performing deep learning model to classify occluded images and generate the corresponding class probability distributions. These distributions are then used to calculate the SMCE, which is subsequently compared to a predefined threshold to detect and classify adversarial examples. The performance of the algorithm is primarily influenced by two key factors: the mask size and the intrinsic classification capability of the model. To systematically assess the impact of these factors on detection performance, we conduct a series of comparative experiments. These experiments examine the effects of mask size, classification accuracy of deep learning models, model architecture (including depth), and the generalization ability of the SWM-AED algorithm under various adversarial attacks. 14 arXiv Template A PREPRINT Table 7: Based on the Vgg-11 model with a classification accuracy rate of 81.3%, the mask detection method based on a 7 × 7 sliding window is used to evaluate its detection success rate for samples generated by various adversarial attack algorithms. Attack Method Precision (%) Recall (%) F1 Score Accuracy (%) JSMA 73.68 98.00 0.84 81.50 PGD 73.28 85.00 0.78 77.00 DeepFool 75.63 90.00 0.82 80.50 FGSM 73.28 85.00 0.78 77.00 BIM 54.78 63.00 0.58 55.50 FFGSM 69.17 92.00 0.78 75.50 APGD 71.68 81.00 0.76 74.50 OnePixel 54.23 77.00 0.63 56.00 Pixle 73.58 78.00 0.75 75.00 PIFGSMPP 68.46 89.00 0.77 74.00 Figure 8: Detection accuracy of the SWM-AED algorithm under various adversarial attack methods using different mask window sizes. The impact of mask window size on the SWM-ADE detection algorithm. Experiments were performed using the same model with three different mask sizes—3 × 3, 7 × 7, and 9 ×',\n",
       "  '74.00 Figure 8: Detection accuracy of the SWM-AED algorithm under various adversarial attack methods using different mask window sizes. The impact of mask window size on the SWM-ADE detection algorithm. Experiments were performed using the same model with three different mask sizes—3 × 3, 7 × 7, and 9 × 9—while evaluating their effects on detection accuracy. The corresponding results are presented in Tables 2, 3, and 4. The evaluation involved ten distinct adversarial attack algorithms and use four key performance metrics, including Precision, Recall, F1-score and Accuracy, for a comprehensive evaluation. For each attack algorithm, multiple classification thresholds were tested, and the optimal results were recorded. By synthesizing and analyzing the data presented in Tables 2, 3, and 4, Figure 8 is constructed. The figure distinctly illustrates substantial variations in the detection accuracy of the SWM-AED algorithm across different mask sizes. The experimental findings underscore the necessity of dynamically adjusting the mask size to optimize detection performance for various adversarial attack strategies. Further comparative analysis reveals that the SWM-AED algorithm attains 15 arXiv Template A PREPRINT Figure 9: Detection accuracy of the SWM-AED algorithm across models with different classification accuracies under various adversarial attack methods. optimal performance in most adversarial example detection tasks when the mask size is configured to 7 × 7. This finding suggests that neither the smallest nor the largest mask necessarily yields optimal results; rather, the choice of mask size should be tailored to the image dimensions and the specific characteristics of the detection task. A mask that is too small may fail to effectively occlude local image features, reducing its ability to capture adversarial perturbations, whereas an excessively large mask may introduce unnecessary noise, compromising local feature detection. The Impact of Deep Neural Network Accuracy on the SWM-ADE Detection Algorithm. Table 5 presents the detection results of the SWM-AED algorithm applied to the ResNet-18 model, which achieves a training accuracy of 80.8%. A comparative analysis with the results from Table 3 (used as a control variable) is conducted, leading to the construction of Figure 9. This figure visually demonstrates a robust positive correlation between the model’s classification accuracy and the detection performance of the SWM-AED algorithm. Specifically, as the model’s classification accuracy increases, the algorithm demonstrates improved effectiveness in detecting adversarial examples generated by different attack algorithms. This phenomenon can be explained from an algorithmic perspective: the SWM-AED detection mechanism relies on the stability of image features, which is quantified using the confidence scores output by the model. When a model with lower classification accuracy is used, even clean samples tend to receive lower confidence scores, leading to higher corresponding confidence entropy values. As a result, the confidence entropy distributions of adversarial and clean samples exhibit substantial overlap, making it difficult for the SWM-AED algorithm to establish an optimal decision threshold for effective differentiation. In contrast, high-accuracy models assign higher confidence scores and lower confidence entropy values to clean samples, creating a more pronounced distinction from the entropy distribution of adversarial examples. This distinction significantly enhances the detection performance of the SWM-AED algorithm by facilitating more',\n",
       "  'establish an optimal decision threshold for effective differentiation. In contrast, high-accuracy models assign higher confidence scores and lower confidence entropy values to clean samples, creating a more pronounced distinction from the entropy distribution of adversarial examples. This distinction significantly enhances the detection performance of the SWM-AED algorithm by facilitating more reliable threshold selection. The Impact of Deep Neural Network Layer Architecture on the SWM-ADE Detection Algorithm. Tables 6 and 5 present a controlled experimental analysis designed to evaluate the impact of model depth on the performance of the proposed SWM-AED algorithm. The experimental setup of Table 6 remains largely consistent with that of Table 5, with the sole difference being an increase in the depth of the ResNet architecture from 18 to 50 layers. The results from 16 arXiv Template A PREPRINT Figure 10: Detection accuracy of the SWM-AED algorithm using different model layer architectures under various adversarial attack methods. Tables 6 and 5 are integrated in Figure 10, which illustrates that a deeper network architecture significantly enhances the detection performance of the SWM-AED algorithm. This improvement can be attributed to two key factors: the algorithmic principles underlying SWM-ADE and the intrinsic characteristics of deep neural networks. First, the SWM-ADE detection mechanism is highly dependent on the model’s classification performance, which benefits from increased network depth. Second, deeper networks exhibit superior feature extraction capabilities, allowing them to capture more fine-grained representations and construct denser, more discriminative feature distributions. This leads to greater stability and reliability in classification outcomes. As a result, utilizing deeper network models for computing Sliding Mask Confidence Entropy significantly improves the effectiveness of the SWM-ADE algorithm in distinguishing between adversarial and clean samples. The Impact of Different Neural Networks on the SWM-ADE Detection Algorithm. To further assess the detection capability of the SWM-AED algorithm across different model architectures, the experimental scope was expanded to include the VGG network, with results summarized in Table 7. By leveraging Tables 5 and 7 as the experimental group, while controlling variables such as mask size and model accuracy, Figure 11 was generated. The experimental findings reveal that the SWM-AED algorithm demonstrates robust detection performance when the Sliding Mask Confidence Entropy (SMCE) is computed using the VGG model. This underscores the critical role of selecting high-performance deep neural networks as the foundation for SWM-AED detection. With the continuous advancements in deep learning technology, the adoption of more powerful architectures can significantly enhance the SWM-AED algorithm’s capacity to detect adversarial examples, reflecting its potential for self-improvement alongside technological progress. By continuously updating advanced models for SMCE computation, the performance of the SWM-AED algorithm can be incrementally refined, ensuring broad applicability across diverse scenarios. Notably, the effectiveness of the SWM-AED algorithm primarily stems from its core logic—the inherent vulnerability of adversarial examples—rather than being dependent on the unique characteristics of a specific model architecture. This highlights its remarkable generalizability and suitability for widespread deployment in practical environments. 17 arXiv Template A PREPRINT Figure 11: Detection accuracy of the SWM-AED algorithm using different models under various adversarial attack methods. Figure 12: The figure presents a performance comparison',\n",
       "  'the unique characteristics of a specific model architecture. This highlights its remarkable generalizability and suitability for widespread deployment in practical environments. 17 arXiv Template A PREPRINT Figure 11: Detection accuracy of the SWM-AED algorithm using different models under various adversarial attack methods. Figure 12: The figure presents a performance comparison between the SWM-AED algorithm and existing adversarial defense methods, including AT (Madry et al., 2017), FAST-AT (Wong et al., 2020), and FREE-AT (Shafahi et al., 2019). It evaluates the defense success rates of these four methods against adversarial examples generated by PGD attacks on the ResNet-18 network architecture. 18 arXiv Template A PREPRINT 4.3.4 Experimental Comparison with Classical Adversarial Defense Algorithms In this section, we perform a comparative analysis between the SWM-AED algorithm and classical adversarial defense methods, namely Adversarial Training (AT) and its variants, FAST-AT and FREE-AT. As shown in Figure 12, the experiments utilize the benchmark PGD adversarial attack algorithm and are conducted on the ResNet-18 classifier. The AT and its variants aim to defend against adversarial examples by retraining a robust ResNet-18 model, achieving a resistance rate of approximately 47% against adversarial examples generated by the PGD algorithm. In contrast, the SWM-AED algorithm leverages SMCE computation within a deep learning model, substantially improving the defense capability to 76%. These findings demonstrate that the SWM-AED algorithm achieves superior accuracy in detecting and defending against adversarial examples. 5 Discussion and Conclusion This study presents, for the first time, a systematic characterization of the intrinsic vulnerabilities of adversarial examples. While adversarial attacks are typically understood as perturbation-based manipulations that induce misclassification, we demonstrate that such perturbations inherently disrupt the local stability of input images. Specifically, the added noise perturbs the pixel value distribution, resulting in fuzzier classification boundaries. Compared to clean inputs, adversarial examples exhibit reduced semantic coherence, making classifiers more prone to prediction errors. To quantify this instability, we introduce a novel metric: Sliding Mask Confidence Entropy (SMCE), which evaluates the sensitivity of model predictions to localized occlusions. SMCE is computed by applying a sliding window across the image and aggregating the resulting confidence entropy values. This metric captures not only the structural consistency of the input image but also the robustness of the classifier to spatially localized perturbations. Building on this insight, we propose a detection framework termed Sliding Window Mask–Adversarial Example Detection (SWM-AED), which identifies adversarial examples by assessing anomalies in their SMCE values. The SMCE metric and SWM-AED algorithm exemplify a broader principle of deriving essential properties from observable phenomena. By leveraging localized fluctuations in model confidence, our approach reveals distinctive entropy patterns that differentiate adversarial examples from clean inputs. This capability enables robust adversarial detection without compromising classification accuracy, offering a pathway towards more secure and trustworthy AI systems. SWM-AED offers several significant innovations: • High Practical Applicability: SWM-AED can be seamlessly integrated into a wide range of existing deep neural network models without requiring architectural modifications. This makes it a lightweight and flexible defense mechanism that is readily deployable in real-world applications. • Model-Aware Adaptability: The effectiveness of SWM-AED naturally improves with more advanced classifiers. As',\n",
       "  'Applicability: SWM-AED can be seamlessly integrated into a wide range of existing deep neural network models without requiring architectural modifications. This makes it a lightweight and flexible defense mechanism that is readily deployable in real-world applications. • Model-Aware Adaptability: The effectiveness of SWM-AED naturally improves with more advanced classifiers. As shown in our experiments, the SMCE metric—proposed for the first time in this work—becomes more discriminative when computed on stronger models, allowing the detection performance to scale with model robustness and accuracy. • Flexible and Future-Proof Design: Unlike traditional detection methods that rely on static heuristics, SWM- AED leverages a dynamic detection mechanism grounded in a fundamental vulnerability of adversarial examples. This makes it inherently adaptable to future advances in model architecture and applicable across various domains involving deep learning. Theoretically, this study is the first to reveal and rigorously demonstrate that adversarial examples exhibit a heightened sensitivity to occlusion compared to their corresponding benign counterparts. This finding addresses a previously overlooked characteristic of adversarial examples and contributes new insights into their intrinsic weaknesses. We believe this insight opens up new avenues for understanding the underlying mechanisms of adversarial vulnerability in deep neural networks (DNNs), potentially aiding in the development of more robust AI systems. Notably, as demonstrated in this experiments, the detection performance of SMCE is closely tied to the robustness of the underlying classifier, making it a flexible and adaptive solution. This enables SWM-AED to be easily integrated into existing DNN-based systems across a wide range of domains. In contrast to traditional detection techniques, which often rely on fixed statistical patterns or input transformations, our method dynamically benefits from advancements in model architecture—yielding increasingly discriminative detection signals as classifier quality improves. This adaptability underscores a crucial aspect often overlooked in current AI development: the overemphasis on accuracy at the cost of systemic security. True AI reliability requires a balanced approach that prioritizes both predictive performance and robustness. The SWM-AED algorithm effectively achieves this balance, enhancing both model robustness and security while maintaining high accuracy. 19 arXiv Template A PREPRINT Data availability All datasets used in this paper were obtained from public data sources and repositories. A complete list of the public data sources with links is available via GitHub at https://github.com/dawei7777/SWM-AED/blob/master/README. md. Code availability The Python code used to compare the SMCE values of adversarial examples and clean samples, visualize the vulnerability of adversarial examples, and evaluate the detection accuracy of adversarial inputs using the SWM-AED algorithm is publicly available on GitHub at https://github.com/dawei7777/SWM-AED, along with documentation, DOIs, and citations (Li et al., 2025a). The code is released under the MIT license without restriction. 20 arXiv Template A PREPRINT A Proof of Property 3.2 Property 3.2. maximum value of SMCE HSMCE(I) ≤log2 m (A.1) Proof. In order to obtain the maximum value of HSMCE(I), We formulate the following constrained optimization problem: max pij 1 n n X i=1 \\uf8eb \\uf8ed− m X j=1 pij log2(pij) \\uf8f6 \\uf8f8 (A.2) s.t. m X j=1 pij = 1 for ∀i (A.3) Let us introduce Lagrange multipliers λi and construct the Lagrangian function as',\n",
       "  'maximum value of HSMCE(I), We formulate the following constrained optimization problem: max pij 1 n n X i=1 \\uf8eb \\uf8ed− m X j=1 pij log2(pij) \\uf8f6 \\uf8f8 (A.2) s.t. m X j=1 pij = 1 for ∀i (A.3) Let us introduce Lagrange multipliers λi and construct the Lagrangian function as follows: L(pij, λi) = 1 n n X i=1 \\uf8eb \\uf8ed− m X j=1 pij log2(pij) \\uf8f6 \\uf8f8+ 1 n n X i=1 λi \\uf8eb \\uf8ed m X j=1 pij −1 \\uf8f6 \\uf8f8 Solve the equations: \\uf8f1 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f3 ∂L(pij, λi) ∂pij = 0 m X j=1 pij = 1 for ∀ i Obtain: ∂L(pij, λi) ∂pij = ∂[−pij log2(pij) + λipij] ∂pij = −log2(pij) −pij · 1 pij ln 2 + λi = −log2(pij) − 1 ln 2 + λi = 0 So log2(pij) = λi − 1 ln 2 Then pij = 2λi− 1 ln 2 (A.4) Substituting pij = 2λi− 1 ln 2 into Pm j=1 pij = 1 gives: m X j=1 2λi− 1 ln 2 = m · 2λi− 1 ln 2 = 1 So 2λi− 1 ln 2 = 1 m (A.5) 21 arXiv Template A PREPRINT By eq. (A.4) and (A.5), pij = 1 m (A.6) Substituting pij = 1 m into HSMCE(I) yields: max HSMCE(I) = 1 n n X i=1 \\uf8eb \\uf8ed− m X j=1 1 m log2 1 m \\uf8f6 \\uf8f8= 1 n n X i=1 log2 m = log2 m Therefore: HSMCE(I) ≤log2 m 22 arXiv Template A PREPRINT References Borkar, T., Heide, F., Karam, L., 2020. Defending against universal attacks through selective feature regeneration, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 709–719. Buades, A., Coll, B., Morel, J.M., 2005. A review of image denoising algorithms, with a new one. Multiscale modeling & simulation 4, 490–530. Bui, M., Doan, T.P., Hong, K., Jung, S., 2025. Boosting black-box transferability of weak audio adversarial attacks with random masking, in: International Conference on Information Security Applications, pp. 96–108. Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., Mukhopadhyay, D., 2021. A survey on adversarial attacks and defences. CAAI Transactions on Intelligence Technology 6, 25–45. Croce, F., Hein, M., 2020. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks, in: International conference on machine learning, PMLR. pp. 2206–2216. Fu, X., Ma, L., Zhang, L., 2025. Remove to regenerate: Boosting adversarial generalization with attack invariance. IEEE Transactions on Circuits and Systems for Video Technology 35, 1999–2012. doi:doi:10.1109/TCSVT.2024.3487761. Gao, L., Zhang, Q., Song, J., Shen, H.T., 2020. Patch-wise++ perturbation for adversarial targeted attacks. arXiv arXiv: 2012.15503. URL: http://arxiv.org/abs/2012.15503. Goodfellow, I.J., Shlens, J., Szegedy, C., 2014. Explaining and harnessing adversarial examples. arXiv URL: https://arxiv.org/abs/1412.6572. Ji, Q., Wang, L., Shi, C., Hu, S., Chen, Y., Sun, L., 2023. Benchmarking and analyzing robust point cloud recognition: Bag of tricks for defending adversarial examples, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4295–4304. Jin, Y., Zhang, X., Lou, J., Ma, X., Wang, Z., Chen, X., 2023. Explaining adversarial robustness of neural networks from clustering',\n",
       "  'Sun, L., 2023. Benchmarking and analyzing robust point cloud recognition: Bag of tricks for defending adversarial examples, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4295–4304. Jin, Y., Zhang, X., Lou, J., Ma, X., Wang, Z., Chen, X., 2023. Explaining adversarial robustness of neural networks from clustering effect perspective, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4522–4531. Kherchouche, A., Fezza, S.A., Hamidouche, W., Déforges, O., 2020. Detection of adversarial examples in deep neural networks with natural scene statistics, in: 2020 International Joint Conference on Neural Networks (IJCNN), pp. 1–7. doi:doi:10.1109/IJCNN48605.2020.9206959. Kim, H., Kim, B.C., Lee, S., Kang, M., Nam, H., Park, S., Kwak, I.Y., Lee, J., 2024. Rapid: Robust multi-patch masker using channel-wise pooled variance with two-stage patch detection. Journal of King Saud University-Computer and Information Sciences 36, 102188. Krizhevsky, A., Hinton, G., et al., 2009. Learning multiple layers of features from tiny images . Kurakin, A., Goodfellow, I.J., Bengio, S., 2018. Adversarial examples in the physical world, in: Artificial intelligence safety and security, pp. 99–112. Le, Q., Chan, F., Ni, J., Yam, S., Lu, N., 2025. Defending against high-intensity adversarial perturbations in deep neural networks: A robust swin transformer approach, in: 2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC), pp. 0270–0275. doi:doi:10.1109/ICAIIC64266.2025.10920873. Lee, B.K., Kim, J., Ro, Y.M., 2023. Mitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4499–4509. Lee, K., Lee, K., Lee, H., Shin, J., 2018. A simple unified framework for detecting out-of-distribution samples and adversarial attacks, in: Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (Eds.), Advances in Neural Information Processing Systems. Li, J., Xu, Y., Hu, Y., 2025a. Deep learning models are vulnerable, but adversarial examples are even more vulnerable. doi:doi:10.5281/zenodo.15508744. Li, J., Xu, Y., Hu, Y., Ma, Y., Yin, X., 2025b. You only attack once: Single-step deepfool algorithm. Applied Sciences 15. doi:doi:10.3390/app15010302. Li, W., Li, B., Nie, W., Wang, L., Liu, A.A., 2025c. Diversified perturbation guided by optimal target code for cross-modal adversarial attack. Information Processing & Management 62, 104214. doi:doi:https://doi.org/10.1016/j.ipm.2025.104214. Liu, S., Lian, Z., Zhang, S., Xiao, L., 2025. Adversarial purification of information masking. Neurocomputing 621, 129214. Ma, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Schoenebeck, G., Song, D., Houle, M.E., Bailey, J., 2018. Characterizing adversarial subspaces using local intrinsic dimensionality. arXiv preprint arXiv:1801.02613 . 23 arXiv Template A PREPRINT Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A., 2017. Towards deep learning models resistant to adversarial attacks. stat 1050. Meng, D., Chen, H., 2017. Magnet: A two-pronged defense against adversarial examples, in: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, Association for Computing Machinery, New York, NY, USA. p. 135–147. doi:doi:10.1145/3133956.3134057. Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P., 2016. Deepfool: a simple and accurate method to fool deep neural networks, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2574–2582. Mustafa, A., Khan, S.H., Hayat, M., Shen, J., Shao, L., 2020. Image super-resolution as a defense',\n",
       "  \"135–147. doi:doi:10.1145/3133956.3134057. Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P., 2016. Deepfool: a simple and accurate method to fool deep neural networks, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2574–2582. Mustafa, A., Khan, S.H., Hayat, M., Shen, J., Shao, L., 2020. Image super-resolution as a defense against adversarial attacks. IEEE Transactions on Image Processing 29, 1711–1724. doi:doi:10.1109/TIP.2019.2940533. Naseer, M., Khan, S., Hayat, M., Khan, F.S., Porikli, F., 2020. A self-supervised approach for adversarial robustness, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 262–271. Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z.B., Swami, A., 2017. Practical black-box attacks against machine learning, in: Proceedings of the 2017 ACM on Asia conference on computer and communications security, Association for Computing Machinery, New York, NY, USA. pp. 506–519. doi:doi:10.1145/3052973.3053009. Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.B., Swami, A., 2016. The limitations of deep learning in adversarial settings, in: 2016 IEEE European symposium on security and privacy (EuroS&P), pp. 372–387. doi:doi:10.1109/EuroSP.2016.36. Pomponi, J., Scardapane, S., Uncini, A., 2022. Pixle: a fast and effective black-box attack based on re- arranging pixels, in: 2022 International Joint Conference on Neural Networks (IJCNN), IEEE. pp. 1–7. doi:doi:10.1109/IJCNN55064.2022.9892966. Qian, Y., He, S., Zhao, C., Sha, J., Wang, W., Wang, B., 2023. Lea2: A lightweight ensemble adversarial attack via non-overlapping vulnerable frequency regions, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4510–4521. Rangwani, H., Aithal, S.K., Mishra, M., Jain, A., Radhakrishnan, V.B., 2022. A closer look at smoothness in domain adversarial training, in: International conference on machine learning, PMLR. pp. 18378–18399. Ryu, G., Choi, D., 2024. Detection of adversarial attacks based on differences in image entropy. International Journal of Information Security 23, 299–314. doi:doi:10.1007/s10207-023-00735-6. Shafahi, A., Najibi, M., Ghiasi, M.A., Xu, Z., Dickerson, J., Studer, C., Davis, L.S., Taylor, G., Goldstein, T., 2019. Adversarial training for free!, in: Wallach, H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., Garnett, R. (Eds.), Advances in Neural Information Processing Systems. Sotgiu, A., Demontis, A., Melis, M., Biggio, B., Fumera, G., Feng, X., Roli, F., 2020. Deep neural rejection against adversarial examples. EURASIP Journal on Information Security 2020, 1–10. Sui, C., Wang, A., Wang, H., Liu, H., Gong, Q., Yao, J., Hong, D., 2025. Isdat: An image-semantic dual adversarial training framework for robust image classification. Pattern Recognition 158, 110968. doi:doi:https://doi.org/10.1016/j.patcog.2024.110968. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., Fergus, R., 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 . Tarchoun, B., Ben Khalifa, A., Mahjoub, M.A., Abu-Ghazaleh, N., Alouani, I., 2023. Jedi: Entropy-based localization and removal of adversarial patches, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4087–4095. Tarchoun, B., Khalifa, A.B., Mahjoub, M.A., Abu-Ghazaleh, N., Alouani, I., 2024. An information-theoretic perspective of physical adversarial patches. Neural Networks 179, 106590. Tian, C., Fei, L., Zheng, W., Xu, Y., Zuo, W., Lin, C.W., 2020. Deep learning on image denoising: An overview. Neural Networks 131, 251–275. Tramèr, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., McDaniel, P., 2017. Ensemble adversarial training:\",\n",
       "  'information-theoretic perspective of physical adversarial patches. Neural Networks 179, 106590. Tian, C., Fei, L., Zheng, W., Xu, Y., Zuo, W., Lin, C.W., 2020. Deep learning on image denoising: An overview. Neural Networks 131, 251–275. Tramèr, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., McDaniel, P., 2017. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204 URL: http://arxiv.org/abs/1705.07204. West, M.T., Tsang, S.L., Low, J.S., Hill, C.D., Leckie, C., Hollenberg, L.C., Erfani, S.M., Usman, M., 2023. Towards quantum enhanced adversarial robustness in machine learning. Nature Machine Intelligence 5, 581–589. Wong, E., Rice, L., Kolter, J.Z., 2020. Fast is better than free: Revisiting adversarial training. arXiv arXiv: 2001.03994. URL: https://arxiv.org/abs/2001.03994. Wu, S., Wang, J., Zhao, J., Wang, Y., Liu, X., 2024. Napguard: Towards detecting naturalistic adversarial patches, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 24367–24376. 24 arXiv Template A PREPRINT Xie, C., Tan, M., Gong, B., Yuille, A., Le, Q.V., 2020. Smooth adversarial training. arXiv preprint arXiv:2006.14536 URL: http://arxiv.org/abs/2006.14536. Xie, C., Wu, Y., Maaten, L.v.d., Yuille, A.L., He, K., 2019. Feature denoising for improving adversarial robustness, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 501–509. Xu, C., Zhang, C., Yang, Y., Yang, H., Bo, Y., Li, D., Zhang, R., 2023. Accelerate adversarial training with loss guided propagation for robust image classification. Information Processing & Management 60, 103143. doi:doi:https://doi.org/10.1016/j.ipm.2022.103143. Xu, W., Evans, D., Qi, Y., 2017. Feature squeezing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155 . Yang, R., Sun, Q., Cao, H., Shen, C., Cai, J., Rong, D., 2025. 1 + 1 > 2: A dual-function defense frame- work for adversarial example mitigation. IEEE Transactions on Information Forensics and Security , 1– 1.doi:doi:10.1109/TIFS.2025.3555186. Zhang, S., Song, Y., Wang, S., 2025. Fa-gan: Defense against adversarial attacks in automatic modulation recognition, in: ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1–5. doi:doi:10.1109/ICASSP49660.2025.10890747. Zhang, X., Zhang, T., Zhang, Y., Liu, S., 2024a. Enhancing the transferability of adversarial attacks with stealth preservation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2915–2925. Zhang, Z., Li, X., Li, H., Dunkin, F., Li, B., Li, Z., 2024b. Dual-branch sparse self-learning with instance binding augmentation for adversarial detection in remote sensing images. IEEE Transactions on Geoscience and Remote Sensing 62, 1–13. doi:doi:10.1109/TGRS.2024.3436841. Zhao, J., Xie, L., Gu, S., Qin, Z., Zhang, Y., Wang, Z., Hu, Y., 2025. Universal attention guided adversarial defense using feature pyramid and non-local mechanisms. Scientific Reports 15, 5237. doi:doi:10.1038/s41598-025-89267-8. Zhao, M., Zhang, L., Kong, Y., Yin, B., 2023. Fast adversarial training with smooth convergence, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4720–4729. 25'],\n",
       " ['Self-adaptive weighting and sampling for physics-informed neural networks Wenqian Chena, Amanda Howarda, Panos Stinisa,∗ aAdvanced Computing, Mathematics and Data Division Pacific Northwest National Laboratory Richland, WA 99354, USA Abstract Physics-informed deep learning has emerged as a promising framework for solving partial differential equations (PDEs). Nevertheless, training these models on com- plex problems remains challenging, often leading to limited accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling and weighting method to en- hance the performance of physics-informed neural networks (PINNs). The adaptive sampling component identifies training points in regions where the solution exhibits rapid variation, while the adaptive weighting component balances the convergence rate across training points. Numerical experiments show that applying only adaptive sampling or only adaptive weighting is insufficient to consistently achieve accurate predictions, particularly when training points are scarce. Since each method empha- sizes different aspects of the solution, their effectiveness is problem dependent. By combining both strategies, the proposed framework consistently improves prediction accuracy and training efficiency, offering a more robust approach for solving PDEs with PINNs. Keywords: Self-adaptive weighting, Self-adaptive sampling, Physics-informed ∗Corresponding author Email address: panos.stinis@pnnl.gov (Panos Stinis ) 1 arXiv:2511.05452v1 [stat.ML] 7 Nov 2025 neural networks 1. Introduction Physics-informed neural networks (PINNs) embed the governing partial differ- ential equation (PDE) into the loss function of a deep neural network, enforcing physical consistency alongside data fidelity [1]. In a standard PINN formulation, the objective is composed of a data term (for initial and boundary conditions) and a PDE residual term evaluated at collocation points in the domain. While this approach has shown promise on a variety of forward and inverse PDE problems, training can be challenging: competing loss components may have wildly different scales, and uni- form sampling of collocation points often fails to focus learning on regions where the solution exhibits sharp gradients or singular behavior [2–5]. To address the multi-objective nature of PINN training due to competing loss terms, a range of self-adaptive weighting strategies have been proposed. Gradient- based methods include learning rate annealing, where weights are updated inversely to back-propagated gradients [6]. Time-structured approaches, such as causal train- ing, assign weights in a temporally ordered fashion for time-dependent problems [7]. Residual-based strategies are also popular: Liu and Wang [8] proposed a minimax method with gradient descent for parameters and ascent for weights; McClenny and Braga-Neto [5] extended this to point-wise weights; and auxiliary networks have been used for point-wise weighting [9, 10]. Anagnostopoulos et al. [11] updated weights by normalized residuals. Lagrangian approaches adapt weights as multipliers for con- straints, including augmented Lagrangian methods (ALM) [12], adaptive ALM [13], dual problem formulations [14], and point-wise multipliers [15]. Finally, kernel-based strategies such as neural tangent kernel (NTK) weighting [16, 17] and the conjugate kernel (CK) [18] update weights according to kernel eigenvalue information. 2 In parallel, adaptive sampling methods in physics-informed machine learning refine training point distributions to better capture the structure of the solution. Residual-based approaches [19, 20] place points in regions of large residuals or select informative samples during training. Importance sampling [21] chooses points ac- cording to',\n",
       "  'kernel eigenvalue information. 2 In parallel, adaptive sampling methods in physics-informed machine learning refine training point distributions to better capture the structure of the solution. Residual-based approaches [19, 20] place points in regions of large residuals or select informative samples during training. Importance sampling [21] chooses points ac- cording to a proposal distribution derived from the loss function to improve efficiency. Residual/gradient-based strategies [22] enhance both accuracy and stability. Deep adaptive sampling [23] employs generative models to sample high-residual regions, while annealed adaptive importance sampling [24] applies expectation–maximization to handle multimodal loss landscapes. For singular or sharp solutions, the expected improvement refinement [25] incorporates residual gradients and boundary sampling, and Gaussian mixture distribution-based sampling [26] uses residual-informed distri- butions for adaptive selection. Collectively, these methods improve convergence and accuracy, though iterative point addition increases computational cost. In this work, we propose a framework that integrates adaptive weighting and adaptive sampling for physics-informed machine learning. The two strategies ad- dress complementary aspects of the training process: adaptive weighting balances the contributions of different loss components, ensuring that no part of the solu- tion dominates or is neglected, while adaptive sampling redistributes training points toward regions that are more challenging to approximate, such as areas with large residuals or sharp gradients. By combining these mechanisms, the framework pro- vides a more comprehensive treatment of training, simultaneously stabilizing op- timization and enhancing data efficiency. Numerical experiments demonstrate that the proposed method consistently achieves high prediction accuracy, highlighting the benefits of this complementary interaction. The rest of the paper is structured as follows. Section 2 reviews the concept of PINNs and summarizes our previous work on adaptive weighting based on the 3 balanced residual decay rate (BRDR) [27]. Section 3.2 introduces the self-adaptive sampling method based on residuals and discusses how to combine it with the adap- tive weighting method. In Section 4 the proposed adaptive weighting and sampling method is tested on four benchmark problems. Finally, some conclusions will be drawn in Section 5. To promote reproducibility and further research, the code and all accompanying data will be made available upon publication. 2. Background 2.1. Physics-Informed Neural Networks Physics-informed neural networks (PINNs) are designed to approximate the solu- tion of PDEs by minimizing a loss function that includes physics-based terms derived from the governing equations. Let’s assume we are solving a general PDE subject to boundary conditions (BCs): N(u(x)) = 0, x ∈Ω (1) B(u(x)) = 0, x ∈∂Ω, (2) where N is a differential operator and u(x) is the solution we seek. The boundary condition is enforced by a general boundary operator B. The goal of the PINN is to approximate u(x) by a neural network uθ(x), where θ represents the network parameters (weights and biases). The PINN loss function usually consists of two main components, Ltotal = LPDE + LBC. (3) The first component is the residual loss, which ensures that the neural network approximation uθ(x) satisfies the PDE: LPDE = Ex∈Ω \\x02 |N(uθ(x))|2\\x03 ≈1 Nr Nr X i=1 |N(uθ(xi))|2 , (4) 4 where Nr is the number of collocation points (randomly selected',\n",
       "  'main components, Ltotal = LPDE + LBC. (3) The first component is the residual loss, which ensures that the neural network approximation uθ(x) satisfies the PDE: LPDE = Ex∈Ω \\x02 |N(uθ(x))|2\\x03 ≈1 Nr Nr X i=1 |N(uθ(xi))|2 , (4) 4 where Nr is the number of collocation points (randomly selected points in the domain where the PDE is enforced) and xi are the coordinates of these points. The second component is the boundary condition loss, which ensures that the solution satisfies the general boundary conditions imposed by B. This loss is formulated as: LBC = Ex∈∂Ω \\x02 |B(uθ(xi))|2\\x03 ≈1 Nb Nb X i=1 |B(uθ(xi))|2 , (5) where Nb is the number of boundary points. 2.2. Adaptive Weighting based on balanced residual decay rate In our recent work [27], we introduced a self-adaptive weighting method based on the balanced residual decay rate (BRDR). This approach assigns a pointwise adaptive weight to each residual term, including both the governing equation and boundary condition residuals. The method dynamically adjusts these weights during training to balance the convergence rates across all training points, thereby improving both accuracy and efficiency in solving PDEs with PINNs. The weighted loss function is defined as follows: L(θ; w, s) = s 1 NR NR X i=1 wi RR2(xi R) + 1 NB NB X i=1 wi BB2(xi B) ! (6) s.t. mean(w) := PNR i=1 wi R + PNB i=1 wi B NR + NB = 1 (7) where wi R > 0 is the weight assigned to the residual of each residual collocation point, wi B > 0 is the weight assigned to each boundary point, and w is the collection of these weights. The scale factor s is employed to scale all the weights, so that the formulation could cover all kinds of possible weight distributions. The basic idea behind BRDR is to update weights based on two critical observa- tions: 5 1. Residuals at different training points may vary significantly across the domain. 2. The point with the smallest residual decay rate often dominates the convergence speed of the global solution. To quantify the speed of residual decay, we use the inverse residual decay rate (IRDR), defined as: IRDR = R2(t) q R4(t) + ϵ (8) where R(t) represents the residual at iteration t, R4(t) is the exponential moving average of R4(t), and ϵ is a small constant to avoid division by 0. The exponential moving average R4(t) is updated using: R4(t) = βc R4(t −1) + (1 −βc) R4(t) (9) where βc is a smoothing constant that controls the influence of past residuals. Since a larger IRDR indicates a slower residual decay, we assign higher weights to loss terms with larger IRDR values. To manage these weights dynamically during training, we compute reference weights at each iteration t based on the relative IRDR values with respect to their global mean. This strategy ensures that the mean of the weights remains at 1, keeping the weights bounded throughout the training process: wref t = IRDRt mean(IRDRt) (10) where IRDRt is the vector of IRDR values for all',\n",
       "  'iteration t based on the relative IRDR values with respect to their global mean. This strategy ensures that the mean of the weights remains at 1, keeping the weights bounded throughout the training process: wref t = IRDRt mean(IRDRt) (10) where IRDRt is the vector of IRDR values for all the training items at iteration t. To minimize noise and stabilize weight updates during training, the weights are adjusted using an exponential moving average: wt = βw wt−1 + (1 −βw) wref t (11) 6 where βw is a smoothing factor that helps to smooth out fluctuations in the weights. For more details on the BRDR method, please refer to our previous work [27]. 3. Adaptive Training in Physics-Informed Machine Learning While adaptive weighting adjusts the importance of different training points, it often requires a high density of points in regions with large gradients, leading to a large number of residual points when using random sampling and increasing significantly the computing cost for training. To reduce the number of training points, we develop an adaptive sampling method so that we can use fewer training points while achieving a higher training accuracy. 3.1. Issues with overfitting Here we take a 1D perturbation equation as an example to showcase the possible issues that can arise when using the adaptive weighting method with a small number of training points. The 1D perturbation equation we use as an example is given by −ϵ2d2u dx2 + u(x) = 1, x ∈[0, 1] u(0) = u(1) = 0 (12) where the parameter ϵ is set to 10−4. The analytical solution of this problem is given by: u(x) = 1 −e−x/ϵ + e(x−1)/ϵ 1 + e−1/ϵ . (13) The solution contains two thin boundary layers at x = 0 and x = 1, with the boundary layer thickness is proportional to ϵ. The stiff gradient within the boundary layers presents a challenge to PINN training. To solve this problem with PINNs, we uniformly sample 128 training points within the domain. The PINN prediction is shown in Fig. 1. The residuals at the training 7 points are minimized to a very small magnitude (less than 10−10), yet the PINN prediction shows a large deviation from the ground truth. This discrepancy arises because the residuals at unseen points remain significant. In other words, the fixed training points underestimate the average residuals, which ultimately leads to the large error of the PINN prediction. 0.0 0.2 0.4 0.6 0.8 1.0 x 0.0 0.2 0.4 0.6 0.8 1.0 Exact solution PINN solution 0.0 0.2 0.4 0.6 0.8 1.0 x −14 −12 −10 −8 −6 −4 −2 0 log10(|residual|2) Residuals at a ﬁne grid Residuals at 128 training points Figure 1: (Left) PINN prediction for the 1D perturbation equation, and (right) residuals at the training points compared with those on a fine grid. 3.2. Self-adaptive sampling based on residuals Considering the overfitting issue discussed in Section 3.1, a straightforward rem- edy is to introduce additional training points in regions where the residuals are large. To achieve this, we first compute the residuals R(x) over a',\n",
       "  'points compared with those on a fine grid. 3.2. Self-adaptive sampling based on residuals Considering the overfitting issue discussed in Section 3.1, a straightforward rem- edy is to introduce additional training points in regions where the residuals are large. To achieve this, we first compute the residuals R(x) over a set of candidate points. New training points are then randomly selected from these candidates according to a probability distribution, as shown in Fig. 2. The probability of selection is proportional to the square of the corresponding residuals: p(x) = R2(x) P x R2(x) (14) 8 To prevent overemphasizing points with extremely large residuals and underem- phasizing those with extremely small residuals, we clip the residuals using: R2 clipped(x) = max \\x10 ˆR2, min \\x10 R2(x), γ ˆR2\\x11\\x11 (15) where ˆR2 is the median value of R2(x) over the candidate points, and γ = 100 is a scaling factor unless stated otherwise. Figure 2: Schematic illustration of the implementation process for self-adaptive sampling based on residuals. Resampling is performed after a specified number of optimization steps. To pre- vent oscillations caused by abruptly replacing all training points, only a fraction pu of the existing points are updated in each resampling cycle, while the remaining points are retained. 3.3. Combination of Adaptive Weighting and Sampling To effectively combine adaptive weighting with adaptive sampling, it is neces- sary to determine the weights, or weight-related quantities, for the newly introduced training points. As described in Section 2.2, both the weights and R4 are updated at every training step. When new points are added through adaptive sampling, their corresponding weights and R4 must also be computed. 9 To estimate these quantities at the new points, we employ inverse distance weight- ing (IDW) interpolation. This interpolation allows us to estimate the weights and R4 for new points based on the existing training points, ensuring a smooth transition in the weighting scheme when training points are updated. wnew = P i wi ri P i 1 ri , R4new = P i R4i ri P i 1 ri (16) where: • wnew and R4new are the weight and exponential moving average of R4 at the new point, • wi and R4i are the corresponding quantities at old points, • ri is the distance between the new point and the i-th old point. The combined algorithm is summarized in Algorithm 1. For adaptive weighting, key parameters include βc (set to 0.999) for smoothing the exponential moving av- erage of R4 and βw (set to 0.999) for weight updates, both of which perform well across various PINN problems [27]. For adaptive sampling, γ (set to 100) controls residual clipping, pu (set to 0.2) is the fraction of points updated per resampling, and Ns (set to 100) determines the update frequency, namely the interval between resampling steps. Default values are found empirically to be effective but may be tuned for specific problems; detailed analyses are provided in Appendix B. 10 Algorithm 1 Adaptive Weighting and Sampling Input: Initial training points X0 1: for each training iteration t = 1, 2, . .',\n",
       "  'the interval between resampling steps. Default values are found empirically to be effective but may be tuned for specific problems; detailed analyses are provided in Appendix B. 10 Algorithm 1 Adaptive Weighting and Sampling Input: Initial training points X0 1: for each training iteration t = 1, 2, . . . , T do 2: Adaptive Weighting: 3: Compute residuals Rt(x) at current training points Xt−1 4: Update R4t(x) = βc R4t−1(x) + (1 −βc) R4 t(x) 5: Calculate IRDR: IRDRt(x) = R2 t(x) q R4t(x) + ϵ 6: Compute reference weights: wref t (x) = IRDRt(x) mean(IRDRt(x)) 7: Update weights: wt(x) = βw wt−1(x) + (1 −βw) wref t (x) 8: Adaptive Sampling: 9: if t mod Ns = 0 then 10: Generate candidate points Xcand 11: Compute residuals Rt(x) for x ∈Xcand 12: Compute selection probabilities with clipped residuals: p(x) = R2 clipped(x) P x∈Xcand R2 clipped(x), where R2 clipped(x) = max \\x10 ˆR2, min \\x10 R2 t(x), γ ˆR2\\x11\\x11 13: Select pu|Xt−1| new points Xnew based on p(x) 14: Select pu|Xt−1| old points Xreplace randomly. 15: IDW Interpolation for New Points: 16: wt(Xnew) = IDW(wt(Xt−1), Xt−1, Xnew ) 17: R4t(Xnew) = IDW(R4t(Xt−1), Xt−1, Xnew ) 18: Update training points: Xt = (Xt−1 \\\\ Xreplace) ∪Xnew 19: end if 20: Assemble the total loss and perform backward propagation 21: Update the parameters with gradient descent 22: end for 11 4. Numerical results To validate the performance of the proposed self-adaptive weighting and sampling approach in training PINNs, we test on four benchmark problems: the perturbation equation, the Allen–Cahn equation, the Burgers equation, and the lid-driven cavity flow. The prediction accuracy is assessed using the L2 relative error defined as ϵL2 = ∥u −uE∥2 ∥uE∥2 (17) where u and uE are the vectors of the predicted and reference solutions on the test set, respectively. In our experiments, we employ the mFCN network architecture (see Appendix A) which consists of six hidden layers with 128 neurons each. The hyperbolic tangent function is utilized as the activation function throughout the network. Network parameters are initialized using Kaiming Uniform initialization [28]; specifically, for a layer with shape (out_features, in_features), the weights and biases are sampled from U \\x10 − √ k, √ k \\x11 with k = 1/in_features. All implementations are carried out in PyTorch [29] and executed on a GPU cluster using a 32-bit single-precision data type on an NVIDIA® Tesla P100 GPU. To assess the impact of adaptive sampling and weighting strategies, Fig. 3 presents the PINN prediction errors and training costs for the four representative problems. Across all training methods, increasing the number of residual points gen- erally reduces the prediction error, but the rate of improvement varies notably among methods and problems. For the perturbation and Burgers equations, the error de- creases steadily as the number of residual points (i.e., the batch size) increases. In contrast, for the Allen–Cahn equation, the error under adaptive sampling remains nearly unchanged with larger batch sizes, while for the lid-driven cavity flow problem, 12 adaptive weighting shows little sensitivity to batch size. These results',\n",
       "  'de- creases steadily as the number of residual points (i.e., the batch size) increases. In contrast, for the Allen–Cahn equation, the error under adaptive sampling remains nearly unchanged with larger batch sizes, while for the lid-driven cavity flow problem, 12 adaptive weighting shows little sensitivity to batch size. These results demonstrate that the effectiveness of adaptive sampling and weighting is strongly problem depen- dent, making it difficult to rely solely on either strategy for robust performance. By contrast, the combined approach consistently achieves the best accuracy across all tested cases. In terms of computational cost, the combined approach increases training time by less than 20% relative to the non-adaptive PINN under the same batch size and maximum iteration settings. More importantly, for a given target prediction error, the proposed method achieves results with substantially less training time than the other strategies. To visualize the distribution of training points and their associated weights for the Allen–Cahn, Burgers, and lid-driven cavity flow problems, we plot scatter dia- grams of the training points colored by their weights, as shown in Figs. C.9–C.11 in Appendix C. While these plots allow for some preliminary observations, they do not clearly reveal the focus of different methods, particularly for the combined approach. To provide a more quantitative assessment, we employ kernel density estimation (KDE) to evaluate the relative importance of different regions. Specifically, we use gaussian_kde in SciPy, which by default applies Scott’s rule for bandwidth selection [30], and incorporate the adaptive weights by assigning them as kernel weights. The detailed results will be discussed in the following subsections. 4.1. Perturbation problem We revisit the 1D perturbation problem introduced in Section 3.1 to evaluate the effectiveness of the proposed adaptive weighting and sampling strategies. The training setup is summarized in Table 1. Figure 3 presents the prediction accuracy of PINN training with and without adaptive weighting and sampling. For this problem, 13 128 256 512 1024 2048 4096 8192 nResi 10−4 10−3 10−2 10−1 error 2.4 × 10−2 2.8 × 10−2 3.2 × 10−2 sec/step SAweight+SAsample SAsample SAweight No adaptivity (a) Perturbation 1000 2000 5000 10000 25000 nResi 10−4 10−3 10−2 10−1 100 error 10−1 6 × 10−2 2 × 10−1 sec/step SAweight+SAsample SAsample SAweight No adaptivity (b) Allen Cahn 1000 2000 4000 800010000 nResi 10−4 10−3 10−2 10−1 100 error 3 × 10−2 4 × 10−2 sec/step SAweight+SAsample SAsample SAweight No adaptivity (c) Burgers 512 1024 2048 4096 nResi 10−3 10−2 10−1 error 8.6 × 10−28.8 × 10−2 9 × 10−2 9.2 × 10−29.4 × 10−29.6 × 10− sec/step SAweight+SAsample SAsample SAweight No adaptivity (d) Lid Driven Figure 3: PINN prediction errors (left) and training time cost (right) for the perturbation, Allen–Cahn, Burgers’ equations, and lid-driven flow. Shaded areas denote the mean ± 2 stan- dard deviations, calculated from 5 independent runs for each case. 14 Table 1: The choice of location of the training points and the BRDR training setup for solving different problems with PINNs. Problems Allen–Cahn Perturbation Burgers Lid-Driven PDE points Latin Hypercube Unifrom Latin Hypercube Random IC points Uniform – Uniform – BC points',\n",
       "  'from 5 independent runs for each case. 14 Table 1: The choice of location of the training points and the BRDR training setup for solving different problems with PINNs. Problems Allen–Cahn Perturbation Burgers Lid-Driven PDE points Latin Hypercube Unifrom Latin Hypercube Random IC points Uniform – Uniform – BC points – Uniform Random Uniform Network [21]+[128]×6+[1] mFCN tanh [2]+[128]×6+[1] mFCN tanh [2]+[128]×6+[1] mFCN tanh [2]+[128]×6+[3] mFCN tanh Adam steps 3e5 1e5 4e4 8E4 Adam Learning rate 0.001 × 0.99t/750 0.005 × 0.99t/250 0.001 × 0.99t/100 0.001 × 0.99t/400 (βc, βw,) in BRDR (0.999, 0.999) (0.999, 0.999) (0.999, 0.999) (0.999, 0.999) (p, γ, Ns) in SAAR (0.2, 100, 100) (0.2, 100, 100) (0.2, 100, 100) (0.2, 100, 100) adaptive weighting alone provides little improvement, whereas adaptive sampling significantly enhances training accuracy. The combination of adaptive weighting and sampling yields the best performance. In particular, the combined approach achieves an L2 relative prediction error of 0.01% with 2048 residual points. For comparison, the prediction error reported in Ref. [31] is 0.43%, and the standard PINN produces an error of about 12.56%, both using a much larger batch size of 10,000. Figure 4 shows the pointwise PINN prediction error together with the weight and residual-point distributions obtained from the combined adaptive weighting and sampling strategy with the batch size 2048. The PINN prediction closely matches the exact solution, even within the thin boundary layers, where the absolute pointwise error remains below 10−3. Residual points are more densely concentrated in these regions, demonstrating that adaptive sampling effectively allocates training efforts 15 where they are most needed. Meanwhile, the weight distribution remains relatively uniform across the domain, indicating that adaptive weighting successfully balances the contribution of each training point according to its residual decay rate. This homogeneous pattern confirms that the method mitigates the dominance of slow- converging points and promotes consistent convergence, consistent with the findings of the previous study [27]. 0.0 0.2 0.4 0.6 0.8 1.0 u −0.0010 −0.0005 0.0000 0.0005 0.0010 Exact Prediction error 0.0 0.2 0.4 0.6 0.8 1.0 x 0 5 10 15 weight 0 20 40 60 80 100 120 Point number weight Point number Figure 4: Perturbation problem: (top) PINN prediction errors and exact solution and (bottom) weight distribution and residual point distribution. 4.2. Allen-Cahn equation The Allen-Cahn equation is defined as follows: ∂u ∂t −5(u −u3) −D∂2u ∂x2 = 0, (x, t) ∈[−1, 1] × [0, 1] u(x, 0) = x2 cos(πx), x ∈[−1, 1] u(±1, t) = 0, t ∈[0, 1] (18) where the viscosity D = 1E −4 is considered. As adopted in reference [11, 27], we use a Fourier feature transformation on x to enhance the network model’s ability to approximate periodic functions. While 16 the Allen-Cahn equation does not explicitly have periodic boundary conditions, this transformation helps improve the model’s expressiveness. With 10 Fourier modes, the two-dimensional input x = (x, t) is expanded into a 21-dimensional feature vector bx, which is then fed into the network through the following mapping: bx = γ(x) = [sin(πBx), cos(πBx), t]T , (19) where B = [1, .',\n",
       "  'transformation helps improve the model’s expressiveness. With 10 Fourier modes, the two-dimensional input x = (x, t) is expanded into a 21-dimensional feature vector bx, which is then fed into the network through the following mapping: bx = γ(x) = [sin(πBx), cos(πBx), t]T , (19) where B = [1, . . . , 10]T. Figure 3 illustrates the prediction error from PINN training with and without adaptive weighting and sampling. For this problem, increasing the batch size under adaptive sampling alone yields no significant improvement. In contrast, adaptive weighting leads to a clear reduction in training error as the batch size increases. The combined use of adaptive weighting and sampling consistently achieves the lowest prediction error. Figure 5 shows the weighted density estimation for the Allen–Cahn equation. With adaptive sampling, the training points become more concentrated in large- gradient regions, enabling more effective learning, particularly for smaller batch sizes. In contrast, solely adaptive weighting fails to identify these large-gradient regions when the batch size is small, requiring larger batch sizes to be effective. Moreover, adaptive weighting places greater emphasis on the initial condition, as indicated by the roughly decreasing density along the t dimension for batch sizes n2–n4. While adaptive sampling effectively captures large-gradient regions, the combined strategy blends the strengths of both methods, focusing simultaneously on the initial condition and the large-gradient regions, thereby achieving improved prediction accuracy. 17 x y Exact solution -1.0 -0.8 -0.5 -0.2 0.0 0.2 0.5 0.8 1.0 (n1) 0.1 0.2 0.2 0.3 0.4 0.5 0.6 0.6 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.1 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 (n2) 0.1 0.2 0.2 0.3 0.4 0.5 0.6 0.6 0.1 0.2 0.2 0.3 0.4 0.5 0.6 0.6 0.7 0.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 0.1 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 (n3) 0.2 0.2 0.3 0.4 0.5 0.6 0.6 0.7 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 0.7 0.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 0.1 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 SAweight+SAsample (n4) 0.1 0.2 0.2 0.3 0.4 0.5 0.6 0.6 0.7 SAsample 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 SAweight 0.0 0.2 0.3 0.5 0.6 0.8 0.9 No adaptivity 0.1 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 Figure 5: Weighted density estimation for Allen-Cahn equation. “n1-n4\" denotes the number of training points 2000, 5000, 10000, 25000, respectively. 18 4.3. Burgers equation The Burgers equation is defined as follows: ∂u ∂t + u∂u ∂x −v∂2u ∂x2 = 0, (x, t) ∈[−1, 1] × [0, 1] u(x, 0) = −sin(πx), x ∈[−1, 1] u(±1, t) = 0, t ∈[0, 1] (20) where u is the flow velocity, and the viscosity of the fluid v = 0.01/π is considered. Figure 3 illustrates the prediction accuracy from PINN training with/without adaptive weighting and with/without adaptive sampling. For this problem, no sig- nificant improvements from increasing batch size is gained from solely adaptive sam- pling. On the contrary, the training error',\n",
       "  'the viscosity of the fluid v = 0.01/π is considered. Figure 3 illustrates the prediction accuracy from PINN training with/without adaptive weighting and with/without adaptive sampling. For this problem, no sig- nificant improvements from increasing batch size is gained from solely adaptive sam- pling. On the contrary, the training error of adaptive weighing decreases with the increase of batch size. The combination of adaptive weighting and adaptive sampling consistently achieves the lowest prediction error. Figure 6 presents the weighted density estimation for the Burgers’ equation. For solely adaptive sampling, the density pattern—concentrated near the shock re- gion—remains largely unaffected by batch size. In contrast, under solely adaptive weighting, the density distribution varies considerably with batch size: when the batch size is 2000, the density is concentrated near the initial condition, but as the batch size increases, it gradually shifts toward the shock region. Compared with solely adaptive sampling, the combined approach places greater emphasis on the ini- tial condition, an effect inherited from adaptive weighting, which ultimately leads to improved prediction accuracy. 4.4. Steady lid-driven cavity flow problem In this subsection, the lid-driven cavity flow problem is used to test the per- formance of the proposed MF approach and also the influence of several hyper- 19 x t Exact solution -1.0 -0.8 -0.5 -0.2 0.0 0.2 0.5 0.8 1.0 (n1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 0.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 0.1 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 (n2) 0.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 (n3) 0.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 0.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 0.1 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 SAweight+SAsample (n4) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 SAsample 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 SAweight 0.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 No adaptivity 0.1 0.1 0.2 0.2 0.3 0.4 0.4 0.5 0.5 0.6 Figure 6: Weighted density estimation for Burgers equation. “n1-n4\" denotes the number of training points 2000, 4000, 8000, and 10000, respectively. 20 parameters. The flow enclosed in a square cavity Ω= [0, 1]2 is described by the (non-dimensional) incompressible Navier-Stokes equations \\uf8f1 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f4 \\uf8f3 ∇· u = 0, x ∈Ω u · ∇u = −∇p + 1 Re∇2u, x ∈Ω u(x) = (uw(x), 0), x ∈Γ1 u(x) = 0, x ∈Γ2 , (21) where u = (u, v) is the velocity in the Cartesian coordinate system x = (x, y), p is the pressure and Re is the Reynolds number. The boundary is ∂Ω= Γ1 ∪Γ2, where Γ1 represents the top moving lid and Γ2 represents the other three static non-slip',\n",
       "  ', (21) where u = (u, v) is the velocity in the Cartesian coordinate system x = (x, y), p is the pressure and Re is the Reynolds number. The boundary is ∂Ω= Γ1 ∪Γ2, where Γ1 represents the top moving lid and Γ2 represents the other three static non-slip walls. uw is the driving velocity of the moving lid. To overcome the singularity at the two upper corner points where the moving lid meets the two stationary vertical walls, a zero-corner-velocity profile uw is employed [32]: uw(x) = 16x2(1 −x)2 (22) Figure 3 illustrates the prediction accuracy from PINN training with/without adaptive weighting and with/without adaptive sampling. For this problem, no signif- icant improvements from increasing batch size is gained from solely adaptive weight- ing. On the contrary, the training error of adaptive sampling decreases with the increase of batch size. The combination of adaptive weighting and adaptive sam- pling consistently achieves the lowest prediction error. Figure 7 shows the weighted density estimation for the lid-driven cavity flow prob- lem. Across different batch sizes, the solely adaptive sampling method concentrates on the top-right corner, where the moving lid meets the stationary wall and gener- ates large gradients. In contrast, adaptive weighting places greater emphasis on the two bottom corners, where resolving the secondary vortices is crucial for accurately 21 capturing the flow structures. The combined strategy effectively incorporates both aspects, focusing on all critical regions and thereby improving overall performance. x y Exact solution: u -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0 (n1) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.2 0.5 0.8 1.0 1.2 1.5 1.8 2.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 (n2) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 0.0 0.2 0.5 0.8 1.0 1.2 1.5 1.8 2.0 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 1.5 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 (n3) 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 0.0 0.4 0.8 1.2 1.6 2.0 2.4 2.8 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 1.5 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 SAweight+SAsample (n4) 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 1.4 SAweight 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 SAsample 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 No adaptivity 0.2 0.3 0.5 0.6 0.8 0.9 1.1 1.2 Figure 7: Weighted density estimation for the lid-driven flow. “n1-n4\" denotes the number of training points 512, 1024, 2048, 4096, respectively. 22 5. Conclusion In plain PINNs, the loss function is defined as a linear combination of the squared residuals from the PDEs and boundary conditions evaluated at a set of training points. However, plain PINNs often struggle with problems involving large gradi- ents. Adaptive weighting methods have been shown to mitigate this issue, but their effectiveness is limited, particularly when the training points are insufficient to resolve large-gradient regions. Adaptive sampling offers a natural complement by directing training points toward regions of the domain where the structure of the',\n",
       "  'involving large gradi- ents. Adaptive weighting methods have been shown to mitigate this issue, but their effectiveness is limited, particularly when the training points are insufficient to resolve large-gradient regions. Adaptive sampling offers a natural complement by directing training points toward regions of the domain where the structure of the solution is more difficult to capture accurately. In this work, we propose a self-adaptive sampling method based on residuals as a complement to our previously developed adaptive weighting approach. In the com- bined framework, more training points are allocated to regions with large gradients, while larger weights are assigned to points where residuals decay slowly. The ef- fectiveness of the proposed strategy is evaluated on four benchmark problems. The results show that neither adaptive weighting nor adaptive sampling alone is sufficient to ensure robust performance across all problems. By contrast, their combination consistently yields superior prediction accuracy. Acknowledgments The work is supported by the U.S. Department of Energy, Advanced Scien- tific Computing Research program, under the Scalable, Efficient and Accelerated Causal Reasoning Operators, Graphs and Spikes for Earth and Embedded Systems (SEA-CROGS) project (Project No. 80278). Pacific Northwest National Laboratory (PNNL) is a multi-program national laboratory operated for the U.S. Department 23 of Energy (DOE) by Battelle Memorial Institute under Contract No. DE-AC05- 76RL01830. Appendix A. Network architectures The modified fully-connected network (mFCN) is introduced in the reference[6], and has demonstrated to be more effective than the standard fully-connected neural network. A mFCN maps the input x to the output y. Generally, a mFCN consists of an input layer, L hidden layers and an output layer. The l-th layer has nl neurons, where l = 0, 1, ..L, L + 1 denotes the input layer, first hidden layer,..., L-th hidden layer and the output layer, respectively. Note that the number of neurons of each hidden layer is the same, i.e., n1 = n2 = ... = nL. The forward propagation, i.e. the function y = fθ(x), is defined as follows U = ϕ(WUx + bU) V = ϕ(WV x + bV ) H1 = ϕ(W1x + b1) Zl = ϕ(WlHl−1 + bl), 2 ≤l ≤L Hl = (1 −Zl) ⊙U + Zl ⊙V, 2 ≤l ≤L fθ(x) = WL+1HL + bL+1 , (A.1) where ϕ(•) is a point-wise activation and ⊙denotes point-wise multiplication. The training parameter in the network is θ = {WU, WV , bU, bV , W1:L+1, b1:L+1}. Appendix B. Hyperparameter study for adaptive sampling The adaptive sampling method involves three key hyperparameters: the fraction of points to be updated pu, the clipping parameter γ, and the update frequency Ns. 24 To investigate their influence on training performance, we conduct a hyperparameter study using the Burgers equation as a test case, as described in Section 4.3. To study the effect of each hyperparameter independently, we vary one while keeping the others fixed at pu = 0.2, γ = 100, and Ns = 100. The other training settings are consistent with those in Section 4.3. Note that all the training cases share the same random seed to ensure that the initial',\n",
       "  'of each hyperparameter independently, we vary one while keeping the others fixed at pu = 0.2, γ = 100, and Ns = 100. The other training settings are consistent with those in Section 4.3. Note that all the training cases share the same random seed to ensure that the initial network parameters and the initial set of residual points are identical. The prediction errors as well as the training time cost for different hyperparame- ter values are presented in Fig. B.8. For the update frequency Ns shown in Fig. B.8 (a), the prediction error roughly increases with Ns, while the training time per step decreases. This suggests that more frequent updates of training points are beneficial for improving prediction accuracy, albeit at a modest increase in computational cost. For a compromise between accuracy and efficiency, we select Ns = 100 as the default value. For the clipping parameter γ shown in Fig. B.8 (b), the prediction error exhibits an approximately concave trend, with the lowest error occurring at about γ = 100. This suggests that clipping is necessary to prevent excessive focus on a few points with very large residuals, which can lead to overfitting. However, overly aggressive clipping (i.e., too small γ) will diminish the effectiveness of adaptive sam- pling, making it similar to random sampling. Therefore, we choose γ = 100 as the default value. The training time per step remains relatively constant across differ- ent γ values with only slight decrease with increasing γ. For the fraction of points updated pu shown in Fig. B.8 (c), the prediction error is very large when pu is too small (e.g., pu = 0.05), as it will get close to non-adaptive sampling. As pu increases, it almost remains nearly unchanged with an approximately increasing trend with pu. For a detailed comparison, Fig. B.8 (d) shows the raw and smoothed prediction error histories for three selected pu. The raw error curves are quite noisy, making it diffi- 25 cult to discern clear trends. To enhance visual clarity, we apply a moving minimum filter with a window size of 1500 training steps to smooth the error histories. The smoothed curves reveal that pu = 0.2 and pu = 0.6 yield similar performance, while pu = 1 results in significantly higher errors, especially at the final stage of training. This indicates that updating all points at each update step is not optimal, as it may lead to excessive fluctuations in the training set, hindering convergence especially when the prediction error is already low. The training time per step remains nearly constant across different pu values. Based on these observations, we recommend to use pu ∈[0.2, 0.6] and select pu = 0.2 as the default value. 26 Figure B.8: Hyperparameter study of adaptive sampling for the Burgers equation. The prediction error and training time per step are shown as functions of (a) the update frequency Ns, (b) the clipping parameter γ, and (c) the fraction of points updated pu. Subfigure (d) compares the raw (left) and smoothed (right) prediction error histories for different pu. The smoothing',\n",
       "  'Burgers equation. The prediction error and training time per step are shown as functions of (a) the update frequency Ns, (b) the clipping parameter γ, and (c) the fraction of points updated pu. Subfigure (d) compares the raw (left) and smoothed (right) prediction error histories for different pu. The smoothing is performed using a moving minimum filter with a window size of 1500 training steps to improve visual clarity and facilitate comparison. 27 Appendix C. Point and weight distribution after training The distributions of training points and weights after applying both adaptive sampling and adaptive weighting are illustrated in Figs. C.9, C.10, and C.11 for the Allen–Cahn equation, Burgers equation, and lid-driven cavity flow problem, respec- tively. The results reveal distinct patterns in relation to the exact solutions. For the Allen–Cahn and Burgers equations, the training points are concentrated in regions with large gradients, while for the lid-driven cavity flow, they are concentrated near the walls. In contrast, the weight distributions appear nearly uniform across the domains for all three problems. −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00 2 4 6 8 Figure C.9: Allen-Cahn equation: (left)Exact solution and (right) weight distribution (color) over the residual points (scatter). −1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00 2 4 6 8 10 12 14 16 Figure C.10: Burgers equation: (left)PINN prediction errors and exact solution and (right) weight distribution (color) over the residual points (scatter). 28 Exact solution: p −0.050 −0.025 0.000 0.025 0.050 0.075 0.100 0.125 Exact solution: u −0.4 −0.2 0.0 0.2 0.4 0.6 0.8 1.0 Exact solution: v −0.5 −0.4 −0.3 −0.2 −0.1 0.0 0.1 0.2 0.3 Weight distribution 2 4 6 8 10 Figure C.11: Lid-driven cavity flow: (left)PINN prediction errors and exact solution and (right) weight distribution (color) over the residual points (scatter). References [1] M. Raissi, P. Perdikaris, G. E. Karniadakis, Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations, Journal of Computational physics 378 (2019) 686–707. [2] S. Wang, B. Yue, T. Xu, G. E. Karniadakis, Understanding and mitigating gradient pathologies in physics-informed neural networks, Journal of Computa- tional Physics 423 (2020) 109381. [3] L. Gao, M. Wang, L. Zhang, Adaptive loss balancing for effective physics- informed neural network training, Proceedings of the AAAI Conference on Ar- tificial Intelligence 35 (5) (2021) 456–464. 29 [4] C. L. Wight, J. Zhao, Solving allen-cahn and cahn-hilliard equations using the adaptive physics informed neural networks, arXiv preprint arXiv:2007.04542 (2020). [5] L. McClenny, U. Braga-Neto, Self-adaptive physics-informed neural networks using a soft attention mechanism, arXiv preprint arXiv:2009.04544 (2020). [6] S. Wang, Y. Teng, P. Perdikaris, Understanding and mitigating gradient flow pathologies in physics-informed neural networks, SIAM Journal on Scientific Computing 43 (5) (2021) A3055–A3081. [7] S. Wang, S. Sankaran, P. Perdikaris, Respecting causality for training physics- informed neural networks, Computer Methods in Applied Mechanics and Engi- neering 421 (2024) 116813. [8] D. Liu, Y. Wang, A dual-dimer method for training physics-constrained neural networks with minimax architecture, Neural Networks 136 (2021) 112–125. [9] Y. Song, H. Wang, H. Yang,',\n",
       "  'Sankaran, P. Perdikaris, Respecting causality for training physics- informed neural networks, Computer Methods in Applied Mechanics and Engi- neering 421 (2024) 116813. [8] D. Liu, Y. Wang, A dual-dimer method for training physics-constrained neural networks with minimax architecture, Neural Networks 136 (2021) 112–125. [9] Y. Song, H. Wang, H. Yang, M. L. Taccari, X. Chen, Loss-attentional physics- informed neural networks, Journal of Computational Physics 501 (2024) 112781. [10] G. Zhang, H. Yang, F. Zhu, Y. Chen, et al., Dasa-pinns: Differentiable ad- versarial self-adaptive pointwise weighting scheme for physics-informed neural networks, SSRN (2023). [11] S. J. Anagnostopoulos, J. D. Toscano, N. Stergiopulos, G. E. Karniadakis, Residual-based attention in physics-informed neural networks, Computer Meth- ods in Applied Mechanics and Engineering 421 (2024) 116805. 30 [12] S. Basir, I. Senocak, Physics and equality constrained artificial neural networks: Application to forward and inverse problems with multi-fidelity data fusion, Journal of Computational Physics 463 (2022) 111301. [13] S. Basir, I. Senocak, An adaptive augmented lagrangian method for train- ing physics and equality constrained artificial neural networks, arXiv preprint arXiv:2306.04904 (2023). [14] S. Basir, Investigating and mitigating failure modes in physics-informed neu- ral networks (pinns), Communications in Computational Physics 33 (5) (2023) 1240–1269. [15] H. Son, S. W. Cho, H. J. Hwang, Enhanced physics-informed neural networks with augmented lagrangian relaxation method (al-pinns), Neurocomputing 548 (2023) 126424. [16] S. Wang, X. Yu, P. Perdikaris, When and why pinns fail to train: A neural tan- gent kernel perspective, Journal of Computational Physics 449 (2022) 110768. [17] S. Wang, H. Wang, P. Perdikaris, Improved architectures and training algo- rithms for deep operator networks, Journal of Scientific Computing 92 (2) (2022) 35. [18] A. A. Howard, S. Qadeer, A. W. Engel, A. Tsou, M. Vargas, T. Chiang, P. Stinis, The conjugate kernel for efficient training of physics-informed deep operator networks, in: ICLR 2024 Workshop on AI4DifferentialEquations In Science. [19] L. Lu, X. Meng, Z. Mao, G. E. Karniadakis, Deepxde: A deep learning library for solving differential equations, Journal of Computational Physics 429 (2021) 109926. 31 [20] W. Gao, C. Wang, Active learning based sampling for high-dimensional nonlin- ear partial differential equations, Journal of Computational Physics 475 (2023) 111848. [21] M. A. Nabian, R. J. Gladstone, H. Meidani, Efficient training of physics- informed neural networks via importance sampling, Computer-Aided Civil and Infrastructure Engineering 36 (8) (2021) 962–977. [22] Z. Mao, X. Meng, Physics-informed neural networks with residual/gradient- based adaptive sampling methods for solving partial differential equations with sharp solutions, Applied Mathematics and Mechanics 44 (7) (2023) 1069–1084. [23] K. Tang, X. Wan, C. Yang, Das-pinns: A deep adaptive sampling method for solving high-dimensional partial differential equations, Journal of Computa- tional Physics 476 (2023) 111868. [24] Z. Zhang, J. Li, B. Liu, Annealed adaptive importance sampling method in pinns for solving high dimensional partial differential equations, Journal of Computa- tional Physics 521 (2025) 113561. [25] Y. Liu, L. Chen, J. Ding, Y. Chen, An adaptive sampling method based on expected improvement function and residual gradient in pinns, IEEE Access 12 (2024) 92130–92141. [26] Y. Jiao, D. Li, X. Lu, J. Z. Yang, C. Yuan,',\n",
       "  'differential equations, Journal of Computa- tional Physics 521 (2025) 113561. [25] Y. Liu, L. Chen, J. Ding, Y. Chen, An adaptive sampling method based on expected improvement function and residual gradient in pinns, IEEE Access 12 (2024) 92130–92141. [26] Y. Jiao, D. Li, X. Lu, J. Z. Yang, C. Yuan, A gaussian mixture distribution-based adaptive sampling method for physics-informed neural networks, Engineering Applications of Artificial Intelligence 135 (2024) 108770. [27] W. Chen, A. A. Howard, P. Stinis, Self-adaptive weights based on balanced 32 residual decay rate for physics-informed neural networks and deep operator net- works, Journal of Computational Physics (2025) 114226. [28] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectifiers: Surpassing human- level performance on imagenet classification, in: Proceedings of the IEEE inter- national conference on computer vision, 2015, pp. 1026–1034. [29] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al., Pytorch: An imperative style, high- performance deep learning library, Advances in neural information processing systems 32 (2019). [30] D. W. Scott, Multivariate Density Estimation: Theory, Practice, and Visualiza- tion, John Wiley & Sons, New York, 1992. [31] Z. Fang, S. Wang, P. Perdikaris, Ensemble learning for physics informed neu- ral networks: A gradient boosting approach, arXiv preprint arXiv:2302.13143 (2023). [32] W. Chen, Y. Ju, C. Zhang, A multidomain multigrid pseudospectral method for incompressible flows, Numerical Heat Transfer, Part B: Fundamentals 74 (1) (2018) 415–431. 33'],\n",
       " ['Multimodal Deep Learning for Prediction of Progression-Free Survival in Patients with Neuroendocrine Tumors Undergoing 177Lu-based Peptide Receptor Radionuclide Therapy Simon Baur1, Tristan Ruhwedel2, Ekin B¨oke1, Zuzanna Kobus3,4, Gergana Lishkova5, Christoph Wetz2, Holger Amthauer2, Christoph Roderburg6, Frank Tacke3, Julian M. Rogasch2, Wojciech Samek1,7,8, Henning Jann3, Jackie Ma1, and Johannes Eschrich3,9 1 Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany 2 Department of Nuclear Medicine, Charit´e—Universit¨atsmedizin Berlin 3 Department of Hepatology and Gastroenterology, Charit´e—Universit¨atsmedizin Berlin 4 Division of Interventional Radiology, Department of Radiology, Memorial Sloan Kettering Cancer Center 5 Department of Endocrinology and Metabolism, Charit´e—Universit¨atsmedizin Berlin 6 Clinic for Gastroenterology, Hepatology and Infectious Diseases, University Hospital D¨usseldorf, Medical Faculty of Heinrich Heine University D¨usseldorf 7 BIFOLD −Berlin Institute for the Foundations of Learning and Data 8 Department of Electrical Engineering and Computer Science, Technische Universit¨at Berlin 9 Berlin Institute of Health at Charit´e – Universit¨atsmedizin Berlin Abstract. Peptide receptor radionuclide therapy (PRRT) is an estab- lished treatment for metastatic neuroendocrine tumors (NETs), yet long- term disease control occurs only in a subset of patients. Predicting progression- free survival (PFS) could support individualized treatment planning. This study evaluates laboratory, imaging, and multimodal deep learn- ing models for PFS prediction in PRRT-treated patients. Methods In this retrospective, single-center study 116 patients with metastatic NETs undergoing [177Lu]Lu-DOTATOC were included. Clin- ical characteristics, laboratory values, and pretherapeutic somatostatin receptor positron emission tomography/computed tomographies (SR- PET/CT) were collected. Seven models were trained to classify low- vs. high-PFS groups, including unimodal (laboratory, SR-PET, or CT) and multimodal fusion approaches. Performance was assessed via repeated 3- fold cross-validation with area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC). Ex- plainability was evaluated by feature importance analysis and gradient maps. Results Forty-two patients (36%) had short PFS (≤1 year), 74 patients long PFS (>1 year). Groups were similar in most characteristics, except arXiv:2511.05169v1 [cs.LG] 7 Nov 2025 2 Baur, Eschrich et al. for higher baseline chromogranin A (p = 0.003), elevated γ-GT (p = 0.002), and fewer PRRT cycles (p < 0.001) in short-PFS patients. The Random Forest model trained only on laboratory biomarkers reached an AUROC of 0.59±0.02. Unimodal three-dimensional convolutional neural networks using SR-PET or CT performed worse (AUROC 0.42±0.03 and 0.54 ± 0.01, respectively). A multimodal fusion model laboratory values, SR-PET, and CT -augmented with a pretrained CT branch - achieved the best results (AUROC 0.72 ± 0.01, AUPRC 0.80 ± 0.01). Conclusion Multimodal deep learning combining SR-PET, CT, and laboratory biomarkers outperformed unimodal approaches for PFS pre- diction after PRRT. Upon external validation, such models may support risk-adapted follow-up strategies. 1 Introduction Neuroendocrine tumors (NETs) arise from neuroendocrine cells and represent a heterogeneous group of neoplasms with variable biological behavior and clinical presentation [1]. Although classified as rare, the reported incidence of NETs has been steadily increasing over recent decades [2]. Most frequently, NETs originate in the gastrointestinal tract or pancreas, collectively referred to as gastroen- teropancreatic neuroendocrine tumors (GEP-NETs) [3]. In a subset of patients, the primary tumor site remains unknown despite extensive diagnostic work-up, referred to as',\n",
       "  'reported incidence of NETs has been steadily increasing over recent decades [2]. Most frequently, NETs originate in the gastrointestinal tract or pancreas, collectively referred to as gastroen- teropancreatic neuroendocrine tumors (GEP-NETs) [3]. In a subset of patients, the primary tumor site remains unknown despite extensive diagnostic work-up, referred to as NETs of unknown primary (CUP-NETs) [4]. For patients with ad- vanced disease, available treatment options are limited and include somatostatin analogues, targeted therapies, chemotherapy, and peptide receptor radionuclide therapy (PRRT). PRRT with [177Lu]Lu-DOTATATE or [177Lu]Lu-DOTATOC has emerged as an effective treatment strategy for patients with metastatic NETs that express high levels of somatostatin receptors [5]. Clinical trials have demon- strated that 177Lu-based PRRT significantly prolongs progression-free survival (PFS) [6]. More recently, the NETTER-2 trial evaluated [177Lu]Lu-DOTATATE as a first-line therapy for patients with advanced grade 2 and 3 gastroenteropan- creatic (GEP) NETs, demonstrating encouraging outcomes that support its ex- panded role in earlier lines of treatment [7]. Nonetheless, meta-analyses have demonstrated that PRRT achieves objective response rates in patients with ad- vanced NETs ranging between 25.0% and 35.0%, depending on the response assessment criteria applied, and disease control rates between 79.0% and 83.0% [8]. Identifying patients who will not achieve long-term disease control or re- mission in advance is a clinical need and defines the rationale of the present study. As of today, histological Ki-67 proliferation index and serum chromo- granin A (CgA) remain the most established prognostic biomarkers in patients undergoing PRRT. Elevated Ki-67 and CgA levels have been consistently associ- ated with shorter PFS [9]. The multigene transcriptomic assay NETest has been proposed as a predictive tool for PRRT outcomes, showing promising initial results, however, its high cost and limited availability currently restrict clini- cal implementation [10]. Recent work by Ruhwedel et al. identified the De Ri- tis ratio (AST/ALT) as a prognostic biomarker in patients undergoing PRRT, Multimodal Deep Learning for PFS prediction in NET Patients 3 with elevated values associated with shorter progression-free survival and overall survival [11,12].Imaging-derived parameters have also been proposed to predict PRRT outcomes. Somatostatin receptor (SR) heterogeneity, high lesional SR ex- pression - as assessed by the Krenning score - and the metastases-to-liver ratio (M/L ratio) have been investigated as potential predictors of therapy response [13,14]. Furthermore, radiomic signatures extracted from baseline somatostatin receptor PET (SR-PET) or CT imaging have demonstrated potential for strat- ifying patients [15]. However, the predictive value of these parameters remains limited, thus restricting their clinical applicability. In recent years, artificial in- telligence (AI) – and particularly deep learning (DL) – has emerged as a po- tent tool to extract high-dimensional patterns from heterogeneous biomedical data, including imaging, genomics, and clinical variables. In oncology, multi- modal DL approaches leverage specialized architectures that process each data type in dedicated branches before combining them into a joint representation for prediction. Typically, convolutional neural networks are employed for im- age analysis, while feed-forward networks handle structured clinical data, and sequence models such as recurrent or transformer architectures are used for ge- nomic or temporal inputs. These modality-specific encoders are integrated via fusion strategies',\n",
       "  'combining them into a joint representation for prediction. Typically, convolutional neural networks are employed for im- age analysis, while feed-forward networks handle structured clinical data, and sequence models such as recurrent or transformer architectures are used for ge- nomic or temporal inputs. These modality-specific encoders are integrated via fusion strategies ranging from simple concatenation to attention-based trans- formers enabling end-to-end optimization across all modalities [16,17]. Recent reviews highlight that such architectures can capture complementary informa- tion, mitigate modality specific biases, and improve generalization across diverse patient populations [16,18]. This paradigm is particularly relevant for PRRT, where reliable biomarkers for predicting durable response remain scarce. Apply- ing multimodal AI frameworks to NETs could therefore facilitate more accurate patient selection and personalized therapeutic strategies. 2 Material and Methods 2.1 Patient Cohort This retrospective, single-center study included 116 consecutive patients with histologically confirmed NETs who received PRRT with [177Lu]Lu-DOTATOC between 2015 and 2022 at Charit´e—Universit¨atsmedizin Berlin. Eligibility cri- teria comprised: (1) metastatic, progressive disease; (2) sufficient SR expression confirmed by pretherapeutic [68Ga]Ga-DOTATOC PET/CT; (3) availability of both pretherapeutic laboratory data and imaging (SR-PET and CT scan); and (4) availability of clinical follow-up data. Baseline laboratory values had to in- clude liver function parameters - namely aspartate transaminase (AST), alanine transaminase (ALT), and gamma-glutamyl transferase (GGT) - as well as the neuroendocrine tumor marker chromogranin A (CgA), all measured within four weeks before initiation of PRRT. Patients were excluded if they had undergone prior PRRT, had incomplete clinical records, or insufficient follow-up to assess disease progression. A majority of the patient cohort analyzed in this study was previously included in earlier publications [11,19]. The present study comprises 4 Baur, Eschrich et al. additional, more recently treated patients. Furthermore, it differs methodologi- cally by employing a multimodal deep learning framework that integrates labo- ratory values and imaging data (SR-PET/CT) for predictive modeling of PFS. Thus, while the patient cohort overlaps with previous studies, the methodolog- ical approach of the current work is distinct. Table 1 illustrates all patient characteristics. 2.2 Imaging Characteristics Prior to PRRT initiation, all patients underwent a pretherapeutic [68Ga]Ga- DOTA-based PET. The median interval between the pretherapeutic SR-PET and initiation of the first PRRT cycle was 36 days (IQR 44; Q1 15.5 – Q3 59.5 days). PET/CT examinations were performed in our center with either a Philips Gemini TF 16 scanner with time-of-flight capability and a 16-row CT scanner [20] or a GE Discovery MI scanner with silicon photomultipliers and time-of- flight capability and a 64-row CT scanner [21]. The CT scans included in our model were exclusively those acquired simultaneously with the pretherapeutic SR-PET to guarantee spatial and temporal alignment between anatomical and functional imaging data. Among the 116 patients included, 74 underwent whole- body contrast-enhanced CT, while the remaining patients received whole-body non-contrast CT. The contrast-enhanced CT images were acquired during the venous contrast phase with a slice thickness of 3 mm. We deliberately used whole- body CTs to ensure that all lesions detected on SR-PET could be anatomically correlated with the corresponding CT scan across the entire field of view. 2.3',\n",
       "  'non-contrast CT. The contrast-enhanced CT images were acquired during the venous contrast phase with a slice thickness of 3 mm. We deliberately used whole- body CTs to ensure that all lesions detected on SR-PET could be anatomically correlated with the corresponding CT scan across the entire field of view. 2.3 Peptide Receptor Radionuclide Therapy and Response Assessment Patients received [177Lu]Lu-DOTATOC PRRT with a median of 3 cycles (range: 1–7), each administered at a standard dose of 200 mCi (7.40 GBq). Treatment cycles were scheduled at intervals of 10 to 12 weeks. Interim response assessment was performed using [68Ga]Ga-DOTATOC PET/CT after every two cycles, with the first evaluation following the second treatment cycle. To minimize the risk of misinterpreting radiogenic edema as disease progression (pseudo-progression), in- terim staging was conducted at least two months after the most recent PRRT cy- cle [22]. Disease progression was determined by an interdisciplinary tumor board. In patients showing progressive disease, no additional PRRT cycles were admin- istered. Following completion of therapy, patients underwent routine follow-up imaging every 3 to 6 months. Morphological evaluation was primarily based on CT. 2.4 Progression-Free Survival PFS was defined as the time from the initiation of the first PRRT cycle until the date of documented disease progression or death from any cause. Disease Multimodal Deep Learning for PFS prediction in NET Patients 5 progression was assessed according to RECIST 1.1 criteria as determined by the local interdisciplinary tumor board. Patients without a progression event were not included in the analysis. For the purpose of this study, we defined the PFS threshold at 1 year, as progression within the first year after PRRT initiation is generally considered to indicate insufficient therapeutic benefit and has been used previously in clinical studies [23]. This threshold was applied to dichotomize patients into low-PFS (≤1 year) and high-PFS (> 1 year) groups for subsequent analyses. 2.5 Deep Learning Models We evaluated seven predictive models for PFS classification, exploring the pre- dictive value of laboratory parameters, imaging, and multimodal data. First we trained a Random Forest classifier using only laboratory biomarkers. The lab- oratory biomarkers included are AST, ALT, CgA, and GGT. Subsequently, we developed two separate 3D convolutional neural networks (3D-CNNs) to pre- dict PFS based on PET and CT imaging data, respectively. To investigate the benefit of multimodal integration, we constructed fusion models that combined (1) PET imaging with laboratory biomarkers, (2) CT imaging with laboratory biomarkers, and (3) PET and CT imaging with laboratory biomarkers. Finally, we assessed an advanced fusion model that integrates PET, CT, and labora- tory data, where the CT branch was initialized with a pretrained MONAI 3D CT segmentation model [24], that we finetuned rather than being trained from scratch. Laboratory biomarkers were fused via concatenation to a flattened vec- tor of image embeddings, and passed to 3 layers of MLPs. An overview of all our evaluated models is given in Figure 1. All models were trained using a learn- ing rate of 0.01. We applied a dropout rate of 0.1. Changing dropout to higher rates did not significantly change results.',\n",
       "  'of image embeddings, and passed to 3 layers of MLPs. An overview of all our evaluated models is given in Figure 1. All models were trained using a learn- ing rate of 0.01. We applied a dropout rate of 0.1. Changing dropout to higher rates did not significantly change results. To prevent overfitting and employed the ADAM optimizer with a weight decay of 0.2. Again, different weight decay did not significantly influence results. We used Binary Cross Entropy Loss for all models. All imaging data underwent preprocessing prior to model training and statistical analysis. Visual artifacts were removed, voxel slopes were harmonized using dicom metadata, and finally the 3D volumes of all scans were normalized across training data. Each scan was subsequently resized to 75 × 50 × 50 voxels. As our architectural design is fairly simple, our models are easily reproducable, given access to the data. 2.6 Statistical Analysis To make sure our evaluation was reliable and not dependent on one specific data split, we used a repeated cross-validation approach. Therefore, we divided the dataset into three parts of equal size (3-fold cross-validation). In each round, two parts were used to train the model and the remaining part was used to test it. This rotation continued until each part had served once as the test set. We then repeated this entire 3-fold process five times, each time reshuffling the data. 6 Baur, Eschrich et al. PET CT Laboratory values Input Setup Description ✓ Laboratory values only ✓ PET only ✓ CT only ✓ ✓ PET Fusion ✓ ✓ CT Fusion ✓ ✓ ✓ PET CT Fusion ✓ ✓ ✓ PET CT Fusion (pretrained CT) Fig. 1: (Top) Overview of the proposed deep learning pipeline for PRRT response prediction. The model integrates 3D PET and 3D CT scans processed through separate 3D convolutional networks, along with laboratory biomarkers, via a concatenation-based fusion layer. The fused features are passed through fully connected layers to generate the prediction output. Interpretability is provided through backward analyses, including gradient maps, feature importance, and UMAP-based feature space analysis. Solid arrows represent forward pass data flow, dashed arrows backward pass dervied post hoc outputs. (Bottom) Overview of input modality combinations evaluated in our experiments. This repetition reduced the chance that the results were influenced by a partic- ular way of splitting the data, giving us a more robust estimate of performance. Finally, we reported the average performance across all 5 repetitions, along with the standard error to indicate how much the results varied. Performance was measured using two widely applied metrics: the area under the receiver oper- ating characteristic curve (AUROC), and the area under the precision–recall curve (AUPRC). To assess whether the observed performance differences be- tween model families were statistically significant, we conducted nonparametric significance testing. Each model was assigned to one of three predefined groups Multimodal Deep Learning for PFS prediction in NET Patients 7 based on its input modality: unimodal (PET-only, CT-only, or Random Forest on tabular data), one-image fusion (PET or CT combined with clinical variables), and dual fusion (both PET and CT',\n",
       "  'Each model was assigned to one of three predefined groups Multimodal Deep Learning for PFS prediction in NET Patients 7 based on its input modality: unimodal (PET-only, CT-only, or Random Forest on tabular data), one-image fusion (PET or CT combined with clinical variables), and dual fusion (both PET and CT combined, with or without pretraining). We first compared one-image fusion models against unimodal models, and then dual- fusion models against one-image fusion models. For each comparison, we used the Mann–Whitney U test (two-sided) to evaluate whether one group achieved higher performance than the other. To control for multiple testing, p-values were adjusted using the Bonferroni correction. In addition, we quantified the effect size with Cliff’s delta, which indicates how strongly one group tends to outper- form the other. According to conventional interpretation, values of |δ| < 0.147, < 0.33, < 0.474, and ≥0.474 correspond to negligible, small, medium, and large effects, respectively. 2.7 Model Analysis and Explainability To elucidate the decision-making process of our deep learning framework, we performed both representation space analysis and biomarker relevance assess- ment as well as qualitative explainability through saliency maps. The aim was to provide transparency on how the model integrates multimodal information— namely, PET imaging and laboratory biomarkers—to arrive at its predictions. Umap Analysis of Feature Embeddings Uniform Manifold Approximation and Projection (UMAP)[25] was employed to visualize the high-dimensional fea- ture embeddings learned by the network. UMAP is a non-linear dimensionality reduction technique that projects complex feature spaces into two dimensions while preserving both local and global structural relationships. This makes it particularly suitable for identifying separable patient subgroups in latent space. Feature Importance Analysis of Laboratory Biomarkers Feature im- portance analysis was performed to quantify the relative contribution of each laboratory biomarker to the predictive performance of different fusion strate- gies. For this, we evaluated the influence of gradients of laboratory values in the fusion layer on the model predictions. Qualitative Explainability Gradient-based saliency maps were computed to localize regions within the PET scans that most strongly influenced the model’s classification decisions. For each patient, voxel-wise gradients from the PET Fu- sion and PET Only models were backpropagated and mapped to the input PET volume to generate saliency heatmaps. These were overlaid on the original scans in three anatomical planes (axial, coronal, and sagittal) to visually highlight spatial patterns of model attention. 8 Baur, Eschrich et al. 3 Results 3.1 Clinical Characteristics A total of 116 patients were included in the final study cohort with a median age of 66 years (range: 36–87). 41% of patients were female. The most common primary sites were the small intestine (42%) and pancreas (29%), with 17% of patients having a cancer of unknown primary (CUP). Most patients presented with hepatic metastases (73%), frequently accompanied by lymphonodal or os- seous spread. The majority of tumors were G2 (71%) with a median Ki-67 index of 5% (range: 1–40). Patients received a median of 3 PRRT cycles (range: 1–7). Key baseline laboratory values, including chromogranin A (CgA) and γ-GT, are summarized in Table 1. 3.2 Progression Free-Survival To evaluate treatment',\n",
       "  'os- seous spread. The majority of tumors were G2 (71%) with a median Ki-67 index of 5% (range: 1–40). Patients received a median of 3 PRRT cycles (range: 1–7). Key baseline laboratory values, including chromogranin A (CgA) and γ-GT, are summarized in Table 1. 3.2 Progression Free-Survival To evaluate treatment outcomes in the study cohort, we first analyzed the PFS. Figure 2 illustrates the PFS distribution of the patient cohort. The median PFS for the total cohort was 15.7 months (interquartile range [IQR]: 9.1–26.7 months), indicating that half of the patients remained progression-free for at least this duration. No censored patients were included, as target values are necessary for a sample to be used in our deep learning setup. Our cohort in- cludes only patients that eventually had a progress. Patients were stratified into short-PFS (≤1 year) and long-PFS (> 1 year) group. When comparing these two subgroups most clinical characteristics did not differ significantly between the groups, including age, sex distribution, primary tumor location, metastatic pattern, tumor functionality, and histological grading (all p > 0.05). Notably, baseline chromogranin A (CgA) level was significantly higher in patients with shorter PFS (p = 0.003), and elevated γ-GT levels (p = 0.02). In addition, pa- tients with early progression had received fewer PRRT cycles (p < 0.01), which is expected, as treatment is typically discontinued in the event of disease pro- gression prior to completion of the planned cycles. 3.3 Deep Learning Predictive Model for Progression-Free Survival We applied a series of unimodal and multimodal deep learning architectures to assess the added value of integrating imaging and laboratory data for predicting PFS. Our results demonstrate that integrating multiple data modalities consis- tently improves model performance in progression-free survival (PFS) classifica- tion (Table 2). The baseline Random Forest model, trained solely on laboratory biomarkers, achieved moderate performance (AUROC: 0.59 ± 0.02, AUPRC: 0.67 ± 0.01, Accuracy: 0.61 ± 0.02). In contrast, unimodal 3D convolutional neural networks trained on PET or CT data alone yielded lower discrimina- tive performance, particularly for the PET-only model (AUROC: 0.42 ± 0.03). The CT-only model performed slightly better (AUROC: 0.54 ± 0.01), though Multimodal Deep Learning for PFS prediction in NET Patients 9 Metric Total PFS ≤1 year PFS > 1 year p-value Patient Statistics Patient Count 116 (100%) 42 (36%) 74 (64%) Age in years 66 (36–87) 66 (36–87) 66 (36–80) 0.945 Male 68 (59%) 23 (55%) 45 (61%) 0.560 Female 48 (41%) 19 (45%) 29 (39%) 0.560 Primary Location Small intestine 49 (42%) 19 (45%) 30 (41%) 0.697 Pancreas 34 (29%) 8 (19%) 26 (35%) 0.090 Colon/Rectum 12 (10%) 4 (10%) 8 (11%) 1.000 Stomach 1 (1%) 0 (0%) 1 (1%) 1.000 CUP 20 (17%) 11 (26%) 9 (12%) 0.073 Metastatic Spread Hepatic 85 (73%) 28 (67%) 57 (77%) 0.276 Lymphonodal 75 (65%) 26 (62%) 49 (66%) 0.689 Osseous 35 (30%) 12 (29%) 23 (31%) 0.836 Peritoneal 19 (16%) 6 (14%) 13 (18%) 0.796 Pulmonal 5 (4%) 1 (2%) 4 (5%) 0.652 Functionality Yes 40 (34%) 19 (45%) 21 (28%) 0.072 No 75 (65%) 22 (52%) 53 (72%)',\n",
       "  '(77%) 0.276 Lymphonodal 75 (65%) 26 (62%) 49 (66%) 0.689 Osseous 35 (30%) 12 (29%) 23 (31%) 0.836 Peritoneal 19 (16%) 6 (14%) 13 (18%) 0.796 Pulmonal 5 (4%) 1 (2%) 4 (5%) 0.652 Functionality Yes 40 (34%) 19 (45%) 21 (28%) 0.072 No 75 (65%) 22 (52%) 53 (72%) 0.045 Unknown 1 (1%) 1 (2%) 0 (0%) 0.362 Grading G1 23 (20%) 8 (19%) 15 (20%) 1.000 G2 82 (71%) 29 (69%) 53 (72%) 0.833 G3 6 (5%) 2 (5%) 4 (5%) 1.000 Unknown 5 (4%) 3 (7%) 2 (3%) 0.351 Ki67-Index % 5 (1–40) 5 (1–25) 5 (1–40) 0.501 Laboratory Parameters CgA in µg/l 419 (24–99590) 821 (25–99590) 262 (24–15100) 0.001 AST in U/l 28 (14–139) 32 (14–123) 28 (14–139) 0.123 ALT in U/l 28 (7–132) 27 (7–96) 28 (10–132) 0.774 γ-GT in U/l 61 (9–691) 95 (21–688) 50 (9–691) 0.014 De Ritis ratio 1.12 (0.46–3.43) 1.16 (0.46–3.43) 1.07 (0.53–2.87) 0.223 PRRT Cycles 4 (1–7) 2 (1–4) 4 (1–7) < 0.001 Table 1: Summary of patient characteristics stratified by PFS. Categorical vari- ables were compared with Fisher’s exact test; continuous variables with the Wilcoxon rank-sum (Mann–Whitney) test. Values are counts with percentages, or median (min–max). “Functionality” refers to the presence of clinically rel- evant hormone secretion by NETs. Parts of the presented cohort overlap with previously published studies [11,19]. See the Methods section for further details. 10 Baur, Eschrich et al. Fig. 2: Kaplan-Meier curve for progression-free survival (PFS) in the total study cohort (n = 116) of patients with neuroendocrine tumors treated with [177Lu]Lu- DOTATOC PRRT. Vertical dashed red line indicates our split into high and low therapy response. No censored patients are included, and all patients eventually had progress. both remained inferior to the model using only laboratory parameters. Intro- ducing laboratory features into imaging-based models led to improvements: the PET-laboratory fusion model achieved an AUROC of 0.68 ± 0.01 and the high- est accuracy overall (0.65 ± 0.01), suggesting strong complementarity between PET imaging and laboratory data. Similarly, the CT-laboratory fusion model improved over the CT-only model across all metrics, reaching an AUROC of 0.62 ± 0.03. Further combining all three modalities—PET, CT, and laboratory data—resulted in additional performance gains. The PET-CT-laboratory fusion model achieved an AUROC of 0.69 ± 0.01 and matched the highest AUPRC score (0.80 ± 0.01), reinforcing the value of multimodal integration. Finally, ini- tializing the CT branch with a pretrained model further boosted the AUROC to 0.72±0.01, indicating that leveraging pretrained representations can enhance predictive performance. Figure 3 gives a visual overview of predictive perfor- mance of all models. For additional insights into predictive performance, Figure 4 displays ROC and Precision-Recall curves of a single exemplary cross vali- dation fold for our PET CT Fusion model. We can clearly see the improved predictive performance compared to the Random Forest model and a random baseline. Statistical testing applied as described in 2.6 suggests that adding it- eratively more modalities improves predictive model performance significantly (p < 0.01). To further explore the clinical relevance of our model predictions, we performed a Kaplan–Meier analysis stratified',\n",
       "  'predictive performance compared to the Random Forest model and a random baseline. Statistical testing applied as described in 2.6 suggests that adding it- eratively more modalities improves predictive model performance significantly (p < 0.01). To further explore the clinical relevance of our model predictions, we performed a Kaplan–Meier analysis stratified by the model-derived probability of long Multimodal Deep Learning for PFS prediction in NET Patients 11 progression-free survival (PFS) (see Figure 5. The plot shows a representative single cross validation of the PET Fusion model. Patients predicted by our model to have a high therapy response (probability ≥0.5) demonstrated a markedly prolonged PFS compared with those in the low-response group (probability ¡ 0.5).The median PFS was 17.25 months in the high-response group versus 12.43 months in the low-response group, corresponding to a statistically significant difference (log-rank p = 0.0001; test statistic 15.18). As all patients in our co- hort eventually experienced progression, no censoring occurred, and the survival curves therefore display the proportion of patients who had progressed at a given time point. These findings indicate that our multimodal model is capable of clin- ically meaningful risk stratification, separating patients into distinct prognostic groups based solely on baseline data prior to therapy initiation. Fig. 3: Barplot comparison of AUROC and AUPRC across models. 3.4 Results Model Analysis and Explainability UMAP Analysis Figure 6 displays the UMAP projections of embeddings de- rived from different fusion strategies, PET imaging combined with laboratory biomarkers (PET Fusion), CT imaging combined with laboratory biomarkers (CT Fusion), joint PET and CT imaging fused with laboratory biomarkers (PET-CT Fusion), and the same PET-CT fusion model with the CT branch ini- tialized from a pretrained network (PET-CT Fusion, Pretrained). PET-derived embeddings alone show weak class separation. In contrast, the fused PET-CT embeddings—particularly when incorporating pretrained CT features—exhibit markedly improved clustering, with tighter intra-class grouping and greater inter-class separation. This suggests multimodal fusion enriches the feature space, enabling the model to capture subtle features associated with PRRT response. 12 Baur, Eschrich et al. Model AUROC AUPRC RF (Laboratory values only) 0.59 ± 0.02 0.67 ± 0.01 PET only 0.42 ± 0.03 0.58 ± 0.03 CT only 0.54 ± 0.01 0.57 ± 0.03 PET Fusion 0.68 ± 0.01∗ 0.80 ± 0.01∗ CT Fusion 0.62 ± 0.03∗ 0.72 ± 0.04∗ PET CT Fusion 0.69 ± 0.01† 0.80 ± 0.01 PET CT Fusion (pretrained CT) 0.72 ± 0.01† 0.80 ± 0.02 Table 2: Performance metrics (AUROC and AUPRC) for all models. Values are reported as mean ± standard error. Bolded values indicate the best performance in each metric. Significance markers denote statistical improvement over the next lower model family: ∗p < 0.01 (vs. unimodal); † p < 0.01 (vs. one-image fusion). All significant differences correspond to large effect sizes (Cliff’s δ > 0.8). Fig. 4: Comparison of predictive performance between the Random Forest base- line (laboratory values only) model and the PET CT Fusion model. The plot is showing the example of a single representative cv fold. Left: ROC curves showing True Positive Rate versus False Positive Rate; the dashed gray line represents a',\n",
       "  '4: Comparison of predictive performance between the Random Forest base- line (laboratory values only) model and the PET CT Fusion model. The plot is showing the example of a single representative cv fold. Left: ROC curves showing True Positive Rate versus False Positive Rate; the dashed gray line represents a random baseline. Right: Precision-Recall curves illustrating the trade-off between precision and recall; the dashed gray line indicates the all-positive baseline. The PET CT Fusion model consistently outperforms the Random Forest baseline, as reflected in higher AUROC and AUPRC values. For cross-validation metrics, refer to Table 2. Feature Importance Our evaluation of feature importance of laboratory val- ues is displayed in Figure 7. Across all model configurations, ALT, AST, CgA, and Gamma-GT consistently emerged as key discriminative variables. Notably, CgA exhibited the highest importance in both PET-CT fusion approaches, high- lighting its strong association with the target outcome when combined with Multimodal Deep Learning for PFS prediction in NET Patients 13 Fig. 5: Kaplan-Meier Curve of study cohort, stratified by our model prediction output probabilities ˆy (low PFS: ˆy < 0.5, high PFS: ˆy >= 0.5). Note that as we did not include censored patients, and all patients in our cohort eventually had progression, the y axis represents the proportion of progression patients at a given time. Log-Rank Test: p = 0.0001. imaging- related features. The pretrained PET-CT fusion model generally pre- served or enhanced biomarker relevance compared to the non-pretrained variant, suggesting that the integration of well-learned CT representations can strengthen the interpretive value of specific laboratory measures. The Random Forest base- line, while ranking the same biomarkers highly, demonstrated lower absolute importance values, underscoring the advantage of deep multimodal learning in capturing non-linear relationships between biochemical and imaging features. Qualitative Explainability Figures 8 and 9 show the results of our qualitative explainability analysis. Firstly, we analyzed the global distributions of gradient magnitudes across all test sets of an entire cross validation (Figure 8 (a)) for PET Only and PET Fusion models. The PET Only model exhibits an irregular, noisy distribution with substantial density fluctuations across the gradient range. In contrast, PET Fusion displays a smooth, unimodal distribution centered around moderate gradient magnitudes. Notably, PET Only shows a pronounced shift toward higher gradient values (spanning the full [0, 1] range with significant density beyond 0.6), which—coupled with the model’s poor generalization per- formance—is indicative of exploding gradients, a well-known phenomenon in deep learning that can hinder stable learning and meaningful representation formation [26,27,28,29]. Quantitative comparisons using multiple statistical dis- tance metrics confirmed substantial divergence between the distributions of the two models: Wasserstein Distance (0.090), Kolmogorov–Smirnov Statistic (0.241, 14 Baur, Eschrich et al. Fig. 6: U-MAP projection of learned feature embeddings from different fusion strategies. Row 1: Embeddings derived from PET imaging alone. Embeddings are mixed up between classes, therefore PET imaging alone is not capable of good discrimination. Row 2: Embeddings derived from CT imaging alone. Sim- ilar to PET imaging only, embeddings are scattered with no clear distinction. Notably, the pretrained CT model displays better clustered embeddings, due to prior exposure',\n",
       "  'Embeddings are mixed up between classes, therefore PET imaging alone is not capable of good discrimination. Row 2: Embeddings derived from CT imaging alone. Sim- ilar to PET imaging only, embeddings are scattered with no clear distinction. Notably, the pretrained CT model displays better clustered embeddings, due to prior exposure to CT imaging. Row 3: Fusion embeddings combining PET, CT, and laboratory biomarkers reveal markedly improved class separation, with tighter intra-class clustering and clearer inter-class boundaries. This illustrates the synergistic effect of multimodal integration in capturing disease-related vari- ation that is not apparent in single-modality embeddings. p < 0.001), Jensen–Shannon Divergence (0.418), Energy Distance (0.183), Bhat- tacharyya Distance (0.207), and Histogram Overlap (0.518) (Figure 8(b)). We further illustrate these differences at the individual-case level. An example of a raw PET scan is shown in Figure 9 (a). Figure 9 (b) and (c) compare the corre- sponding saliency maps of the PET Only and PET Fusion models for the same patient. Brighter colors indicate voxels with stronger contributions to the model’s prediction. The PET Fusion model predominantly focuses on relevant tumorous regions, while the PET Only model assigns high importance to the bladder. In addition to visual inspection, we compared the gradient magnitude distributions of both models (Figure 9 (d) and (e)). The PET Only model exhibits numerous Multimodal Deep Learning for PFS prediction in NET Patients 15 Fig. 7: Feature importance of selected laboratory biomarkers across different fu- sion strategies and a Random Forest baseline. Importance values were computed using a permutation-based approach, with higher values indicating stronger con- tribution to model predictions. Four biomarkers—ALT, AST, CgA, and Gamma- GT—consistently ranked among the most relevant features across all model con- figurations. CgA exhibited the highest importance in both PET-CT fusion mod- els, suggesting a strong association with the target outcome when combined with imaging-derived features. large gradients and an irregular, fragmented distribution, whereas the PET Fu- sion model produces a smoother, more coherent gradient distribution with fewer extreme values. In total, our gradient analyses support our earlier findings from model performance (Table 2) and internal feature representations (Figure 6): the PET Fusion model learns more physiologically meaningful signal patterns associated with PRRT effectiveness, while the PET Only model fails to capture relevant information. The contrasting distributional shapes underscore funda- mental differences in training stability: PET Fusion’s concentrated, bell-shaped profile reflects well-regulated gradient flow, while PET Only’s diffuse, erratic pattern signals optimization instability. 4 Discussion In this study, we developed and evaluated a multimodal deep learning model integrating somatostatin receptor PET, CT imaging, and laboratory biomarkers to predict progression-free survival in patients undergoing [177Lu]Lu-DOTATOC PRRT. Our results show that unimodal imaging models alone — whether based on SR-PET or CT — were insufficient to provide clinically meaningful predictive performance and, in fact, performed worse than the model using only laboratory data. In contrast, combining complementary imaging modalities with laboratory biomarkers in a fusion architecture substantially enhanced predictive accuracy and robustness. Importantly, we incorporated explainability into the model by leveraging three-dimensional gradient maps and biomarker relevance analyses, 16 Baur, Eschrich et al. (a) Comparison',\n",
       "  'worse than the model using only laboratory data. In contrast, combining complementary imaging modalities with laboratory biomarkers in a fusion architecture substantially enhanced predictive accuracy and robustness. Importantly, we incorporated explainability into the model by leveraging three-dimensional gradient maps and biomarker relevance analyses, 16 Baur, Eschrich et al. (a) Comparison of gradient magnitude distributions between PET Only and PET Fu- sion models. Metric Value Range & Interpretation Wasserstein Distance 0.090 [0, ∞) ↑(9% of range) KS Statistic (p < 0.001) 0.241 [0, 1] ↑(moderate) Jensen–Shannon Divergence 0.418 [0, 1] ↑(moderate) Energy Distance 0.183 [0, ∞) ↑(18% of range) Bhattacharyya Distance 0.207 [0, ∞) ↑(moderate) Histogram Overlap 0.518 [0, 1] ↓(52% overlap) (b) Quantitative comparison of gradient distributions between PET Only and PET Fusion models across multiple distance metrics. Fig. 8: Visual and quantitative comparison of global gradient distributions for PET only and PET Fusion models. (a) Histograms of global gradient distribu- tions for both models. (b) Quantitative comparison of global gradient distribu- tions for both models. enabling interpretation of decision-driving features. As reported previously, analyses of the cohort characteristics showed that base- line levels of CgA and gamma-GT were higher in patients with shorter PFS [11]. Consistent with this finding, earlier studies have demonstrated an inverse association between baseline CgA and clinical outcome in NET patients under- going PRRT [30]. A similar pattern was observed for gamma-GT, which has been linked to hepatic tumor burden and poorer prognosis in NET [31], and which in our analysis was consistently higher in patients with early progression. Interest- ingly, in the explainability analysis of our multimodal model, CgA also emerged as a parameter with notable contribution to prediction (see Figure 4). This sug- gests that despite its limited value as a stand-alone biomarker — being strongly influenced by non-tumor–related factors such as proton-pump inhibitor therapy, renal dysfunction, or other comorbidities — CgA can still provide complemen- tary prognostic information when integrated with imaging and other laboratory features. Although previous authors have applied machine learning approaches Multimodal Deep Learning for PFS prediction in NET Patients 17 (a) Example of a three-dimensional 68Ga-DOTA-PET scan. (b) Gradient heatmap PET Only. (c) Gradient heatmap PET Fusion. (d) Gradient distribution for PET Only. (e) Gradient distribution for PET Fusion. Fig. 9: Example PET scan (a) and comparison of gradient maps for a single sam- ple between PET Only and PET Fusion models. Panels (b)–(c) show the gradient heatmap overlays, while (d)–(e) show the corresponding gradient distributions. Gradient maps were filtered with vmin = 0.3, omitting smaller gradients. to predict the prognosis of patients with NETs [32,33], these models have typ- ically relied on single data modalities, predominantly clinical and laboratory 18 Baur, Eschrich et al. data. For example, Jiang et al. used deep learning on population-based data from the SEER registry—containing only demographic, clinical, and pathologi- cal variables—to predict survival in pancreatic NETs [32]. Likewise, Gao et al. developed a machine learning model for prognosis estimation in gastroenteropan- creatic NET patients with liver metastases using solely clinical parameters [33]. In addition to clinical and laboratory parameters, imaging information has in-',\n",
       "  'registry—containing only demographic, clinical, and pathologi- cal variables—to predict survival in pancreatic NETs [32]. Likewise, Gao et al. developed a machine learning model for prognosis estimation in gastroenteropan- creatic NET patients with liver metastases using solely clinical parameters [33]. In addition to clinical and laboratory parameters, imaging information has in- creasingly been explored as a means of predicting outcomes after PRRT in NET patients. SR-PET/CT provide essential information on tumor burden and re- ceptor expression, and several groups have investigated whether quantitative or radiomic features could be used for prognostic modeling. For example, in a re- cent study, Opali´nska et al. found that a significant decrease in liver-normalized SUVmax in NET lesions on [68Ga]Ga-DOTA-TATE PET/CT following PRRT was associated with a lower risk of disease progression over a 20-month follow-up [34]. This suggests that PET/CT-derived SUVlmax in NET lesions may serve as an additional and independent predictor of treatment outcome. Further, Laudi- cella et al. reported that the [68Ga]Ga-DOTA-TATE PET/CT radiomic features HISTO Skewness and HISTO Kurtosis predicted PRRT response for individual lesions of both primary and metastatic GEP-NETs, regardless of tumor origin, with AUCs of 0.745 and 0.722, respectively [15]. Importantly, in the CLARINET trial, Pavel et al. reported that deep learning models based on CT imaging alone failed to outperform conventional laboratory markers such as chromogranin A and specific growth rate (SLDr) [35]. Similarly, in our study, the model based solely on CT scans or SR-PET showed no meaningful prognostic value and per- formed worse than a baseline model using laboratory biomarkers. Only when laboratory and imaging data were combined in a multimodal fusion model did we observe a relevant increase in predictive performance. These results further underscore the complementary nature of PET and CT imaging in capturing distinct yet clinically relevant aspects of disease biology. While SR-PET empha- sizes functional and metabolic activity, CT provides higher resolution anatomi- cal detail. From a clinical perspective, this implies that radiomic signatures from combined PET and CT imaging—augmented by biochemical markers—may re- flect pathological differences more accurately than any modality alone. When integrated within a shared feature space alongside laboratory biomarkers, the combined modality seems to offer a richer and more complete representation of patient status. This multimodal synergy enables the network to detect patterns that may be too subtle to discern in either modality alone, thereby improving the robustness and generalizability of the learned representations. We recognize several limitations of our study. First, the sample size was rel- atively small, which raises concerns about the robustness and generalizability of the findings. Training deep learning models on limited data can lead to over- fitting; although we employed cross-validation and regularization techniques, a larger dataset would be needed to ensure the model’s performance is con- sistent and not an artifact of our particular cohort. Second, our analysis was retrospective. This inherently carries risks of selection bias (e.g. only patients Multimodal Deep Learning for PFS prediction in NET Patients 19 who completed PRRT were included) and confounding factors that prospective studies could better control. A key limitation of our study is the absence of',\n",
       "  'cohort. Second, our analysis was retrospective. This inherently carries risks of selection bias (e.g. only patients Multimodal Deep Learning for PFS prediction in NET Patients 19 who completed PRRT were included) and confounding factors that prospective studies could better control. A key limitation of our study is the absence of an external validation cohort, which restricts the generalizability of our find- ings. Nonetheless, the PET/CT images were acquired using different scanner systems, potentially introducing variability due to differences in reconstruction algorithms. Given that all required inputs—laboratory parameters as well as SR- PET and CT imaging—are routinely obtained as part of the standard diagnostic work-up in patients scheduled for PRRT, prospective validation in larger multi- center cohorts appears feasible. Another methodological limitation concerns the heterogeneity of CT acquisition protocols. Among the 116 patients included, 42 underwent non-contrast CT, whereas the remaining patients received contrast- enhanced whole-body scans acquired during the venous contrast phase. This ensured complete anatomical coverage and optimal alignment between PET and CT images. However, arterial phase imaging could have improved the visualiza- tion of certain lesions, especially hepatic metastases, and might have enhanced the accuracy of image-based analyses. A further limitation of our study is that we considered only a relatively small fraction of the potentially available clinical information. Additional data such as genetic profiles, advanced laboratory pa- rameters, histological images, or multiplex staining might have provided further predictive value. At the same time, novel biomarkers such as the NETest are gaining increasing attention. Future models that integrate such high-specificity biomarkers with deep learning predictions could further enhance the accuracy and clinical utility of prognostic tools in NET patients undergoing PRRT. In contrast to many previous studies in this field, our work makes a contribution with respect to explainability, moving beyond the paradigm of “black-box” deep learning models. While gradient-based visualization did highlight tumor regions, as expected, it also consistently emphasized areas such as kidneys, spleen, and urinary bladder. In line with the observed AUROC of 0.42 for the SR-PET–only model, these findings indicate that SR-PET data alone did not provide predictive value for PFS. Consistent with the inferior predictive performance, PET-only models produced noisier and less structured gradient maps, with strong activa- tions concentrated in medically irrelevant regions (Figure 9). In contrast, the PET Fusion model—though not entirely free of spurious correlations, which are expected to some extent in any explainability method—yielded clearer, more co- herent gradient patterns that tend to focus more on clinically relevant tumorous regions. In general, saliency in non-tumor regions likely reflects a mix of relevant and spurious correlations inherent to the imaging data. Importantly, these corre- lations are not necessarily harmful in our setting: predictive performance emerges only after fusion with laboratory features, as supported by our UMAP analysis of embedding space, suggesting that the model leverages clinically meaningful interactions rather than relying solely on non-medically relevant image cues. From a clinical perspective, the ability to stratify patients by likely PFS has significant implications. PRRT is an expensive and resource-intensive therapy, 20 Baur, Eschrich et al. and not without toxicity, therefore, optimizing patient selection',\n",
       "  'the model leverages clinically meaningful interactions rather than relying solely on non-medically relevant image cues. From a clinical perspective, the ability to stratify patients by likely PFS has significant implications. PRRT is an expensive and resource-intensive therapy, 20 Baur, Eschrich et al. and not without toxicity, therefore, optimizing patient selection is critical. If a model identifies a patient as high risk for early progression, clinicians might con- sider adapting the treatment strategy. Such patients could benefit from closer monitoring during therapy and earlier response evaluation. The multimodal deep learning framework presented in this study builds on routinely available labo- ratory and imaging data, which may facilitate integration into interdisciplinary tumor board discussions and clinical workflows. Moreover, the architecture is designed to flexibly incorporate additional data sources in the future, such as genetic profiling, thereby further enhancing its predictive potential. Multimodal Deep Learning for PFS prediction in NET Patients 21 References 1. Sergio Pedraza-Ar´evalo, Manuel D Gahete, Emilia Alors-P´erez, Ra´ul M Luque, and Justo P Casta˜no. Multilayered heterogeneity as an intrinsic hallmark of neuroen- docrine tumors. Reviews in Endocrine and Metabolic Disorders, 19(2):179–192, 2018. 2. Eric H. Liu. Neuroendocrine Tumors: Epidemiology, pages 37–50. Springer Inter- national Publishing, Cham, 2024. 3. James C. Yao, Manal M. Hassan, Alexandria T. Phan, Cecile G. Dagohoy, Colleen C. Leary, Jeannette E. Mares, Eddie K. Abdalla, Jason B. Fleming, Jean- Nicolas Vauthey, Asif Rashid, and Douglas B. Evans. One hundred years after ”carcinoid”: epidemiology of and prognostic factors for neuroendocrine tumors in 35,825 cases in the united states. Journal of clinical oncology : official journal of the American Society of Clinical Oncology, 26 18:3063–72, 2008. 4. John D Hainsworth, F Anthony Greco, and Jonathan R Strosberg. Neuroendocrine neoplasms of unknown primary site, 2024. 5. Anna Pellat, Anne S´egol`ene Cottereau, Benoit Terris, and Romain Coriat. Neu- roendocrine carcinomas of the digestive tract: what is new? Cancers, 13(15):3766, 2021. 6. Jonathan Strosberg, Ghassan El-Haddad, Edward Wolin, Andrew Hendifar, James Yao, Beth Chasen, Erik Mittra, Pamela L Kunz, Matthew H Kulke, Heather Jacene, et al. Phase 3 trial of 177lu-dotatate for midgut neuroendocrine tumors. New England Journal of Medicine, 376(2):125–135, 2017. 7. Simron Singh, Daniel Halperin, Sten Myrehaug, Ken Herrmann, Marianne Pavel, Pamela L Kunz, Beth Chasen, Salvatore Tafuto, Secondo Lastoria, Jaume Capdev- ila, et al. [177lu] lu-dota-tate plus long-acting octreotide versus high-dose long- acting octreotide for the treatment of newly diagnosed, advanced grade 2–3, well- differentiated, gastroenteropancreatic neuroendocrine tumours (netter-2): an open- label, randomised, phase 3 study. The lancet, 403(10446):2807–2817, 2024. 8. Ying et al. Wang. The therapeutic efficacy of 177lu-dotatate/dotatoc in advanced neuroendocrine tumors: A meta-analysis. Medicine (Baltimore), 99(10):e19304, 2020. 9. Kosmas Daskalakis, Marina Tsoli, G¨oran Wallin, Angelika Kogut, Raj Srirajaskan- than, Christopher Harlow, Georgios Giovos, Martin O Weickert, Beata Kos-Kudla, and Gregory Kaltsas. Modified histopathological grading optimizes prediction of survival outcomes in small intestinal neuroendocrine tumors. The Journal of Clin- ical Endocrinology & Metabolism, 109(12):e2222–e2230, 2024. 10. U Knigge, J Capdevila, DK Bartsch, E Baudin, Jenny Falkerby, R Kianmanesh, B Kos-Kudla, B Niederle, E Nieveen van Dijkum, D O’Toole, et al. Enets consensus recommendations for the standards of',\n",
       "  'of survival outcomes in small intestinal neuroendocrine tumors. The Journal of Clin- ical Endocrinology & Metabolism, 109(12):e2222–e2230, 2024. 10. U Knigge, J Capdevila, DK Bartsch, E Baudin, Jenny Falkerby, R Kianmanesh, B Kos-Kudla, B Niederle, E Nieveen van Dijkum, D O’Toole, et al. Enets consensus recommendations for the standards of care in neuroendocrine neoplasms: follow-up and documentation. Neuroendocrinology, 105(3):310–319, 2017. 11. T. Ruhwedel, J. M. Rogasch, K. Huang, H. Jann, I. Schatka, C. Furth, H. Amthauer, and C. Wetz. The prognostic value of the de ritis ratio for progression-free survival in patients with net undergoing [177lu]lu-dotatoc-prrt: a retrospective analysis. Cancers, 13:635, 2021. 12. Tristan Ruhwedel, Julian Rogasch, Imke Schatka, Markus Galler, Peter Steinhagen, Christoph Wetz, and Holger Amthauer. Beyond similarities: overall survival and prognostic insights from [177lu] lu-dotatoc therapy in neuroendocrine tumors. Eu- ropean Journal of Nuclear Medicine and Molecular Imaging, pages 1–10, 2025. 22 Baur, Eschrich et al. 13. Rudolf A Werner, Constantin Lapa, Harun Ilhan, Takahiro Higuchi, Andreas K Buck, Sebastian Lehner, Peter Bartenstein, Frank Bengel, Imke Schatka, Dirk O Muegge, et al. Survival prediction in patients undergoing radionuclide therapy based on intratumoral somatostatin-receptor heterogeneity. Oncotarget, 8(4):7039, 2016. 14. Christoph Wetz, Philipp Genseke, Ivayla Apostolova, Christian Furth, Sammy Ghazzawi, Julian MM Rogasch, Imke Schatka, Michael C Kreissl, Frank Hofheinz, Oliver S Grosser, et al. The association of intra-therapeutic heterogeneity of so- matostatin receptor expression with morphological treatment response in patients undergoing prrt with [177lu]-dotatate. PLoS One, 14(5):e0216781, 2019. 15. Riccardo Laudicella, Albert Comelli, Virginia Liberini, Antonio Vento, Alessandro Stefano, Alessandro Spataro, Ludovica Croc`e, Sara Baldari, Bambaci Michelangelo, Desiree Deandreis, et al. [68ga] dotatoc pet/ct radiomics in the prediction of response in gep-nets undergoing [177lu] dotatoc prrt: the “theragnomics” concept, 2022. 16. Juli´an N. Acosta, Guido J. Falcone, Pranav Rajpurkar, and Eric J. Topol. Multi- modal biomedical AI. Nature Medicine, 28(9):1773–1784, 2022. 17. Simon Baur, Alexandra Benova, Emilio Dolgener Cant´u, and Jackie Ma. On the effectiveness of multimodal privileged knowledge distillation in two vision trans- former based diagnostic applications. arXiv preprint arXiv:2508.06558, 2025. 18. Pranav Rajpurkar, Emma Chen, Oishi Banerjee, and Eric J. Topol. Ai in health and medicine. Nature Medicine, 28(1):31–38, 2022. 19. Christoph Wetz, Tristan Ruhwedel, Imke Schatka, Jane Grabowski, Henning Jann, Giulia Metzger, Markus Galler, Holger Amthauer, and Julian MM Rogasch. Plasma markers for therapy response monitoring in patients with neuroendocrine tumors undergoing peptide receptor radionuclide therapy. Cancers, 15(24):5717, 2023. 20. Suleman Surti, Austin Kuhn, Matthew E Werner, Amy E Perkins, Jeffrey Koltham- mer, and Joel S Karp. Performance of philips gemini tf pet/ct scanner with spe- cial consideration for its time-of-flight imaging capabilities. Journal of Nuclear Medicine, 48(3):471–480, 2007. 21. Delphine Vandendriessche, Jorge Uribe, Hugo Bertin, and Frank De Geeter. Per- formance characteristics of silicon photomultiplier based 15-cm afov tof pet/ct. EJNMMI Physics, 6(1):8, 2019. 22. Tessa Brabander, Wouter A van der Zwan, Jaap JM Teunissen, Boen LR Kam, Wouter W de Herder, Richard A Feelders, Eric P Krenning, and Dik J Kwekke- boom. Pitfalls in the response evaluation after peptide receptor radionuclide ther- apy with [177 lu-dota 0, tyr 3] octreotate. Endocrine-related cancer, 24(5):243–251, 2017. 23.',\n",
       "  'Brabander, Wouter A van der Zwan, Jaap JM Teunissen, Boen LR Kam, Wouter W de Herder, Richard A Feelders, Eric P Krenning, and Dik J Kwekke- boom. Pitfalls in the response evaluation after peptide receptor radionuclide ther- apy with [177 lu-dota 0, tyr 3] octreotate. Endocrine-related cancer, 24(5):243–251, 2017. 23. E. Baudin, T. Walter, C. Docao, M. Haissaguerre, J. Hadoux, D. Taieb, C. Ansquer, L. Dierickx, L. De Mestier, E. Deshayes, E. Quak, and S. Foulon. First multicentric randomized phase ii trial investigating the antitumor efficacy of peptide receptor radionuclide therapy with 177lutetium–octreotate (oclu) in unresectable progres- sive neuroendocrine pancreatic tumor: Results of the oclurandom trial. Annales d’Endocrinologie, 83(5):289–290, 2022. On behalf of the ENDOCAN RENATEN network and GTE. 24. Jakob Wasserthal, Hanns-Christian Breit, Manfred T Meyer, Maurice Pradella, Daniel Hinck, Alexander W Sauter, Tobias Heye, Daniel T Boll, Joshy Cyriac, Shan Yang, et al. Totalsegmentator: robust segmentation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence, 5(5):e230024, 2023. Multimodal Deep Learning for PFS prediction in NET Patients 23 25. Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approx- imation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. 26. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international confer- ence on artificial intelligence and statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010. 27. Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term depen- dencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157–166, 1994. 28. Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universit¨at M¨unchen, 91(1):31, 1991. 29. Andrea Ceni. Random orthogonal additive filters: a solution to the vanish- ing/exploding gradient of deep neural networks. IEEE Transactions on Neural Networks and Learning Systems, 2025. 30. E. A. Aalbersberg, D. M. V. d. Vries–Huizing, I. Walraven, B. J. d. W. d. Veen, H. Kulkarni, A. Singh, M. P. M. Stokkel, and R. P. Baum. Parameters to predict progression-free and overall survival after peptide receptor radionuclide therapy: a multivariate analysis in 782 patients. Journal of Nuclear Medicine, 60:1259–1265, 2019. 31. Benjamin Christopher Schmidt, Miriam Theresa Leiderer, Tania Amin, Fabrice Viol, Samuel Huber, Frank Oliver Henes, and J¨org Schrader. Does gamma- glutamyltransferase correlate with liver tumor burden in neuroendocrine tumors? Endocrine, 83(2):511–518, 2024. 32. Chen Jiang, Kan Wang, Lizhao Yan, Hailing Yao, Huiying Shi, and Rong Lin. Predicting the survival of patients with pancreatic neuroendocrine neoplasms us- ing deep learning: A study based on surveillance, epidemiology, and end results database. Cancer Medicine, 12(11):12413–12424, 2023. 33. Fuli Gao, Jian Chen, and Xiaodan Xu. Machine learning predicts prognosis in patients with gastroenteropancreatic neuroendocrine tumors with liver metastases. Discover Oncology, 16(1):743, 2025. 34. Marta Opali´nska, Karolina Morawiec-S lawek, Adrian Kania-Kuc, Ibraheem Al Maraih, Anna Sowa-Staszczak, and Alicja Hubalewska-Dydejczyk. Potential value of pre-and post-therapy [68ga] ga-dota-tate pet/ct in the prognosis of re- sponse to prrt in disseminated neuroendocrine tumors. Frontiers in Endocrinology, 13:929391, 2022. 35. Marianne Pavel, Clarisse Dromain, Maxime Ronot, Niklaus Schaefer, Dalvinder Mandair, Delphine Gueguen, David Elvira, Simon J´egou, F´elix Balazard, Olivier Dehaene, et al. The use of',\n",
       "  'value of pre-and post-therapy [68ga] ga-dota-tate pet/ct in the prognosis of re- sponse to prrt in disseminated neuroendocrine tumors. Frontiers in Endocrinology, 13:929391, 2022. 35. Marianne Pavel, Clarisse Dromain, Maxime Ronot, Niklaus Schaefer, Dalvinder Mandair, Delphine Gueguen, David Elvira, Simon J´egou, F´elix Balazard, Olivier Dehaene, et al. The use of deep learning models to predict progression-free survival in patients with neuroendocrine tumors. Future Oncology, 19(32):2185–2199, 2023. Statements and Declarations Funding This work was supported by the Senate of Berlin and the European Commision’s Digital Europe Programme (DIGITAL) as grant TEF-Health (101100700). Jo- hannes Eschrich is a participant in the BIH Charit´e Junior Digital Clinician Scientist Program funded by the Charit´e – Universit¨atsmedizin Berlin, and the Berlin Institute of Health at Charit´e. 24 Baur, Eschrich et al. Competing Interests The authors have no relevant financial or non-financial interests to disclose. Author Contributions All authors contributed to the conception and design of the study. Data collec- tion was performed by Tristan Ruhwedel, Zuzanna Kobus, Gergana Lishkova and Johannes Eschrich. Imaging evaluation was supervised by Julian M. M. Ro- gasch, Christoph Wetz, and Holger Amthauer. Model development and compu- tational analysis were conducted by Simon Baur, Ekin B¨oke, Jackie Ma, and Wojciech Samek. Manuscript review and editing were performed by Christoph Roderburg, Frank Tacke, Holger Amthauer, Christoph Wetz, Henning Jann, Ju- lian M. M. Rogasch, Jackie Ma, and Wojciech Samek. Study conception, clinical oversight, and supervision were provided by Johannes Eschrich. The first draft of the manuscript was written by Johannes Eschrich and Simon Baur. All authors read and approved the final version of the manuscript. Data Availability The datasets generated and analysed during the current study are available from the corresponding author on reasonable request. Due to institutional and ethical restrictions, data are not publicly available. Ethics Approval This study was performed in accordance with the ethical standards of the in- stitutional research committee and with the 1964 Helsinki Declaration and its later amendments. Approval was granted by the Ethics Committee of Charit´e – Universit¨atsmedizin Berlin (Approval No.: EA1/016/23; Date: 24 February 2023). Consent to Participate Informed consent was obtained from all individual participants included in the study. Consent to Publish Not applicable. This manuscript does not contain any individual person’s data in any form (including individual details, images, or videos); therefore, consent for publication was not required.'],\n",
       " ['PRECIPITATION NOWCASTING OF SATELLITE DATA USING PHYSICALLY-ALIGNED NEURAL NETWORKS Antônio Catão ∗ antonio.catao@impa.br Melvin Poveda∗ melvin.poveda@impa.br Leonardo Voltarelli∗ leonardo.voltarelli@impa.br Paulo Orenstein∗ pauloo@impa.br ABSTRACT Accurate short-term precipitation forecasts predominantly rely on dense weather-radar networks, limiting operational value in places most exposed to climate extremes. We present TUPANN (Transferable and Universal Physics-Aligned Nowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike most deep learning models for nowcasting, TUPANN decomposes the forecast into physically meaningful components: a variational encoder–decoder infers motion and intensity fields from recent imagery under optical-flow supervision, a lead-time-conditioned MaxViT evolves the latent state, and a differentiable advection operator reconstructs future frames. We evaluate TUPANN on both GOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro, Manaus, Miami, La Paz) at 10–180-min lead times using the CSI and HSS metrics over 4–64 mm/h thresholds. Comparisons against optical-flow, deep learning and hybrid baselines show that TUPANN achieves the best or second-best skill in most settings, with pronounced gains at higher thresholds. Training on multiple cities further improves performance, while cross-city experiments show modest degradation and occasional gains for rare heavy-rain regimes. The model produces smooth, interpretable motion fields aligned with numerical optical flow and runs in near real time due to the low latency of GOES-16. These results indicate that physically aligned learning can provide nowcasts that are skillful, transferable and global. Keywords precipitation nowcasting, neural networks, physical conditioning, satellite data 1 Introduction Extreme precipitation events are projected to become more frequent and intense under climate change, increasing the risk of floods and landslides, particularly in vulnerable regions (K. et al., 2023). Nowcasting—forecasting the atmosphere on time horizons up to 6 h at high spatial resolution—is critical for early warnings and disaster management. While numerical weather prediction has improved steadily, its finite resolution and latency limit the accuracy of short-term precipitation forecasts. Radar-based nowcasting methods provide detailed observations but often require dense and well-maintained radar networks that are absent or degraded in much of the world. For example, Rio de Janeiro experiences recurrent flood-induced disasters yet lacks reliable radar coverage due to topographic blocking and limited infrastructure. Recent advances in machine learning have shown that deep networks can outperform traditional numerical models in precipitation nowcasting when trained on high-resolution radar data. However, reliance on radar restricts their applicability to radar-rich regions, leaving large parts of South America, Africa and Asia underserved. Furthermore, purely data-driven architectures often struggle with physical interpretability: they may produce realistic-looking precipitation maps while neglecting physically consistent motion fields, hindering forecasters’ trust and operational uptake. This paper addresses both accessibility and interpretability by leveraging geostationary satellites, which provide global coverage with near real-time latency, and by incorporating explicit physical structure into the neural network. We present TUPANN (Transferable and Universal Physics-Aligned Nowcasting Network), a model that uses only satellite-derived precipitation fields and decomposes the forecasting problem into physically motivated submodules. ∗Instituto Nacional de Matemática Pura e Aplicada, Rio de Janeiro, RJ, Brazil arXiv:2511.05471v1 [cs.LG] 7 Nov 2025 Precipitation nowcasting of satellite data using physically-aligned neural networks TUPANN comprises a variational encoder–decoder trained under optical-flow supervision',\n",
       "  'that uses only satellite-derived precipitation fields and decomposes the forecasting problem into physically motivated submodules. ∗Instituto Nacional de Matemática Pura e Aplicada, Rio de Janeiro, RJ, Brazil arXiv:2511.05471v1 [cs.LG] 7 Nov 2025 Precipitation nowcasting of satellite data using physically-aligned neural networks TUPANN comprises a variational encoder–decoder trained under optical-flow supervision to recover motion and intensity fields, a lead-time-conditioned transformer to evolve latent states, and a differentiable advection operator to reconstruct future frames. A strong physical alignment is done by explicitly penalizing the encoder-decoder output to match results from optical flows algorithms, which numerically infers motion fields. We evaluate TUPANN on data from four climate regimes — tropical rainforest (Manaus), subtropical highland (La Paz), tropical savanna with coastal influence (Rio de Janeiro) and tropical monsoon (Miami) — and report critical success indices (CSI) and Heidke skill scores (HSS) across lead times from 10 to 180 minutes and various precipitation thresholds. We benchmark against state-of-the-art baselines, including optical–flow methods (PySTEPS), deep learning models (Earthformer, CasCast), and hybrid approaches (NowcastNet). Our main contributions are: • We develop a physically aligned satellite-only nowcasting model that separates motion inference, latent dynamics and advection. Unlike prior works that learn motion implicitly from final frame loss, our variational encoder–decoder is directly supervised by numerical optical flow, yielding smooth and interpretable motion fields. • We leverage a lead-time-conditioned MaxViT transformer to evolve the latent representation and allow long lead-time prediction with a single network, reducing memory requirements compared with recurrent decoding. • We perform extensive experiments on GOES-16’s Rain Rate Quantitative Precipitation Estimation (RRQPE) and IMERG datasets in up to four cities with different climate regimes, comparing TUPANN with well- established and operational baselines. We demonstrate state-of-the-art CSI and HSS scores at multiple thresholds, analyze the effect of adding a generative adversarial network, and evaluate cross-city and multi-city training for transferability. • We discuss operational considerations, including runtime and latency, and outline limitations and future directions for satellite-based nowcasting. The remainder of this paper is structured as follows. Section 2 summarizes related work in numerical, optical-flow, deep learning and satellite-only nowcasting. Section 3 describes the datasets and study regions. Section 4 details the TUPANN architecture and its components. Section 5 presents our experimental design, baselines and results. Section 6 discusses limitations and future work, and Section 7 concludes our work. 2 Related work 2.1 Numerical nowcasting methods Precipitation nowcasting emerged in the late 1980s (Browning and Collier, 1989) and remains a fundamental tool to mitigate the impacts of extreme precipitation events (An et al., 2025). Early approaches relied primarily on Lagrangian extrapolation of radar echoes (Germann and Zawadzki, 2002), while later developments incorporated physical constraints and stochastic perturbations to enable ensemble-based probabilistic forecasts (Seed et al., 2013). Among these, PySTEPS (Pulkkinen et al., 2019) has become a widely adopted open-source Python library providing a reproducible platform for numerical nowcasting. It integrates multiple optical-flow algorithms, including Lucas–Kanade (Lucas and Kanade, 1981) and DARTS (Ruzanski et al., 2009), to estimate motion fields and applies the STEPS model (Bowler et al., 2006) for probabilistic extrapolation enhanced with downscaled numerical weather prediction input. Despite',\n",
       "  'Python library providing a reproducible platform for numerical nowcasting. It integrates multiple optical-flow algorithms, including Lucas–Kanade (Lucas and Kanade, 1981) and DARTS (Ruzanski et al., 2009), to estimate motion fields and applies the STEPS model (Bowler et al., 2006) for probabilistic extrapolation enhanced with downscaled numerical weather prediction input. Despite their interpretability and operational maturity, these numerical schemes typically experience a rapid decline in forecast skill with increasing lead time, and ensemble configurations such as STEPS can incur substantial computational cost. 2.2 Deep Learning models Deep learning (DL) approaches have recently achieved strong performance in precipitation nowcasting, often surpassing traditional numerical methods in both accuracy and scalability while enabling faster inference once trained. Early models include ConvLSTM (Shi et al., 2015) and PredRNN (Wang et al., 2023), which introduced convolutional and recurrent architectures to capture spatiotemporal dependencies in radar imagery. Transformer-based architectures have since extended this line of work. Earthformer (Gao et al., 2022) adapts the Transformer framework for general Earth-system forecasting through a modified Cuboid Attention mechanism that models three-dimensional spatial interactions. Similarly, Rainformer (Bai et al., 2022) extracts local and global features by combining a window-based multi-head self-attention with a gating mechanism component. These deterministic models, typically trained with pixel-wise L1 or L2 losses, target mean or median intensities and therefore tend to produce overly smooth forecasts. 2 Precipitation nowcasting of satellite data using physically-aligned neural networks To address this limitation, generative DL models have been proposed to better reproduce fine-scale variability by learning the underlying data distribution. The first class comprises Generative Adversarial Networks (GANs), including DGMR (Ravuri et al., 2021) and NowcastNet (Zhang et al., 2023), which have shown competitive performance on radar-based benchmarks. NowcastNet augments the GAN structure with an Evolution Network that estimates motion and intensity fields used to generate intermediate predictions before adversarial refinement. Despite its physics-inspired design, NowcastNet does not explicitly encode physical constraints and inherits known GAN instabilities, including mode collapse and artifact generation (Saxena and Cao, 2021). More recently, diffusion-based generative models have emerged as stable alternatives to GANs, inspired by advances in computer vision. PreDiff (Gao et al., 2023) employs a Latent Diffusion Model (LDM) with a knowledge-alignment mechanism that enforces domain-specific physical consistency during the sampling process. Evaluated on the SEVIR dataset (Veillette et al., 2020)—a combination of GOES-16 satellite imagery and NEXRAD radar data over the United States—PreDiff guides denoising steps toward physically plausible predictions by aligning generated intensities with those estimated from a time-series model applied to context-frame averages. CasCast (Gong et al., 2024a) extends this approach through a cascaded LDM framework: it first conditions on deterministic Earthformer predictions and then refines them in latent space to generate high-resolution, small-scale structures. CasCast achieves superior results on SEVIR and other benchmarks, though it remains limited to radar data, short lead times (up to one hour), and lacks explicit physical regularization—sometimes producing noisy outputs and incurring high inference costs typical of diffusion models (Salimans and Ho, 2022). 2.3 Physically conditioned Deep Learning Deep learning models are often regarded as black boxes, offering limited interpretability of the processes guiding their predictions.',\n",
       "  'lead times (up to one hour), and lacks explicit physical regularization—sometimes producing noisy outputs and incurring high inference costs typical of diffusion models (Salimans and Ho, 2022). 2.3 Physically conditioned Deep Learning Deep learning models are often regarded as black boxes, offering limited interpretability of the processes guiding their predictions. In geophysical applications, incorporating domain-specific physical knowledge can regularize training and promote physically consistent outputs. Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019) exemplify this approach and have inspired numerous extensions and applications (McClenny and Braga-Neto, 2023; Kovacs et al., 2022; Wang et al., 2022; Karniadakis et al., 2021). Their core principle is to augment the loss function with a term enforcing that the network outputs satisfy a governing Partial Differential Equation (PDE). In precipitation nowcasting, several DL models include such physical conditioning. PID-GAN (Yin et al., 2024) combines a GAN framework with a physics-informed loss derived from the moisture conservation equation, trained on radar data. FourCastNet (Kurth et al., 2023) employs an Adaptive Fourier Neural Operator architecture (Guibas et al., 2021), a physics-inspired design widely used for PDE solutions, and applies it to global-scale forecasting with ERA5 reanalysis inputs. The previously discussed PreDiff and NowcastNet also incorporate elements of physical conditioning: NowcastNet’s design draws inspiration from the continuity equation without numerical constraints, whereas PreDiff explicitly penalizes deviations from numerical-model intensities. The conditioning proposed in this work builds on both ideas by enforcing the continuity equation through an explicit loss between predicted physical terms and those derived from a numerical optical-flow method, applied to a specific module of the architecture. As shown later, this formulation achieves competitive predictive skill while improving interpretability and physical plausibility. 2.4 Use of satellite data The use of satellite observations for precipitation nowcasting remains relatively limited. Several studies have explored this direction. Shukla et al. (2025) evaluated the use of PySTEPS with satellite imagery under different optical flow methods. Lebedev et al. (2019) employed a variant of the U-Net architecture (Ronneberger et al., 2015) to predict precipitation up to two hours ahead over Russia using EUMETSAT data. Rahimi et al. (2024) proposed a hybrid U-Net–ConvLSTM model evaluated on IMERG and Global Forecast System (GFS) data, using GFS as ground truth but without comparison against baseline models. More recently, Park et al. (2025) introduced a two-phase neural network that first predicts future satellite imagery using a video prediction model and then performs image-to-image translation to obtain radar reflectivity. Their approach leverages the Sat2Rdr dataset, derived from the Korean GK2A geostationary satellite and ten ground-based radar stations. While effective for light precipitation, reported results are limited to low critical success index (CSI) thresholds (below 8 mm/h), leaving high-intensity events mostly unassessed. Agrawal et al. (2025) also leverage geostationary satellite mosaics, providing global skillful forecasts up to 12 hours into the future. The model uses an encoder-decoder architecture with multiple high-dimensional inputs, including numerical weather prediction (NWP), leading to an intensive hardware use of more than 500 TPU cores during training. Other studies, including Niu et al. (2024), Zheng et al. (2024), and Andrychowicz et al. (2023), integrate both satellite',\n",
       "  'the future. The model uses an encoder-decoder architecture with multiple high-dimensional inputs, including numerical weather prediction (NWP), leading to an intensive hardware use of more than 500 TPU cores during training. Other studies, including Niu et al. (2024), Zheng et al. (2024), and Andrychowicz et al. (2023), integrate both satellite and radar data as inputs, which limits their applicability in radar-sparse regions. In contrast, the present work relies exclusively on satellite imagery, enabling global scalability. Comparisons are conducted against established baselines and evaluated across a range of precipitation intensities, including high and extreme-rate events. 3 Precipitation nowcasting of satellite data using physically-aligned neural networks 3 Data and study regions We train and evaluate TUPANN using precipitation data from satellite products. For each dataset we identify rain events and split them into training, validation and test sets as described below. 3.1 GOES-16 RRQPE The primary data source is the GOES-R Advanced Baseline Imager Rain Rate Quantitative Precipitation Estimation (RRQPE) (GOES-R Algorithm Working Group and GOES-R Program Office, 2018). RRQPE provides precipitation estimates over the Americas every 10 min at 2 km spatial resolution with a latency of approximately 5 min, enabling real-time nowcasting. This product is highly correlated with rain-related bands and has been validated against ground radars and the GPM CORRA dataset (Agrawal et al., 2025), highlighting the value of predicting geostationary satellite observations. We use RRQPE from January 2020 to December 2023. Rain events are defined as contiguous periods when the precipitation rate exceeds a chosen threshold (see Section A.5 for details); we sample events uniformly at random and allocate 70% to training, 15% to validation and 15% to testing. Figure 1 shows the proportion of observations above various thresholds for four different cities, while Figure 2 displays the accumulated precipitation and dataset splits. >1 >2 >4 >8 >16 >32 >64 Rainfall Threshold (mm/h) 0.00 0.02 0.04 0.06 0.08 0.10 Proportion of Pixels Proportion of pixels exceeding thresholds Manaus La Paz Rio de Janeiro Miami Figure 1: Proportion of observations above different precipitation thresholds in the GOES-16 RRQPE dataset for each of the four study regions. Manaus exhibits the highest frequency of heavy rainfall across thresholds, while Rio de Janeiro and La Paz show intermediate levels 3.2 IMERG To test generalization across data sources we also use the Integrated Multi-satellitE Retrievals for GPM (IMERG) Final Run product (Huffman et al., 2014). IMERG provides precipitation estimates every 30 min at 10 km resolution and is widely used in remote sensing research. Its latency is about 3.5 months (the Early Run version has 4 h latency), which precludes real-time use but offers an independent validation dataset. We extract IMERG data from January 2020 to December 2023, split it using the same event-based procedure, and consider only the Rio de Janeiro region. Figure 3 compares the proportion of heavy-rain observations and accumulated precipitation for IMERG. 3.3 Study regions To evaluate model performance across different climates we select four 512 km × 512 km subregions of the GOES-16 domain centered on Rio de Janeiro (Brazil), La Paz (Bolivia), Manaus (Brazil) and Miami (USA). These regions',\n",
       "  'compares the proportion of heavy-rain observations and accumulated precipitation for IMERG. 3.3 Study regions To evaluate model performance across different climates we select four 512 km × 512 km subregions of the GOES-16 domain centered on Rio de Janeiro (Brazil), La Paz (Bolivia), Manaus (Brazil) and Miami (USA). These regions span coastal, high-altitude, rainforest and subtropical environments. For IMERG we consider only a 2560 km × 2560 km area surrounding Rio de Janeiro. The dominant precipitation processes include orographic and mesoscale convection in Rio (Luiz-Silva and Oscar-Júnior, 2022), high-altitude convective storms in La Paz (Garreaud, 2001), monsoon-driven 4 Precipitation nowcasting of satellite data using physically-aligned neural networks Figure 2: Accumulated precipitation in GOES-16 RRQPE from January 2020 to December 2023 over each study region. Shaded areas denote training, validation and test splits. Seasonal variability differs markedly between regions, with pronounced dry and wet seasons in La Paz and Rio de Janeiro convection in Manaus (Oliveira et al., 2016) and sea-breeze thunderstorms in Miami (Burpee, 1979). Figure 2 illustrates the seasonal cycle and dataset splits for GOES-16 across regions. >1 >2 >4 >8 >16 >32 Rainfall Threshold (mm/h) 0.00 0.02 0.04 0.06 0.08 Proportion of Pixels Proportion of pixels exceeding thresholds Manaus La Paz Rio de Janeiro Miami (a) Proportion of observations above different precipitation thresholds in the IMERG dataset for each study region. Un- like RRQPE, Manaus only dominates the lowest thresholds. (b) Estimated precipitation from IMERG (2020–2023) over Rio de Janeiro. The data splits are color-coded; low-level precipitation events are excluded from training and evalua- tion. Figure 3: Statistics of IMERG data, highlighting differences with respect to the RRQPE dataset 5 Precipitation nowcasting of satellite data using physically-aligned neural networks 4 Methods The proposed model, TUPANN, forecasts precipitation fields from a sequence of past satellite images 𝑋−𝑇:0 ∈ R(𝑇+1)×𝑛×𝑛, where 𝑛denotes the spatial resolution (i.e., number of pixels per dimension) and 𝑇+ 1 is the number of past observations. It produces predicted fields b𝑋1:𝑇𝑓∈R𝑇𝑓×𝑛×𝑛, where 𝑇𝑓is the forecast horizon. The architecture comprises two modules: (i) a variational encoder–decoder (VED) that learns a latent representation 𝐿𝑘, 𝑘= 1, . . . ,𝑇𝑓 of the evolution of the precipitation fields under optical flow supervision; (ii) a visual transformer (MaxViT) that evolves the latent representation 𝐿𝑘so that the resulting application of a differentiable advection operator (warp) on the decoded motion and intensity fields, b𝑠𝑘−1→𝑘and b𝑣𝑘−1→𝑘, closely match the ground truth frame. See Figure 4. The training procedure is sequential: the VED module is initially trained to infer the first set of motion and intensity fields, then its weights are fixed and used for the training of the MaxVit module. Details on the VED and MaxViT training are discussed in Sections 4.1 and 4.2, respectively. Even though MaxViT offers linear complexity in the image size, the choice of such encoder-decoder architecture is guided by the idea that, apart from compressing the images, the VED is also responsible for learning the dynamics of a single step evolution. MaxViT, on the other hand, learns to extrapolate the dynamics to further lead times. Figure 4: TUPANN architecture. The',\n",
       "  'the choice of such encoder-decoder architecture is guided by the idea that, apart from compressing the images, the VED is also responsible for learning the dynamics of a single step evolution. MaxViT, on the other hand, learns to extrapolate the dynamics to further lead times. Figure 4: TUPANN architecture. The VED and MaxViT modules displayed are learned; motion fields and the final predictions are extrapolated through a warp function 4.1 Variational encoder–decoder We use a variational encoder–decoder to learn an efficient representation of the precipitation evolution. Instead of reconstructing the input images as in classical variational autoencoders, our VED outputs motion fields b𝑣0→1 ∈R2×𝑛×𝑛 and intensity corrections b𝑠0→1 ∈R𝑛×𝑛given the past sequence 𝑋−𝑇:0. To enforce physically plausible motion we compute the ground truth motion fields 𝑣0→1 applying an optical flow algorithm to 𝑋−˜𝑇+2:1, where ˜𝑇is the context length provided to the optical flow algorithm. After that, the ground truth intensity correction 𝑠0→1 is obtained by subtracting the advected frame ˜𝑋1, obtained using 𝑣0→1, from the true frame 𝑋1 (see Figure 5). Thus, the estimated fields b𝑣0→1 and b𝑠0→1 can be used to extrapolate the last observed frame 𝑋0 to an estimate b𝑋1 of the next frame via an advection operator (see Section 4.2.2). 4.1.1 Target loss To supervise the predicted b𝑣0→1, ℓ1 and cosine-similarity losses are used with respect to 𝑣0→1. The intensity discrepan- cies between 𝑠0→1 and b𝑠0→1 are penalized via ℓ1 loss. Finally, a Kullback-Leibler divergence term is added to ensure the regularity of the learned latent space. The VED loss is LossVED(b𝑠0→1,b𝑣0→1, 𝑠0→1, 𝑣0→1) := 𝜆int ℓ1(𝑠0→1,b𝑠0→1) + 𝜆motion ℓ1(𝑣0→1,b𝑣0→1) + 𝜆cos CosSimilarity(𝑣0→1,b𝑣0→1) + 𝜆KL 𝐾𝐿(𝑝, b𝑝𝜃). (1) Here, b𝑝𝜃is the latent distribution inferred by the encoder, 𝑝is a standard normal distribution and 𝜆int, 𝜆motion, 𝜆cos, 𝜆KL are hyperparameters tuned on the validation set. This loss encourages accurate motion fields, intensity corrections and latent regularity. 4.1.2 Optical flow An optical flow algorithm is able to infer motion fields between two images, and is thus essential to obtain the derived ground-truth motion and intensity fields (e.g., 𝑣0→1, 𝑠0→1) used in (1). We consider two options: Lucas–Kanade (LK), 6 Precipitation nowcasting of satellite data using physically-aligned neural networks Optical Flow Advect Subtract Figure 5: Ground truth motion fields are obtained using an optical flow method from a pair of past and future images. The past image is advected to obtain an intermediate one, ˜𝑋1. Finally, ground truth intensity is the subtraction of ˜𝑋1 from the future image which solves a local least-squares problem under the assumption of small displacements, and DARTS, a spectral method tailored to radar imagery that solves the optical-flow equation in Fourier space. For TUPANN, we have taken the choice of optical flow method as a hyperparameter; below we use DARTS for GOES-16 and LK for IMERG results. 4.2 MaxViT Given the latent representation 𝐿1 from the VED, a visual transformer evolves the latent state forward in time. We adopt MaxViT (Tu et al., 2022), which combines local and grid attention to efficiently capture global context while avoiding quadratic attention cost. 4.2.1 Lead time conditioning To predict the',\n",
       "  '4.2 MaxViT Given the latent representation 𝐿1 from the VED, a visual transformer evolves the latent state forward in time. We adopt MaxViT (Tu et al., 2022), which combines local and grid attention to efficiently capture global context while avoiding quadratic attention cost. 4.2.1 Lead time conditioning To predict the latent state at lead time 𝑘, we condition the transformer on 𝑘via one-hot encoding and linear embedding, yielding 𝐿𝑘= VT(𝐿1, 𝑘), 𝑘= 2, . . . ,𝑇𝑓, where where VT(·) represents the MaxViT model. Unlike recurrent decoding, this conditioning enables the same transformer to produce all lead times while reducing memory overhead (Andrychowicz et al., 2023). Applying the VED decoder to 𝐿𝑘yields motion and intensity fields \\x00b𝑣𝑘−1→𝑘,b𝑠𝑘−1→𝑘 \\x01 = 𝐷(𝐿𝑘), where 𝐷(·) is the decoder module of the VED. Thus, all the necessary elements to predict the sequence recursively are obtained. 4.2.2 Warp function Following NowcastNet (Zhang et al., 2023), we implement a fixed differentiable advection operator that can reconstruct future precipitation frames using the predicted motion and intensity fields. Thus, given a frame b𝑋𝑘−1, motion field b𝑣𝑘−1→𝑘and intensity field b𝑠𝑘−1→𝑘, the extrapolated frame b𝑋𝑘is b𝑋𝑘= warp \\x10 b𝑣𝑘−1→𝑘,b𝑠𝑘−1→𝑘, b𝑋𝑘−1 \\x11 . (2) 4.2.3 Target loss We compute the target loss in the original image space. We assume that b𝑋𝑘−1 = 𝑋𝑘−1 in equation (2) to avoid a costly recursive loss and calculate the ℓ1 loss between the warped frame b𝑋𝑘and the observed frame 𝑋𝑘. Thus, LossMaxViT = ℓ1 (warp (b𝑣𝑘−1→𝑘,b𝑠𝑘−1→𝑘, 𝑋𝑘−1) , 𝑋𝑘) . (3) 7 Precipitation nowcasting of satellite data using physically-aligned neural networks The gradients of this loss will flow through the VED decoder module and the MaxViT transformer. The VED is pre-trained separately, thus optimizing this loss only affects the MaxViT modules. 5 Experiments and results We compare TUPANN against four nowcasting benchmarks using several evaluation metrics and across regions with different climates. We also run several ablation experiments. 5.1 Evaluation framework We evaluate TUPANN and baselines using the critical success index (CSI) and Heidke skill score (HSS). Both metrics depend on a choice of threshold 𝑡, so that pixel values above the threshold are assigned as positive and, otherwise, negative. CSI and HSS are given by CSI𝑡= TP𝑡 TP𝑡+ FN𝑡+ FP𝑡 , HSS𝑡= 2 (TP𝑡× TN𝑡−FN𝑡× FP𝑡) (TP𝑡+ FN𝑡) (TN𝑡+ FN𝑡) −(TP𝑡+ FP𝑡) (TN𝑡+ FP𝑡) , where TP stands for true positive, TN for true negatives, FN for false negatives and FP for false positives. Thus, CSI quantifies the overlap between forecasted and observed precipitation, ignoring true negatives (i.e., disregarding correct predictions of no precipitation) while HSS compares the forecast against random chance. We report CSI and HSS at thresholds of 4, 8, 16, 32 and 64 mm/h and compute both pixel-wise scores (POOL1) and max-pooled scores over 4 × 4 blocks (POOL4). For aggregated metrics we denote CSI–M and HSS–M as the mean across all thresholds. CSI values are reported in Section 5.3, and HSS ones are included in Appendix A. 5.2 Baseline models and tuning In our experiments, TUPANN is compared with four baselines from different nowcasting paradigms: • PySTEPS (LK) and PySTEPS (DARTS) (Pulkkinen',\n",
       "  'denote CSI–M and HSS–M as the mean across all thresholds. CSI values are reported in Section 5.3, and HSS ones are included in Appendix A. 5.2 Baseline models and tuning In our experiments, TUPANN is compared with four baselines from different nowcasting paradigms: • PySTEPS (LK) and PySTEPS (DARTS) (Pulkkinen et al., 2019): optical-flow baselines that estimate a motion field (LK: local Lucas–Kanade; DARTS: DFT-based spectral) from recent frames and then semi-Lagrangianly advect the precipitation field forward. As purely physical extrapolation methods, they are strong for very short lead times; • Earthformer (Gao et al., 2022): a space-time Transformer for Earth-system data that uses Cuboid Attention (local block attention with global tokens) in a hierarchical encoder–decoder to predict future frames; • NowcastNet (Zhang et al., 2023): a hybrid model combining a U-Net–based learnable semi-Lagrangian advec- tion (Evolution Network) with a physics-conditioned generative network trained with a temporal discriminator to inject high-resolution convective structure; • CasCast (Gong et al., 2024b): a cascaded scheme that first uses a deterministic predictor (e.g., Earthformer) to capture mesoscale evolution, then conditions a latent-space diffusion transformer on that coarse forecast to generate small-scale features and improve extreme-precipitation skill. For TUPANN and Earthformer, we tune learning rate, dropout rate and loss weights on the validation set by maximizing mean CSI in the city of Rio de Janeiro, and use these for the other cities. Hyperparameters for NowcastNet and CasCast are mostly those presented in their original paper (see Appendix A). The optimizer for all models is Adam. After selecting the best values, we retrain on the combined training and validation data and evaluate on a held-out test set. Training uses a single NVIDIA A100 GPU, and inference typically takes under two seconds per forecast (for all 18 lead times). 5.3 GOES-16 results Table 1 presents CSI scores across the four study regions and thresholds. TUPANN consistently ranks first or second for each metric. In Rio de Janeiro, it achieves the highest CSI at all thresholds; NowcastNet is the closest competitor, followed by Earthformer. In Miami, TUPANN again dominates most metrics, but now CasCast performs well. In Manaus and La Paz, Earthformer and NowcastNet obtain slightly better CSI for low thresholds, but TUPANN leads for higher thresholds and is therefore better at forecasting extreme rainfall. The 64 mm/h CSI values are small across models, reflecting the rarity of such intense events, yet TUPANN’s scores remain the highest. Overall, the table highlights TUPANN’s performance across different climate regimes and precipitation thresholds. Similar results are true for HSS (see Table 7), although by that metric Earthformer is much more competitive. 8 Precipitation nowcasting of satellite data using physically-aligned neural networks Table 1: Aggregated CSI metrics for GOES-16 data across cities. Bold values denote the best, underlined values the second best. TUPANN obtains state-of-the-art performance at most thresholds and regions, particularly for high rain-rate events. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 Rio de Janeiro Earthformer 0.237 0.222 0.326 0.320 0.287 0.236 0.326 0.312 0.238',\n",
       "  'state-of-the-art performance at most thresholds and regions, particularly for high rain-rate events. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 Rio de Janeiro Earthformer 0.237 0.222 0.326 0.320 0.287 0.236 0.326 0.312 0.238 0.237 0.009 0.006 NowcastNet 0.244 0.269 0.313 0.374 0.282 0.293 0.318 0.325 0.247 0.278 0.059 0.074 PySTEPS (LK) 0.165 0.169 0.242 0.262 0.212 0.195 0.226 0.226 0.142 0.156 0.005 0.008 PySTEPS (DARTS) 0.166 0.166 0.231 0.243 0.216 0.191 0.229 0.228 0.140 0.152 0.013 0.015 CasCast 0.170 0.187 0.308 0.343 0.205 0.249 0.164 0.162 0.159 0.156 0.016 0.027 TUPANN (ours) 0.259 0.277 0.330 0.384 0.289 0.289 0.330 0.336 0.274 0.287 0.072 0.090 Miami Earthformer 0.141 0.126 0.274 0.270 0.180 0.160 0.154 0.122 0.097 0.078 0.000 0.000 NowcastNet 0.137 0.160 0.248 0.299 0.170 0.207 0.128 0.139 0.097 0.106 0.040 0.047 PySTEPS (LK) 0.113 0.116 0.188 0.202 0.133 0.136 0.120 0.111 0.079 0.079 0.045 0.053 PySTEPS (DARTS) 0.112 0.113 0.189 0.202 0.135 0.138 0.118 0.107 0.073 0.071 0.044 0.046 CasCast 0.146 0.170 0.258 0.298 0.188 0.229 0.144 0.167 0.117 0.135 0.020 0.028 TUPANN (ours) 0.169 0.187 0.267 0.312 0.189 0.211 0.177 0.177 0.135 0.141 0.079 0.094 Manaus Earthformer 0.276 0.256 0.355 0.341 0.323 0.297 0.316 0.292 0.265 0.245 0.124 0.104 NowcastNet 0.253 0.278 0.323 0.366 0.296 0.324 0.283 0.303 0.233 0.258 0.130 0.137 PySTEPS (LK) 0.200 0.196 0.258 0.266 0.237 0.233 0.218 0.212 0.160 0.156 0.125 0.112 PySTEPS (DARTS) 0.197 0.194 0.259 0.268 0.239 0.235 0.219 0.213 0.158 0.154 0.109 0.099 CasCast 0.265 0.286 0.344 0.377 0.303 0.333 0.295 0.307 0.260 0.269 0.126 0.141 TUPANN (ours) 0.290 0.293 0.339 0.367 0.316 0.321 0.315 0.312 0.278 0.274 0.200 0.193 La Paz Earthformer 0.303 0.270 0.337 0.312 0.329 0.281 0.359 0.319 0.323 0.291 0.167 0.146 NowcastNet 0.291 0.301 0.330 0.376 0.303 0.321 0.321 0.315 0.300 0.296 0.202 0.197 PySTEPS (LK) 0.212 0.208 0.248 0.250 0.243 0.230 0.247 0.237 0.197 0.195 0.126 0.131 PySTEPS (DARTS) 0.225 0.218 0.263 0.262 0.262 0.244 0.264 0.250 0.206 0.201 0.127 0.132 CasCast 0.228 0.235 0.309 0.337 0.251 0.270 0.245 0.237 0.232 0.222 0.101 0.111 TUPANN (ours) 0.314 0.317 0.336 0.363 0.327 0.323 0.350 0.340 0.327 0.322 0.232 0.239 The graphs in Figure 6 show mean CSI (averaged across thresholds) versus lead time. TUPANN maintains the highest or second-highest skill across all lead times; the advantage over NowcastNet grows for early lead times, reflecting the benefit of explicit motion supervision and the efficiency of lead-time conditioning (see also Figure 7). Beyond aggregated metrics, Figure 10 illustrates a TUPANN prediction for a rain event in Manaus, compared with NowcastNet, CasCast and Earthformer. Generative models such as NowcastNet and CasCast produce detailed textures but may introduce artifacts, whereas TUPANN and Earthformer yield smoother predictions. Despite the blurred appearance, TUPANN captures the timing and location of heavy rain more accurately, leading to higher CSI values. 5.4 Ablation results We will study how different experiments affect TUPANN: investigating its motion fields against another physics-DL hybrid model, adding a GAN head, evaluating TUPANN’s cross-city generalization and training the',\n",
       "  'the blurred appearance, TUPANN captures the timing and location of heavy rain more accurately, leading to higher CSI values. 5.4 Ablation results We will study how different experiments affect TUPANN: investigating its motion fields against another physics-DL hybrid model, adding a GAN head, evaluating TUPANN’s cross-city generalization and training the model jointly on all cities. 5.4.1 Interpretability and motion fields TUPANN’s interpretability stems from its explicitly learned motion fields. Figure 7 compares motion fields predicted by TUPANN and NowcastNet (which relies on an Evolution Network submodule for its motion fields). The TUPANN fields are smooth and closely resemble the numerical optical flow computed by DARTS, whereas the baselines’ fields exhibit unrealistic patterns. This underscores the benefit of supervising motion fields directly. 9 Precipitation nowcasting of satellite data using physically-aligned neural networks 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Rio de Janeiro Manaus Earthformer NowcastNet TUPANN (ours) PySTEPS (LK) PySTEPS (DARTS) CasCast 30 60 90 120 150 180 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Miami 30 60 90 120 150 180 La Paz Mean CSI value Lead Time (min) CSI-M Figure 6: Mean CSI (CSI–M) versus lead time for the four study regions using GOES-16 data. TUPANN consistently outperforms baselines across lead times 5.4.2 GAN-TUPANN Generative adversarial networks can improve visual realism at the cost of evaluation metrics. To study its impact on TUPANN predictions, we evaluate the variant GAN-TUPANN, which adds a GAN head to TUPANN outputs. Figure 8 shows that GAN-TUPANN produces significantly sharper images. Still, Table 2 shows this does not always lead to improvements in CSI scores. While in Rio de Janeiro GAN-TUPANN increases low-threshold CSI, in Miami the gains are either non-existent or negative. Given the computational overhead and mixed impact on metrics, there is no clear advantage in including this module unless visual fidelity is paramount. 5.4.3 Cross-city generalization To assess generalization we train TUPANN on one city and evaluate on others. Table 3 compares TUPANN trained on Rio de Janeiro (TUPANN–Rio) against models trained separately on each city. In Manaus and La Paz, TUPANN–Rio yields lower CSI at all thresholds, as expected due to climate differences. Surprisingly, in Miami the Rio-trained model performs comparably or better at high thresholds, possibly because heavy-rain events are rare in Miami (see Figure 1) and the Rio model may bias towards such events. Overall, cross-city degradation is modest (at most 20%) and TUPANN–Rio still outperforms or matches baselines trained on the target city, highlighting the transferability of the architecture. 10 Precipitation nowcasting of satellite data using physically-aligned neural networks mm/h Figure 7: Comparison of motion fields. Top row: ground-truth DARTS motion fields for future frames. Second row: motion fields from TUPANN. Subsequent rows: motion fields estimated by NowcastNet/Evolution Network. TUPANN yields smoother fields that align with physical intuition 11 Precipitation nowcasting of satellite data using physically-aligned neural networks mm/h Figure 8: Visual comparison of TUPANN and GAN-TUPANN for the same Manaus event as in Figure 10. GAN-TUPANN reduces blur but yields mixed changes in CSI (Table 2) 5.4.4 Multi-city training Rather than training a model for each',\n",
       "  'intuition 11 Precipitation nowcasting of satellite data using physically-aligned neural networks mm/h Figure 8: Visual comparison of TUPANN and GAN-TUPANN for the same Manaus event as in Figure 10. GAN-TUPANN reduces blur but yields mixed changes in CSI (Table 2) 5.4.4 Multi-city training Rather than training a model for each city, training on multiple cities can further improve skill. Table 4 compares TUPANN trained separately on each city with a multi-city model trained jointly on all regions (including Toronto, see Appendix A). The multi-city model (TUPANN–Multicity) yields higher CSI across most thresholds and regions. Access to diverse climates helps the network learn more generalizable features, especially for extreme rainfall events. 5.5 IMERG results Table 5 compares TUPANN to baselines on the IMERG dataset for Rio de Janeiro. Without pooling (POOL1), TUPANN achieves the best CSI across all thresholds. With pooling (POOL4), generative models (NowcastNet, GAN-TUPANN) slightly outperform TUPANN at low thresholds but TUPANN remains competitive and leads at higher thresholds. Figure 9 plots CSI–M versus lead time, showing TUPANN’s superior performance at most lead times and small gaps only at 150 min. These results demonstrate that TUPANN generalizes to coarser spatial resolution and longer latency datasets, where its physics-aligned architecture can become slightly less beneficial. 12 Precipitation nowcasting of satellite data using physically-aligned neural networks Table 2: CSI metrics comparing TUPANN and its GAN variant (GAN-TUPANN) on GOES-16 data. Bold values denote the best, underlined values the second best. GAN-TUPANN improves low-threshold performance in Rio de Janeiro but degrades or yields marginal improvements in other cities. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 Rio de Janeiro NowcastNet 0.244 0.269 0.313 0.374 0.282 0.293 0.318 0.325 0.247 0.278 0.059 0.074 TUPANN 0.259 0.277 0.330 0.384 0.289 0.289 0.330 0.336 0.274 0.287 0.072 0.090 GAN-TUPANN 0.265 0.290 0.350 0.413 0.296 0.319 0.342 0.348 0.270 0.286 0.070 0.086 Miami NowcastNet 0.137 0.160 0.248 0.299 0.170 0.207 0.128 0.139 0.097 0.106 0.040 0.047 TUPANN 0.169 0.187 0.267 0.312 0.189 0.211 0.177 0.177 0.135 0.141 0.079 0.094 GAN-TUPANN 0.152 0.174 0.252 0.300 0.180 0.211 0.160 0.164 0.116 0.128 0.052 0.066 Manaus NowcastNet 0.253 0.278 0.323 0.366 0.296 0.324 0.283 0.303 0.233 0.258 0.130 0.137 TUPANN 0.290 0.293 0.339 0.367 0.316 0.321 0.315 0.312 0.278 0.274 0.200 0.193 GAN-TUPANN 0.274 0.285 0.331 0.359 0.318 0.329 0.310 0.313 0.258 0.267 0.156 0.156 La Paz NowcastNet 0.291 0.301 0.330 0.376 0.303 0.321 0.321 0.315 0.300 0.296 0.202 0.197 TUPANN 0.314 0.317 0.336 0.363 0.327 0.323 0.350 0.340 0.327 0.322 0.232 0.239 GAN-TUPANN 0.306 0.312 0.336 0.367 0.327 0.333 0.344 0.336 0.308 0.305 0.214 0.218 Table 3: Cross-city CSI comparison between TUPANN trained on each city and TUPANN trained on Rio de Janeiro (TUPANN–Rio). Bold values denote the best, underlined values the second best. Cross-city performance declines in Manaus and La Paz but remains competitive; in Miami TUPANN–Rio improves high-threshold scores. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1',\n",
       "  '(TUPANN–Rio). Bold values denote the best, underlined values the second best. Cross-city performance declines in Manaus and La Paz but remains competitive; in Miami TUPANN–Rio improves high-threshold scores. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 Miami TUPANN 0.169 0.187 0.267 0.312 0.189 0.211 0.177 0.177 0.135 0.141 0.079 0.094 TUPANN–Rio 0.166 0.185 0.248 0.289 0.182 0.199 0.178 0.182 0.138 0.152 0.085 0.103 Manaus TUPANN 0.290 0.293 0.339 0.367 0.316 0.321 0.315 0.312 0.278 0.274 0.200 0.193 TUPANN–Rio 0.236 0.249 0.283 0.319 0.257 0.276 0.254 0.264 0.214 0.221 0.172 0.167 La Paz TUPANN 0.314 0.317 0.336 0.363 0.327 0.323 0.350 0.340 0.327 0.322 0.232 0.239 TUPANN–Rio 0.279 0.288 0.305 0.339 0.288 0.295 0.306 0.304 0.279 0.279 0.216 0.221 5.6 Operational deployment Downloading and processing GOES-16 data and making predictions can be done in less than three minutes. Due to its low latency, this enables an efficient near real-time 10 to 180 minutes nowcast with 10 minutes temporal resolution. The whole pipeline, including the predictions generated by TUPANN, can be run in a machine with relatively modest requirements: 16GB of memory, GeForce RTX 3080 GPU. Predictions can also include the derived motion fields to help meteorologists interpret the results. This system is currently operational and has been deployed to aid Rio de Janeiro’s Instituto Estadual do Ambiente (INEA) agency to prepare for floods and other extreme precipitation disasters. Due to TUPANN’s reliance on widely available geostationary satellite data, we expect that this solution can be broadened to other regions as well. 6 Limitations and future work Our study has important limitations. First, although TUPANN may use globally available satellite products, it currently relies on GOES-16 coverage for real-time nowcasting. The architecture should be retrained and validated on other geostationary satellites (e.g., Himawari, Meteosat) to ensure generalization across platforms. Second, the optical-flow supervision requires additional computation and may not perfectly capture complex convection dynamics; using the 13 Precipitation nowcasting of satellite data using physically-aligned neural networks Table 4: Comparison between single-city TUPANN and a multi-city version (TUPANN–Multicity) trained on all regions simultaneously. Bold denotes the best, underlined the second best. The multi-city model improves performance in most cases, demonstrating the benefit of pooled training. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 Rio de Janeiro TUPANN 0.259 0.277 0.330 0.384 0.289 0.289 0.330 0.336 0.274 0.287 0.072 0.090 TUPANN–Multicity 0.271 0.286 0.337 0.390 0.303 0.297 0.353 0.353 0.293 0.304 0.071 0.086 Miami TUPANN 0.169 0.187 0.267 0.312 0.189 0.211 0.177 0.177 0.135 0.141 0.079 0.094 TUPANN–Multicity 0.178 0.190 0.273 0.313 0.196 0.211 0.188 0.182 0.151 0.151 0.082 0.095 Manaus TUPANN 0.290 0.293 0.339 0.367 0.316 0.321 0.315 0.312 0.278 0.274 0.200 0.193 TUPANN–Multicity 0.291 0.296 0.340 0.369 0.316 0.324 0.314 0.314 0.278 0.275 0.208 0.198 La Paz TUPANN 0.314 0.317 0.336 0.363 0.327 0.323 0.350 0.340 0.327 0.322 0.232 0.239 TUPANN–Multicity 0.324 0.325 0.339 0.363 0.334 0.327 0.359 0.348 0.335',\n",
       "  '0.290 0.293 0.339 0.367 0.316 0.321 0.315 0.312 0.278 0.274 0.200 0.193 TUPANN–Multicity 0.291 0.296 0.340 0.369 0.316 0.324 0.314 0.314 0.278 0.275 0.208 0.198 La Paz TUPANN 0.314 0.317 0.336 0.363 0.327 0.323 0.350 0.340 0.327 0.322 0.232 0.239 TUPANN–Multicity 0.324 0.325 0.339 0.363 0.334 0.327 0.359 0.348 0.335 0.331 0.251 0.254 Table 5: Aggregated CSI metrics for Rio de Janeiro using IMERG data. Bold denotes the best, underlined the second best. Without pooling TUPANN is clearly superior; with pooling, generative baselines perform slightly better at low thresholds, but TUPANN remains competitive overall. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 Earthformer 0.153 0.141 0.361 0.343 0.270 0.243 0.130 0.114 0.006 0.006 0.000 0.000 NowcastNet 0.209 0.271 0.387 0.453 0.322 0.384 0.229 0.304 0.103 0.201 0.001 0.013 PySTEPS (LK) 0.114 0.099 0.300 0.269 0.191 0.164 0.075 0.061 0.002 0.003 0.000 0.000 PySTEPS (DARTS) 0.107 0.096 0.287 0.262 0.179 0.157 0.069 0.057 0.002 0.003 0.000 0.000 CasCast 0.180 0.229 0.362 0.420 0.288 0.339 0.188 0.248 0.061 0.139 0.000 0.000 TUPANN (ours) 0.218 0.248 0.414 0.454 0.344 0.379 0.241 0.280 0.087 0.123 0.005 0.006 GAN-TUPANN 0.210 0.274 0.391 0.461 0.327 0.393 0.234 0.313 0.095 0.199 0.002 0.004 proposed scheme with additional covariates could yield further improvements under a more detailed physical modeling. Third, CSI and HSS values decline at longer lead times (>2h), reflecting inherent predictability limits; integrating uncertainty quantification and ensemble approaches may better characterize forecast skill. Finally, GAN-based enhancements improve visual realism but degrade or inconsistently affect skill metrics; stabilizing adversarial training and assessing perceptual quality remain open challenges. Future work includes coupling TUPANN with probabilistic post-processing to provide calibrated uncertainty estimates, extending the model to include additional inputs such as cloud imagery or microwave precipitation retrievals, and investigating transfer learning across continents. 7 Conclusions We have presented TUPANN, a physically aligned neural network for precipitation nowcasting using satellite imagery. TUPANN’s modular design — combining a variational encoder–decoder supervised by optical flow, and a transformer capable of evolving the latent representation according to physical constraints — yields interpretable motion fields and competitive forecast skill. Extensive experiments on GOES-16 and IMERG data across four climates show that TUPANN matches or surpasses state-of-the-art baselines, particularly at high precipitation thresholds. Training on multiple cities improves performance and cross-city evaluations reveal modest degradation, highlighting the model’s transferability. With its low latency and reliance on globally available satellites, TUPANN supports equitable access to short-term rainfall forecasts and provides a foundation for operational applications in radar-sparse regions. Acknowledgements The authors thank Adriana Monteiro, João Vitor Romano, Lucas Nissenbaum, and Thiago Ramos as well as the help and support from Google, in particular, Shreya Agrawal, Boris Babenko and Samier Merchant. 14 Precipitation nowcasting of satellite data using physically-aligned neural networks 30 60 90 120 150 180 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 CSI-M for Rio de Janeiro Earthformer NowcastNet TUPANN (ours) PySTEPS (LK) PySTEPS (DARTS) CasCast Mean CSI value Lead Time (min) Figure 9: Mean CSI versus lead time',\n",
       "  'Precipitation nowcasting of satellite data using physically-aligned neural networks 30 60 90 120 150 180 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 CSI-M for Rio de Janeiro Earthformer NowcastNet TUPANN (ours) PySTEPS (LK) PySTEPS (DARTS) CasCast Mean CSI value Lead Time (min) Figure 9: Mean CSI versus lead time for IMERG data in Rio de Janeiro. TUPANN outperforms baselines at most lead times; NowcastNet overtakes slightly at 150 min but lags at shorter lead times Funding Statement A.C. was supported by a “FAPERJ Nota 10\" grant (SEI-260003/004731/2025) from Fun- dação de Amparo à Pesquisa do Estado do Rio de Janeiro (FAPERJ). M.P. and L.V. were supported by scholarships from Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES). P.O. was supported by grant SEI- 260003/001545/2022 from FAPERJ. Competing Interests The authors declare no competing interests. Data Availability Statement GOES-16 RRQPE data were obtained from the NOAA GOES-R Product Distribution and Access (PDA) system. IMERG Final Run data were obtained from the NASA GPM data portal. Processed datasets, event splits and TUPANN code are available at https://github.com/acataos/tupann. Ethical Standards The research meets all ethical guidelines, including adherence to the legal requirements of Brazil and the United States. Author Contributions Conceptualization: A.C., M.P., L.V., P.O.; Methodology: A.C., M.P., L.V.; Data curation and visualization: A.C.; Writing—original draft: A.C., M.P., L.V.; Writing—review and editing: A.C., M.P., L.V., P.O.. All authors approved the final manuscript. References Agrawal, S., Hassen, M. A., Brempong, E. A., Babenko, B., Zyda, F., Graham, O., Li, D., Merchant, S., Potes, S. H., Russell, T., Cheresnick, D., Kakkirala, A. P., Rasp, S., Hassidim, A., Matias, Y., Kalchbrenner, N., Gupta, P., Hickey, J., and Bell, A. (2025). An operational deep learning system for satellite-based high-resolution global nowcasting. An, S., Oh, T.-J., Sohn, E., and Kim, D. (2025). Deep learning for precipitation nowcasting: A survey from the perspective of time series forecasting. Expert Systems with Applications, 268:126301. Andrychowicz, M., Espeholt, L., Li, D., Merchant, S., Merose, A., Zyda, F., Agrawal, S., and Kalchbrenner, N. (2023). Deep learning for day forecasts from sparse observations. Bai, C., Sun, F., Zhang, J., Song, Y., and Chen, S. (2022). Rainformer: Features extraction balanced network for radar-based precipitation nowcasting. IEEE Geoscience and Remote Sensing Letters, 19:1–5. Bowler, N. E., Pierce, C. E., and Seed, A. W. (2006). Steps: A probabilistic precipitation forecasting scheme which merges an extrapolation nowcast with downscaled nwp. Quarterly Journal of the Royal Meteorological Society, 132(620):2127–2155. Browning, K. A. and Collier, C. G. (1989). Nowcasting of precipitation systems. Reviews of Geophysics, 27(3):345–370. 15 Precipitation nowcasting of satellite data using physically-aligned neural networks Burpee, R. W. (1979). Peninsula-scale convergence in the south florida sea breeze. Monthly Weather Review, 107(7):852–865. Gao, Z., Shi, X., Han, B., Wang, H., Jin, X., Maddix, D., Zhu, Y., Li, M., and Wang, Y. (2023). Prediff: precipitation nowcasting with latent diffusion models. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS ’23, Red Hook, NY, USA. Curran Associates Inc. Gao, Z., Shi, X., Wang, H., Zhu, Y., Wang, Y., Li, M., and Yeung, D.-Y. (2022). Earthformer: exploring space-time transformers',\n",
       "  'Y. (2023). Prediff: precipitation nowcasting with latent diffusion models. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS ’23, Red Hook, NY, USA. Curran Associates Inc. Gao, Z., Shi, X., Wang, H., Zhu, Y., Wang, Y., Li, M., and Yeung, D.-Y. (2022). Earthformer: exploring space-time transformers for earth system forecasting. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA. Curran Associates Inc. Garreaud, R. D. (2001). Interannual rainfall variability over the south american altiplano and related circulation anomalies. Journal of Climate, 14(12):2779–2789. Germann, U. and Zawadzki, I. (2002). Scale-dependence of the predictability of precipitation from continental radar images. part i: Description of the methodology. Monthly Weather Review, 130(12):2859 – 2873. GOES-R Algorithm Working Group and GOES-R Program Office (2018). Noaa goes-r series advanced baseline imager (abi) level 2 rainfall rate / qpe. https://doi.org/10.7289/V5W66J21. Gong, J., Bai, L., Ye, P., Xu, W., Liu, N., Dai, J., Yang, X., and Ouyang, W. (2024a). Cascast: skillful high-resolution precipitation nowcasting via cascaded modelling. In Proceedings of the 41st International Conference on Machine Learning, ICML’24. JMLR.org. Gong, J., Bai, L., Ye, P., Xu, W., Liu, N., Dai, J., Yang, X., and Ouyang, W. (2024b). Cascast: skillful high-resolution precipitation nowcasting via cascaded modelling. In Proceedings of the 41st International Conference on Machine Learning, ICML’24. JMLR.org. Guibas, J., Mardani, M., Li, Z.-Y., Tao, A., Anandkumar, A., and Catanzaro, B. (2021). Adaptive fourier neural operators: Efficient token mixers for transformers. ArXiv, abs/2111.13587. Huffman, G., Bolvin, D., Braithwaite, D., Hsu, K., Joyce, R., and Xie, P. (2014). Integrated multi-satellite retrievals for gpm (imerg), version 4.4. K., M., Su, W., Delgado, R., Aarons, S., Chatterjee, A., Garcia, M., Hausfather, Z., Hayhoe, K., Hence, D., Jewett, E., Robel, A., Singh, D., Tripati, A., Vose, R., Khan, A., Crimmins, A., Avery, C., Easterling, D., Kunkel, K., and Maycock, T. (2023). Fifth National Climate Assessment: Chapter 2 Climate Trends. Karniadakis, G. E., Kevrekidis, I. G., Lu, L., Perdikaris, P., Wang, S., and Yang, L. (2021). Physics-informed machine learning. Nat. Rev. Phys., 3(6):422–440. Kovacs, A., Exl, L., Kornell, A., Fischbacher, J., Hovorka, M., Gusenbauer, M., Breth, L., Oezelt, H., Yano, M., Sakuma, N., Kinoshita, A., Shoji, T., Kato, A., and Schrefl, T. (2022). Conditional physics informed neural networks. Commun. Nonlinear Sci. Numer. Simul., 104(106041):106041. Kurth, T., Subramanian, S., Harrington, P., Pathak, J., Mardani, M., Hall, D., Miele, A., Kashinath, K., and Anandkumar, A. (2023). Fourcastnet: Accelerating global high-resolution weather forecasting using adaptive fourier neural operators. In Proceedings of the Platform for Advanced Scientific Computing Conference, PASC ’23, New York, NY, USA. Association for Computing Machinery. Lebedev, V., Ivashkin, V., Rudenko, I., Ganshin, A., Molchanov, A., Ovcharenko, S., Grokhovetskiy, R., Bushmarinov, I., and Solomentsev, D. (2019). Precipitation nowcasting with satellite imagery. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, page 2680–2688, New York, NY, USA. Association for Computing Machinery. Lucas, B. D. and Kanade, T. (1981). An iterative image registration technique with an application to stereo vision. In Proceedings of the 7th International Joint Conference',\n",
       "  'the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’19, page 2680–2688, New York, NY, USA. Association for Computing Machinery. Lucas, B. D. and Kanade, T. (1981). An iterative image registration technique with an application to stereo vision. In Proceedings of the 7th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI’81, page 674–679, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Luiz-Silva, W. and Oscar-Júnior, A. C. (2022). Climate extremes related with rainfall in the state of rio de janeiro , brazil: a review of climatological characteristics and recorded trends. Natural Hazards, 114(1):713–732. McClenny, L. D. and Braga-Neto, U. M. (2023). Self-adaptive physics-informed neural networks. J. Comput. Phys., 474(111722):111722. Niu, D., Li, Y., Wang, H., Zang, Z., Jiang, M., Chen, X., and Huang, Q. (2024). Fsrgan: A satellite and radar- based fusion prediction network for precipitation nowcasting. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 17:7002–7013. Oliveira, R. et al. (2016). Characteristics and diurnal cycle of gpm rainfall estimates over the central amazon region (around manaus). Remote Sensing, 8(7):544. 16 Precipitation nowcasting of satellite data using physically-aligned neural networks Park, Y.-J., Kim, D., Seo, M., Jeon, H.-G., and Choi, Y. (2025). Data-driven precipitation nowcasting using satellite imagery. In Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence and Thirty-Seventh Conference on Innovative Applications of Artificial Intelligence and Fifteenth Symposium on Educational Advances in Artificial Intelligence, AAAI’25/IAAI’25/EAAI’25. AAAI Press. Pulkkinen, S., Nerini, D., Pérez Hortal, A. A., Velasco-Forero, C., Seed, A., Germann, U., and Foresti, L. (2019). Pysteps: an open-source python library for probabilistic precipitation nowcasting (v1.0). Geoscientific Model Development, 12(10):4185–4219. Rahimi, R., Ravirathinam, P., Ebtehaj, A., Behrangi, A., Tan, J., and Kumar, V. (2024). Global precipitation nowcasting of integrated multi-satellite retrievals for gpm: A u-net convolutional lstm architecture. Journal of Hydrometeorology, 25(6):947 – 963. Raissi, M., Perdikaris, P., and Karniadakis, G. E. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys., 378:686–707. Ravuri, S., Lenc, K., Willson, M., Kangin, D., Lam, R., Mirowski, P., Fitzsimons, M., Athanassiadou, M., Kashem, S., Madge, S., Prudden, R., Mandhane, A., Clark, A., Brock, A., Simonyan, K., Hadsell, R., Robinson, N., Clancy, E., Arribas, A., and Mohamed, S. (2021). Skilful precipitation nowcasting using deep generative models of radar. Nature, 597(7878):672–677. Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In Navab, N., Hornegger, J., Wells, W. M., and Frangi, A. F., editors, Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, pages 234–241, Cham. Springer International Publishing. Ruzanski, E., Wang, Y., and Chandrasekar, V. (2009). Development of a real-time dynamic and adaptive nowcasting system. 25th Conference on International Interactive Information and Processing Systems (IIPS) for Meteorology, Oceanography, and Hydrology, 2(12). Salimans, T. and Ho, J. (2022). Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations. Saxena, D. and Cao, J. (2021). Generative adversarial networks (gans): Challenges, solutions, and future directions. ACM Comput. Surv., 54(3). Seed, A. W., Pierce, C. E., and Norman, K.',\n",
       "  'Hydrology, 2(12). Salimans, T. and Ho, J. (2022). Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations. Saxena, D. and Cao, J. (2021). Generative adversarial networks (gans): Challenges, solutions, and future directions. ACM Comput. Surv., 54(3). Seed, A. W., Pierce, C. E., and Norman, K. (2013). Formulation and evaluation of a scale decomposition-based stochastic precipitation nowcast scheme. Water Resources Research, 49(10):6624–6641. Shi, X., Chen, Z., Wang, H., Yeung, D.-Y., Wong, W.-k., and Woo, W.-c. (2015). Convolutional lstm network: a machine learning approach for precipitation nowcasting. In Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 1, NIPS’15, page 802–810, Cambridge, MA, USA. MIT Press. Shukla, B. P., Vyas, J., Chhari, A., Shah, S., Panda, S. K., and Varma, A. K. (2025). Satellite based rainfall nowcasting using geospatial techniques. Meteorology and Atmospheric Physics, 137(1):3. Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., and Li, Y. (2022). MaxViT: Multi-axis Vision Transformer. In Computer Vision – ECCV 2022, pages 459–479. Springer, Cham, Switzerland. Veillette, M. S., Samsi, S., and Mattioli, C. J. (2020). Sevir: a storm event imagery dataset for deep learning applications in radar and satellite meteorology. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA. Curran Associates Inc. Wang, S., Yu, X., and Perdikaris, P. (2022). When and why PINNs fail to train: A neural tangent kernel perspective. J. Comput. Phys., 449(110768):110768. Wang, Y., Wu, H., Zhang, J., Gao, Z., Wang, J., Yu, P. S., and Long, M. (2023). Predrnn: A recurrent neural network for spatiotemporal predictive learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(2):2208–2225. Yin, J., Meo, C., Roy, A., Cher, Z., Lic˘a, M., Wang, Y., Imhoff, R., Uijlenhoet, R., and Dauwels, J. (2024). Precipitation nowcasting using physics informed discriminator generative models. In 32nd European Signal Processing Confer- ence, EUSIPCO 2024 - Proceedings, European Signal Processing Conference, pages 1967–1971. European Signal Processing Conference, EUSIPCO. Green Open Access added to TU Delft Institutional Repository ‘You share, we take care!’ – Taverne project https://www.openaccess.nl/en/you-share-we-take-care Otherwise as indicated in the copyright section: the publisher is the copyright holder of this work and the author uses the Dutch legislation to make this work public. ; 32nd European Signal Processing Conference, EUSIPCO 2024, EUSIPCO 2024 ; Conference date: 26-08-2024 Through 30-08-2024. Zhang, Y., Long, M., Chen, K., Xing, L., Jin, R., Jordan, M. I., and Wang, J. (2023). Skilful nowcasting of extreme precipitation with nowcastnet. Nature, 619(7970):526–532. 17 Precipitation nowcasting of satellite data using physically-aligned neural networks Zheng, K., He, L., Ruan, H., Yang, S., Zhang, J., Luo, C., Tang, S., Zhang, J., Tian, Y., and Cheng, J. (2024). A cross-modal spatiotemporal joint predictive network for rainfall nowcasting. IEEE Transactions on Geoscience and Remote Sensing, 62:1–23. 18 Precipitation nowcasting of satellite data using physically-aligned neural networks A Additional results Some additional results related to metrics and evaluation are presented below, such as HSS and CSI scores for the city of Toronto. Prediction samples in both GOES-16 and IMERG data are also included. A.1',\n",
       "  'Remote Sensing, 62:1–23. 18 Precipitation nowcasting of satellite data using physically-aligned neural networks A Additional results Some additional results related to metrics and evaluation are presented below, such as HSS and CSI scores for the city of Toronto. Prediction samples in both GOES-16 and IMERG data are also included. A.1 Model Hyperparameters The hyperparameters for Earthformer and TUPANN (both VED and MaxViT) are shown below. Hyperparameters Evolution Network/NowcastNet were the ones selected in the original paper (Zhang et al., 2023). Since CasCast uses a Denoising Transformer (DiT) model trained with a different image size (384x384), slight adaptations were made to support the 256x256 images used in this work. The hyperparameters modified were input_size to 32 and hidden_size to 512; other hyperparameters were chosen as in their original paper (Gong et al., 2024b). All remaining hyperparameters and early stopping criteria were selected by maximizing the mean CSI value in the validation set during training. Table 6: Model hyperparameters for VED, Earthformer, and MaxViT. Architectural and loss parameters are detailed in Section 4. (a) VED Parameter Value batch_size 8 learning_rate 0.0001 channels 128 embed_dim 4 reduc_factor 4 𝜆cos 0.00165 𝜆KL 1.0e-06 𝜆motion 0.0033 𝜆int 0.995 dropout 0.2 (b) Earthformer Parameter Value batch_size 4 learning_rate 0.0001 num_global_vectors 6 num_heads 2 base_units 64 (c) MaxViT Parameter Value batch_size 8 learning_rate 0.0001 MaxViT_depth 4 MaxViT_dim 64 A.2 HSS results Table 7 summarizes aggregated HSS metrics for the four cities using GOES-16 data. TUPANN achieves the best scores for high thresholds and remains competitive for lower thresholds, mirroring the CSI behaviour. A.3 Visual inspection of model predictions Figure 10 includes predictions for several models at a given moment in time. While TUPANN and EarthFormer show relatively blurred predictions, note NowcastNet and CasCast add several artifacts to its predictions. Overall, TUPANN achieves a reasonable trade-off between good evaluation metrics and reasonable precipitation plots. Figure 11 also includes precipitation plots for the IMERG dataset, where the temporal and spatial resolution of the image is coarser. A.4 Results for Toronto For completeness Table 8 reports CSI scores for the city of Toronto using GOES-16 data. This city was excluded from the main text once it did not present as many extreme precipitation events as the other selected alternatives. The multi-city TUPANN model performs best across most thresholds, while the single-city TUPANN model ranks second. Although extreme precipitation events are rare in Toronto, these results demonstrate that TUPANN generalizes to additional regions. A.5 Rain events dataset selection The datasets used to train and evaluate our models comprise a subsample of rainy windows drawn from either the GOES-16 RRQPE or the IMERG products. We define a rainy window as follows. For each 10-min timestamp 𝑡from 2020-01-01 00:00 UTC to 2023-12-31 23:50 UTC, we (i) form a symmetric 60-min window [𝑡−30 min, 𝑡+ 30 min]; (ii) compute the spatiotemporal precipitation accumulation over that window (10-min steps over all grid points); and 19 Precipitation nowcasting of satellite data using physically-aligned neural networks mm/h Figure 10: Predictions up to 3h ahead for a rain event in Manaus, starting at 2021-11-20 22:00 UTC. Rows show the ground',\n",
       "  'min]; (ii) compute the spatiotemporal precipitation accumulation over that window (10-min steps over all grid points); and 19 Precipitation nowcasting of satellite data using physically-aligned neural networks mm/h Figure 10: Predictions up to 3h ahead for a rain event in Manaus, starting at 2021-11-20 22:00 UTC. Rows show the ground truth, TUPANN, and other models; columns represent lead times. TUPANN displays greater skill in predicting the movement and intensity of the rain event, while generative models produce visually sharper but less accurate predictions 20 Precipitation nowcasting of satellite data using physically-aligned neural networks mm/h Figure 11: Prediction sample from the IMERG test dataset centered in Rio de Janeiro, starting from 2023-01-08 07:30 UTC. The red square represents the same area as in the GOES-16 figures, demonstrating the difference in spatial resolution 21 Precipitation nowcasting of satellite data using physically-aligned neural networks Table 7: Aggregated HSS metrics for GOES-16 data across cities. Bold values denote the best, underlined values the second best. TUPANN excels at high thresholds and is second or first at lower thresholds. Model HSS-M ↑ HSS4 ↑ HSS8 ↑ HSS16 ↑ HSS32 ↑ HSS64 ↑ Rio de Janeiro Earthformer 0.360 0.473 0.438 0.488 0.382 0.017 NowcastNet 0.373 0.454 0.429 0.478 0.394 0.112 PySTEPS (LK) 0.266 0.369 0.341 0.365 0.247 0.010 PySTEPS (DARTS) 0.268 0.356 0.347 0.369 0.244 0.025 CasCast 0.266 0.438 0.321 0.271 0.270 0.032 TUPANN (ours) 0.393 0.473 0.439 0.492 0.428 0.135 Miami Earthformer 0.230 0.412 0.299 0.265 0.176 0.000 NowcastNet 0.224 0.368 0.277 0.223 0.177 0.076 PySTEPS (LK) 0.195 0.299 0.227 0.213 0.147 0.087 PySTEPS (DARTS) 0.192 0.301 0.231 0.210 0.135 0.083 CasCast 0.237 0.388 0.304 0.247 0.208 0.039 TUPANN (ours) 0.277 0.398 0.309 0.298 0.237 0.146 Manaus Earthformer 0.417 0.502 0.476 0.472 0.414 0.220 NowcastNet 0.384 0.457 0.437 0.429 0.372 0.228 PySTEPS (LK) 0.319 0.385 0.369 0.349 0.271 0.222 PySTEPS (DARTS) 0.314 0.386 0.371 0.350 0.269 0.195 CasCast 0.401 0.486 0.447 0.444 0.406 0.222 TUPANN (ours) 0.435 0.479 0.464 0.469 0.430 0.333 La Paz Earthformer 0.454 0.487 0.486 0.523 0.485 0.285 NowcastNet 0.439 0.472 0.451 0.479 0.458 0.335 PySTEPS (LK) 0.340 0.378 0.381 0.390 0.326 0.223 PySTEPS (DARTS) 0.356 0.398 0.405 0.412 0.339 0.225 CasCast 0.351 0.441 0.381 0.381 0.370 0.183 TUPANN (ours) 0.468 0.482 0.481 0.512 0.490 0.376 (iii) if the accumulation exceeds a threshold 𝜏, label the larger window [𝑡−4 hours, 𝑡+ 4 hours] as a rainy window. Finally, we merge rainy windows that intersect. The threshold 𝜏= 120,000 was chosen empirically to balance excluding near-dry periods against obtaining a dataset large enough for effective learning. Using a symmetric ±4-hour window ensures that events include both onset and dissipation phases (from no precipitation to mild or heavy precipitation and back). 22 Precipitation nowcasting of satellite data using physically-aligned neural networks Table 8: Aggregated CSI metrics for Toronto with GOES-16 data. Bold denotes the best, underlined the second best. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 Earthformer 0.209 0.187 0.288 0.280 0.233 0.203 0.154 0.133 0.102 0.084 0.000',\n",
       "  'for Toronto with GOES-16 data. Bold denotes the best, underlined the second best. Model CSI-M ↑ CSI4 ↑ CSI8 ↑ CSI16 ↑ CSI32 ↑ CSI64 ↑ POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 POOL1 POOL4 Earthformer 0.209 0.187 0.288 0.280 0.233 0.203 0.154 0.133 0.102 0.084 0.000 0.000 NowcastNet 0.206 0.216 0.278 0.294 0.225 0.238 0.151 0.161 0.100 0.105 0.000 0.000 PySTEPS (LK) 0.179 0.173 0.250 0.240 0.200 0.186 0.130 0.120 0.078 0.070 0.000 0.000 PySTEPS (DARTS) 0.181 0.174 0.252 0.242 0.203 0.188 0.133 0.121 0.080 0.071 0.000 0.000 CasCast 0.192 0.203 0.270 0.286 0.217 0.231 0.145 0.158 0.096 0.105 0.000 0.000 TUPANN 0.219 0.222 0.295 0.295 0.239 0.231 0.159 0.158 0.106 0.105 0.000 0.000 TUPANN–Multicity 0.229 0.230 0.305 0.298 0.250 0.241 0.170 0.165 0.114 0.107 0.000 0.000 23'],\n",
       " ['Online motion recognition . M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 1 of 8 arXiv:2511.05250v1 [cs.CV] 7 Nov 2025 Highlights Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks Mohamed Sanim AKREMI, Rim SLAMA, Hedi TABIA • The \"SPD Siamese Network\" is efficient to resolve motion recognition problem • An online system based on a detector and a classifier achieves well performances • The proposed algorithm is accurate in different contexts and on challenging datasets Accurate online action and gesture recognition system using detectors and Deep SPD Siamese Networks⋆ Mohamed Sanim AKREMIa, Rim SLAMAb and Hedi TABIAa aIBISC, Univ Evry. Université Paris Saclay , Evry, Paris, France bUniv. Eiffel, ENTPE, LICIT-ECO7, F-69518, Lyon, France A R T I C L E I N F O Keywords: Manifold approaches SPD learning model Siamese network Deep learning Online gesture and action recognition 3D skeletal data A B S T R A C T Online continuous motion recognition is a hot topic of research since it is more practical in real life application cases. Recently, Skeleton-based approaches have become increasingly popular, demonstrating the power of using such 3D temporal data. However, most of these works have focused on segment-based recognition and are not suitable for the online scenarios. In this paper, we propose an online recognition system for skeleton sequence streaming composed from two main components: a detector and a classifier, which use a Semi-Positive Definite (SPD) matrix representation and a Siamese network. The powerful statistical representations for the skeletal data given by the SPD matrices and the learning of their semantic similarity by the Siamese network enable the detector to predict time intervals of the motions throughout an unsegmented sequence. In addition, they ensure the classifier capability to recognize the motion in each predicted interval. The proposed detector is flexible and able to identify the kinetic state continuously. We conduct extensive experiments on both hand gesture and body action recognition benchmarks to prove the accuracy of our online recognition system which in most cases outperforms state-of-the-art performances. 1. Introduction Human activity recognition is an important research topic in pattern recognition field. It has been the subject of many studies in the past two decades because of its importance in numerous areas such as security, health, daily activity, energy consumption and robotics. Recently, some works on the recognition of hand gestures or human ac- tions from skeletal data are based on the modeling of the skeleton’s movement as manifold-based representation and proposed deep neural networks on this structure [1, 2, 3]. These approaches demonstrated their potential in the pro- cessing of skeletal data. Most of them are applied on offline human action recognition which is useful in time-limited tasks. However, in many applications, simply recognizing a single gesture in a given segmented sequence is not enough, especially in monitoring systems and virtual-reality devices which need to detect human movements moment by moment in continuous videos. In these online recognition systems, it is important to detect the existence of an action as early as possible after its beginning. It is',\n",
       "  'a given segmented sequence is not enough, especially in monitoring systems and virtual-reality devices which need to detect human movements moment by moment in continuous videos. In these online recognition systems, it is important to detect the existence of an action as early as possible after its beginning. It is also essential to determine the nature of the movement within a sequence of frames, without having information about the number of gestures present within the video, their starting times or their durations, unlike the segmented action recognition. In this paper, we propose to use a manifold-based model in order to build an online motion recognition system that detects and identifies different human activities in unseg- mented skeletal sequences. As demonstrated in our previous work [4], the SPD Siamese network is accurate for hand mohamedsanim.akremi@univ-evry.fr (M.S. AKREMI); rim.slamasalmi@entpe.fr (R. SLAMA); hedi.tabia@univ-evry.fr (H. TABIA) ORCID(s): 0000-0000-0000-0000 (M.S. AKREMI) gesture classification. Thus, we decide to generalize our work on 3D body skeleton representation and adapt it for action recognition problem. Besides, we upgrade our model to make it as suitable as possible for online recognition criteria [5]. Our proposed online recognition system is composed from (i) a detector which segments the sequence and (ii) a classifier which identifies the action or the gesture. In the following, we review both segmented and continu- ous human motion recognition methods. A focus is given to manifold deep learning approaches applied to 3D skeleton data in the offline recognition approaches. For the Online recognition system, we review the approaches applied for sequence segmentation and online recognition, whatever the type of data. An overview of our approach is also given to outline the main steps of the proposed solution. The pretrained models and the code are provided at the following link. 1.1. Related works Many methods based on manifold learning approaches are proposed in proceeding 3D skeletal data. Vemulapalli et al. [3] worked on the special Euclidean Group denoted by SE(3). They mapped the action curves from SE(3) to the lie Algebra se(3). Then, they computed a nominal curve using Dynamic Time Warping (DTW) and used Fourier Temporal Pyramid (FTP) representation to eliminate the noise issue before the use of the classification with SVM. Huang et al. [6] proposed a Lie group Network (LieNet) which based on an input rotation matrix, applies mapping transformation in order to construct the best rotation matrix. Other studies focus on the SPD manifold-based approaches: a Riemannian metric learning for SPD Matrices was proposed by Huang et al. [2]; a learning matrix neural network using mean and covariance statistics was proposed by [7] . A deep M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 1 of 8 Online motion recognition neural network on Grassmann manifold, denoted by GrNet, was proposed by [1]. It is composed of 3 major blocks: a projection block used for the transformation of the orthonor- mal input matrices, a pooling block designed to map the orthonormal matrices and apply a mean pooling on them, and an output block utilized for mapping and classification. These approaches, as proposed, work well on segmented',\n",
       "  'of 3 major blocks: a projection block used for the transformation of the orthonor- mal input matrices, a pooling block designed to map the orthonormal matrices and apply a mean pooling on them, and an output block utilized for mapping and classification. These approaches, as proposed, work well on segmented sequences. In order to adapt them to an online recognition task, introducing online approaches into these methods is required. The well-known adopted strategy in identifying online motions is based mainly on determining the time interval for each motion, then brings the problem to an offline recognition task. Some studies, such as [8], went to monitor movements by studying the changes that occur during the transition from stagnation to active state. Then, they analyzed spatially limbs behavior in the stagnation. However, this automatic segmentation did not generally obtain the required efficacy. Some works have also gone away from segmentation and are content with knowing the movement locally [5, 9]. Other approaches proposed various sliding window strategies for online recognition purpose [10, 11, 12]. A Temporal Recurrent Network (TRN) on fixed length windows was proposed by [13] to anticipate the future using a temporal decoder incorporating future infor- mation with the historical information to improve online action detection. Delamare et al. [10] used a sliding window approach to execute Spatio-Temporal Graph Convolutional Network (ST-GCN). 1.2. Contributions and method overview The major contributions of this paper are as follows: (1) A new portioning for the human body skeleton is proposed, enabling the \"SPD Siamese Network\" to operate on action recognition context. (2) A detector and a verification process are introduced in order to upgrade our method from an offline to an online system able to identify hand gestures or body ac- tions in unsegmented sequences with high performance and fast reaction. (3) The efficiency of the proposed algorithm is proved in different contexts and datasets. First, we propose to build a network that recognizes hand gestures and body actions using a proposed partitioning of body/hand joints, SPD matrices spatio-temporal representa- tions of the skeletal sequence and the characteristics of the Siamese network. In a next stage, we build an online system based on two main components: a detector and a classifier that use the proposed SPD Siamese network for the identification of the kinetic state and the recognition of the motion respec- tively. The detector has as a role to identify continuously the kinetic state and find the segments of each performed motion throughout a long sequence: it predicts the starting frame and the end frame of each performed motion by detecting a change in the kinetic state of the volunteer. As illustrated in Figure 1, the change in the kinetic state may refer to an action beginning, an action end or an unexpected idle period. In order to verify the nature of the state, we realize a Verification Process (VP). After each segmentation, the classifier is activated in order to recognize the motion segment. The rest of the paper is constructed as follows: section 2 presents the proposed network for motion recognition. Section 3',\n",
       "  'period. In order to verify the nature of the state, we realize a Verification Process (VP). After each segmentation, the classifier is activated in order to recognize the motion segment. The rest of the paper is constructed as follows: section 2 presents the proposed network for motion recognition. Section 3 presents the aspects of our online system. In section 4, the experiments results are reported. Finally, the last section is dedicated for the conclusion. 2. SPD Siamese neural network for motion recognition The SPD Siamese network aims to build a classifier capable of recognizing the actions performed by humans, and a detector that identifies the kinetic state. It uses specific partitioning for the body and hand joints. Then, it learns an SPD matrix representing a motion sequence. Finally, a Siamese network is used to finalize the classification of this sequence. 2.1. Proposed architecture The proposed network, illustrated in Figure 2, is com- posed of five principal components: preprocessing compo- nent, partitioning component, SPD learning features com- ponent, SPD Siamese network component, and classification component. Having the 3D skeleton sequence, we divide the joint set into parts in order to perform the spatial analysis. The joints from each part must have a correlation between each other. In the SPD learning component, we analyze the spatial- temporal evolution and the temporal-spatial evolution in order to obtain the best SPD matrix representing the skeleton sequence. For the SPD Siamese network component, we use as a base model the network proposed by [2] without trans- formation blocks since we realized the SPD learning in the previous component. We twin the two previous components and use the contrastive loss function to train our model. Finally, we use the K-NN algorithm on the learnt model parameters applied to the base SPD network component for the classification. 2.2. Proposed representation for body/hand parts Having 3D skeletal joint coordinates, we divide the hand/body skeleton into parts in order to study the evolution of the coordinates locally. As explained in [4], for the hand, we consider that each finger represents a part (Figure 3(a)). As for the body, the proposed partitioning consists in combining each joint of the body to an adjacent joint, starting from the head down to the other parts of the body. When a joint has more than one adjacent, it produces ramifications which we follow and divide the body into into four parts (Figure 3(b)): (i) upper right part that runs from the head down to the palm of the right hand, (ii) upper left part that goes from the head part to the palm of the left hand, (iii) lower right part which includes the spine connected to the right leg, (iv) and lower left part which includes the spine connected to the left leg. M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 2 of 8 Online motion recognition 2.3. SPD matrix learning In order to learn the better SPD matrix representation, we use the ST-TS-HGR-Network inspired by [7]. This network is composed from four principal phases. The first phase is the convolution layer. It highlights',\n",
       "  'R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 2 of 8 Online motion recognition 2.3. SPD matrix learning In order to learn the better SPD matrix representation, we use the ST-TS-HGR-Network inspired by [7]. This network is composed from four principal phases. The first phase is the convolution layer. It highlights the correlation between the neighboring joints, and it learns the filter weight associated to each neighbor of a given joint. We apply 3×3 convolution on the matrix representation of the partitioning. Then, we divide the sequence into 6 sub-sequences: the first represents the whole sequence, the second and the third represent the two halves of the sequence and the rest represents its three thirds. We perform a spatial-temporal analysis in the ST- GA-NET phase and a temporal-spatial analysis TS-GA-NET phase of body/hand parts evolution. Each phase contains five layers. The first layer, Gaussian Aggregation (GA) outputs the SPD matrix 𝑌 = [ Σ + 𝜇𝜇𝑇 𝜇 𝜇𝑇 1 ] , where 𝜇and Σ denote respectively the mean and the covariance of the joints positions spatially in ST-GA-NET and temporally in TS- GA-NET. Then, we apply a ReEig layer to rectify the SPD matrices, outputs of GA layer, by tuning up their small positive eigenvalues using a chosen threshold. The LogEig and VecMap layers are used to map the resulting SPD matrices into the euclidean space. Then, a second GA layer computes the mean and the covariance along the temporal domain in ST-GA-NET and along the spatial domain (parts) in TS-GA-NET. In the final phase, SPDC-NET has as a function to generate more compact and discriminative SPD matrix. It takes as input the resulting SPD matrices from both ST-GA-NET and TS-GA-NET, returning as output 𝑌= ∑𝑊𝑖𝑋𝑖𝑊𝑇 𝑖, where its parameters {𝑊𝑖} are Stiefel weights and {𝑋𝑖} are the outputs from both previous components. For more details on equations used in each layer, we refer readers to [4]. 2.4. SPD Siamese network and motion recognition The SPD Siamese network consists of two identical SPD sub-networks joined at their outputs and it is char- acterized by its margin parameter 𝑔. We have as input, pair of two SPD matrices, with a binary label 𝑏. In each sub-network of the SPD Siamese network, we map an SPD matrix into a tangent space, and we extract a feature vector using a fully connected layer. Then, we use the Contrastive Loss (𝐶𝐿) function to measure the distance between the two extracted feature vectors and minimize distance between positive pairs using the following equation: 𝐶𝑙(𝑦1, 𝑦2, 𝑏) = 𝑏||𝑦1 −𝑦2||2 + (1 −𝑏)𝑚𝑎𝑥(0, 𝑔−||𝑦1 −𝑦2||2) Where 𝑦1 and 𝑦2 are the outputs of the two twin SPD sub-networks and ||.||2 is the Euclidean distance. For the motion recognition, we apply K-Nearest Neighbor (K-NN) algorithm with 𝐾= 1. 3. Online action and gesture recognition using a detector The effectiveness of our online system is related to the performance of the proposed detector and its ability to predict the motions intervals throughout a long sequence. We are also interested to study the efficiency of our system within an early classification. 3.1.',\n",
       "  'action and gesture recognition using a detector The effectiveness of our online system is related to the performance of the proposed detector and its ability to predict the motions intervals throughout a long sequence. We are also interested to study the efficiency of our system within an early classification. 3.1. Proposed detector The detector preparation goes through two stages: train- ing loop and testing loop. As a first step, we train the detector with the segmented sequences. Then, we evaluate its performance on unsegmented sequences. Training loop: In most sequences, there is a period of inactivity between every two successive motions, called idle period or stagnation state. In order to distinguish between such an idle period (State \"0\") and a period of activity (State \"1\") and detect the transition of state, we use a binary detector. Sometimes, we have sequences in which there is not nec- essarily an idle separating between two actions. In this case, we use a multi-class object detector to detect the transition of the kinetic states between one action and another. Let N be the number of classes in the training dataset. While a subject is performing an action 𝑖, we attribute the state 𝑖to each possible subsequence extracted from its interval. When a transition from a state 𝑖to another state 𝑗is detected and verified, we consider that there is a transition from an action to another. Actually, in the training process, we extract from each sequence randomly subsequences with equal size: 𝑤𝑠frames. This size should be long enough for a person to perform part from an action (longer than half a second). We attribute to the subsequence the dominant state class, i.e., when the trained sample contains two different kinetic states, it will be labeled according to the longer state within it. Same as the classifier, we use the SPD Siamese Network to train the detector (see section 2). Testing loop: In the testing loop, we adopt the sliding window method proposed by [10], augmenting it by the trained detector and a Verification Process (V.P) that helps our system in reducing False Positive (FP) rate and giving more accurate segmentation. Unlike the method proposed by [11] who relies on clustering sub-activities using his proposed detector (that he calls SSD), we propose a su- pervised method to train our detector. Instead of having unknown numerous groups of sub-activities, we consider these sub-activities belonging to more general classes (\"0\" and \"1\" in the case of a binary detector). Besides, we use majority voting rather than Markov probability matrix for sequence segmentation. since in case of high similarity of sub-activities (like in \"Swipe X\" and \"Swipe V\"), the perfor- mance of Markov probability matrix in predicting the classes of the consecutive sub-activities is generally not accurate. Let 𝑤𝑠be the size of the window (the same size of the training subsequences), 𝑟be the refresh rate of the detector and 𝑐𝑟be the capturing rate of the used sensor. Each 𝑟frames, the detector reveals the kinetic state on each window, i.e., the segments on which the detector is applied are [0, 𝑤𝑠], [𝑟,',\n",
       "  'of the window (the same size of the training subsequences), 𝑟be the refresh rate of the detector and 𝑐𝑟be the capturing rate of the used sensor. Each 𝑟frames, the detector reveals the kinetic state on each window, i.e., the segments on which the detector is applied are [0, 𝑤𝑠], [𝑟, 𝑤𝑠+𝑟], [2𝑟, 𝑤𝑠+2𝑟]... As we are dealing with continuous sequences, we have to set 𝑟≤0.3 ∗𝑐𝑟frames and ensure that the running time of the detector does not exceed 𝑟frames. M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 3 of 8 Online motion recognition When the state of the detector moved from a state 𝐴to a different state 𝐵, we need to verify if it is a real change of a kinetic state or a false detection. To this end, we realize a verification process based on a bunch of repetitive tests to confirm this transition. Let 𝑡𝑒be the number of these tests. The verification starts from the second test until the 𝑡𝑒𝑡ℎtest while retaining the states of the detector within each window. We use the method of \"majority voting\" on states resulting from each test to make our decision on the transition to be checked. For example (see Figure 5), with 𝑡𝑒 = 5, when a binary detector detects a transi- tion in the kinetic state from state \"0\" to \"1\" at the win- dow [𝑁−𝑤𝑠, 𝑁], we realize 4 other tests on the segments [𝑁+ 𝑘× 𝑟−𝑤𝑠, 𝑁+ 𝐾× 𝑟](𝑘=1,2,3,4). We suppose that the states detected are [0, 1, 1, 1, 0]. The change of state is con- firmed in 3 tests among 5. So, we can conclude that (𝑁−𝑟)𝑡ℎ frame is the starting frame of an action. It is important to notice that the detected starting frame is recognized only after (𝑡𝑒× 𝑟) frames. So, we need to minimize 𝑡𝑒and 𝑟as much as possible, relying on a high performing detector with fast reaction time. 3.2. Early classification In the context of daily human activities [9, 14] or hand gesture context [15, 16], we can tolerate a delay of no more than one second. However, in industrial context [17] no delay is tolerated, the online recognition should be instantaneously even before the action is finished (Early classification is needed (see Figure 6). We define 𝑇as the maximum time used to recognize a motion. In order to ensure that we will not exceed 𝑇seconds, we set 𝑡𝑒≤ 𝑇 𝑟∗𝑐𝑟. Once a frame is recognized as the starting frame of a motion, we wait until the next 𝑇frames coordinates are provided. Then the classifier recognizes the motion. If the end frame of a motion is detected before reaching the deadline we set, the classifier will give the class of the motion between the two predicted boundaries, just like the no-early classification case. 4. Experiments and results Our method works for both online hand and action recog- nition purpose. Thus, we evaluate it on four challenging datasets captured in different contexts. In the following, we extensively provide details on the experimental settings and results obtained for each dataset. We also compare the state- of-the-art',\n",
       "  'Experiments and results Our method works for both online hand and action recog- nition purpose. Thus, we evaluate it on four challenging datasets captured in different contexts. In the following, we extensively provide details on the experimental settings and results obtained for each dataset. We also compare the state- of-the-art methods to our approach using online metrics. Our model is implemented on an octa-core CPU running at 3.2 GHz with 32GB RAM in python 3.9.7 environment. In the whole sequence, R.T denotes running time. 4.1. Datasets Online Dynamic Hand Gesture dataset (ODHG) [15]: It consists of 280 relatively long video clips taken with 28 volunteers. Each video contains 10 hand gestures. The data is captured by the depth camera (30 fps) and consists of 14 hand gesture sequences performed in two ways: using one finger and the whole hand. SHREC 2021 gesture benchmark dataset [16]: It consists of 180 unsegmented hand skeletal sequences cap- tured by LeapMotion sensor at rate of 25 fps. 60% of these sequences are used in the training loop and and the rest are dedicated for the test. The dataset dictionary is made of 17 gestures divided in static gestures and dynamic gestures. Online Action Detection dataset (OAD) [9]: It in- cludes 59 long sequences representing 839 actions, captured at 8 fps. The possible actions revolve around 10 action classes. It gives the coordinates of 25 joints from different body parts. We have 30 sequences for training and 20 se- quences for testing. UOW Online Action 3D dataset [14]: It consists of 48 skeleton sequences recorded using the kinect V2 sensor (20 fps). It has 21 action classes. Without a stagnation between two consecutive actions, its actions are performed with two manners: repeatably and continuously. The sequences containing repeated actions are used in the training and the continuous sequence is dedicated for the testing loop. Industrial Human Action Recognition Dataset (In- Hard) [17]: It is collected in industrial context of human robot collaboration. It contains 38 long sequences: 26 se- quences used in the training and 12 sequences are dedicated for the test. The possible actions are 13 Meta actions. The skeletal data comprises the 3D coordinates and the 3 rota- tions around the axis of 21 body joints. 4.2. Evaluation of the classifier on segmented sequences The experiments performed for the evaluation of the classifier follow the concept of the segmented motion recog- nition. The training process and the validation of the classi- fication are based on the proposed SPD Siamese network. First of all, we start by data cleaning to remove corrupt or inaccurate records. For InHard dataset, we remove two long sequences P04_R01 and P04_R02 in which there is no de- scription of the evolution of the 3D coordinates. Moreover, for this dataset, we remark that working with the derivates of the sequences gives better performance comparing to that given by the position evolution studying. After data preprocessing, we prepare the segmented sequences. Using the groundtruth information, we extract the actions sequences performed within the continuous se- quences with respect to their train/test protocols mentioned',\n",
       "  'that working with the derivates of the sequences gives better performance comparing to that given by the position evolution studying. After data preprocessing, we prepare the segmented sequences. Using the groundtruth information, we extract the actions sequences performed within the continuous se- quences with respect to their train/test protocols mentioned in their description. In the partitioning component (as illus- trated in Figure 3), we follow the matrix representation of the hand parts for ODHG and SHREC 2021 datasets and the matrix presentation of the body parts for the other datasets. As preprocessing, the input sequences are normalized and interpolated to 500 frames for ODHG and SHREC 2021, to 200 frames for OAD and UOW datasets and to 600 frames for InHard. The difference of the number of the interpolated frames are due to the difference between the capturing rate and the average duration of the motion in each dataset. For the rest of the components, we keep the same configurations set on the experiment section of [4]. M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 4 of 8 Online motion recognition Partioning 1 Partioning 2 Ours 1 0 6 7 21 22 12 13 14 15 16 17 18 19 8 9 10 11 23 24 5 4 20 2 3 Acc: 94.46% Acc: 94.81% Acc: 96.19% Table 1 SPD Siamese network performance with respect to partitioning strategy on OAD segmented sequences. Table 2 SPD Siamese network performance on the datasets classifiers Dataset ODHG SHREC OAD UOW InHard Accuracy(%) 95.60 95.92 96.19 98.84 79.38 Running time(ms) 173 253 138 118 352 Body partitioning strategies: Different partitioning strategies are assessed and compared in order to select the more efficient one for body skeleton. As illustrated in Table 1, three strategies are considered. The first proposed by [18], divide the skeleton into 5 parts, the second, proposed by Ji et al. [19] divide the body into 10 parts and the third is the one we proposed. Experiments, conducted on segmented sequences of OAD dataset have proven the efficiency of the third strategy we propose in term of classification accuracy. This can be explained by the fact that our strategy created more interconnection between adjacent joints and enhanced it in the convolution component. In the following, we exploit it for the action recognition context. Classifiers performance: Using the settings described in this section and the the body/hand partitioning mentioned in section 2.2, we apply the SPD Siamese network on the different benchmarks: ODHG and SHREC 2021 for hand gesture recognition and OAD, UOW and InHard datasets for body action recognition. According to the Table 2, the per- formance of the proposed classifier are proved on both hand gesture and action recognition scenarios. In fact, an accuracy over 95% is achieved in almost experimented datasets except for InHard which is a new dataset very challenging one and is collected in real industrial environment. The achieved per- formances on action recognition datasets demonstrate that we have succeeded to generalize the SPD Siamese network on body skeleton sequences with an acceptable running time. 4.3. Ablation study of the',\n",
       "  'for InHard which is a new dataset very challenging one and is collected in real industrial environment. The achieved per- formances on action recognition datasets demonstrate that we have succeeded to generalize the SPD Siamese network on body skeleton sequences with an acceptable running time. 4.3. Ablation study of the detector and online experiments There are different levels of configurations to consider for the proposed detector: the one related to neural network and classification performance and the configuration of slid- ing window and verification process parameters. The neural network of our detector, the SPD Siamese neural network, is kept with the same parameters set for the classifier. Only interpolation step is considered with fewer number of frames since the detector training sequences has shorter. Besides, many experiments are conducted to evaluate our system in term of running speed. For the evaluation of the detection process, we use the following metrics: Jaccard index, predic- tion accuracy, prediction accuracy, F1-score, SL-score, EL- score. To better understand these metrics, we refer readers to [9, 16]. Influence of window size variation We recall that window size has to be long enough to detect the kinetic of human in continuous sequences but not too long that it exceeds some human action duration or affects the detector rapidity. The kinetic state needs to be detected 4 or 5 times per second at least. We set 𝑟= 6, 5, 2, 4, 6 frames to be the refresh rates respectively of ODHG, SHREC 2021, OAD, UOW and InHard datasets. Then, we vary 𝑤𝑠values corresponding to each dataset (see Table 3). The different variations of 𝑤𝑠regarding each dataset is due to the difference of the sensors capturing rate. According to the results described in Table 3, the detector running time did not exceed 120 ms even when varying 𝑤𝑠in different intervals. It enables us to work online and detect the kinetic state 4 or 5 times per second, which is an acceptable frequency on human action recognition task. Regarding the performance of the detector, we remark that the accuracy of the detector is enhanced by the elevation of the window size in the datasets. This is expected since with this elevation, the detector is able to generate more information about the motion evolution. However, the be- havior of the other metrics is different, as they continue to rise until a peak value ̂𝑤𝑠is reached. ( ̂𝑤𝑠=18, 20, 6, 12, 18 are respectively the peak values of window sizes for ODHG, SHREC 2021, OAD, UOW and InHard datasets). For a longer window size, the reported performances shrink. This can be explained by to the fact that the exaggerated increase of windows size makes the window covering rapidly different transitions of kinetic status in the case of actions with short duration. This causes ambiguity in identifying the true kinetic state and affects the system performance, especially in motion boundaries prediction (SL-score and EL-score remarkably decrease). Reducing the number of frames excessively will lead to a lack of available informa- tion about the ongoing state for the detector and thus will lead to difficulty',\n",
       "  'ambiguity in identifying the true kinetic state and affects the system performance, especially in motion boundaries prediction (SL-score and EL-score remarkably decrease). Reducing the number of frames excessively will lead to a lack of available informa- tion about the ongoing state for the detector and thus will lead to difficulty in the online recognition. It is remarkable that the best performances correspond to a window size compromise between 0.6s and 0.8s. This Time interval can be considered to be the interval of visibility for the detector. Influence of the V.P parameters variation In this experiment, we vary 𝑡𝑒: the number of tests performed by the detector during the verification process. The results are shown in the Table 4. We remark that the best performance is given by 𝑡𝑒= 3 for all the datasets except OAD for which the peak is given by 𝑡𝑒= 2. This difference is due to different reasons. Firstly, the detector performance of OAD dataset is the highest performance comparing to the other detectors. Since the tests are performed in order to reduce the error rate at the detector level, it is expected that the detector of the higher performance will need a smaller M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 5 of 8 Online motion recognition Table 3 Model performance for different settings of 𝑤𝑠 ODHG SHREC 2021 OAD UOW Online Action 3D 𝑤𝑠(frames) 12 18 24 15 20 30 4 6 10 8 12 20 Detector R.T (ms) 93 100 105 52 58 69 77 85 99.7 80 82 85 Detector accuracy (%) 73.11 75.89 77.12 92.76 92.82 93.31 86.97 88.34 89.34 61.13 76.09 77.5 Detection accuracy 0.723 0.771 0.739 0.737 0.770 0.615 0.619 0.901 0.801 0.802 0.84 0.812 SL-score 0.627 0.676 0.645 0.674 0.694 0.544 0.561 0.796 0.715 0.708 0.747 0.719 EL-score 0.601 0.661 0.632 0.658 0.691 0.526 0.593 0.803 0.715 0.73 0.733 0.725 Gloabl F1-score 0.713 0.769 0.732 0.696 0.720 0.632 0.763 0.915 0.859 0.852 0.881 0.862 Table 4 Evolution of model performance according to 𝑡𝑒 ODHG SHREC 2021 OAD UOW Online Action 3D InHard te 3 4 5 3 5 2 3 5 2 3 5 3 Detector accuracy 75.89% 92.82% 88.34% 76.09% 67.33% Detection accuracy 0.771 0.752 0.742 0.770 0.737 0.901 0.829 0.801 0.74 0.84 0.414 0.675 SL-score 0.676 0.645 0.63 0.694 0.665 0.796 0.762 0.727 0.675 0.747 0.351 0.578 EL-score 0.661 0.645 0.65 0.691 0.66 0.803 0.775 0.762 0.664 0.733 0.341 0.564 Gloabl F1-score 0.769 0.736 0.746 0.720 0.716 0.915 0.89 0.876 0.826 0.881 0.499 0.661 number of tests. More importantly, the actions durations are arbitrary in OAD dataset: some actions last less than half a second and others last more than half a minute. The detector extends the window size to 10 frames (1.25s) and may reach the idle period while performing 3 tests. In contrast, motions in other datasets example are more regular and without stagnation. Working with 𝑡𝑒> 5 gives performances slightly inferior to those reported with 𝑡𝑒= 3 and with more time. So, we prefer to not exceed 3 tests. 4.4. Early classification experiments We set different',\n",
       "  'while performing 3 tests. In contrast, motions in other datasets example are more regular and without stagnation. Working with 𝑡𝑒> 5 gives performances slightly inferior to those reported with 𝑡𝑒= 3 and with more time. So, we prefer to not exceed 3 tests. 4.4. Early classification experiments We set different deadlines for classification: 𝑇= [0.5s, 1s...3s]. We evaluate the performance of our system and to what extent it can maintain its efficiency. The obtained F1- scores are described in Figure 6. Observing the obtained curves, we remark that the model didn’t show an efficiency in the lower deadlines. This can be explained that human activities generally need more than a second to be completely performed in the most cases. Higher than one second, the curves show F1-scores amelioration especially for OAD, UOW and SHREC 2021 which approach rapidly to the ideal case. For ODHG and InHard, it needs more time to move towards the F1-score of the no-early classification case because the performances of their detectors are lower than those of the other datasets. Also, they have some motions which are performed in the same manner at the beginning. For example, the first part of gesture \"Tap\" and gesture \"Swipe Down\" in ODHG dataset are too similar. 4.5. Comparison with the state-of-the art In order to assert the efficiency of our system, we need to compare its performance with the previous proposed approaches. Our experiments are reported on both online hand gesture and online action recognition contexts. Comparison of Running Speed: Experiments on answer time are carried out on SHREC 2021 challenge [16] and comparison is made with different approaches proposed by four groups who give their perfor- mances on this dataset. The average running time taken by Group Class time (s) Total time (s) 1 [12, 16] 1.36 435.5 2 [20, 16] 0.41 48781.0 3 [21, 16] 0.6 × 10-4 66.7 4 [22, 16] 0.066 94.6 𝑂𝑢𝑟𝑠 0.038 3836.88 Table 5 Execution time corresponding to different approaches on SHREC 2021 the detector to identify the kinetic state in each window and in the whole sequences are described in Table 5. We remark that the average running times per single prediction (class time) are very discarded. Our system takes about 69 ms to give a single prediction, enabling it to identify the kinetic states of more than 12 windows per second in average. This frequency is acceptable in real live applications. It is faster than approaches used by groups 1, 2 and group 4. However, group 3 proposed a faster approach, thanks to the simple complexity of its model which helps for better responding time but not highly efficient in term of performance 6. For the global classification, our system takes about 3837 seconds to compute all the test sequences. These sequences contain more than 119K frames, capturing at 25 fps. They last about 4700s, which is close to the total time taken by our system because the refresh rate of the system is designed to make the system execution in phase with real streaming. State-of-the-art on hand gesture context datasets: For',\n",
       "  'contain more than 119K frames, capturing at 25 fps. They last about 4700s, which is close to the total time taken by our system because the refresh rate of the system is designed to make the system execution in phase with real streaming. State-of-the-art on hand gesture context datasets: For hand gesture recognition, we evaluate and compare the performance of our online system on SHREC 2021 dataset [16]. Best performances of research groups par- ticipating in SHREC 2021 challenge are reported in Table 6. Only skeletal based approaches are considered for this comparison. The proposed metrics for evaluation in this challenge are Detection rate, Jaccard index and FP rate. According to Table 6, our system reveals interesting performance and shows its superiority over the majority of the proposed models. Only adapted ST-GCN approach seems to outperform our system performance. This can be M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 6 of 8 Online motion recognition Method Detection rate Jaccard index FP rate Dissimilarity-based Classification [12, 16] 0.3993 0.2566 0.764 Transformer module [20, 16] 0.7292 0.6029 0.257 uDeepGRU+TSGR [21, 16] 0.7431 0.6238 0.271 ST-GCN [22, 16] 0.8993 0.8526 0.066 Ours 0.770 0.667 0.230 Table 6 Performance comparison with the state-of-the-art online meth- ods on SHREC 2021 gesture benchmark dataset Table 7 Performance comparison with the state-of-the-art methods on body action context datasets Dataset OAD UOW InHard Method Metric F1- score Pred. Acc F1- score Pred. Acc F1- score Pred. Acc RNN-SW [23] 0.600 - - - - - JCR-RNN [9] 0.653 0.788 - - - - ST-LSTM [24] - 0.770 - - - - Attention Net [25] - 0.783 - - - - RF+ST [26] 0.672 - - - - - MM-MT-RNN [27] 0.795 - - - - - FSNet [28] - 0.800 - - - SSNet [28] - 0.820 - - - VGG16+L1 [29] - 0.868 - - - - SW-CNN [10] - - 0.680 0.680 - - SW-GCN [10] - - 0.750 0.755 - - Our model 0.915 0.901 0.881 0.840 0.661 0.631 explained by the fact that the used approach is not only based on online ST-GCN but uses also Trajectory-based fine tuning approach to simplify the recognition in some specific gestures. Besides, energy-based detection module seems to be a good choice on such context. State-of-the-art on body action context datasets: Comparison with state-of-the-art approaches on online action recognition datasets regarding the commonly used metrics F1-score and prediction accuracy are reported in Table 7. We can notice that our model outperforms other models, especially in UOW Online Action 3D dataset, which witnessed an increase of 17%. Besides, we achieve the best performance on OAD dataset in term of F1-score and prediction accuracy. This proves the efficiency of the de- tector, the key role played by the SPD Siamese model in ensuring the quality of the results given by the system and the importance of the tests realized in the beginning and the end of each action. Finally, promising results are highlighted in the challenging InHard dataset which represents specific industiral context. In fact, actions in continuous sequences are performed',\n",
       "  'in ensuring the quality of the results given by the system and the importance of the tests realized in the beginning and the end of each action. Finally, promising results are highlighted in the challenging InHard dataset which represents specific industiral context. In fact, actions in continuous sequences are performed without stagnation periods and with human- robot collaboration scenarios in some cases. 5. Conclusion We proposed an accurate manifold-based approach which works either for hand gesture sequences or for human body action recognition. We built an online system composed from a detector and a classifier. The detector is used for the prediction of the action intervals using a verification process and the sliding window approach. The classifier uses the segmentation predicted by the detector for motion recognition. We provided the experimental evaluation on five benchmark datasets and compared our approach with the recent state of the art methods. However, our approach can be more optimized, especially the classifier architecture which can help to obtain lower execution time. As future work, we plan to study more the human motion recognition challenges especially in industrial context such as human robot interaction in real case study and in virtual environ- ment. References [1] Zhiwu Huang, Jiqing Wu, and Luc Van Gool. Building deep networks on grassmann manifolds. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. [2] Zhiwu Huang and Luc Van Gool. A riemannian network for spd matrix learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017. [3] Raviteja Vemulapalli, Felipe Arrate, and Rama Chellappa. Human action recognition by representing 3d skeletons as points in a lie group. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 588–595, 2014. [4] Mohamed Akremi., Rim Slama., and Hedi Tabia. Spd siamese neural network for skeleton-based hand gesture recognition. In Proceedings of the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 4: VISAPP,, pages 394–402. INSTICC, SciTePress, 2022. [5] Okan Köpüklü, Ahmet Gunduz, Neslihan Kose, and Gerhard Rigoll. Real-time hand gesture detection and classification using convolu- tional neural networks. In 2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019), pages 1–8. IEEE, 2019. [6] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning on lie groups for skeleton-based action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6099–6108, 2017. [7] Xuan Son Nguyen, Luc Brun, Olivier Lézoray, and Sébastien Bougleux. A neural network based on spd manifold learning for skeleton-based hand gesture recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12036–12045, 2019. [8] Maxime Devanne, Panagiotis Papadakis, et al. Recognition of activities of daily living via hierarchical long-short term memory networks. In 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), pages 3318–3324. IEEE, 2019. [9] Yanghao Li, Cuiling Lan, Junliang Xing, Wenjun Zeng, Chunfeng Yuan, and Jiaying Liu. Online human action detection using joint classification-regression recurrent neural networks. In European conference on computer vision, pages 203–220. Springer,',\n",
       "  'memory networks. In 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), pages 3318–3324. IEEE, 2019. [9] Yanghao Li, Cuiling Lan, Junliang Xing, Wenjun Zeng, Chunfeng Yuan, and Jiaying Liu. Online human action detection using joint classification-regression recurrent neural networks. In European conference on computer vision, pages 203–220. Springer, 2016. [10] Mickael Delamare, Cyril Laville, Adnane Cabani, and Houcine Chafouk. Graph convolutional networks skeleton-based action recog- nition for continuous data stream: A sliding window approach. In VISIGRAPP (5: VISAPP), pages 427–435, 2021. [11] Farhood Negin, Abhishek Goel, Abdelrahman G Abubakr, Francois Bremond, and Gianpiero Francesca. Online detection of long- term daily living activities by weakly supervised recognition of sub- activities. In 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), pages 1–6. IEEE, 2018. [12] Ariel Caputo, Andrea Giachetti, Franca Giannini, Katia Lupinetti, Marina Monti, Marco Pegoraro, and Andrea Ranieri. Sfinge 3d: A novel benchmark for online detection and recognition of hetero- geneous hand gestures from 3d fingers’ trajectories. Computers & Graphics, 91:232–242, 2020. [13] Mingze Xu, Mingfei Gao, Yi-Ting Chen, Larry S Davis, and David J Crandall. Temporal recurrent networks for online action detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5532–5541, 2019. M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 7 of 8 Online motion recognition [14] Chang Tang, Pichao Wang, and Wanqing Li. Online action recogni- tion based on incremental learning of weighted covariance descrip- tors. arXiv preprint arXiv:1511.03028, 2015. [15] Quentin De Smedt, Hazem Wannous, and Jean-Philippe Vandeborre. Skeleton-based dynamic hand gesture recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2016. [16] Ariel Caputo, Andrea Giachetti, Simone Soso, Deborah Pintani, Andrea D’Eusanio, Stefano Pini, Guido Borghi, Alessandro Simoni, Roberto Vezzani, Rita Cucchiara, et al. Shrec 2021: Skeleton-based hand gesture recognition in the wild. Computers & Graphics, 99:201– 211, 2021. [17] M. DALLEL, V. HAVARD, D. BAUDRY, and X. SAVATIER. Inhard - industrial human action recognition dataset in the context of indus- trial collaborative robotics. In 2020 IEEE International Conference on Human-Machine Systems (ICHMS), pages 1–6, 2020. [18] Liafisu Sina Yekini, Tomasz Piotr Wisniewski, and Yuval Millo. Market reaction to the positiveness of annual report narratives. The British Accounting Review, 48(4):415–430, 2016. [19] Xiaopeng Ji, Jun Cheng, Wei Feng, and Dapeng Tao. Skeleton embedded motion body partition for human action recognition using depth sequences. Signal Processing, 143:56–68, 2018. [20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. At- tention is all you need. Advances in neural information processing systems, 30, 2017. [21] Mehran Maghoumi. Deep recurrent networks for gesture recognition and synthesis. Ph.D. thesis, 2020. [22] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for skeleton-based action recognition. In Thirty-second AAAI conference on artificial intelligence, 2018. [23] Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li Shen, and Xiaohui Xie. Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks. In Proceedings of the AAAI conference on artificial intelligence, vol- ume 30, 2016.',\n",
       "  'In Thirty-second AAAI conference on artificial intelligence, 2018. [23] Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li Shen, and Xiaohui Xie. Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks. In Proceedings of the AAAI conference on artificial intelligence, vol- ume 30, 2016. [24] Jun Liu, Amir Shahroudy, Dong Xu, Alex C Kot, and Gang Wang. Skeleton-based action recognition using spatio-temporal lstm network with trust gates. IEEE transactions on pattern analysis and machine intelligence, 40(12):3007–3021, 2017. [25] Jun Liu, Gang Wang, Ping Hu, Ling-Yu Duan, and Alex C Kot. Global context-aware attention lstm networks for 3d action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1647–1656, 2017. [26] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Real-time online action detection forests using spatio-temporal contexts. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 158–167. IEEE, 2017. [27] Jiaying Liu, Yanghao Li, Sijie Song, Junliang Xing, Cuiling Lan, and Wenjun Zeng. Multi-modality multi-task recurrent neural network for online action detection. IEEE Transactions on Circuits and Systems for Video Technology, 29(9):2667–2682, 2018. [28] Jun Liu, Amir Shahroudy, Gang Wang, Ling-Yu Duan, and Alex C Kot. Skeleton-based online action prediction using scale selection network. IEEE transactions on pattern analysis and machine intelli- gence, 42(6):1453–1467, 2019. [29] Nassim Mokhtari, Alexis Nédélec, and Pierre De Loor. Human activity recognition: A spatio-temporal image encoding of 3d skeleton data for online action detection. In VISIGRAPP (5: VISAPP), pages 448–455, 2022. Window size Detector queue Skeletal sequence stream Detector KINETIC STATE No change Change VP VP SF EF Classifier Motion Next refresh frame Figure 1: The proposed online motion recognition system. CONTRASTIVE LOSS FUNCTION ST-TS-HGR-NET PARTITIONING SPDNET ST-TS-HGR-NET PARTITIONING SPDNET LABEL 1 LABEL 2 BINARY LABEL PREPROCESSING PREPROCESSING KNN LABEL 1 KNN LABEL 2 Figure 2: The overview of the proposed SPD Siamese network. 16 17 16 17 17 18 16 17 16 17 19 20 16 17 16 17 13 14 16 17 16 17 15 16 16 17 16 17 9 10 16 17 16 17 11 12 16 17 16 17 5 6 16 17 16 17 7 8 16 17 16 17 1 2 16 17 16 17 3 4 (a) (b) 4 3 2 1 16 15 14 13 12 11 10 9 8 7 6 5 20 19 18 17 16 17 16 17 24 23 16 17 16 17 11 10 16 17 16 17 19 18 16 17 16 17 17 16 16 17 16 17 15 14 16 17 16 17 13 12 16 17 16 17 22 21 16 17 16 17 7 6 16 17 16 17 9 8 16 17 16 17 20 2 16 17 16 17 0 1 16 17 16 17 20 2 16 17 16 17 0 1 16 17 16 17 20 2 16 17 16 17 5 4 16 17 16 17 20 2 17 17 3 17 17 3 17 17 3 17 17 3 1 0 6 7 21 22 12 13 14 15',\n",
       "  '16 17 20 2 16 17 16 17 0 1 16 17 16 17 20 2 16 17 16 17 5 4 16 17 16 17 20 2 17 17 3 17 17 3 17 17 3 17 17 3 1 0 6 7 21 22 12 13 14 15 16 17 18 19 8 9 10 11 23 24 5 4 20 2 3 Figure 3: Skeleton parts and matrix representation for (a) the hand and (b) the body. M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 8 of 8 Online motion recognition GA REEIG LOGEIG VECMAP GA TS-GA-NET GA REEIG LOGEIG VECMAP GA TS-GA-NET SPDAGG LOGEIG VECMAP FC CONVOLUTION CONVOLUTION SPDC Figure 4: ST-TS-HGR-NET architecure ws r Possible start frame Frame of decision Tests performed by the detector Kinetic state A # Kinetic state B Figure 5: Verification process with 𝑡𝑒= 5. 0.5 1 1.5 2 2.5 3 EF Deadlines(seconds) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 F1-score ODHG SHREC2021 OAD UOW InHard Figure 6: Model behavior toward early classification different deadlines. M.S.AKREMI, R.SLAMA, H.TABIA: Preprint submitted to Elsevier Page 9 of 8'],\n",
       " ['Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding Xinheng He1#, Yijia Zhang2,3#, Haowei Lin4, Xingang Peng4, Xiangzhe Kong5, Mingyu Li6,7, and Jianzhu Ma2,3⋆ 1 Lingang Laboratory, Shanghai, China 2 Department of Electronic Engineering, Tsinghua University, Beijing, China 3 Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China 4 Institute for Artificial Intelligence, Peking University, Beijing, China 5 Department of Computer Science and Technology, Tsinghua University, Beijing, China 6 Department of Pharmaceutical and Artificial-Intelligence Sciences, Shanghai Jiao Tong University School of Medicine, Shanghai, China 7 Institute of Medical Artificial Intelligence, Shanghai Jiao Tong University, Shanghai, China majianzhu@tsinghua.edu.cn Abstract. Structure-based drug design has seen significant advancements with the integration of artificial intelligence (AI), particularly in the gener- ation of hit and lead compounds. However, most AI-driven approaches ne- glect the importance of endogenous protein interactions with peptides, which may result in suboptimal molecule designs. In this work, we present Pep- tide2Mol, an E(3)-equivariant graph neural network diffusion model that gen- erates small molecules by referencing both the original peptide binders and their surrounding protein pocket environments. Trained on large datasets and leveraging sophisticated modeling techniques, Peptide2Mol not only achieves state-of-the-art performance in non-autoregressive generative tasks, but also produces molecules with similarity to the original peptide binder. Addition- ally, the model allows for molecule optimization and peptidomimetic design through a partial diffusion process. Our results highlight Peptide2Mol as an effective deep generative model for generating and optimizing bioactive small molecules from protein binding pockets. Keywords: Small molecule design · Diffusion model · Structure based drug design · Peptide mimicry ⋆Corresponding author. arXiv:2511.04984v1 [cs.LG] 7 Nov 2025 2 X. He et al. 1 Introduction Small molecules have long been the cornerstone of drug discovery due to their ease of synthesis, cell permeability, oral bioavailability, and cost-effectiveness in manufacturing [1,2]. In contrast, peptides, despite their high affinity and specificity for protein targets, often suffer from poor membrane permeability and metabolic instability, which severely limit their therapeutic application [3]. To combine the strength of both modalities, recent strategies aim to transform native peptide or protein binders into small molecules that preserve key binding interactions [4]. This concept has been validated in several landmark cases, such as the conversion of the snake-venom peptide Teprotide into the antihyper- tensive drug Captopril and the rational design of the HIV protease inhibitor Saquinavir through peptide bond isosteres [5,6]. However, these successes remain isolated, and no systematic or scalable framework exists for general peptide-to-small-molecule conversion. With the rapid advancement of artificial intelligence (AI), especially the remarkable success of generative models, drug design has entered a new era [7,8,9,10,11,12]. Early gen- erative models learned structural distributions from known ligands [13,14], while recent methods incorporate pocket structures to generate target-specific molecules [10,15,16]. This shift is motivated by the recognition that incorporating receptor-specific informa- tion is vital for drug design, because only through precise binding to the target protein can a drug exert its therapeutic effect [17]. Recent advancements in predictive modeling have provided promising approaches for molecular generation. Modern all-atom models have demonstrated the capability to pre-',\n",
       "  'the recognition that incorporating receptor-specific informa- tion is vital for drug design, because only through precise binding to the target protein can a drug exert its therapeutic effect [17]. Recent advancements in predictive modeling have provided promising approaches for molecular generation. Modern all-atom models have demonstrated the capability to pre- dict small molecule-protein complexes with atomic-level precision [18]. Furthermore, work from the Baker group has shown has demonstrated that deep learning can be used to de- sign diverse, high-affinity protein binders [19,20]. However, small molecule generation encompasses a broader chemical space and presents challenges in terms of validation, which can be costly and time-consuming [21]. Despite this, most existing models focus solely on small molecule-protein complex data, often overlooking the abundant and biologically significant protein-protein and protein-peptide interactions [22]. This narrow focus on protein-ligand complexes intro- duces several challenges. This limited scope leads to a lack of diversity in the generated small molecules, as the available protein-ligand complexes often represent similar scaf- folds, thereby constraining the exploration of novel molecular designs [23,24]. Moreover, current models fail to incorporate protein-protein or protein-peptide interaction structural data, despite the growing emphasis on mimicking peptide binders in small molecule de- sign. Consequently, a significant gap exists in the ability to effectively link small molecules and peptides/proteins at the atomic scale for generation. To address this gap, we propose Peptide2Mol, the first AI model that learns to translate peptide or protein binding interfaces into small molecules directly in three- dimensional space. Peptide2Mol is formulated as an E(3)-equivariant graph neural net- work (EGNN) diffusion model, trained on diverse datasets encompassing small-molecule conformation ensembles, protein–ligand complexes, and both experimentally determined and computationally predicted protein–protein interactions. This design enables Pep- tide2Mol to generate target-aware small molecules that reliably mimic peptide-like bind- ing interactions while maintaining favorable drug-like properties towards the target pro- tein. Such an algorithm does not conflict with existing diffusion based methods [10,11,25] and can be combined to generate small molecule binders to mimic peptide behavior. Finally, by analyzing antibody-antigen surface interactions, we identify preferred chemi- Title Suppressed Due to Excessive Length 3 cal groups for replacing amino acids, which provides valuable insights into the design of peptide mimicry. 2 Methods Fig. 1: Overview of the Peptide2Mol model. (a) Dataset composition, training, and in- ference workflow. The model is trained on peptide and small molecule structures, with inference generating candidate ligands for target protein pockets. (b) Schematic repre- sentation of edge for non-covalent interactions between ligands and the protein pocket. (c) Model architecture of Peptide2Mol. 2.1 Dataset Construction To construct our training dataset, we aggregated molecular structures from multiple sources. Small molecules were obtained from the Geometric Ensemble Of Molecules (GEOM) [26] drug dataset, while protein-ligand and protein-peptide complexes were obtained from the PDBBind [27] and BioLip2 [28] databases. Protein-peptide interac- tion were also derived from the monomeric models in the AlphaFold Database [29]. In these models, loops that are fully buried and exhibit interactions with other parts are treated as ligands, while the remaining parts are considered receptors. All molecular data were filtered using RDKit [30] to',\n",
       "  'databases. Protein-peptide interac- tion were also derived from the monomeric models in the AlphaFold Database [29]. In these models, loops that are fully buried and exhibit interactions with other parts are treated as ligands, while the remaining parts are considered receptors. All molecular data were filtered using RDKit [30] to ensure successful parsing, yielding a final dataset com- prising 304,322 drug-like small molecules, 38,860 protein-ligand complexes, and 39,499 peptide-protein interfaces. For evaluation, we assembled a test set comprising 10 protein-ligand complexes ran- domly selected from the CrossDock2020 dataset, consistent with prior publications [15,31]. The selected complexes correspond to PDB IDs: 1BVR, 1ZYU, 2ATI, 4BNW, 5G3N, 1U0F, 2AH9, 2HW1, 4I91, and 5LVQ. Additionally, we included antibody-antigen pairs sourced from the Structural Antibody Database (SAbDab) for showing the replacement of residues by small molecule fragments [32]. 4 X. He et al. 2.2 Model overview Peptide2Mol is a non-autoregressive diffusion based generative model designed to gener- ate and optimize small molecules within protein pockets, leveraging peptide-binder struc- tural data. As shown in Figure 1a, the model is trained on a curated dataset combining small-molecule conformation ensembles [26], protein-ligand/peptide complexes [27,28], and protein-peptide models [29]. During training, ligand geometries undergo progressive disruption via a diffusion process, while peptide sidechains are partially diffused and binding pocket residues remain fixed. This framework establishes an invertible mapping between the base Gaussian distribution and the ground truth graph. At inference, the model iteratively transfers Gaussian noise into molecular structures at each step un- til convergence. Non-covalent interactions within 5Å are explicitly modeled to capture pocket–ligand contacts (Figure 1b). As depicted in Figure 1c, the model embeds ligand and pocket atoms into node and edge features, augmented with timestep embeddings and pocket embedding to distinguish atomic contexts. These representations are processed by six E(3)-equivariant GNN layers that iteratively update atomic features and coordinates through rotation-equivariant con- volutions. Finally, the refined embeddings are decoded into molecular graphs. Pocket2Mol [33] can be optionally used to resolve steric clashes to further refine ligand-pocket com- plementarity. 2.3 Molecular Featurization Ligands and ligand–protein complexes were represented as undirected atomic graphs, denoted as M = (V, E). Each node vi ∈V corresponds to an atom and is associated with two attributes: its spatial coordinate ri ∈R3 and its element-type feature ai ∈R8, where ai is implemented as a one-hot encoding over common atom symbols (C, N, O, F, P, S, Cl, Br). Each edge eij ∈E corresponds to an atom pair and is described by a bond feature vector bij ∈R6, encoding single, double, triple, aromatic, and non-bonded proximity interactions, plus an absorbing state for no interaction. 2.4 Diffusion Model Architeture A diffusion model is then employed in the generation process, characterized by two Markov random processes. The forward process incrementally introduces noise to the data accord- ing to a predefined noise schedule, while the reverse process leverages neural networks to denoise the data, ultimately reconstructing the original data from the noise. Let the superscript t denote variables at time step t with M0 representing the 3D molecule or complex drawn from the real distribution. At each',\n",
       "  'ing to a predefined noise schedule, while the reverse process leverages neural networks to denoise the data, ultimately reconstructing the original data from the noise. Let the superscript t denote variables at time step t with M0 representing the 3D molecule or complex drawn from the real distribution. At each step, Mt is sampled from the condi- tional distribution q(Mt | Mt−1), dependent solely on Mt−1: q(Mt | Mt−1, Mt−2, · · · , M0) := q(Mt | Mt−1) (1) For atom positions ri, atom types ai and bond types bij, which are discrete, categorical distributions are used for their representation. The forward process is defined as: q(rt i | rt−1 i ) := N(rt i | p 1 −βtrt−1 i , βtI) (2) q(at i | at−1 i ) := N(at i | p 1 −βtat−1 i , βtI) (3) q(bt ij | bt−1 ij ) := N(bt ij | p 1 −βtbt−1 ij , βtI) (4) Title Suppressed Due to Excessive Length 5 with βt ∈[0, 1] denotes the predefined noise scaling schedule, I ∈R3×3 is the identity matrix. For atom position ri, atom type ai, and bond type bij, scaled standard Gaussian noise is incrementally added. In addition, time embedding and a binary pocket indicator (0/1) were concatenated with the node embeddings, resulting in a unified representation that integrates atomic, temporal, and contextual information. Leveraging the Markov property, M can be directly derived from the original sample M0. By defining αt := 1 −βt, and ¯αt := Qt s=1 αs, the sample can be expressed as the following equations: where ¯αt = Qt s=1(1 −βs) denotes the fraction of information retained at step t. From this formulation, ¯αt represents the proportion of information from the original data retained at step t. We refer to ¯αt as the \"information level,\" which is determined by the noise level βt. q(rt i | r0 i ) := N(rt i | √ ¯αtr0 i , (1 −¯αt)I) (5) q(at i | a0 i ) := N(at i | √ ¯αta0 i , (1 −¯αt)I) (6) q(bt ij | b0 ij) := N(bt ij | √ ¯αtb0 ij, (1 −¯αt)I) (7) As t →∞, the atom positions, types and bond types approximate a standard Gaussian distribution. These resulting prior distributions serve as the initial distributions for the reverse process. In the reverse process, we invert the Markov chain to reconstruct the original sample from prior distributions, employing E(3)-equivariant neural networks to parameterize the transition probability pθ(Mt−1 | Mt). Specifically, we model all the three predicted distri- butions as a Gaussian distribution N(Xt−1 i | (µθ(Mt, t), Σt)), where X represents variable and µθ is the neural network. The neural network is trained to recover Mt−1 from Mt by optimizing the predicted distribution pθ(Mt−1 | Mt) to approximate the true posterior q(Mt−1 | Mt, M0). During training, the loss function was defined in equations (8)–(11). Lt−1 pos = 1 N X i ∥rt−1 i −µθ(Mt, t)i∥2 (8) Lt−1 atom = 1 N X i ∥at−1 i −µθ(Mt, t)i∥2 (9) Lt−1 bond = 1 N X',\n",
       "  'Mt) to approximate the true posterior q(Mt−1 | Mt, M0). During training, the loss function was defined in equations (8)–(11). Lt−1 pos = 1 N X i ∥rt−1 i −µθ(Mt, t)i∥2 (8) Lt−1 atom = 1 N X i ∥at−1 i −µθ(Mt, t)i∥2 (9) Lt−1 bond = 1 N X i ∥bt−1 ij −µθ(Mt, t)i∥2 (10) Lt−1 = Lt−1 pos + λaLt−1 atom + λbLt−1 bond (11) where λa and λb were set both 30 for atom and bond. A timestep t was randomly sampled during training and the neural network was applied to recover the unbiased molecule, where its parameters was optimized by minimizing the loss Lt−1. In inference process, we sample atom position, type and symmetric bond type in Gaussian distribution and repeatedly sample from t = T, T −1, · · · , 1 to denoise the molecule. 2.5 Molecule Generation Inference process is used for the generation of small molecules. During inference, the model takes the receptor pocket originally defined by the peptide–protein interface but does not include the peptide backbone as a structural scaffold. Instead, the model initializes from Gaussian noise and iteratively denoises the atomic positions, atom types, and bond connectivity to generate a small molecule directly within the peptide’s binding pocket. 6 X. He et al. In this way, the generated molecules adopt drug-like geometries while preserving the essential interaction pattern of the original peptide because of the diverse training data. This design allows Peptide2Mol to effectively translate peptide binding interfaces into small molecule mimetics rather than reconstructing peptide structures. Peptide2Mol can also make molecule optimization and peptidomimetic design when pointing fixed atoms during diffusion process. After generation, a pocket-aware refinement stage can be applied using the Pocket2Mol optimization module [33]. This step performs local atom and bond adjustment to remove steric clashes, correct unreasonable geometries, and improve shape complementarity be- tween the ligand and pocket. Such refinement is necessary because diffusion sampling may yield high-energy or overlapping conformations that are not physically realizable. The Pocket2Mol-based relaxation ensures that the final small molecules correspond to chemically valid, energetically stable binding poses consistent with the protein pocket environment. 2.6 Fragmentation of Small Molecules To identify which fragments were most frequently used to replace residue side chains, we filtered the SabDab dataset for complementarity-determining region (CDR) domains in complex with antigens. Antigens were defined as residues within 5Å of the CDR domain, and only complexes where the number of antigen residues exceeded the number of CDR residues were retained. These CDRs were then converted into small molecules using Pep- tide2Mol, and the resulting molecules were fragmented based on their rotatable bonds. A fragment was defined as replacing an amino acid if it was located within 4 Å of any heavy atom of the residue. To rank the likelihood of fragment–residue replacements, we computed the Pointwise Mutual Information (PMI) as follows: PMI(res, frag) = log \\x12 P(res, frag) P(res) · P(frag) \\x13 (12) 3 Results 3.1 Benchmarking Molecular Properties of Peptide2Mol We first assessed the general properties of molecules generated by Peptide2Mol, focus- ing on evaluating their chemical',\n",
       "  'likelihood of fragment–residue replacements, we computed the Pointwise Mutual Information (PMI) as follows: PMI(res, frag) = log \\x12 P(res, frag) P(res) · P(frag) \\x13 (12) 3 Results 3.1 Benchmarking Molecular Properties of Peptide2Mol We first assessed the general properties of molecules generated by Peptide2Mol, focus- ing on evaluating their chemical validity and plausibility. To this end, we selected and computed the following evaluation metrics, which have been widely adopted in previ- ous studies to characterize the properties of sampled candidates [33,34,35]: (1) QED (Quantitative Estimation of Drug-likeness) [36], quantifies the likelihood of a molecule being a viable drug candidate based on its physicochemical properties and conformity to drug-like characteristics; (2) SA (Synthetic Accessibility) [37], measuring the ease of molecular synthesis, with higher scores indicating greater synthetic feasibility; (3) LogP (Octanol–Water Partition Coefficient), a metric of molecular hydrophobicity derived from the distribution between octanol and aqueous phases; and (4) PBrate (PoseBusters pass- ing rate) [38] integrates 19 criteria to comprehensively assess docking quality, including molecular structural integrity and conformational validity, which provides a rigorous and comprehensive measure of docking plausibility and makes it a reliable benchmark for assessing generative models. Title Suppressed Due to Excessive Length 7 Table 1: The comparison of properties of the generated molecules in the test set. Method QED(↑) SA(↑) LogP PBrate(%. ↑) LiGAN [31] 0.428 0.546 1.224 39.50 Pocket2Mol [33] 0.587 0.758 1.063 71.60 TargetDiff [25] 0.430 0.550 1.249 36.90 PocketFlow [15] 0.497 0.769 3.521 46.00 Peptide2Mol 0.501 0.612 0.638 45.30 Peptide2Mol-Fixed 0.509 0.637 0.729 83.80 Table 1 summarizes the generative performance of Peptide2Mol compared with repre- sentative molecular generation methods on the same benchmark used in LiGAN [31] and PocketFlow [15]. In terms of QED, our model (0.501) already surpasses LiGAN (0.428) and TargetDiff (0.430), and achieves a comparable level to PocketFlow (0.497), situating it within a competitive range. For SA, while peptide-like molecules naturally score lower compared to small-molecule–oriented methods such as Pocket2Mol and PocketFlow, Pep- tide2Mol maintains parity with LiGAN and TargetDiff, highlighting its balance between peptide-specific features and overall synthetic feasibility. Regarding hydrophobicity, the lower LogP values generated by Peptide2Mol reflect the intrinsic physicochemical prop- erties of peptides, making the results consistent with the intended design space. Fig. 2: Waterfall diagram illustrating the stepwise evaluation of AI-generated molecules against the PoseBusters criteria. Each method was designed to generate 100 molecules per target across the testset targets. Panels show results for LiGAN (a), Pocket2Mol (b), TargetDiff (c), PocketFlow (d), Peptide2Mol (e), and Peptide2Mol- Fixed (f). 8 X. He et al. Although Peptide2Mol does not achieve the highest scores in QED or SA, its per- formance is comparable to established approaches trained exclusively on small-molecule datasets. Importantly, when a partially masked autoregressive refinement step is applied (Peptide2Mol-Fixed), the overall chemical validity is further improved, yielding the high- est PBrate (83.80%). This indicates that molecules generated by Peptide2Mol, although not optimized exclusively for drug-likeness metrics, achieve competitive quality and can be effectively refined to ensure robust structural integrity and docking plausibility. To further dissect the structural quality of the generated molecules, we visualize the in- dividual contributions',\n",
       "  'high- est PBrate (83.80%). This indicates that molecules generated by Peptide2Mol, although not optimized exclusively for drug-likeness metrics, achieve competitive quality and can be effectively refined to ensure robust structural integrity and docking plausibility. To further dissect the structural quality of the generated molecules, we visualize the in- dividual contributions to the PoseBusters score using a waterfall plot (Figure 2). This rep- resentation highlights which specific geometric and conformational criteria most strongly influence the PBrate for each method. For instance, Pocket2Mol demonstrates strong per- formance in satisfying bond length distribution constraints and in generating molecules with favorable internal energies. In contrast with Pocket2Mol, Peptide2Mol achieves su- perior control over intermolecular distance constraints with the target, thereby effec- tively reducing steric clashes. Leveraging these complementary strengths, we employed Pocket2Mol for partially-refinement of our generated molecules, which yielded the most favorable overall evaluation outcomes. 3.2 Bond Length Distribution Analysis In addition to the benchmark comparison, we further examined the bond length distri- butions of generated molecules. As shown in Figure 3a-i, Nine kinds of chemical bonds are analyzed, including C-C, C=C, C-O, C=O, C-N, C=N, C-Cl, C-S, and C-F. Fig. 3: Geometric and property-based evaluation of generated molecules. (a–i) Bond length distributions of molecules generated by different AI-based methods compared with those in the training set. Nine representative bond types are analyzed: C–C (a), C=C (b), C–O (c), C=O (d), C–N (e), C=N (f), C–Cl (g), C–S (h), and C–F (i). Title Suppressed Due to Excessive Length 9 Notably, the results from Peptide2Mol closely match the overall distribution of the training dataset, while also notably capturing the characteristic bond length patterns specific to peptides. This highlights the model’s ability to generate peptide-like molecules that are both chemically realistic and structurally consistent with experimental observa- tions. 3.3 Residue replacement analysis To investigate the residue-level mimicry capability of Peptide2Mol, we applied the model to an antibody–antigen dataset to generate small-molecule fragments substituting native antibody side chains. Four representative residues—tyrosine (Y), aspartic acid (D), argi- nine (R), and leucine (L)—were analyzed (Figure 4). Fragments were ranked by PMI with the corresponding residue, reflecting association strength. High-PMI fragments gen- erally preserve key chemical features: tyrosine substitutes retain aromatic or hydroxyl groups; aspartic acid replacements are enriched in polar oxygen-containing groups; argi- nine mimics maintain nitrogen-rich motifs; and leucine substitutes comprise carbon-rich hydrophobic chains. These results indicate that Peptide2Mol generates chemically plau- sible, residue-specific side-chain mimics while allowing structural diversity. Fig. 4: The histogram to show the top replacement fragment from small molecules with 4 representative residues (ARG, ASP, LEU and TYR), the color reflects the composition proportion of elements (green: Carbon, Blue: Nitrogen, Red: Oxygen, Gray, others) 4 Discussion In this work, we introduced Peptide2Mol, a structure-based generative framework de- signed to bridge the gap between peptides and small molecules in drug discovery. Un- like previous generative models that primarily focus on small-molecule–protein com- plexes, Peptide2Mol integrates structural information from both protein–ligand and pro- tein–protein (or protein–peptide) interactions. This enables the model to translate peptide or antibody CDR binders into small molecules that mimic their native binding modes',\n",
       "  'small molecules in drug discovery. Un- like previous generative models that primarily focus on small-molecule–protein com- plexes, Peptide2Mol integrates structural information from both protein–ligand and pro- tein–protein (or protein–peptide) interactions. This enables the model to translate peptide or antibody CDR binders into small molecules that mimic their native binding modes (Fig. 5). This design enables the generation of peptide-mimicking small molecules that preserve the functional essence of native residues while retaining drug-like chemical prop- erties or generate peptidomimetics from original peptide. 10 X. He et al. Fig. 5: Representative examples showing that Peptide2Mol can transform (a) a peptide binder (PDB ID: 7WXO) and (b) an antibody CDR (PDB ID: 3NGB) into corresponding small molecules that mimic their binding interfaces. One strength of Peptide2Mol is its principled use of diverse structural datasets in train- ing. Existing models often inherit biases from protein–ligand complexes [15,33,34,35]. By systematically incorporating both experimentally determined and computationally pre- dicted peptide and protein interaction data, Peptide2Mol broadens the generative chemi- cal space. This approach improves the diversity of generated molecules, while still yielding competitive performance in standard benchmarks. Importantly, refinement with a par- tially masked autoregressive step significantly improved structural plausibility, achieving the highest PoseBusters passing rate, thereby demonstrating the potential of combining complementary modeling strategies. Despite these advances, several limitations remain. The generated molecules tend to inherit physicochemical features closer to peptides than to conventional small molecules, which may explain their modest performance on QED and SA relative to models opti- mized exclusively for drug-likeness. While this aligns with the goal of peptide mimicry, practical applications will require balancing peptide-like fidelity with pharmacokinetic constraints [39]. Moreover, although we demonstrated residue-level replacement analysis, the current model does not yet provide a quantitative metric for peptide–small molecule similarity. Looking forward, we envision several directions for extending this work. One is to couple Peptide2Mol with physics-based simulation pipelines to assess stability and bind- ing mechanisms beyond docking scores [40]. Moreover, systematic benchmarking across a broader range of “undruggable” protein–protein interaction targets will be critical to Title Suppressed Due to Excessive Length 11 establish the generality of this approach and to uncover patterns of residue substitution that may inform rational drug design [41]. In conclusion, Peptide2Mol represents a step toward unifying peptide- and small- molecule–based design strategies. By capturing the structural logic of peptide binders while ensuring drug-like feasibility, our framework highlights a new frontier for genera- tive drug discovery. Just as the development of protein language models expanded the interpretability of sequence variation, the integration of peptide-derived binding infor- mation into molecular generation holds promise to unlock new chemical modalities and accelerate the translation of peptide insights into therapeutically viable small molecules. 5 Code Availability The source code, pretrained models, and a minimal test dataset for Peptide2Mol are publicly available at https://github.com/BLUE-Flowing/Peptide2Mol/. References 1. K. Wu, S. H. Kwon, X. Zhou, C. Fuller, X. Wang, J. Vadgama, and Y. Wu, Overcoming challenges in small- molecule drug bioavailability: A review of key factors and approaches, International Journal of Molecular Sciences, 2024, 25(23), 13121. 2. A. M. Vargason, A. C. Anselmo, and S.',\n",
       "  'https://github.com/BLUE-Flowing/Peptide2Mol/. References 1. K. Wu, S. H. Kwon, X. Zhou, C. Fuller, X. Wang, J. Vadgama, and Y. Wu, Overcoming challenges in small- molecule drug bioavailability: A review of key factors and approaches, International Journal of Molecular Sciences, 2024, 25(23), 13121. 2. A. M. Vargason, A. C. Anselmo, and S. Mitragotri, The evolution of commercial drug delivery technologies, Nature Biomedical Engineering, 2021, 5(9), 951–967. 3. M. Muttenthaler, G. F. King, D. J. Adams, P. F. Alewood: Trends in peptide drug discovery. Nature Reviews Drug Discovery 20(4), 309–325 (2021). 4. W. Brytan and L. Padrela, Structural modifications for the conversion of proteins and peptides into stable dried powder formulations: A review, Journal of Drug Delivery Science and Technology, 2023, 89, 104992. 5. C. Odaka and T. Mizuochi, Angiotensin-converting enzyme inhibitor captopril prevents activation-induced apoptosis by interfering with T cell activation signals, Clinical & Experimental Immunology, 2000, 121(3), 515–522. 6. N. A. Roberts, J. A. Martin, D. Kinchington, A. V. Broadhurst, J. C. Craig, I. B. Duncan, S. A. Galpin, B. K. Handa, J. Kay, A. Kröhn, et al., Rational design of peptide-based HIV proteinase inhibitors, Science, 1990, 248(4953), 358–361. 7. A. Gangwal and A. Lavecchia, Unleashing the power of generative AI in drug discovery, Drug Discovery Today, 2024, 29(6), 103992. 8. X.-h. He, J.-r. Li, J. Xu, H. Shan, S.-y. Shen, S.-h. Gao, and H. E. Xu, AI-driven antibody design with generative diffusion models: current insights and future directions, Acta Pharmacologica Sinica, 2025, 46(3), 565–574. 9. S. Luo, Y. Su, X. Peng, S. Wang, J. Peng, and J. Ma, Antigen-specific antibody design and optimization with diffusion-based generative models for protein structures, Advances in Neural Information Processing Systems, 2022, 35, 9754–9767. 10. H. Lin, Y. Huang, O. Zhang, S. Ma, M. Liu, X. Li, L. Wu, J. Wang, T. Hou, and S. Z. Li, Diffbp: Generative diffusion of 3D molecules for target protein binding, Chemical Science, 2025, 16(3), 1417–1431. 11. L. Huang, T. Xu, Y. Yu, P. Zhao, X. Chen, J. Han, Z. Xie, H. Li, W. Zhong, K.-C. Wong, et al., A dual diffusion model enables 3D molecule generation and lead optimization based on target pockets, Nature Communications, 2024, 15(1), 2657. 12. S. Luo, J. Guan, J. Ma, J. Peng: A 3D generative model for structure-based drug design. Advances in Neural Information Processing Systems 34, 6229–6239 (2021). 13. W. J. Godinez, E. J. Ma, A. T. Chao, L. Pei, P. Skewes-Cox, S. M. Canham, J. L. Jenkins, J. M. Young, E. J. Martin, W. A. Guiguemde: Design of potent antimalarials with generative chemistry. Nature Machine Intelligence 4(2), 180–186 (2022). 14. V. Bagal, R. Aggarwal, P. K. Vinod, U. D. Priyakumar: MolGPT: molecular generation using a transformer- decoder model. Journal of Chemical Information and Modeling 62(9), 2064–2076 (2021). 15. Y. Jiang, G. Zhang, J. You, H. Zhang, R. Yao, H. Xie, L. Zhang, Z. Xia, M. Dai, Y. Wu, et al.: Pocketflow is a data-and-knowledge-driven structure-based molecular generative model. Nature Machine Intelligence 6(3), 326–337 (2024). 16. S. Choi, S. Seo, B. J. Kim, C. Park, S. Park: PIDiff: Physics informed diffusion model for protein',\n",
       "  'You, H. Zhang, R. Yao, H. Xie, L. Zhang, Z. Xia, M. Dai, Y. Wu, et al.: Pocketflow is a data-and-knowledge-driven structure-based molecular generative model. Nature Machine Intelligence 6(3), 326–337 (2024). 16. S. Choi, S. Seo, B. J. Kim, C. Park, S. Park: PIDiff: Physics informed diffusion model for protein pocket- specific 3D molecular generation. Computers in Biology and Medicine 180, 108865 (2024). 12 X. He et al. 17. R. Santos, O. Ursu, A. Gaulton, A. P. Bento, R. S. Donadi, C. G. Bologa, A. Karlsson, B. Al-Lazikani, A. Hersey, T. I. Oprea, et al.: A comprehensive map of molecular drug targets. Nature Reviews Drug Discovery 16(1), 19–34 (2017). 18. R. Roy, H. M. Al-Hashimi: AlphaFold3 takes a step toward decoding molecular behavior and biological computation. Nature Structural & Molecular Biology 31(7), 997–1000 (2024). 19. J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, et al., De novo design of protein structure and function with RFdiffusion, Nature, 2023, 620(7976), 1089–1100. 20. D. R. Fox, C. Taveneau, J. Clement, R. Grinter, and G. J. Knott, Code to complex: AI-driven de novo binder design, Structure, 2025, in press. 21. X. Zeng, F. Wang, Y. Luo, S.-G. Kang, J. Tang, F. C. Lightstone, E. F. Fang, W. Cornell, R. Nussinov, F. Cheng: Deep generative molecular design reshapes drug discovery. Cell Reports Medicine 3(12), 108865 (2022). 22. J. F. Greenblatt, B. M. Alberts, N. J. Krogan: Discovery and significance of protein-protein interactions in health and disease. Cell 187(23), 6501–6517 (2024). 23. H. Zhu, X. Li, B. Chen, N. Huang: Augmented BindingNet dataset for enhanced ligand binding pose predic- tions using deep learning. npj Drug Discovery 2(1), 1 (2025). 24. O. Zhang, H. Lin, H. Zhang, H. Zhao, Y. Huang, C.-Y. Hsieh, P. Pan, T. Hou: Deep lead optimization: leveraging generative AI for structural modification. Journal of the American Chemical Society 146(46), 31357–31370 (2024). 25. J. Guan, W. W. Qian, X. Peng, Y. Su, J. Peng, J. Ma: 3D equivariant diffusion for target-aware molecule generation and affinity prediction. arXiv preprint arXiv:2303.03543 (2023). 26. S. Axelrod, R. Gomez-Bombarelli: GEOM, energy-annotated molecular conformations for property prediction and molecular generation. Scientific Data 9(1), 185 (2022). 27. R. Wang, X. Fang, Y. Lu, C.-Y. Yang, S. Wang: The PDBbind database: methodologies and updates. Journal of Medicinal Chemistry 48(12), 4111–4119 (2005). 28. C. Zhang, X. Zhang, L. Freddolino, Y. Zhang: BioLiP2: an updated structure database for biologically relevant ligand–protein interactions. Nucleic Acids Research 52(D1), D404–D412 (2024). 29. M. Varadi, S. Anyango, M. Deshpande, S. Nair, C. Natassia, G. Yordanova, D. Yuan, O. Stroe, G. Wood, A. Laydon, et al., AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models, Nucleic Acids Research, 2022, 50(D1), D439–D444. 30. G. Landrum: RDKit: A software suite for cheminformatics, computational chemistry, and predictive model- ing. Available at: https://www.rdkit.org (2013). 31. M. Ragoza, T. Masuda, D. R. Koes: Generating 3D molecules conditional on receptor binding sites with deep generative models. Chemical Science 13, 2701–2713 (2022).',\n",
       "  'Nucleic Acids Research, 2022, 50(D1), D439–D444. 30. G. Landrum: RDKit: A software suite for cheminformatics, computational chemistry, and predictive model- ing. Available at: https://www.rdkit.org (2013). 31. M. Ragoza, T. Masuda, D. R. Koes: Generating 3D molecules conditional on receptor binding sites with deep generative models. Chemical Science 13, 2701–2713 (2022). 32. D. James, K. Konrad, L. Jinwoo, B. Terry, F. Angelika, G. Guy and S. Jiye and D. Charlotte, SAbDab: the structural antibody database, Nucleic acids research, 2014, 42(D1), D1140–D1146. 33. X. Peng, S. Luo, J. Guan, Q. Xie, J. Peng, J. Ma: Pocket2mol: efficient molecular sampling based on 3D protein pockets. In: International Conference on Machine Learning, pp. 17644–17655 (2022). 34. X. Peng, J. Guan, Q. Liu, J. Ma: MolDiff: addressing the atom-bond inconsistency problem in 3D molecule diffusion generation. arXiv preprint arXiv:2305.07508 (2023). 35. X. Zhou, J. Guan, Y. Zhang, X. Peng, L. Wang, J. Ma: Reprogramming pretrained target-specific diffusion models for dual-target drug design. Advances in Neural Information Processing Systems 37, 87255–87281 (2024). 36. G. R. Bickerton, G. V. Paolini, J. Jérémy Besnard, S. Muresan, A. L. Hopkins: Quantifying the chemical beauty of drugs. Nature Chemistry 4(2), 90–98 (2012). 37. P. Ertl, A. Schuffenhauer: Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of Cheminformatics 1, 1–11 (2009). 38. M. Buttenschoen, G. M. Morris, C. M. Deane: PoseBusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences. Chemical Science 15(9), 3130–3139 (2024). 39. A. Datta-Mannan: Mechanisms influencing the pharmacokinetics and disposition of monoclonal antibodies and peptides. Drug Metabolism and Disposition 47(10), 1100–1110 (2019). 40. N. van Hilten, J. Methorst, N. Verwei, and H. J. Risselada, Physics-based generative model of curvature sensing peptides; distinguishing sensors from binders, Science Advances, 2023, 9(11), eade8839. 41. Q. Sun, H. Wang, J. Xie, L. Wang, J. Mu, J. Li, Y. Ren, and L. Lai, Computer-Aided Drug Discovery for Undruggable Targets, Chemical Reviews, 2025, in press.'],\n",
       " ['Code Review Automation using Retrieval Augmented Generation QIANRU MENG, Leiden University, The Netherlands XIAO ZHANG, University of Groningen, The Netherlands ZHAOCHUN REN, Leiden University, The Netherlands JOOST VISSER, Leiden University, The Netherlands Code review is essential for maintaining software quality but is labor-intensive. Automated code review generation offers a promising solution to this challenge. Both deep learning-based generative techniques and retrieval-based methods have demonstrated strong performance in this task. However, despite these advancements, there are still some limitations where generated reviews can be either off-point or overly general. To address these issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages Retrieval- Augmented Generation (RAG) to combine retrieval-based and generative methods, explicitly incorporating external domain knowledge into the code review process. RARe uses a dense retriever to select the most relevant reviews from the codebase, which then enrich the input for a neural generator, utilizing the contextual learning capacity of large language models (LLMs), to produce the final review. RARe outperforms state-of- the-art methods on two benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively. Its effectiveness is further validated through a detailed human evaluation and a case study using an interpretability tool, demonstrating its practical utility and reliability. ACM Reference Format: Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser. 2018. Code Review Automation using Retrieval Augmented Generation. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym ’XX). ACM, New York, NY, USA, 20 pages. https://doi.org/XXXXXXX. XXXXXXX 1 Introduction Code review is an essential component of software quality assurance, requiring substantial human effort. To reduce this effort, automated code review generation has been used to assist human reviewers, for instance by alleviating their workload through the suggestion of review comments, or to replace human reviewers by autonomously generating reviews that can be used immediately by developers. Various approaches have been employed to automate the code review process, evolving from initial rule-based static analysis techniques [1, 4, 7, 22, 31] to more advanced deep learning (DL) methods [14, 24, 27, 28, 39, 41]. These DL-based methods address the code review task as a generation task, employing pre-trained language models [24, 27, 41] or LLMs [28] to produce reviews. Alternatively, code review can also be formulated as an information retrieval task, where a given code change is used as a query to search for the most relevant reviews from a code review Authors’ Contact Information: Qianru Meng, q.r.meng@liacs.leidenuniv.nl, Leiden University, The Netherlands; Xiao Zhang, xiao.zhang@rug.nl, University of Groningen, The Netherlands; Zhaochun Ren, z.ren@liacs.leidenuniv.nl, Leiden University, The Netherlands; Joost Visser, j.m.w.visser@liacs.leidenuniv.nl, Leiden University, The Netherlands. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to',\n",
       "  'for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ’XX, Woodstock, NY © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX , Vol. 1, No. 1, Article . Publication date: November 2018. arXiv:2511.05302v1 [cs.SE] 7 Nov 2025 2 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser corpus. The retrieval-based approach has demonstrated its effectiveness with greater computational efficiency compared to DL-based generation methods [16]. Although both retrieval-based and DL-based generative methods have shown effectiveness in code review generation, each still comes with limitations. On one hand, retrieval-based methods, as noted by prior research [16], heavily rely on existing reviews in the training dataset, which restricts their ability to generate new reviews. On the other hand, DL-based generative methods can compensate for this limitation by learning patterns from the given code-review context, thereby generating new reviews. In recent years, LLMs have demonstrated impressive capabilities, particularly when fine-tuned with domain-specific data [28]. However, despite these advancements, significant challenges still remain in applying LLMs effectively to code review tasks. LLM-generated reviews often suffer from a lack of domain-specific knowledge, leading to outputs that are either overly general or lacking in relevance [24]. Moreover, while fine-tuning can improve the specificity of reviews by applying the model to learn patterns, it could results in reviews that are off-point due to the model’s overfitting on the limited training data. To mitigate these issues, integrating retrieval-based methods alongside generative models offers a promising solution. By introducing a wide-ranging external knowledge base, retrieval methods can provide relevant example reviews, thereby enriching the generated content with insights from similar past cases. This hybrid approach has the potential to significantly improve the relevance, accuracy, and contextual understanding of the generated reviews. Motivated by the limitations of both retrieval-based and generative approaches, we introduce the Retrieval-Augmented Reviewer (RARe), a novel method that combines the strengths of both paradigms for automated code review generation. Fundamentally, RARe embodies the concept of retrieval-augmented generation (RAG), which improves the quality and relevance of generative models by incorporating external knowledge from a retrieval codebase. RARe is composed of two main components: the retriever and the generator. The retriever first uses the given code snippet to search for the most relevant reviews within the codebase. These retrieved reviews, combined with the given code snippet and instructions, form a prompt that is then fed into the generator. The generator, utilizing this comprehensive prompt, produces a review tailored to the context. Extensive experiments demonstrate that RARe outperforms state-of-the-art methods in the code review generation task across two benchmark datasets, achieving the best BLEU-4 scores of 12.31 and 12.96, respectively. Compared to its performance without retrieval augmentation, RARe shows remarkable improvements, with a 153% increase in BLEU-4,',\n",
       "  'review tailored to the context. Extensive experiments demonstrate that RARe outperforms state-of-the-art methods in the code review generation task across two benchmark datasets, achieving the best BLEU-4 scores of 12.31 and 12.96, respectively. Compared to its performance without retrieval augmentation, RARe shows remarkable improvements, with a 153% increase in BLEU-4, a 240% rise in ROUGE-L, and a 228% gain in METEOR. In addition, we conduct a comprehensive human evaluations and a case study analysis using an interpretability tool to further confirm the effectiveness of RARe. The detailed investigation results show that by recommending relevant examples and leveraging the contextual understanding capabilities of LLMs, RARe effectively reduce the number of overly general or off-point reviews, providing more accurate and relevant reviews. Our main contributions are as follows: • We are the first to apply the RAG technique to the code review generation task, achieving state-of-the-art results across two datasets compared with the baselines. • We conduct comprehensive comparisons among various retrievers (Normal Dense Retrieval and Dense Passage Retrieval) and generators (Llama 3.1, Mistral, and CodeGemma), providing a detailed analysis of these components. • We are the first to utilize an interpretability tool to provide a novel evaluation insight for code review generation. • All of our code is available in anonymous repository: https://anonymous.4open.science/r/ GAR-9EE2 , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 3 The structure of the paper is as follows. We introduce the related work in Section 2. Details of the RARe are provided in Section 3. In Section 4, we outline three research questions and detail the experiments conducted to investigate them. We then answer these questions in the analysis presented in Section 5 and Section 6. Implications and limitations are discussed in Section 7. Potential threats to validity are listed in Section 8. Finally, Section 9 concludes the paper. 2 Related Work 2.1 Automation of Code Review Generation Static analysis tools are commonly used in early studies [1, 7, 22, 31] to integrate predefined rules for automating review processes. For instance, Balachandran et al. [1] developed a Review Bot [1] to offer code modification suggestions by integrating outputs from multiple static analyzers. Recently, Palvannan et al. [31] developed a Suggestion Bot for GitHub, using a Python static analyzer. Moreover, Chatley et al. [4] also employs data mining techniques to extract relevant rules from history code commit information as predefined validation rules. However, traditional rule-base methods lack flexibility and portability. To overcome these limi- tations, some studies leverage deep learning techniques to enhance the relevance and semantic understanding of code and reviews. Gupta et al. [14] introduced DeepCodeReviewer, which employs deep learning to determine the relevance of reviews to a code snippet and recommends the most relevant review from a repository of historical reviews. Siow et al. [39] presented a deep learning method called CORE, which utilizes two-level embedding and an attention-based Long Short-Term Memory (LSTM) model to generate relevant reviews. With the advent of pre-trained sequence-to-sequence models (transformer-based [42]), the focus in code review task has shifted towards model pre-training,',\n",
       "  'of historical reviews. Siow et al. [39] presented a deep learning method called CORE, which utilizes two-level embedding and an attention-based Long Short-Term Memory (LSTM) model to generate relevant reviews. With the advent of pre-trained sequence-to-sequence models (transformer-based [42]), the focus in code review task has shifted towards model pre-training, particularly with the Text-to- Text Transfer Transformer (T5) framework [24, 27, 41]. Related studies tailor their pre-training framework to achieve different task objectives. The AUGER method [24] was specifically designed for code review generation, while the other approaches by CodeReviewer [27] and Tufano et al. [41] are adopted to various code review downstream tasks, including review necessity prediction, code review generation, and code refinement. Recently, the excellent contextual learning capabilities of LLMs have been demonstrated in code review generation task. Lu et al. [28] compared different training methods (LoRA [17] and prefix tuning [26]) for LLMs and proposed the LLaMA-Reviewer. It leverages the capabilities of the LLM (LLaMA [40]) to automatically generate code reviews through fine-tuning. Different from above deep learning based methods that consume substantial computational resources, to improve efficiency, Hong et al. [16] firstly proposed retrieval-based method for this task, which uses a given changed method as a query to retrieve the most similar changed methods from the code-review corpus. They represented code token vectors utilizing the Bag-of-Words model [46], and then employed cosine similarity along with a text similarity method known as Gestalt Pattern Matching (GPM) to identify the most relevant reviews. In conclusion, recent advanced methods addressing automatic code review generation as either a generation task or an information retrieval task, with both methods proving effective. However, it is worth noting that there is currently no work that combines the two methods and no work utilizes external knowledge to enhance the generation performance of the model. In this case, we propose to use the RAG method in code review generation task, which utilizes the in-context learning capabilities of LLMs to incorporate retrieval results into generation models. , Vol. 1, No. 1, Article . Publication date: November 2018. 4 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser 2.2 Retrieval-Augmented Generation The RAG technique has already been proposed in early development of neural models [20, 23] and applied in various domains, such as question answering [8, 15, 23, 25] and dialogue systems [5, 9, 21, 38], etc. Its expansion was previously limited because early neural models, lacking zero- shot learning capabilities, were confined to specific datasets and tasks and required fine-tuning for optimal performance. Performance would often degrade significantly due to changes in input formats. However, LLMs, such as GPT [3], Llama [40], Mistral [18], have reignited a broad interest in RAG [11]. The excellent contextual learning and understanding abilities allow LLMs to understand varying inputs (also known as prompts) and provide relevant responses [3]. This breakthrough makes the LLMs feasible to incorporate with the input with external information. And as expected, LLMs have shown to benefit from RAG, such as reducing the hallucinations[11]. Furthermore, the RAG method has also been employed in various code-related tasks. Parvez et al.',\n",
       "  'prompts) and provide relevant responses [3]. This breakthrough makes the LLMs feasible to incorporate with the input with external information. And as expected, LLMs have shown to benefit from RAG, such as reducing the hallucinations[11]. Furthermore, the RAG method has also been employed in various code-related tasks. Parvez et al. [33] applied the RAG method to code generation and summarization tasks. They employed the Dense Passage Retriever (DPR) [20] to encode both code and text, using the similarity of these encoded embeddings for retrieval. Based on the retrieved results, they then generated the corresponding output. Lu et al. [29] presented ReACC, a retrieval-augmented method for code completion, leveraging external context by retrieving codes that are semantically and lexically similar in the codebase. Nashid et al. [30] introduced CEDAR, which uses two different retrieval augmented strategies (embedding-based using SRoBERTA [36] and frequency-based using BM-25 [37]) for creating prompts, enabling the Codex [34] generator to produce targeted outputs for assertion generation and program repair tasks. Geng et al. [12] introduced RAG method in multi- intent code comment generation task, using two retrieval strategies (token-based and semantic- based) to retrieve the most similar code and comment pairs and then using Codex to generate code comments with corresponding intents. Zhang et al [45] applied RAG method into commit message generation. Their framework employs a hybrid retriever that combines sparse and dense retrieval methods, to enhance the generation performance of Pre-traing language models (PLM) and LLMs. The task of code review generation, different with code summarization and comment generation, requires that the model possess not only an understanding of the code but also domain-specific knowledge. Therefore, employing RAG for this task, which is capable of effectively leveraging external knowledge, is fully-motivated. 3 Methodology In this section, we introduce the architecture of RARe, focusing on its two key components: retriever and generator. We explain the method from a probabilistic perspective. Figure 1 illustrates the methodologies employed in our proposed RARe: Retrieval, Generation and RAG. In previous approaches, retrieval and generation were distinct processes. For example, the green pathway, labeled 1, represents the retrieval phase, where similar code snippets and their corresponding reviews are extracted from the codebase. The blue pathway, labeled 2, corresponds to the generation phase, where the input code is directly fed into a generative model to produce a review. The RAG process, shown by the orange pathway labeled 3, acts as a “bridge” between these two methods, integrating both retrieval and generation. The RAG methodology can be outlined across the following three stages: (1) Use a retrieval system to identify the top 𝑘most relevant code-review pairs from the codebase in relation to the target code. (2) Construct a comprehensive prompt for the generative model by combining the target code with the retrieved reviews. , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 5 Target Code Generator Review Retriever Top k Reviews Prompt Instruction Instruction ❖Target Code ❖Top k Review Query # Code Snippet # Review Top 1 def max(a, b): if a > b: Return a',\n",
       "  '1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 5 Target Code Generator Review Retriever Top k Reviews Prompt Instruction Instruction ❖Target Code ❖Top k Review Query # Code Snippet # Review Top 1 def max(a, b): if a > b: Return a return is not complete ... Top k Fig. 1. The overall architecture of RARe. Different colors distinguish the retrieval, generation, and RAG processes. Within the review of the target code, the text related to generation is marked in blue, while those related to retrieval are marked in green. (3) Feed this augmented prompt into the generator, allowing it to produce a review that is both contextually informed and tailored to the target code. 3.1 Retriever The key of retrieval process is to acquire representations of both code and review, subsequently comparing their similarities. This distinction determines whether the retriever is Normal Dense Retrieval (NDR) or Dense Passage Retrieval (DPR). NDR for code review generation task follows a uni-encoder architecture, which means that both code and review representations are generated by a single encoder. The probability model of NDR is shown in Equation 1: 𝑝(𝑧|𝑥) ∝exp(𝑐(𝑧)𝑇𝑐(𝑥)), (1) 𝑐(𝑧) = encoder𝑐(𝑧), (2) 𝑐(𝑥) = encoder𝑐(𝑥), (3) 𝑜= 𝑚(𝑧), (4) where 𝑥denotes a input code, 𝑧represents the retrieved code and 𝑜is the output review. 𝑐sym- bolizes the dense representation of the code, encoded by the 𝑒𝑛𝑐𝑜𝑑𝑒𝑟𝑐. The product between two representations represents their distance. The 𝑒𝑥𝑝function is used to ensure that the distance is positive. This distance directly correlates to the likelihood 𝑝of the code being retrieved. Specifically, Normal Dense Retrieval selects the code with closest representations from the codebase. Then the review associated with the retrieved code (store in the mapping 𝑚) is utilized as final output. Differently, Dense Passage Retrieval for code review generation follows a bi-encoder architecture, indicating that code and review representations are generated by two independent encoders. The probability model of DPR is shown in Equation 5: , Vol. 1, No. 1, Article . Publication date: November 2018. 6 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser 𝑝(𝑜|𝑥) ∝exp(𝑟(𝑜)𝑇𝑐(𝑥)), (5) 𝑟(𝑜) = encoder𝑟(𝑜), (6) 𝑐(𝑥) = encoder𝑐(𝑥), (7) where 𝑥denotes a input code and 𝑜represents the output review. 𝑟(𝑜) represents the dense representation of a review, encoded by the 𝑒𝑛𝑐𝑜𝑑𝑒𝑟𝑟. DPR trains two encoders simultaneously, one for code (encoder𝑐) and another for reviews (encoder𝑟), using contrastive loss to enhance retrieval by maximizing the similarity between relevant pairs [20]. In this case, DPR allows to directly compute the similarity between representations 𝑟(𝑜) and 𝑐(𝑥), thereby facilitating the direct comparison of code and its corresponding review. Therefore, DPR does not need to retrieve the code and map it to the review, allowing for the direct retrieval of reviews instead, thus streamlining the process. 3.2 Generator Transformers-based sequence-to-sequence models have become predominant in generation tasks, primarily encompassing two architectures: decoder-only and encoder-decoder. Concisely, in the encoder-decoder framework, the encoder is tasked with analyzing and understanding the input data, while the decoder is responsible for crafting output based on this acquired comprehension. On',\n",
       "  'the process. 3.2 Generator Transformers-based sequence-to-sequence models have become predominant in generation tasks, primarily encompassing two architectures: decoder-only and encoder-decoder. Concisely, in the encoder-decoder framework, the encoder is tasked with analyzing and understanding the input data, while the decoder is responsible for crafting output based on this acquired comprehension. On the other hand, the decoder-only architecture, prevalent in LLMs, specializes in generating content directly from the given context. The underlying probabilistic models of these two architectures are similar: 𝑝𝑒𝑛𝑐𝑜𝑑𝑒𝑟−𝑑𝑒𝑐𝑜𝑑𝑒𝑟(𝑜|𝑥) = 𝑛 Ö 𝑖=1 𝑝𝜃(𝑜𝑖|𝑜1:𝑖−1,𝑥), (8) 𝑝𝑑𝑒𝑐𝑜𝑑𝑒𝑟−𝑜𝑛𝑙𝑦(𝑜|𝑥) = 𝑛 Ö 𝑖=1 𝑝𝜃(𝑜𝑖|𝑥,𝑜1:𝑖−1), (9) where 𝑥is the input code processed by the encoder, 𝑜1:𝑖−1 represents the review sequence generated up to the current step, and 𝑜𝑖is the output being generated at the current step. 𝜃is the parameters of the encoder and decoder. 𝑝denotes the probability (likelihood) of generating an output sequence 𝑜given an input sequence 𝑥. The encoder-decoder model (as shown in Equation 8) first encodes the input coder 𝑥into a fixed-length vector representation using the encoder, and then the decoder uses this vector along with the previously generated outputs to generate the next output 𝑜𝑖. In contrast, for decoder-only models, both 𝑥and 𝑜1:𝑖−1 are provided to the model as a continuous sequence, and the model generates the next output 𝑜𝑖based on the entire context provided. In our work, we exclusively utilize LLMs with a decoder-only architecture, which leads us to mainly rely on Equation 9. 3.3 Retrieval-Augmented Reviewer The integration of retrieval and generation serves to enhance the code review process by leveraging external knowledge that directly informs the generation of output sequences. From a mathematical standpoint, the retrieval step introduces a probabilistic component, 𝑝(𝑜′|𝑥), that models the like- lihood of retrieving relevant reviews 𝑜′ given the input sequence 𝑥. This retrieval probability is then combined multiplicatively with the generation probability, as shown in Equation 10. The prod- uct of these probabilities indicates that both the retrieval and generation components contribute meaningfully to the final output. , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 7 𝑃𝑅𝐴𝑅𝑒(𝑜|𝑥) = 𝑝(𝑜′|𝑥)𝑝𝑑𝑒𝑐𝑜𝑑𝑒𝑟−𝑜𝑛𝑙𝑦(𝑜|𝑥,𝑜′) = 𝑝(𝑜′|𝑥) 𝑁 Ö 𝑖=1 𝑝𝜃(𝑜𝑖|𝑥,𝑜′,𝑜1:𝑖−1) (10) By integrating retrieved reviews, the model gains additional context beyond just the input sequence and training data, allowing it to generate more accurate and contextually relevant reviews by leveraging references. 4 Experiments In this section, we outline the research questions that guide our study and describe the experimental setup designed to address them. We detail the datasets used, the evaluation metrics employed, the baseline methods for comparison, the models involved, and the relevant experimental settings. 4.1 Research Questions We list three research questions to guide the rest of experiments, each targeting a distinct aspect of the evaluation: the performance of individual components, the overall performance of the proposed system, and a fine-grained analysis of the system’s performance. • RQ1: What are the performances of different retrievers and generators? We aim to separately compare the effectiveness of different retrievers and generators. • RQ2: What is the performance of RARe compared to the state-of-the-art method? This question is the main',\n",
       "  'system, and a fine-grained analysis of the system’s performance. • RQ1: What are the performances of different retrievers and generators? We aim to separately compare the effectiveness of different retrievers and generators. • RQ2: What is the performance of RARe compared to the state-of-the-art method? This question is the main focus of the paper. As introduced in Section 1 and Section 3, the retriever could offer external knowledge (similar reviews from the codebase) to the generator, which make it have potential to outperform the current models. • RQ3: What explanation can be provided for the performance of RARe? While performance scores offer a quantitative measure of RARe’s effectiveness, they don’t tell the whole story. We believe that human evaluation and interpretatbility of neural models are also crucial to demonstrate the impact of RARe. 4.2 Dataset We use two publicly available code review datasets for experiments: the CodeReviewer dataset (CRer.) [27] and the Tufano dataset (Tuf.) [41]. The CRer dataset collects code from GitHub reposi- tories across nine programming languages, including Java, Python, PHP, and others. In contrast, the Tuf dataset consists exclusively of Java code, sourced from both GitHub and Gerrit. Table 1 details the statistics of these two datasets. The code change inputs differ slightly between the two datasets, as illustrated in Table 1. The CRer dataset uses a code-difference format, where ′+′ and ′−′ symbols indicate the changes, whereas the Tuf dataset includes only the original code snippet. To give a clear insight of the formats, we provide two examples from these datasets in Table 2. Table 1. Statistics of CodeReviewer and Tufano dataset. Dataset Train Test Val Total CRer. ∼118k ∼10k ∼10k ∼138k Tuf. ∼134k ∼17k ∼17k ∼168k We do not apply any preprocessing to these datasets, since they have been already cleaned to reduce duplicates. The data split ratios are consistent with those used in previous works [27, 41]. , Vol. 1, No. 1, Article . Publication date: November 2018. 8 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser Table 2. Two examples of CRer. dataset and Tuf. dataset. CodeReviewer Code: @@ -53,7 +53,7 @@ public class ProtocGapicPluginGeneratorTest { model.getFiles().stream() .map(ProtoFile::getProto) .collect(Collectors.toList())) // Only the file to generate a client for (don\\'t generate dependencies) .addFileToGenerate(multiple_services.proto) - .setParameter(language=java) + .setParameter(language=java,transport=grpc) .build(); CodeGeneratorResponse response = ProtocGeneratorMain.generate(codeGeneratorRequest); CodeReviewer Review: Can we also test for ‘transport=rest‘? Tufano Code: public static <T> TypeToken<T> getTypeToken(TypeToken<T> token, Class<? super T> raw) { if (raw != null) return TypeToken.of((Class<T>) raw); if (token != null) return token; throw new IllegalStateException(\"Both indicators of type are null\"); } Tufano Review: Should this not prefer the token over the raw? 4.3 Evaluation Following prior works on code review generation [16, 24, 27, 28, 41], we apply three metrics in our model evaluation: BLEU-4, ROUGE-L, and METEOR. These metrics together provide a comprehensive evaluation of the quality of generated reviews, capturing different aspects of text similarity and fluency. BLEU [32] is the most widely used metric to measure the quality of generated reviews. Specifically, we use BLEU-4, which calculates the overlap of n-grams, where n ranges from 1 to 4.',\n",
       "  'provide a comprehensive evaluation of the quality of generated reviews, capturing different aspects of text similarity and fluency. BLEU [32] is the most widely used metric to measure the quality of generated reviews. Specifically, we use BLEU-4, which calculates the overlap of n-grams, where n ranges from 1 to 4. It is computed as: BLEU = exp 4 ∑︁ 𝑛=1 𝑤𝑛· log𝑝𝑛 ! (11) where 𝑝𝑛is the precision of n-grams, 𝑤𝑛are the weights (typically uniform). ROUGE [6] measures text similarity by counting overlapping units such as n-grams, word pairs, and sequences. Among its variants, ROUGE-L is the most popular [24], which uses the longest common subsequence (LCS) for evaluation. The ROUGE-L score is computed as: ROUGE-L = 𝐿𝐶𝑆(𝐶, 𝑅) length(𝑅) (12) where 𝐿𝐶𝑆(𝐶, 𝑅) is the length of the longest common subsequence between the candidate text 𝐶 and the reference text 𝑅. , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 9 METEOR [2] improves upon BLEU by considering synonyms, stemming, and alignment based on recall and precision. METEOR is designed to address some of the limitations of BLEU, particularly in terms of word matching flexibility. The METEOR score is computed as: METEOR = \\x12 1 −𝛾· 𝑐ℎ 𝑚 \\x13 · 𝐹mean (13) where 𝐹mean is the harmonic mean of precision and recall, 𝑚is the number of matches, 𝑐ℎis the number of chunks, and 𝛾is a parameter that penalizes fragmentation in the matched words. BLEU-4 and METEOR metrics use the implementations provided by the Python NLTK library1, and ROUGE-L is sourced from the Python Rouge library2. 4.4 Baselines As mentioned in Section 2, we select two main types of state-of-the-art approaches: retrieval-based methods and DL-based generative methods. The retrieval-based method involves CommentFinder [16], and the DL-based generative methods include Tufano et al. [41], CodeReviewer [27], AUGER [24], and LLaMA-Reviewer [28]. In addition, we include the pre-trained language model CodeT5 as one of the baseline models, due to its capability in code generation fields [44]. 4.5 Model Selection As described in Section 3, the RARe includes two components, with each process including several methods/models to choose. In this section, we describe the methods and models we use to compare. 4.5.1 Retriever. We compare three kinds of retrievers: the first two are based on Normal Dense Retrieval (NDR), and the third is based on Dense Passage Retrieval (DPR). The first kind of retriever, termed as Normal Dense Retriever (NDR), utilizes CodeBERT [10] as the encoder, employing cosine distance as the similarity measurement. Inspired by CommentFinder [16], the second kind of retriever improves upon the first by enhancing the similarity calculation, which combines cosine and Gestalt Pattern Matching (GPM) similarity. And we name it as GPM dense retriever (GDR). GPM [35] is a text similarity approach that takes into account the order of code tokens, allowing for a nuanced comparison of code snippets. Specifically, cosine similarity is used to select the top k candidates, followed by reranking them using GPM to get the final top k results. For the third kind of retriever, DPR, we follow',\n",
       "  'that takes into account the order of code tokens, allowing for a nuanced comparison of code snippets. Specifically, cosine similarity is used to select the top k candidates, followed by reranking them using GPM to get the final top k results. For the third kind of retriever, DPR, we follow the work on code summarization [19], leveraging CodeBERT and GraphCodeBERT [13] as encoders for code and reviews, respectively, and also use cosine similarity. 4.5.2 Generator. Inspired by the work of Wang et al.[43], we also apply following two methods in generation: inferencing and fine-tuning. We employ three open-source LLMs as our genera- tion models: Llama 3.13, Mistral4, and CodeGemma5. These models have been pre-trained on code, making them well-suited for our tasks. We compare four generation types to verify RARe’s effective- ness: direct inference, fine-tuning, retrieval-augmented direct inference, and retrieval-augmented fine-tuning. 1https://www.nltk.org/ 2https://pypi.org/project/rouge/ 3https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct 4https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 5https://huggingface.co/google/codegemma-7b-it , Vol. 1, No. 1, Article . Publication date: November 2018. 10 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser Table 3. Prompts for direct inference and RARe. Direct Inference Prompt: RARe Prompt: Your task is to write a concise code review for the given code snippet. Your output should only be a brief code review, no extra information. Here is an example code review: {retrieved reviews}. Your task is to write a concise code review for the given code snippet. Your output should only be a brief code review, no extra information. 4.5.3 Prompt Construction. For direct inference and fine-tuning, we followed the official prompt examples provided for each model. Since prompt tuning is not the focus of this paper, we kept the templates simple and concise. The prompt template we designed for LLMs is shown in Table 3. 4.6 Training Setting Among the retrievers, only DPR requires training. We train two encoders on the training data from the two datasets, following the well-adjusted hyperparameters provided by the original work [33]. Fine-tuning LLMs can be inefficient for a single task due to their vast number of parameters. Therefore, we use Low-Rank Adaptation (LoRA), which introduces trainable low-rank matrices to modify the weights of the model’s layers, specifically targeting the attention and feedforward layers. LoRA modifies the weight matrix 𝑊0 as follows: 𝑊=𝑊0 + Δ𝑊, where Δ𝑊= 𝐴× 𝐵 (14) Here, 𝐴and 𝐵are low-rank matrices with rank(𝐴) = rank(𝐵) ≪rank(𝑊0), reducing the number of parameters needed for adaptation. While there are alternative strategies, such as prefix tuning, we follow the findings of the LLaMa Reviewer [28], which demonstrate the superiority of LoRA over prefix tuning. We configure the training to run for 5 epochs with a learning rate of 10−4 and a batch size of 16. We employed 4 NVIDIA A100 GPUs to accelerate the training process. Additionally, to ensure the reliability of our results, we averaged the results across three runs for each experiment. 5 Results and Analysis In this section, we compare the performance of different retrievers and generators, and present the results of our proposed RARe architecture in comparison with baseline methods. Table 4. Comparison of three retrievers: NDR (Normal Dense Retrieval),',\n",
       "  'we averaged the results across three runs for each experiment. 5 Results and Analysis In this section, we compare the performance of different retrievers and generators, and present the results of our proposed RARe architecture in comparison with baseline methods. Table 4. Comparison of three retrievers: NDR (Normal Dense Retrieval), GDR (GPM Dense Retrieval), DPR (Dense Passage Retrieval). Method CRer. Tuf. BLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR NDR 9.53 5.65 5.54 12.06 8.45 8.17 GDR 9.55 5.66 5.55 12.33 8.67 8.43 DPR 9.71 5.70 5.60 11.80 7.98 7.74 5.1 Comparisons of Retrievers The retrieval performance comparison involves three approaches: NDR, GDR, and DPR (introduced in Section 4), as shown in Table 4. Notably, in this comparison, we primarily focus on the top-1 retrieval results. We observe that on the CRer. dataset, DPR attains the highest score in all metrics, , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 11 with 9.71 in BLEU-4, 5.70 in ROUGE-L, and 5.60 in METEOR. And GDR demonstrates the best performance on the Tuf. dataset, with 12.33 in BLEU-4, 8.67 in ROUGE-L and 8.43 in METEOR. The results that GDR outperforms NDR aligns with expectations. GDR employs GPM, a specially designed metric that reorganizes retrieval results while incorporating NDR, leading to superior performance on both datasets. These two methods excel when dealing with single-programming- language dataset (Tuf.), likely because in such a dataset, where all the code shares the same language structure and syntax, only comparing code similarity is sufficient for retrieving useful reviews. However, both methods are less effective than DPR on a multi-programming dataset (CRer.) because DPR assesses the similarity between code and review, thus avoiding potential issues that arise from comparing codes across different languages. Table 5. Performance of models on CRer. dataset and Tuf. dataset with and without Retrieval Augmentation (+RA). Note: DI=Direct Inference, FT=Fine Tuning, B=BLEU-4, R=ROUGE-L, M=METEOR. Model CRer. Tuf. BLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR Llama3.1 DI 5.84 3.18 3.18 5.12 2.55 2.67 +RA 12.31 ↑110% 6.43 ↑102% 8.33 ↑161% 12.96 ↑153% 8.67 ↑240% 8.76 ↑228% FT 8.82 6.75 5.29 8.06 5.87 4.57 +RA 9.05 ↑3% 6.90 ↑2% 5.42 ↑2% 9.19 ↑14% 7.00 ↑19% 5.66 ↑24% Mistral DI 5.07 2.69 2.72 5.21 2.75 3.09 +RA 11.15 ↑114% 5.75 ↑114% 6.87 ↑153% 9.74 ↑87% 7.69 ↑179% 7.58 ↑145% FT 9.07 6.56 5.32 8.42 6.21 4.91 +RA 9.59 ↑6% 7.40 ↑13% 6.59 ↑24% 9.58 ↑14% 7.40 ↑19% 6.05 ↑23% CodeGemma DI 3.34 1.53 1.40 5.27 3.02 2.95 +RA 4.76 ↑43% 2.56 ↑67% 2.49 ↑78% 5.51 ↑5% 3.02 — 3.53 ↑20% FT 9.26 6.48 5.22 8.39 5.70 4.53 +RA 9.33 ↑1% 6.51 ↑1% 5.25 ↑1% 9.44 ↑13% 6.88 ↑16% 5.60 ↑24% 5.2 Comparisons of Generators As introduced in Section 4, we compare three LLMs (LLama 3.1, Mistral and CodeGemma) with employing four generation approaches: direct inference (DI), fine-tuning (FT), retrieval-augmented direct inference, and retrieval-augmented fine-tuning. The results are shown in Table 5. Firstly, we focus on the results without RA. Among three models, Llama 3.1 generally outper- forms the other two models in direct',\n",
       "  '(LLama 3.1, Mistral and CodeGemma) with employing four generation approaches: direct inference (DI), fine-tuning (FT), retrieval-augmented direct inference, and retrieval-augmented fine-tuning. The results are shown in Table 5. Firstly, we focus on the results without RA. Among three models, Llama 3.1 generally outper- forms the other two models in direct inference, highlighting its strong code understanding ability. Compared to direct inference, fine-tuning consistently improves the performance of all three models across both datasets. Overall, the performance of Mistral and CodeGemma is comparable, slightly outperforming that of Llama 3.1. Llama 3.1 performs better in direct inference, likely because of its broader pre-training. The similar performance of the three models after fine-tuning suggests that, despite differences in their architectures or pre-training, they can all effectively adapt to task-specific datasets when given additional training. Then for direct inference (DI), retrieval augmentation (+RA) shows substantial performance improvements. Llama 3.1 demonstrates the highest improvement on both datasets, especially on , Vol. 1, No. 1, Article . Publication date: November 2018. 12 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser the Tuf. dataset, with a 153% increase in BLEU-4, a 240% rise in ROUGE-L, and a 228% increase in METEOR. Mistral shows considerable improvements across both datasets, notably on the CRer. dataset, with a 114% increase in both BLEU-4 and ROUGE-L, and a 145% rise in METEOR. Although CodeGemma also shows improvements, they are relatively modest compared to the other two models. For fine-tuning (FT), retrieval also enhances generation performance, with Mistral showing the greatest improvement across both datasets. On the CRer. dataset, Mistral achieves a 6% increase in BLEU-4, a 13% rise in ROUGE-L, and a 24% gain in METEOR. Similarly, on the Tuf. dataset, Mistral delivers a 14% boost in BLEU-4, a 19% improvement in ROUGE-L, and a 23% increase in METEOR. Obviously, retrieval can greatly improve model performance in the code review generation task, especially when applied in direct inference. However, this is not always the case. For example, in CodeGemma, the benefits of retrieval are very limited. We attribute this to the model’s relatively weaker ability to understand prompts compared to the other two models, as evidenced by its lower direct inference results. Notably, when retrieval augmentation is applied to direct inference, Llama 3.1 achieves the highest performance among the three models across both datasets. On the CRer. dataset, it reaches a BLEU-4 score of 12.31, a ROUGE-L score of 6.43, and a METEOR score of 8.33. On the Tuf. dataset, it attains a BLEU-4 score of 12.96, a ROUGE-L score of 8.67, and a METEOR score of 8.76. Answer to RQ1: For retrievers, DPR and GDR achieve the best retrieval results on multi-programming- language and single-programming-language datasets, respectively. For generators, Llama 3.1 performs the best on both datasets, particularly in direct inference when combined with retrieval augmentation. Table 6. Overall comparison between baseline methods and RARe. −indicates that the previous study did not provide the predictions. For RARe, the number of retrieved reviews are in brackets. Method CRer. Tuf. BLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR Tufano et al. [41] - - -',\n",
       "  'when combined with retrieval augmentation. Table 6. Overall comparison between baseline methods and RARe. −indicates that the previous study did not provide the predictions. For RARe, the number of retrieved reviews are in brackets. Method CRer. Tuf. BLEU-4 ROUGE-L METEOR BLEU-4 ROUGE-L METEOR Tufano et al. [41] - - - 12.31 8.72 7.83 CodeT5 [44] 7.34 7.41 5.86 7.10 6.61 5.13 CodeReviewer [27] 6.02 5.39 3.68 - - - CommentFinder [16] 9.47 5.69 5.44 12.71 8.81 8.61 AUGER [24] 8.09 6.50 4.74 7.76 5.77 4.36 LLaMA-Reviewer [28] 8.23 6.12 5.34 7.85 5.82 4.38 RARe (random) 11.29 6.41 7.97 10.88 6.66 7.82 RARe (top1) 12.32 6.43 8.33 12.96 8.67 8.76 RARe (top3) 11.76 6.14 8.14 11.74 6.89 8.00 RARe (top5) 10.81 5.89 7.76 10.80 6.47 8.23 , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 13 5.3 Comparisons of Code Reviewers Based on the previous comparison of retrievers and generators, we selected the best-performing retriever and generator for RARe to compare against the baseline methods. Specifically, for the CRer. dataset, we use the DPR retriever, while for the Tuf. dataset, we use the GDR retriever. And Llama 3.1 serves as the generator for both datasets. We conducted performance comparisons among RARe and baseline methods on both datasets, as shown in Table 6. Compared to baselines, RARe achieves the best performance across both datasets in three metrics. Especially on the CRer. dataset, RARe significantly outperforms Commentfinder by 30% in BLEU-4 and 53% in METEOR, respectively. On the Tufano dataset, RARe shows a modest improvement, surpassing CommentFinder by 2% in both BLEU-4 and METEOR scores. These results highlight the superior performance of the RARe method compared to state-of- the-art retrieval-based and deep learning-based generative methods. To further understand how different retrieval options affect RARe’s performance, we implement different retrieval strategies in RARe, as Table 6 presented. ’Random’ refers to the random selection of an example review for the source code. The results indicate that RARe with the top_1 retrieval setting attains the best performance. This suggests that the most relevant review contributes most significantly to the performance of the generated code review. We also observe that with the increase of example reviews (from 1 to 5), the performance of RARe decreases. This implies that considering more reviews may introduce irrelevant information, potentially confusing the generator. Answer to RQ2: Our RARe system outperforms previous methods in the code review generation task, achieving the best BLEU scores with 12.32 on the CRer. dataset and 12.96 on the Tuf. dataset. And retrieval quality is critical for the effectiveness of code review generation, where selecting the most relevant review yields the best results rather than using more reviews. 6 Case Study In this section, we conduct three case studies: comparing different reviewers using a specific instance, evaluating the benefits of retrieval augmentation through human evaluation, and probing the models with an interpretable tool. We randomly select instances from the Tuf. dataset and compare the reviews generated by different reviewers on this code snippet, as shown in Table 7. Through these',\n",
       "  'different reviewers using a specific instance, evaluating the benefits of retrieval augmentation through human evaluation, and probing the models with an interpretable tool. We randomly select instances from the Tuf. dataset and compare the reviews generated by different reviewers on this code snippet, as shown in Table 7. Through these instances, we attribute the improvements of retrieval augmentation shown in Table 6 to two key factors: review writing style and bug location. 6.1 Comparative Analysis of Reviewers For review writing style, we observe that fine-tuned models are good at following the writing patterns of original dataset, such as CodeT5, Tufano, AUGER and our FT model. In contrast, model using direct inference (DI) tends to produce general, long and summary-like reviews, resulting in lower scores. The retrieval-augmented direct inference partly alleviates the problem of writing style of DI, by just providing one retrieved review, which make it generate short and more review-like reviews. For bug location, FT model does not locate the \"unnecessary synchronization\" problem, but after referring to the retrieved review (result for DPR retriever), RAFT model successfully locate this issue. , Vol. 1, No. 1, Article . Publication date: November 2018. 14 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser Table 7. An example from Tuf. dataset comparing RARe (CodeGemma) with other baselines. We omit some particularly long reviews. Data/Models Code/Reviews BLEU-4 Code snippet public void onClose(Client client) { synchronized(clients) { clients.remove(client); } } Ground Truth This synchronization seems unnecessary? CodeT5 Should this be a WARN instead? 0.0839 CommentFinder Off topic: I’ve been writing this as \"client == null\". I think this conveys just as much information in fewer characters, so by Shipley’s law I should switch over. The drawback is that \"client == null\" is a boolean expression, and is consistent with \"limit < 0\" etc. 0.0442 Tufano I think you should have done something like: if (clients!= null) {clients.remove(client);} 0.0255 AUGER Remove client from the interface but keep it on the i imp 0.0530 GDR retriever I don’t see any other method that this synchronization would avoid overlapping with. 0.2741 DI The code snippet synchronizes on the ‘clients‘ object before removing a client from the list. This is necessary to avoid race conditions where multiple threads attempt to remove the same client from the list simultaneously... 0.1154 RADI The synchronized block is not necessary... 0.1228 FT synchronized (clients) clients.remove(client); 0.2328 RAFT this can be simplified to clients.remove(client) 0.0895 However, we notice that the BLEU score of RADI is lower than DI, due to its longer length, which increases the matches of overlap n-grams with the Ground Truth. Similarly, FT’s BLEU-4 is also higher than RAFT in some cases, even the latter is shown to be better after human check. This is also due to the limitation of BLEU’s semantic similarity. Other metrics (ROUGE, METEOR) also have this issue. Therefore, to better investigate the models’ performance, we further conduct a human evaluation in following section. 6.2 Human evaluation of RARe Based on the classification criteria from prior work [16, 41], we randomly selected 100 samples from the Tuf. dataset',\n",
       "  'semantic similarity. Other metrics (ROUGE, METEOR) also have this issue. Therefore, to better investigate the models’ performance, we further conduct a human evaluation in following section. 6.2 Human evaluation of RARe Based on the classification criteria from prior work [16, 41], we randomly selected 100 samples from the Tuf. dataset and the CRer. dataset, respectively. These samples are selected from Llama 3.1 model’s predictions, due to its overall superior performance. The quality of the code reviews can be evaluated by four criteria [16]: • Perfect Prediction (PP): the review is syntactically equivalent to the actual one. • Semantically Equivalent (SE): the review is semantically equivalent to the actual one. • Alternative Solution (AS): a review that is different from the actual one but is also valuable. • Incorrect (IN): there is no useful information in the review. In the annotation process, two expert annotators, each with 6 or 8 years of software engineering experience, use a 0-5 rating system to evaluate the review categories as follows: 0-1 as IN, 2-3 as AS, 4 as SE, and 5 as PP. To ensure reliable results, if there is a disagreement between the annotators, a discussion is held to reach a consensus on the final rating. The human-checked results of direct inference and fine-tuning are shown in Table 8. We consider reviews categorized as Perfect Prediction, Semantically Equivalent, and Alternative Solution to be , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 15 valuable. On the Tuf. dataset, RARe significantly increases the proportion of valuable reviews, with an increase of 266% (from DI to RADI) and an improvement of 8% (from FT to RAFT). Similarly, on the CRer. dataset, valuable reviews increase by 275% (from DI to RADI) and by 21% (from FT to RAFT). This highlights the effectiveness of RARe in enhancing the models, particularly in direct inference with LLMs, supporting our conclusion in RQ1. Table 8. Human evaluation of RARe’s generated reviews on 100 samples from the CRer. and Tuf. datasets. Metrics CRer. Tuf. DI RADI FT RAFT DI RADI FT RAFT Perfect Prediction 0 0 0 0 0 0 0 3 Semantically Equivalent 0 1 1 4 1 3 7 11 Alternative Solution 12 44 55 64 14 52 52 50 Incorrect 88 55 44 32 85 45 41 36 In addition, a closer investigation of reviews generated through direct inference reveals that they often begin with a summary of the overall code but tend to be overly general, redundant, and vague, resulting in a high rate of Incorrect reviews. After retrieval augmentation, the reviews become more specific, leading to a sharp increase of AS reviews. While fine-tuning produces more valuable reviews overall, retrieval augmentation further enhances their accuracy, as evidenced by the increase of SE and PP reviews. This finding is consistent with our observation in Section 6.1. 6.3 Interpretative Analysis of RARe We use Inseq, an interpretability toolkit for sequence generation models, to make visible of the models’ attention score. It can provide faithfulness, plausibility and usefulness of explanations for the',\n",
       "  'increase of SE and PP reviews. This finding is consistent with our observation in Section 6.1. 6.3 Interpretative Analysis of RARe We use Inseq, an interpretability toolkit for sequence generation models, to make visible of the models’ attention score. It can provide faithfulness, plausibility and usefulness of explanations for the output of decoder-only or encoder-decoder Transformer architectures. Simply, it generates a attention score between each pair of the input token and output token to quantify the dependence of the output on the input. Here, we present an example to illustrate the enhancement of RARe, as depicted in Figure 2. Due to extensive length the input, we exclude tokens with lower scores and only display the important tokens. The performance of RARe is notably superior, as partly evidenced by its higher BLEU-4 score (0.342 v.s. 0.042). Looking deeper into the generated output reveals additional evidence of RARe’s enhancement. Specifically, the text retrieved (marked in green) receives considerable focus in RARe’s predictions, as indicated by its high attention score. This demonstrates that the model effectively leverages external information from retrieval, which is a suggestion to \"incorporate a null check function\" in this example. Moreover, RARe doesn’t simply copy the retrieved results, it also concentrates on the code. This is evident from the attention scores assigned to the code (highlighted in blue) from the output in Table (b), showing RARe conducts a balanced integration of external knowledge with original code analysis. Going back to a broader perspective, it becomes apparent that retrieval results can significantly enhance the model’s attention, preventing it from concentrating on less relevant code. For instance, without RAG, the model focus on the getStartTime() function, diverging from our expected review. However, with RAG, this deviation is corrected, redirecting the model’s attention to the null check in volume.getAsyncTask() function. It is important to clarify that the output in Table (a) is not necessarily incorrect; rather, it deviates from the reviewer’s intended focus. This deviation suggests , Vol. 1, No. 1, Article . Publication date: November 2018. 16 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser that the output lacks specific domain knowledge necessary for concentrating on the aspects of the code that truly require review. _I _think _this _is _not _need ed. _The _start _time _is _alrea dy _set _in _the _const rcucto r [instruction] … … _private 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.01 [code…] … … _volume.getAsync Task 0.01 0.02 0.01 0.02 0.00 0.01 0.01 0.02 0.04 0.01 0.01 0.02 0.01 0.00 0.00 !stepslist. isEmpty()) 0.00 0.01 0.00 0.01 0.00 0.01 0.00 0.00 0.02 0.04 0.00 0.05 0.01 0.00 0.01 _entity.setStartTim e(stepslist.get(0).ge tStartTime 0.08 0.07 0.10 0.11 0.21 0.08 0.06 0.19 0.21 0.11 0.23 0.15 0.10 0.09 0.22 […code] … … _can _you _please _add _the _check _for _volume. getAsync Task() _in _this _method [instruction] … … _private 0.01 0.00 0.00 0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.00 [code…] … … _volume.getAsyncTa sk() 0.01 0.12 0.19 0.16 0.01 0.33 0.18 0.47 0.48 0.11 0.21 !stepslist. isEmpty( )) 0.00 0.00 0.00 0.00',\n",
       "  '_you _please _add _the _check _for _volume. getAsync Task() _in _this _method [instruction] … … _private 0.01 0.00 0.00 0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.00 [code…] … … _volume.getAsyncTa sk() 0.01 0.12 0.19 0.16 0.01 0.33 0.18 0.47 0.48 0.11 0.21 !stepslist. isEmpty( )) 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 _entity.setStartTime( stepslist.get(0).getSta rtTime( )) 0.01 0.03 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 […code] … … [retrieved review…] … … _(asyncTask 0.01 0.04 0.04 0.22 0.23 0.44 0.10 0.33 0.31 0.12 0.22 _!= 0.04 0.04 0.07 0.12 0.12 0.56 0.19 0.12 0.11 0.23 0.28 _null) 0.01 0.08 0.17 0.19 0.23 0.29 0.22 0.19 0.11 0.01 0.01 [Instruction] Your task is to write a concise code review for the given code snippet. Your output should only be a brief code review with no other information. [code] private GlusterVolumeTaskStatusEntity fetchTaskStatusDetails() { ... GlusterAsyncTask asyncTask = volume.getAsyncTask(); if (asyncTask != null) { List<Step> stepsList = ...; if (stepsList != null && !stepsList.isEmpty()) { entity.setStartTime(stepsList.get(0 ).getStartTime()); } } return entity; } [retrieved review] it could be more readable if (asyncTask != null && asyncTask.getTaskId() != null ) { ... } [expected review] null check should done for asyncTask.getTaskId() as well (a) Prediction without RARe (BLUE-4: 0.042) (b) Prediction with RARe (BLUE-4: 0.342) Fig. 2. An example from Tuf. dataset and the saliency heatmap comparison for fine-tuned Llama 3.1 with and without retrieval augmentation. The horizontal rows of tables display the models’ output. Different input components are differentiated using colors, and higher attention scores are highlighted in red within the table. Answer to RQ3: Through these case studies, we explain how retrieval enhances the generation in details, which are fine-grained evidences apart from simple scores. The retrieval is proven to be able to explicitly supplies generative models with external domain knowledge by referencing relevant reviews, which allows RARe to produce more domain-appropriate reviews for the code snippet in the desired writing style and precise location. 7 Discussion The findings from the research questions, along with the detailed analysis in Section 5 and Section 6 have demonstrated the effectiveness of our RARe approach in code review generation, compared to previous work. In this section, we further discuss the implications, and limitations of RARe. Implications. Firstly, in the domain of software engineering, the significant goal of automated code review generation approaches often lies in reducing human effort. Therefore, to a certain extent, mimicking human behavior and thinking in the process of generating reviews is useful. In our methodology, the retrieval process simulates the recall of prior professional knowledge and , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 17 experience, allowing RARe to generate high-quality, relevant code reviews. By achieving state-of- the-art performance, RARe can effectively save developers time and effort. Secondly, RARe extends the generalization of Retrieval-Augmented Generation technique. As mentioned in Section2, RAG has been widely adopted for generation tasks across various domains. Our RARe further validates its effectiveness in the field of software engineering and also offers insights',\n",
       "  'the-art performance, RARe can effectively save developers time and effort. Secondly, RARe extends the generalization of Retrieval-Augmented Generation technique. As mentioned in Section2, RAG has been widely adopted for generation tasks across various domains. Our RARe further validates its effectiveness in the field of software engineering and also offers insights for future research. Limitations. There are some limitations in our experiments. First, our evaluation of RARe components is limited in diversity, focusing on only three LLMs for the generator and exclusively dense retrieval methods. Future research could explore a broader range of models, from smaller variants like DistilBERT to larger ones like GPT, while incorporating diverse retrieval methods, such as sparse or hybrid retrievers, to gain deeper insights. Second, the case study evaluation could benefit from including more cases in Section 6.1 or Section 6.3 to provide a more comprehensive understanding of RARe’s performance. Additionally, the reliance on only two annotators may introduce bias and limit the diversity of human evaluations. Future studies should involve multiple annotators to improve reliability. 8 THREATS TO VALIDITY Although we conducted comprehensive experiments and evaluations in our study, there are still the following threats that may affect our research. Internal Validity. A notable threat to internal validity comes from the pre-training data of the LLMs used in RARe. Because the details of the datasets used to pre-train these LLMs are often not publicly disclosed, there is uncertainty about the diversity, quality, and relevance of the data these models are exposed to. This lack of transparency can affect the performance of the models, as biases or gaps in the pre-training data can lead to inappropriate code reviews. One solution could be to carefully select diverse and representative test datasets that cover a wide range of programming languages and domain-specific scenarios. Additionally, cross-validation techniques can be employed to assess the robustness of the models across various tasks. It is also important to conduct manual check of model outputs to identify potential biases or gaps in performance. The other potential threat to internal validity is the external knowledge base. A rich multilingual knowledge base covering various programming languages, libraries, and coding practices can significantly improve the effectiveness of RARe. In our study, we did not construct a new external database, but used the training dataset as the knowledge base. This is due to our main goal is to show the effectiveness of RARe in code review generation relative to retrieval-only and generation-only methods, rather than to build the best external knowledge base. Construct Validity. A potential threat to construct validity in this research lies in the evaluation metrics for assessing the quality of generated code reviews. Although we combine model evaluation that measure the text similarity and human evaluation that measure the valuable reviews. But there is no quality metric to define what is a good code review. This oversight can result in measurements that are inconsistent with what was intended, affecting the accuracy and relevance of our findings. Additionally, the quality of datasets is also a threat. How to ensure that the quality of the reviews in the',\n",
       "  'metric to define what is a good code review. This oversight can result in measurements that are inconsistent with what was intended, affecting the accuracy and relevance of our findings. Additionally, the quality of datasets is also a threat. How to ensure that the quality of the reviews in the dataset is good or bad. This issue also has a great impact on the generation performance. Therefore, further work can explore specific quality metrics for reviews. External Validity. An external threat in this research lies in the generalization of the results. Although we used a multi-programming language dataset and a Java-specific dataset to validate our approach, the data for each language in the multi-language dataset are relatively limited. This raises concerns about the model’s ability to generalize effectively to other programming languages or broader contexts. Consequently, larger and more diverse datasets are essential to improve the generalization of our method in various programming environments. , Vol. 1, No. 1, Article . Publication date: November 2018. 18 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser 9 Conclusion In this study, we have employed Retrieval-Augmented Generation (RAG) to incorporate exter- nal domain knowledge into the input of LLMs for the task of code review generation. Through comprehensive evaluation on two datasets, our approach, RARe achieves superior performance over the existing state-of-the-art methods, and on the other hand, it outperforms the retrieve-only and generation-only methods. Notably, retrieval-augmented direct inference with LLMs yields the best results. Furthermore, detailed investigation through human evaluations and a case study offers strong evidence that RARe can effectively improve the accuracy and relevance of reviews. Compared to direct inference, it provides examples for reference and potential reviews, avoiding overly general and vague reviews. Compared to fine-tuning, retrieval augmentation improves the precision by incorporating a set of broader and relevant examples, rather than relying solely on patterns in a limited training dataset, thereby mitigating the risk of overfitting. However, that doesn’t mean it’s perfect. For example, the dependency on the accuracy of retrieval significantly impacts the generative performance of the model. In addition, retrieval significantly increases the input length of the generative model, posing challenges to the model’s long text processing. Furthermore, incorporating retrieval does not have an efficiency advantage compared to the retrieval-only method. On the other hand, it is essential to integrate our approach into the reviewer workflow and conduct thorough testing with software developers and reviewers to assess its practicality and effectiveness in real-world scenarios. Therefore, future work should focus not only on optimizing accuracy but also on improving efficiency to ensure that this method becomes more practical and adaptable to real-world applications. 10 Data Availability The data and code that support the findings of this study are available in anonymous repository: https://anonymous.4open.science/r/GAR-9EE2 References [1] Vipin Balachandran. 2013. Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation. In 2013 35th International Conference on Software Engineering (ICSE). 931–940. [2] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of',\n",
       "  'Reducing human effort and improving quality in peer code reviews using automatic static analysis and reviewer recommendation. In 2013 35th International Conference on Software Engineering (ICSE). 931–940. [2] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. 65–72. [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS) 33 (2020), 1877–1901. [4] Robert Chatley and Lawrence Jones. 2018. Diggit: Automated code review via software repository mining. In 2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER). 567–571. [5] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2024. Lift yourself up: Retrieval-augmented text generation with self-memory. Advances in Neural Information Processing Systems (NeurIPS) 36 (2024). [6] Lin Chin-Yew. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out, 2004. [7] Ewen Denney and Bernd Fischer. 2009. Generating code review documentation for auto-generated mission-critical software. In 2009 Third IEEE International Conference on Space Mission Challenges for Information Technology. 394–401. [8] Alexander Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. 2020. Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question Answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). 4508–4513. [9] Angela Fan, Claire Gardent, Chloé Braud, and Antoine Bordes. 2021. Augmenting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computational Linguistics (TACL) 9 (2021), 82–99. [10] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In Findings of the Association for Computational Linguistics (EMNLP). 1536–1547. , Vol. 1, No. 1, Article . Publication date: November 2018. Code Review Automation using Retrieval Augmented Generation 19 [11] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 (2023). [12] Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and Xiangke Liao. 2024. Large language models are few-shot summarizers: Multi-intent comment generation via in-context learning. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering (ICSE). 1–13. [13] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366 (2020). [14] Anshul Gupta and Neel Sundaresan. 2018. Intelligent code reviews using deep learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) Deep Learning Day. [15] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training.',\n",
       "  '[14] Anshul Gupta and Neel Sundaresan. 2018. Intelligent code reviews using deep learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD) Deep Learning Day. [15] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International Conference on Machine Learning (ICML). 3929–3938. [16] Yang Hong, Chakkrit Tantithamthavorn, Patanamon Thongtanunam, and Aldeida Aleti. 2022. Commentfinder: a simpler, faster, more accurate code review comments recommendation. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). 507–519. [17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [19] Xin Jin, Jonathan Larson, Weiwei Yang, and Zhiqiang Lin. 2023. Binary code summarization: Benchmarking chatgpt/gpt- 4 and other large language models. arXiv preprint arXiv:2312.09601 (2023). [20] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 6769–6781. [21] Brendan King and Jeffrey Flanigan. 2023. Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking. In Findings of the Association for Computational Linguistics (ACL). 5570–5585. [22] Markus Klinik, Pieter Koopman, and Rick van der Wal. 2021. Personal Prof: Automatic Code Review for Java Assignments. In Proceedings of the 10th Computer Science Education Research Conference. 31–38. [23] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS). Article 793, 16 pages. [24] Lingwei Li, Li Yang, Huaxi Jiang, Jun Yan, Tiejian Luo, Zihan Hua, Geng Liang, and Chun Zuo. 2022. AUGER: automatically generating review comments with pre-training models. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). 1009–1021. [25] Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified Demonstration Retriever for In-Context Learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL). 4644–4668. [26] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL/IJCNLP). 4582–4597. [27] Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svy- atkovskiy, Shengyu Fu, et al. 2022. Automating code review activities by large-scale pre-training. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on',\n",
       "  'Language Processing (ACL/IJCNLP). 4582–4597. [27] Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svy- atkovskiy, Shengyu Fu, et al. 2022. Automating code review activities by large-scale pre-training. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). 1035–1047. [28] Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, and Chun Zuo. 2023. LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning. In 2023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). 647–658. [29] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. 2022. ReACC: A Retrieval- Augmented Code Completion Framework. 6227–6240. https://doi.org/10.18653/v1/2022.acl-long.431 [30] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based prompt selection for code-related few-shot learning. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 2450–2462. [31] Nivishree Palvannan and Chris Brown. 2023. Suggestion bot: analyzing the impact of automated suggested changes on code reviews. In 2023 IEEE/ACM 5th International Workshop on Bots in Software Engineering (BotSE). 33–37. [32] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics (ACL). , Vol. 1, No. 1, Article . Publication date: November 2018. 20 Qianru Meng, Xiao Zhang, Zhaochun Ren, and Joost Visser 311–318. [33] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval Augmented Code Generation and Summarization. In Findings of the Association for Computational Linguistics (EMNLP). 2719–2734. [34] Luca Pasquini, Stefano Cristiani, Ramón García López, Martin Haehnelt, Michel Mayor, Jochen Liske, Antonio Manescau, Gerardo Avila, Hans Dekker, Olaf Iwert, et al. 2010. Codex. In Ground-based and Airborne Instrumentation for Astronomy III, Vol. 7735. 957–968. [35] John W Ratcliff, David E Metzener, et al. 1988. Pattern matching: The gestalt approach. Dr. Dobb’s Journal 13, 7 (1988), 46. [36] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 3982–3992. [37] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333–389. [38] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval Augmentation Reduces Hallucination in Conversation. In Findings of the Association for Computational Linguistics (EMNLP). 3784–3803. [39] Jing Kai Siow, Cuiyun Gao, Lingling Fan, Sen Chen, and Yang Liu. 2020. Core: Automating review recommendation for code changes. In 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). 284–295. [40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [41] Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota. 2022. Using pre-trained models to boost code review automation. In Proceedings of the 44th',\n",
       "  'Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [41] Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota. 2022. Using pre-trained models to boost code review automation. In Proceedings of the 44th International Conference on Software Engineering (ICSE). 2291–2302. [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems (NeurIPS), Vol. 30. [43] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R Lyu. 2022. No more fine- tuning? an experimental evaluation of prompt tuning in code intelligence. In Proceedings of the 30th ACM joint European software engineering conference and symposium on the foundations of software engineering (ESEC/FSE). 382–394. [44] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder- decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021). [45] Linghao Zhang, Hongyi Zhang, Chong Wang, and Peng Liang. 2024. RAG-Enhanced Commit Message Generation. arXiv preprint arXiv:2406.05514 (2024). [46] Yin Zhang, Rong Jin, and Zhi-Hua Zhou. 2010. Understanding bag-of-words model: a statistical framework. International journal of machine learning and cybernetics 1 (2010), 43–52. Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009 , Vol. 1, No. 1, Article . Publication date: November 2018.'],\n",
       " ['DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme ∆∆G Prediction Abigail Lin* Department of Computer & Information Science & Engineering, University of Florida Abstract Predicting the effect of amino acid mutations on enzyme thermodynamic stability (∆∆G) is fun- damental to protein engineering and drug design. While recent deep learning approaches have shown promise, they often process sequence and structure information independently, failing to capture the in- tricate coupling between local structural geometry and global sequential patterns. We present DGTN (Diffused Graph-Transformer Network), a novel architecture that co-learns graph neural network (GNN) weights for structural priors and transformer attention through a diffusion mechanism. Our key inno- vation is a bidirectional diffusion process where: (1) GNN-derived structural embeddings guide trans- former attention via learnable diffusion kernels, and (2) transformer representations refine GNN message passing through attention-modulated graph updates. We provide rigorous mathematical analysis showing this co-learning scheme achieves provably better approximation bounds than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson ρ = 0.87, RMSE = 1.21 kcal/mol), with 6.2% improvement over best baselines. Ablation studies confirm the diffusion mechanism contributes 4.8 points to correlation. Our theoretical analysis proves the diffused attention converges to optimal structure-sequence coupling, with convergence rate O(1/ √ T) where T is diffusion steps. This work establishes a principled framework for integrating heterogeneous protein representations through learnable diffusion. 1 Introduction Motivation and Background The Gibbs free energy change upon mutation (∆∆G = Gmutant−Gwild-type) governs protein thermodynamic stability, a critical determinant of enzyme function, disease pathogenesis, and therapeutic efficacy (Tokuriki and Tawfik [2009]). Accurate ∆∆G prediction enables rational protein design for industrial biocatalysis (Braun et al. [2024, 2025], Listov et al. [2025]), therapeutic antibody engineering (Jain et al. [2017]), and understanding disease mechanisms (Yue et al. [2005], Bashour et al. [2024]). Traditional computational approaches employ physics-based energy functions (FoldX [Schymkowitz et al., 2005a], Rosetta [Kellogg et al., 2011a]), achieving limited accuracy (Pearson ρ ≈0.5) due to approx- imations in energy calculations and conformational sampling. Recent machine learning methods leverage either sequence information via protein language models ([Meier et al., 2021b]) or structural data through 3D convolutional networks ([Zhou et al., 2023]), but typically process these modalities separately. Key Challenges There are three key challenges when deploying a transformer-based learning algorithm to perform highly accurate ∆∆G predictions. 1) Modal Heterogeneity. Sequence data (1D) and structural data (3D graph) *Corresponding author: abigail.lin@ufl.edu 1 arXiv:2511.05483v1 [cs.LG] 7 Nov 2025 have fundamentally different mathematical representations. Naive concatenation or late fusion fails to cap- ture cross-modal dependencies. 2) Local-Global Coupling. Mutation effects involve both local geometric perturbations (captured by GNNs) and long-range sequential patterns (captured by Transformers). Exist- ing methods lack mechanisms for mutual refinement between these representations. 3) Attention Myopia. Standard transformer attention is agnostic to 3D spatial relationships. Residues spatially proximal but se- quentially distant receive inadequate attention, missing critical structural contacts. Our Contributions We propose DGTN, which addresses these challenges through: 1. Diffused Graph-Transformer Architecture (§2.1): A novel co-learning framework where GNN weights and Transformer attention are jointly optimized through bidirectional diffusion processes. 2. Learnable Diffusion Kernels',\n",
       "  'Residues spatially proximal but se- quentially distant receive inadequate attention, missing critical structural contacts. Our Contributions We propose DGTN, which addresses these challenges through: 1. Diffused Graph-Transformer Architecture (§2.1): A novel co-learning framework where GNN weights and Transformer attention are jointly optimized through bidirectional diffusion processes. 2. Learnable Diffusion Kernels (§2.2): We introduce structure-guided attention diffusion that prop- agates spatial information into sequence attention weights, and attention-modulated graph diffusion that refines message passing using sequence context. 3. Rigorous Theoretical Analysis (§C): We prove: • The diffused attention converges to the optimal structure-aware attention matrix (Theorem 1) • The joint optimization has lower approximation error than independent models (Theorem 2) • The diffusion rate achieves O(1/ √ T) convergence (Proposition 1) 4. State-of-the-Art Empirical Results (§3): DGTN achieves ρ = 0.87 on ProTherm, outperforming ESM-1v (ρ = 0.78), DeepDelta Delta G (ρ = 0.73), and MutFormer (ρ = 0.76). 2 Methodology Formally, we formulate our problem as: given a protein sequence s = (s1, . . . , sL), where si ∈A (20 amino acids), a 3D structure graph G = (V, E), where V = {v1, . . . , vL} represents residues and E contains edges (i, j) if ∥ri −rj∥< rc (distance cutoff), and a mutation specification m = (p, swt p , where smut p ) indicating position p, wild-type residue, and mutant residue, predict ∆∆G value in kcal/mol. Essentially, our key objective is to learn a mapping fθ : (s, G, m) →∆∆G by minimizing L(θ) = E(s,G,m,∆∆G∗)[(∆∆G − ∆∆G∗)2] + λR(θ), where R(θ) is the regularization. 2.1 Architecture Overview Structure Graph (G) Geometric GNN Encoder (ϕG) Sequence (s) Sequence Transformer Encoder (ϕT ) Bidirectional Diffusion Module (Ψ) Prediction Head (ϕP ) ∆∆G Figure 1: Architecture of our multi-modal framework with diffusively gated attention. 2 DGTN consists of four key components (Figure 1): 1) Geometric GNN Encoder ϕG: Processes struc- ture graph G producing residue embeddings HG ∈RL×d. 2) Sequence Transformer Encoder ϕT : Processes sequence s producing embeddings HT ∈RL×d. 3) Bidirectional Diffusion Module Ψ: Co- learns GNN weights and Transformer attention through diffusion. 4) Prediction Head ϕP : Generates ∆∆G from fused representations. The forward pass is governed by i) HG, HT = Ψ(ϕG(G), ϕT (s)); ii) hm = Aggregate(HG, HT , m); iii) ∆∆G = ϕP (hm). 2.2 Bidirectional Diffusion Mechanism This is our key innovation. We introduce learnable diffusion processes that couple GNN and Transformer. Structure-Guided Attention Diffusion. Our key innovation lies in introducing a learnable diffusion process that couples the Graph Neural Network (GNN) and Transformer components into a unified ar- chitecture. Standard self-attention (Eq. 33) captures dependencies along the sequence but disregards the underlying 3D geometry. To integrate spatial structure, we construct a graph-based affinity matrix S that encodes geometric proximity between residues: Sij = ( exp(−d2 ij/σ2), if (i, j) ∈E, 0, otherwise. (1) The affinities are then symmetrically normalized as ˜S = D−1/2SD−1/2, (2) where D denotes the degree matrix. This normalization ensures numerically stable propagation across the graph. We inject geometric priors into the Transformer by diffusing the normalized',\n",
       "  'residues: Sij = ( exp(−d2 ij/σ2), if (i, j) ∈E, 0, otherwise. (1) The affinities are then symmetrically normalized as ˜S = D−1/2SD−1/2, (2) where D denotes the degree matrix. This normalization ensures numerically stable propagation across the graph. We inject geometric priors into the Transformer by diffusing the normalized structural affinity ˜S into the attention maps. Starting from the vanilla attention A(ℓ), the diffusion proceeds iteratively as A(t+1) diff = (1 −β)A(t) diff + β ˜S A(t) diff, (3) where β ∈(0, 1) is a learnable diffusion rate controlling the degree of geometric influence. After T itera- tions, the process yields the structure-aware attention matrix Astruct = A(T) diff . (4) This mechanism enables the Transformer to internalize 3D structural context while maintaining the expres- sive flexibility of sequence-based attention. Learnable Diffusion Kernel. To enable adaptive control over the diffusion strength, we introduce a learnable diffusion kernel that dynamically adjusts the diffusion rate across layers. Specifically, the diffusion coefficient is parameterized as βℓ= σ(w⊤ β LayerFeatures(ℓ)), (5) where LayerFeatures(ℓ) encodes properties such as layer depth, current loss, and attention entropy, and σ(·) ensures βℓ∈(0, 1). This learnable formulation allows the model to modulate how strongly structural infor- mation propagates within each transformer layer, depending on the current training state and representational needs. Attention-Modulated Graph Diffusion Conversely, we introduce an attention-modulated graph dif- fusion mechanism, where the sequence-level attention informs and reshapes the GNN message-passing process. A pseudo-graph is constructed from the averaged attention maps across heads: Gattn = 1 H H X h=1 Ah, (6) 3 where H is the number of attention heads. This attention-derived graph captures dynamic, context-dependent interactions that may not be explicit in the original structure. To refine the graph, we apply thresholding and normalization: ˜Gattn,ij = ( Gattn,ij, if Gattn,ij > τ, 0, otherwise, (7) ensuring that only meaningful attention-based connections influence subsequent graph diffusion. We enhance the structural encoder by introducing learnable graph diffusion, which refines the initial adjacency matrix through iterative propagation of structural information. Starting from the original normal- ized adjacency matrix ˜S, we define a diffusion process over T steps: ˜S(0) diff = ˜S, ˜S(t+1) diff = (1 −γ)˜S(t) diff + γ ˜Gattn˜S(t) diff, (8) where γ ∈(0, 1) is a learnable mixing coefficient and ˜Gattn is a geometric attention matrix encoding distance- and orientation-aware relationships between residues. This process effectively smooths and en- riches the graph connectivity by allowing information to diffuse beyond immediate neighbors in a data- driven manner. The resulting diffused adjacency matrix ˜S(T) diff is then used to define updated neighborhoods Ndiff(i) for each residue i. In the GNN message-passing layers, node representations are computed using these refined neighborhoods: h(ℓ+1) i = GNN-Layer \\x00h(ℓ) i , {h(ℓ) j }j∈Ndiff(i) \\x01 . (9) This enables the GNN to aggregate information from structurally relevant but potentially non-adjacent residues, thereby capturing longer-range geometric dependencies critical for accurate stability prediction. Algorithm 1 Co-Learning via Bidirectional Diffusion 1: Input: Structure G, sequence s, mutation m 2: Initialize: GNN parameters θG, Transformer parameters θT 3: for layer ℓ= 1 to L do 4: //',\n",
       "  'information from structurally relevant but potentially non-adjacent residues, thereby capturing longer-range geometric dependencies critical for accurate stability prediction. Algorithm 1 Co-Learning via Bidirectional Diffusion 1: Input: Structure G, sequence s, mutation m 2: Initialize: GNN parameters θG, Transformer parameters θT 3: for layer ℓ= 1 to L do 4: // GNN Forward 5: HG ℓ= GNN-Layerℓ(G, θG) 6: // Transformer Forward 7: Aℓ= Attentionℓ(s, θT ) 8: // Structure-guided attention diffusion 9: Astruct,ℓ= Diffuse-Attention(Aℓ, S, βℓ, T) (Eq. 3) 10: // Attention-modulated graph diffusion 11: Sdiff,ℓ= Diffuse-Graph(S, Astruct,ℓ, γℓ, T) 12: // Update with diffused structures 13: HG ℓ+1 = GNN-Layerℓ+1(Sdiff,ℓ, HG ℓ) 14: HT ℓ+1 = Transformer-Layerℓ+1(Astruct,ℓ, HT ℓ) 15: end for 16: ∆∆G = PredictionHead(HG L, HT L, m) 17: return ∆∆G Joint Training Procedure. Our co-learning algorithm jointly refines structural and sequential representa- tions through bidirectional diffusion. At each layer ℓ, the GNN computes geometric-aware structural embed- dings HG ℓ, while the Transformer generates sequence embeddings with mutation-aware attention Aℓ. These modalities interact in two directions: (1) structural priors diffuse into the attention mechanism to produce geometry-guided attention weights Astruct,ℓ, and (2) these attention weights modulate the graph adjacency to 4 yield a refined, attention-informed structure Sdiff,ℓ. Both the GNN and Transformer are then updated using these diffused representations. This layer-wise mutual refinement enables the model to iteratively align 3D spatial context with evolutionary sequence patterns, culminating in a fused representation from which the final ∆∆G prediction is made. To generate a precise and context-aware prediction, we perform mutation-specific aggregation of the learned structural and sequential representations. Let HG and HT denote the final node-level embeddings from the GNN and Transformer, respectively. Around the mutation position p, we compute three comple- mentary representations: (1) a local context vector hlocal, obtained by averaging the concatenated GNN and Transformer embeddings over a window W(p) of size w centered at p; (2) a global context vector hglobal, formed by concatenating the max-pooled structural representation and mean-pooled sequential represen- tation across the entire protein; and (3) a mutation-specific encoding hmut, which embeds the wild-type residue, mutant residue, and normalized position p/L to capture the identity and location of the substitution. Formally, hlocal = 1 |W(p)| X i∈W(p) [HG i ; HT i ], (10) hglobal = [MaxPool(HG); MeanPool(HT )], (11) hmut = [e(swt p ); e(smut p ); epos(p/L)]. (12) These three vectors are concatenated into a unified feature vector and passed through a dedicated pre- diction head implemented as a three-layer MLP. The first layer applies a GELU activation, the second adds dropout for regularization followed by another GELU, and the final linear layer outputs the scalar ∆∆G prediction: ∆∆G = MLP \\x00[hlocal; hglobal; hmut] \\x01 , (13) where the MLP architecture is defined as z(1) = GELU(W(1)h + b(1)), (14) z(2) = Dropout \\x00GELU(W(2)z(1) + b(2)) \\x01 , (15) ∆∆G = w(3)⊤z(2) + b(3). (16) This design enables accurate estimation of stability changes by jointly leveraging local perturbation effects, global protein context, and explicit mutation identity. 3 Experiments We evaluate our framework on four widely used benchmark datasets for protein stability',\n",
       "  '= Dropout \\x00GELU(W(2)z(1) + b(2)) \\x01 , (15) ∆∆G = w(3)⊤z(2) + b(3). (16) This design enables accurate estimation of stability changes by jointly leveraging local perturbation effects, global protein context, and explicit mutation identity. 3 Experiments We evaluate our framework on four widely used benchmark datasets for protein stability prediction. ProTherm (Ku- mar et al. [2006]) contains 5,166 experimentally measured single-point mutations across 1,228 proteins; we use the standard split of 70% training (3,616), 15% validation (775), and 15% test (775) samples. To as- sess generalization to protein–protein interfaces, we use SKEMPI 2.0 (Jankauskait˙e et al. [2019]), which provides 7,085 mutations in 319 protein complexes. For out-of-distribution evaluation on engineered pro- teins, we include Ssym (Kellogg et al. [2011b]), a dataset of 628 symmetry-derived mutations. Finally, FireProtDB (Stourac et al. [2021]) supplies 8,196 thermostability-focused mutations, enabling assessment on industrially relevant design tasks. All structures are processed uniformly: PDB files are parsed using Biopython, Cα coordinates are ex- tracted, and residue–residue edges are constructed for pairs within a 10 A cutoff. Secondary structure assign- ments are computed using DSSP (Kabsch and Sander [1983]), and solvent-accessible surface areas (SASA) 5 are calculated with NACCESS. This standardized pipeline ensures consistent structural feature extraction across all datasets. We compare our method against a comprehensive set of baselines spanning both physics-based and ma- chine learning approaches. On the physics-based side, we include FoldX 5.0 (Schymkowitz et al. [2005b]), a widely used empirical force field method, and Rosetta ∆∆G monomer (Kellogg et al. [2011b]), which employs all-atom energy minimization with conformational sampling. Among machine learning methods, we evaluate DDGun3D (Montanucci et al. [2022]), which uses gradient boosting on handcrafted structural features; DeepDDG (Cao et al. [2019]), combining 3D convolutional networks with sequence informa- tion; ThermoNet (Li et al. [2020a]), which applies graph convolutional networks (GCNs) to residue contact graphs; MutFormer (Zhou et al. [2020]), a graph-based Transformer architecture; and ESM-1v (Meier et al. [2021a]), a state-of-the-art protein language model fine-tuned for stability prediction. This diverse set of baselines allows us to assess the relative contribution of structural modeling, sequence context, and archi- tectural design in ∆∆G prediction. Table 1: Model architecture and training configuration. Component Configuration Model Architecture GNN 4 layers, hidden dim 256, 8 attention heads, dropout 0.1 Transformer 6 layers, hidden dim 256, 8 heads, FFN dim 1024, dropout 0.1 Diffusion T = 5 steps, learnable β, γ ∈[0.1, 0.5] Prediction Head MLP: 768 →384 →192 →1 Training Optimizer AdamW, lr = 10−4, weight decay 10−2 Batch Size 32 Epochs 100 (early stopping, patience = 15) Loss MSE with gradient clipping (max norm = 1.0) Hardware NVIDIA A100 40GB GPU Training Time 12 hours We assess model performance using four standard metrics: (1) Pearson correlation coefficient (ρ), which measures the strength of the linear relationship between predicted and experimental ∆∆G values; (2) Spear- man rank correlation (ρs), which evaluates the monotonic agreement in mutation rankings; (3) Root Mean Squared Error (RMSE, in kcal/mol), which emphasizes larger errors; and (4) Mean Absolute Error (MAE, in kcal/mol), which provides a robust measure of average prediction accuracy. Together,',\n",
       "  'between predicted and experimental ∆∆G values; (2) Spear- man rank correlation (ρs), which evaluates the monotonic agreement in mutation rankings; (3) Root Mean Squared Error (RMSE, in kcal/mol), which emphasizes larger errors; and (4) Mean Absolute Error (MAE, in kcal/mol), which provides a robust measure of average prediction accuracy. Together, these metrics capture both correlation and calibration quality across the full range of stability effects. 4 Results Our model, DGTN, achieves state-of-the-art performance on the ProTherm test set with a Pearson correla- tion of 0.87, substantially outperforming the best prior method, ESM-1v (ρ = 0.78), and all physics-based and machine learning baselines. This represents a 9% relative improvement in correlation and a 20% reduc- tion in RMSE (from 1.52 to 1.21 kcal/mol), indicating not only better ranking of mutations but also more accurate absolute ∆∆G predictions (critical for practical protein engineering). The ablation variant DGTN (no diffusion) already surpasses all existing methods (ρ = 0.81), highlighting the strength of our multi-modal GNN–Transformer architecture; however, the full model with diffusively gated attention provides an addi- tional +0.06 gain in Pearson correlation, confirming that bidirectional structural–sequential diffusion is a key 6 Table 2: Performance on ProTherm test set. Best results in bold, second best underlined. Method Pearson ρ Spearman ρs RMSE MAE Physics-based FoldX 0.46 0.43 2.83 1.92 Rosetta 0.53 0.51 2.61 1.78 Machine Learning DDGun3D 0.68 0.65 1.87 1.34 DeepDDG 0.73 0.71 1.65 1.21 ThermoNet 0.71 0.69 1.72 1.27 MutFormer 0.76 0.74 1.58 1.15 ESM-1v 0.78 0.76 1.52 1.12 Ours DGTN (no diffusion) 0.81 0.79 1.38 1.02 DGTN (full) 0.87 0.85 1.21 0.94 Improvement over best baseline +9% +9% -20% -16% driver of performance. Together, these results demonstrate that explicitly modeling the interplay between 3D geometry and evolutionary sequence context (via a co-learned, diffused attention mechanism) yields both statistically and practically significant improvements in protein stability prediction. 4.1 Cross-Dataset Generalization Fig. 2 shows cross-dataset generalization performance of four methods (DeepDDG, MutFormer, ESM-1v, and DGTN) trained on ProTherm and evaluated on three unseen datasets: SKEMPI 2.0 (protein complexes), Ssym (symmetric proteins), and FireProtDB (thermostability-focused mutations). Performance is measured by Pearson correlation (ρ) between predicted and experimental ∆∆G values. DGTN consistently outper- forms all baselines across all three datasets, achieving ρ = 0.78 (SKEMPI), 0.73 (Ssym), and 0.80 (Fire- ProtDB). The consistent margin ( 0.05–0.07 points) over the strongest baseline (ESM-1v) demonstrates that DGTN’s bidirectional diffusion mechanism enables more robust and transferable learning of fundamental stability principles, rather than overfitting to dataset-specific biases. This superior generalization highlights the benefit of explicitly integrating 3D structural priors with sequence context in a co-learned framework. Ablation studies reveal that the bidirectional diffusion mechanism is central to DGTN’s performance, contributing a 6-point gain in Pearson correlation over simple cross-modal fusion. This improvement stems from the complementary roles of its two components: attention diffusion (structure →sequence) alone yields a 5-point boost, while graph diffusion (sequence →structure) adds 3 points, indicating that structural guidance has a stronger immediate impact on sequence modeling than vice versa. Crucially, combining both directions produces a synergistic effect, surpassing the sum of individual contributions and achieving',\n",
       "  'two components: attention diffusion (structure →sequence) alone yields a 5-point boost, while graph diffusion (sequence →structure) adds 3 points, indicating that structural guidance has a stronger immediate impact on sequence modeling than vice versa. Crucially, combining both directions produces a synergistic effect, surpassing the sum of individual contributions and achieving the highest accuracy. Remarkably, this substantial performance gain comes at minimal cost (only a 4% increase in model parameters) demonstrating the efficiency and effectiveness of our co-learning design. 4.2 Diffusion Step Analysis Optimal at T = 5. Beyond this, performance plateaus (consistent with Theorem 1) while computational cost increases. 7 DeepDDG MutFormer ESM-1v DGTN Method 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 Pearson Correlation ( ) 0.62 0.67 0.71 0.78 0.58 0.63 0.66 0.73 0.66 0.70 0.74 0.80 Cross-Dataset Generalization (Trained on ProTherm) SKEMPI 2.0 Ssym FireProtDB Figure 2: Cross-dataset generalization performance. Models trained on ProTherm and evaluated on un- seen datasets. DGTN consistently outperforms baselines, demonstrating superior generalization through co-learned structural-sequential representations. Sequence only (Transformer) Structure only (GNN) GNN + Trans (concat, no diffusion) GNN + Trans (learned fusion) + Attention diffusion only + Graph diffusion only + Bidirectional diffusion (full) Model Configuration 0.65 0.70 0.75 0.80 0.85 0.90 Pearson Correlation ( ) 0.74 0.70 0.79 0.81 0.84 0.82 0.87 Ablation Study on ProTherm Test Set Pearson RMSE Full Model 1.2 1.3 1.4 1.5 1.6 1.7 RMSE (kcal/mol) 1.59 1.71 1.42 1.38 1.31 1.35 1.21 Figure 3: Ablation study showing the contribution of each component. Bidirectional diffusion yields the largest improvement in both correlation and error reduction. 8 Table 3: Effect of diffusion steps T on performance. Diffusion Steps T Pearson ρ RMSE Time (ms) T = 1 0.83 1.35 12 T = 3 0.85 1.27 18 T = 5 0.87 1.21 26 T = 7 0.87 1.22 35 T = 10 0.86 1.23 48 4.3 Learned Diffusion Rates Observations: • Attention diffusion rate β increases from 0.15 (layer 1) to 0.42 (layer 6) • Deeper layers rely more on structural guidance • Graph diffusion rate γ stable around 0.25 • Supports hypothesis: Early layers learn local features, late layers integrate global structure-sequence coupling 4.4 Attention Visualization 0 10 20 30 40 Residue Index 0 10 20 30 40 Residue Index Early Layer (Local Focus) 0 10 20 30 40 Residue Index 0 10 20 30 40 Middle Layer (Emerging Long-Range) 0 10 20 30 40 Residue Index 0 10 20 30 40 Late Layer (Diffused Attention) 0.0 0.2 0.4 0.6 0.8 1.0 Attention Weight Figure 4: Attention weight matrices at early, middle, and late layers of the Transformer in DGTN. Early layers focus on local sequence context (diagonal dominance), while later layers (guided by bidirectional diffusion) develop strong attention between sequentially distant but spatially proximate residues (e.g., posi- tions 5 and 45), demonstrating successful integration of 3D structural priors into the attention mechanism. Diffused attention successfully identifies spatially proximal residues (12-18 ˚A in sequence, ¡8 ˚A in 3D) that vanilla attention misses. 4.5 Case Study: Antibody Stabilization Objective: Stabilize therapeutic IgG1 antibody (PDB: 1HZH) while preserving binding. 9 Table',\n",
       "  '5 and 45), demonstrating successful integration of 3D structural priors into the attention mechanism. Diffused attention successfully identifies spatially proximal residues (12-18 ˚A in sequence, ¡8 ˚A in 3D) that vanilla attention misses. 4.5 Case Study: Antibody Stabilization Objective: Stabilize therapeutic IgG1 antibody (PDB: 1HZH) while preserving binding. 9 Table 4: Computational cost comparison (single mutation prediction). Method Time (GPU) Time (CPU) FoldX - 180 s Rosetta - 300 s DeepDDG 2.1 s 12 s ESM-1v 1.2 s 8 s DGTN (ours) 1.8 s 15 s Approach: Screen 2,000 single mutations in framework regions using DGTN. Top predictions: • L15V: ∆DeltaGpred = −1.8 kcal/mol • T43A: ∆DeltaGpred = −1.5 kcal/mol • S88L: ∆DeltaGpred = −1.3 kcal/mol Experimental validation (differential scanning fluorimetry): • L15V: ∆DeltaGexp = −1.6 kcal/mol, Tm +2.1°C • T43A: ∆DeltaGexp = −1.4 kcal/mol, Tm +1.8°C • S88L: ∆DeltaGexp = −1.1 kcal/mol, Tm +1.5°C • Combined variant: Tm +4.9°C, binding affinity maintained (KD within 1.5-fold) Interpretation: DGTN correctly identified hydrophobic core packing improvements detected by dif- fused attention connecting framework residues to CDR-proximal positions. 4.6 Computational Efficiency DGTN is 100× faster than physics-based methods while achieving higher accuracy. Comparable to other deep learning methods despite additional diffusion computation. 4.7 Error Analysis Large errors (|∆| > 2 kcal/mol) occur in: • Oligomeric interfaces (18% of errors): Model trained on monomers • Cofactor binding sites (12%): Cofactors not explicitly modeled • Large |∆DeltaG| values (>5 kcal/mol) (22%): Training data imbalance • Flexible loops (15%): Static structure limitation Future improvements: Include oligomeric structures, model cofactors, use ensemble structures for flexibility. 10 5 Discussion Key Insights Our analysis reveals four core findings. First, bidirectional diffusion is essential: unidirectional infor- mation flow (e.g., structure to sequence alone) yields limited gains, whereas mutual refinement between modalities enables iterative co-adaptation that significantly boosts performance. Second, the model learns depth-dependent diffusion rates, placing greater emphasis on structural guidance in deeper layers, consis- tent with a hierarchical integration strategy where early layers capture local features and later layers fuse global structure–sequence relationships. Finally, attention visualization confirms the mechanism’s efficacy: the diffused attention maps consistently highlight spatially proximal residues that are sequentially distant, directly demonstrating the model’s ability to bridge 3D geometry and sequence context as intended. Comparison with Related Approaches Our method, DGTN, offers distinct advantages over existing approaches by design. Compared to ESM-1v (a large protein language model trained on 250 million sequences with 650M parameters) DGTN achieves superior prediction performance despite using 40× fewer parameters, primarily by explicitly integrating 3D structural information rather than relying solely on evolutionary sequence patterns. In contrast to Mut- Former, which applies Transformers to protein graphs but treats structural context uniformly across layers, DGTN employs learnable, depth-adaptive diffusion rates that enable dynamic, context-sensitive coupling between sequence and structure, strengthening their interaction in deeper layers where global dependencies matter most. Finally, while DeepDDG relies on 3D CNNs that assume translational invariance and operate on fixed voxel grids, DGTN leverages graph neural networks, which are inherently permutation-equivariant and better suited to the irregular, graph-structured nature of protein residues and their spatial interactions. This architectural choice',\n",
       "  'layers where global dependencies matter most. Finally, while DeepDDG relies on 3D CNNs that assume translational invariance and operate on fixed voxel grids, DGTN leverages graph neural networks, which are inherently permutation-equivariant and better suited to the irregular, graph-structured nature of protein residues and their spatial interactions. This architectural choice not only respects the native geometry of proteins but also enables more faithful modeling of long-range and orientation-dependent interactions critical for stability prediction. Study Limitations and Future Plan DGTN operates on a single static conformation of the wild-type protein, which cannot capture dynamic or entropic contributions to stability. Future work will incorporate ensembles of structures (either from molecular dynamics simulations or predicted conformational distributions) to better model flexibility and allostery. Moreover, the current DGTN framework evaluates mutations in isolation and does not explicitly model epistatic (non-additive) interactions between multiple substitutions. Extending the architecture to jointly represent combinatorial mutations (e.g., via graph-based mutation encoding or interaction-aware attention) remains an important direction. Finally, the ProTherm dataset exhibits significant class imbalance, with certain protein families (e.g., lysozyme, constituting ˜8% of entries) overrepresented. This may limit generalization to underrepresented folds. Strategies such as domain-adversarial training, balanced sampling, or cross-family transfer learning could improve robustness and fairness. 6 Related Work Recent advances in protein stability prediction fall into three broad categories, each with notable limitations that our work addresses. Sequence-based methods, particularly protein language models like ESM-1v and others (Meier et al. [2021a], Rives et al. [2021]), leverage evolutionary information from millions of se- quences and achieve strong performance (ρ ≈0.75 –0.78 ), but they operate solely on amino acid sequences and lack explicit awareness of 3D structural constraints. In contrast, structure-based approaches incorporate 11 spatial information: DeepDDG (Cao et al. [2019]) uses 3D convolutions (ρ=0.73 ), while ThermoNet (Li et al. [2020a]) applies graph convolutional networks (GCNs) to residue interaction graphs (ρ=0.71 ). Al- though effective, these methods often treat structure in isolation and do not fully exploit evolutionary se- quence signals. Hybrid models (Zhou et al. [2020]) attempt to combine both modalities but typically rely on simple concatenation or late fusion of independently encoded features, missing opportunities for deep cross-modal interaction during representation learning. This gap is especially pronounced in the architectural design of modern components. While Graph Neu- ral Networks (Kipf and Welling [2017], Veliˇckovi´c et al. [2018]) naturally model proteins as graphs (where nodes are residues and edges encode spatial proximity) and recent geometric GNNs (Jing et al. [2021]) further incorporate distance and angular features, they remain disconnected from sequence-level context. Similarly, Transformers (Vaswani et al. [2017]) excel at capturing long-range dependencies in sequences, and protein-specific variants (Rives et al. [2021], Rao et al. [2021]) learn rich evolutionary representations, yet their attention mechanisms cannot natively integrate 3D geometric information. Although techniques like graph diffusion (Klicpera et al. [2019]) and attention smoothing (Li et al. [2020b]) have improved mes- sage passing in deep GNNs, none enable bidirectional, co-adaptive information flow between structural and sequential modalities. Our work is the first to introduce a bidirectionally diffused attention mechanism that couples GNN and Transformer representations during',\n",
       "  'diffusion (Klicpera et al. [2019]) and attention smoothing (Li et al. [2020b]) have improved mes- sage passing in deep GNNs, none enable bidirectional, co-adaptive information flow between structural and sequential modalities. Our work is the first to introduce a bidirectionally diffused attention mechanism that couples GNN and Transformer representations during training, supported by formal convergence guarantees and designed specifically for interpretable, multi-modal protein property prediction. 7 Conclusion We present DGTN, a multi-modal framework that unifies geometric graph neural networks and mutation- aware Transformers through bidirectional diffusion, enabling mutual refinement of structural and sequential representations. By allowing GNN-derived structural priors to guide attention and Transformer-derived signals to modulate graph connectivity, DGTN effectively captures the coupling between 3D geometry and evolutionary sequence context. This yields state-of-the-art performance on ProTherm (Pearson ρ = 0.87, RMSE = 1.21 kcal/mol),surpassing ESM-1v with 40x fewer parameters, and ablation studies confirm the diffusion mechanism alone contributes +4.8 points to correlation. Attention visualizations further validate that DGTN correctly identifies spatially proximate yet sequentially distant residue interactions. Beyond accuracy, these capabilities of DGTN enable practical protein design, as demonstrated in case studies on antibody thermostabilization, industrial enzyme engineering, and rescue of disease-causing mu- tations (all experimentally validated). Although currently limited to static structures and single-point mu- tations, DGTN establishes a principled, efficient, and interpretable paradigm for rational protein design, proving that integration, interaction, and interpretability are as essential as predictive performance in bridg- ing AI with real-world biological discovery. Acknowledgments A Geometric Graph Neural Network A.1 Node Features Each residue i is initialized as: h(0) i = [eaa(si); epos(ri); fi] (17) where: • eaa(si) ∈Rda: learnable amino acid embedding 12 • epos(ri) ∈Rdp: coordinate encoding via MLP • fi ∈Rdf : additional features (secondary structure, solvent accessibility, B-factor) A.2 Edge Features For edge (i, j) ∈E: eij = ϕedge(dij, vij, θijk) (18) where: • dij = ∥ri −rj∥: Euclidean distance • vij = (rj −ri)/dij: unit direction vector • θijk: bond angles (for backbone connectivity) Edge encoder: ϕedge(d, v, θ) = MLP([RBF(d); v; RBF(θ)]) (19) where RBF is radial basis function expansion: RBF(x) = [exp(−γ(x −µk)2)]K k=1 (20) A.3 Geometric Message Passing At layer ℓ, messages are computed as: m(ℓ) ij = ϕ(ℓ) msg(h(ℓ) i , h(ℓ) j , eij) (21) We use geometric attention ([Jing et al., 2021]): α(ℓ) ij = exp(a(ℓ) ij ) P k∈N(i) exp(a(ℓ) ik ) (22) a(ℓ) ij = 1 √ d q(ℓ)⊤ i k(ℓ) ij (23) k(ℓ) ij = W(ℓ) k [h(ℓ) j ; eij] (24) Aggregation: h(ℓ+1) i = σ \\uf8eb \\uf8edW(ℓ) s h(ℓ) i + X j∈N(i) α(ℓ) ij W(ℓ) m [h(ℓ) j ; eij] \\uf8f6 \\uf8f8 (25) After LG layers: HG = [h(LG) 1 , . . . , h(LG) L ]⊤ (26) B Sequence Transformer B.1 Token Embedding X(0) = Eaa(s) + Epos([1, . . . , L]) (27) where Epos uses sinusoidal encoding: PE(p, 2i) = sin(p/100002i/d) (28) PE(p, 2i + 1) = cos(p/100002i/d) (29) 13 B.2 Multi-Head Self-Attention Standard transformer attention at layer ℓ: Q(ℓ) = X(ℓ)W(ℓ) Q (30) K(ℓ) = X(ℓ)W(ℓ) K (31) V(ℓ) = X(ℓ)W(ℓ) V (32)',\n",
       "  '+ Epos([1, . . . , L]) (27) where Epos uses sinusoidal encoding: PE(p, 2i) = sin(p/100002i/d) (28) PE(p, 2i + 1) = cos(p/100002i/d) (29) 13 B.2 Multi-Head Self-Attention Standard transformer attention at layer ℓ: Q(ℓ) = X(ℓ)W(ℓ) Q (30) K(ℓ) = X(ℓ)W(ℓ) K (31) V(ℓ) = X(ℓ)W(ℓ) V (32) A(ℓ) = softmax Q(ℓ)K(ℓ)⊤ √dk ! (33) X(ℓ+1) = FFN(A(ℓ)V(ℓ) + X(ℓ)) (34) After LT layers: HT = X(LT ) (35) C Theoretical Analysis We now provide rigorous mathematical analysis of the diffusion mechanism. C.1 Convergence of Diffused Attention Definition 1 (Optimal Structure-Aware Attention) The optimal attention matrix A∗that best incorpo- rates structural information is defined as: A∗= argmin A ∥A −Asemantic∥2 F + λ∥A −S∥2 F (36) subject to row-stochasticity constraints, where Asemantic is vanilla attention and S is structural adjacency. Theorem 1 (Convergence of Attention Diffusion) Let A(t) diff be the diffused attention at step t (Eq. 3) with diffusion rate β ∈(0, 1). Then: lim t→∞A(t) diff = A∗ (37) where A∗= (1 −β)A(0) + β˜SA∗is the unique fixed point. Proof 1 The diffusion process (Eq. 3) can be written as: A(t) diff = (1 −β) t−1 X k=0 (β˜S)kA(0) + (β˜S)tA(0) (38) Since ˜S is normalized and β < 1, the spectral radius ρ(β˜S) < 1. By Neumann series: lim t→∞A(t) diff = (1 −β) ∞ X k=0 (β˜S)kA(0) (39) = (1 −β)(I −β˜S)−1A(0) (40) 14 Let A∗= (1 −β)(I −β˜S)−1A(0). Then: A∗= (1 −β) ∞ X k=0 (β˜S)kA(0) (41) = (1 −β)A(0) + (1 −β)β ∞ X k=0 ˜S(β˜S)kA(0) (42) = (1 −β)A(0) + β˜SA∗ (43) This shows A∗is a fixed point. Uniqueness follows from contraction mapping theorem since ∥β˜S∥< 1. Proposition 1 (Convergence Rate) The convergence rate of attention diffusion is: ∥A(t) diff −A∗∥F ≤C · ρ(β˜S)t ≤C · e−t/τ (44) where τ = −1/ log(βλmax(˜S)) is the time constant and C depends on initialization. Proof 2 From the error recurrence: A(t) diff −A∗= (β˜S)t(A(0) −A∗) (45) Taking Frobenius norm: ∥A(t) diff −A∗∥F = ∥(β˜S)t(A(0) −A∗)∥F (46) ≤∥(β˜S)t∥2∥A(0) −A∗∥F (47) ≤[βλmax(˜S)]t · C (48) Since β < 1 and λmax(˜S) ≤1 (normalized), we have exponential convergence. C.2 Approximation Error Bounds Definition 2 (Function Space) Define Fjoint as the space of functions learned by joint GNN-Transformer with diffusion, and Fsep as functions from separate GNN and Transformer without interaction. Theorem 2 (Superior Approximation of Joint Model) For any target function f∗: (s, G) →R satisfying structural-sequential coupling, the joint model achieves better approximation: inf f∈Fjoint∥f −f∗∥2 ≤κ · inf f∈Fsep∥f −f∗∥2 (49) where κ < 1 depends on the coupling strength. Proof 3 Consider target function with cross-modal interaction: f∗(s, G) = g(s) + h(G) + c(s, G) | {z } coupling term (50) Separate Model: Can only approximate g(s) + h(G), ignoring c(s, G). Therefore: inf f∈Fsep∥f −f∗∥2 ≥∥c(s, G)∥2 (51) 15 Joint Model: Through diffusion, structural information enters transformer (via Astruct) and sequence information enters GNN (via Sdiff). The effective representation becomes: fjoint = g(s, S) + h(G, A) + c′(s, G) (52) By universal approximation theorem for GNNs and Transformers, there exist parameters such',\n",
       "  'f∈Fsep∥f −f∗∥2 ≥∥c(s, G)∥2 (51) 15 Joint Model: Through diffusion, structural information enters transformer (via Astruct) and sequence information enters GNN (via Sdiff). The effective representation becomes: fjoint = g(s, S) + h(G, A) + c′(s, G) (52) By universal approximation theorem for GNNs and Transformers, there exist parameters such that: ∥c′(s, G) −c(s, G)∥2 ≤ϵ (53) Therefore: inf f∈Fjoint∥f −f∗∥2 ≤ϵ ≪∥c(s, G)∥2 (54) The ratio κ = ϵ/∥c∥2 is small when coupling is significant. Corollary 1 (Sample Complexity) For ϵ-approximation with probability 1 −δ, the joint model requires: Njoint = O \\x12d log(1/δ) ϵ2 \\x13 (55) samples, compared to: Nsep = O \\x12d log(1/δ) κ2ϵ2 \\x13 (56) for separate models, where d is effective dimension. C.3 Information Flow Analysis Lemma 1 (Mutual Information Lower Bound) The diffusion mechanism ensures mutual information be- tween structure and sequence representations satisfies: I(HG; HT ) ≥I(HG sep; HT sep) + ∆I (57) where ∆I > 0 quantifies additional information flow from diffusion. Proof 4 By data processing inequality, standard separate encoding has: I(HG sep; HT sep) ≤I(G; s) (58) With diffusion, attention Astruct depends on G: I(HT ; G) ≥I(HT sep; G) + I(HT ; G|Astruct) (59) ≥I(HT sep; G) + H(G|Astruct) | {z } structural information in attention > I(HT sep; G) (60) Similarly for I(HG; s) through graph diffusion. Therefore: I(HG; HT ) = H(HG) + H(HT ) −H(HG, HT ) (61) is larger due to increased shared information. 16 C.4 Generalization Bound Theorem 3 (Generalization Error) With probability at least 1 −δ over training set S of size N: Etest[L(fS)] ≤ˆLS(f) + O Rdiff(F) √ N + r log(1/δ) N ! (62) where Rdiff(F) is the Rademacher complexity of the diffused function class. Proof 5 The diffusion operation adds smoothness to the function class. By Ledoux-Talagrand contraction lemma: Rdiff(F) ≤(1 −β)T R(Foriginal) (63) Since diffusion is a contraction with rate (1 −β), the Rademacher complexity is reduced. Applying standard generalization bounds: Etest[L(f)] −ˆLS(f) (64) ≤2Rdiff(F) + 3 r log(2/δ) 2N (65) ≤2(1 −β)T R(Foriginal) + 3 r log(2/δ) 2N (66) This shows diffusion improves generalization by reducing complexity. C.5 Stability Analysis Proposition 2 (Lipschitz Continuity) The diffused attention is Lipschitz continuous with respect to input perturbations: ∥Adiff(s) −Adiff(s′)∥F ≤L∥s −s′∥2 (67) where L = 1 1−βλmax is the Lipschitz constant. Proof 6 From the fixed point equation: A∗= (1 −β)A(0) + β˜SA∗ (68) Taking difference for two inputs: A∗(s) −A∗(s′) = (1 −β)[A(0)(s) −A(0)(s′)] (69) + β˜S[A∗(s) −A∗(s′)] (70) Rearranging: (I −β˜S)[A∗(s) −A∗(s′)] = (1 −β)[A(0)(s) −A(0)(s′)] (71) Solving: A∗(s) −A∗(s′) = (1 −β)(I −β˜S)−1[A(0)(s) −A(0)(s′)] (72) Since ∥(I −β˜S)−1∥2 ≤1/(1 −βλmax) and vanilla attention is L0-Lipschitz: ∥A∗(s) −A∗(s′)∥F ≤ 1 −β 1 −βλmax L0∥s −s′∥2 (73) ≤L∥s −s′∥2 (74) 17 References Habib Bashour, Eva Smorodina, Matteo Pariset, Jahn Zhong, Rahmad Akbar, Maria Chernigovskaya, Khang Lˆe Qu´y, Igor Snapkow, Puneet Rawat, Konrad Krawczyk, Geir Kjetil Sandve, Jose Gutierrez-Marcos, Daniel Nakhaee-Zadeh Gutierrez, Jan Terje Andersen, and Victor Greiff. Biophysical cartography of the native and human-engineered antibody landscapes quantifies the plasticity of antibody developability. Communications Biology, 7(1):922, Jul 2024. ISSN 2399-3642. doi: 10.1038/s42003-024-06561-3.',\n",
       "  'Akbar, Maria Chernigovskaya, Khang Lˆe Qu´y, Igor Snapkow, Puneet Rawat, Konrad Krawczyk, Geir Kjetil Sandve, Jose Gutierrez-Marcos, Daniel Nakhaee-Zadeh Gutierrez, Jan Terje Andersen, and Victor Greiff. Biophysical cartography of the native and human-engineered antibody landscapes quantifies the plasticity of antibody developability. Communications Biology, 7(1):922, Jul 2024. ISSN 2399-3642. doi: 10.1038/s42003-024-06561-3. URL https://doi.org/10.1038/s42003-024-06561-3. Markus Braun, Adrian Tripp, Morakot Chakatok, Sigrid Kaltenbrunner, Massimo Totaro, David Stoll, Alek- sandar Bijelic, Wael Elaily, Shlomo Yakir Hoch, Matteo Aleotti, M´elanie Hall, and Gustav Oberdor- fer. Computational design of highly active de novo enzymes. bioRxiv, 2024. doi: 10.1101/2024.08.02. 606416. URL https://www.biorxiv.org/content/early/2024/08/03/2024.08.02. 606416. Markus Braun, Adrian Tripp, Morakot Chakatok, Sigrid Kaltenbrunner, Celina Fischer, David Stoll, Alek- sandar Bijelic, Wael Elaily, Massimo G. Totaro, Melanie Moser, Shlomo Y. Hoch, Horst Lechner, Federico Rossi, Matteo Aleotti, M´elanie Hall, and Gustav Oberdorfer. Computational enzyme de- sign by catalytic motif scaffolding. bioRxiv, 2025. doi: 10.1101/2024.08.02.606416. URL https: //www.biorxiv.org/content/early/2025/06/11/2024.08.02.606416. Hong Cao, Jielin Wang, Lin He, Yan Chen, Kaixian Chen, Hualiang Jiang, and Jian Li. Deepddg: Predicting the stability change of protein point mutations using neural networks. Journal of Chemical Information and Modeling, 59(4):1508–1514, 2019. Tanya Jain, Tong Sun, St´ephane Durand, Andrew Hall, Nathan R. Houston, Jason H. Nett, Brian Sharkey, Boguslaw Bobrowicz, Isaac Caffry, Yingda Yu, Yi Cao, Heather Lynaugh, Michael Brown, Himadri Baruah, Laura T. Gray, Eric M. Krauland, Yan Xu, Martha V´asquez, and K. Dane Wittrup. Biophysical properties of the clinical-stage antibody landscape. Proceedings of the National Academy of Sciences of the United States of America, 114(5):944–949, January 2017. doi: 10.1073/pnas.1616408114. URL https://doi.org/10.1073/pnas.1616408114. Jovita Jankauskait˙e, Bruno Jim´enez-Garc´ıa, Justas Dapk¯unas, Joost Schymkowitz, Ignacio Ferreira, Francesco Luigi Gervasio, Johannes S´oding, Wim Vranken, Adri´an Velazquez-Campoy, and Yves De- houck. Skempi 2.0: an updated benchmark of changes in protein–protein binding energy, kinetics and thermodynamics upon mutation. Bioinformatics, 35(3):462–469, 2019. Bowen Jing, Stephan Eismann, Patricia A. Suriana, Raphael J. L. Townshend, and Ron Dror. Learning from protein structure with geometric vector perceptrons. In International Conference on Learning Represen- tations (ICLR), 2021. Wolfgang Kabsch and Chris Sander. Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features. Biopolymers, 22(12):2577–2637, 1983. Elizabeth H. Kellogg, Andrew Leaver-Fay, and David Baker. Role of conformational sampling in com- puting mutation-induced changes in protein structure and stability. Proteins: Structure, Function, and Bioinformatics, 79(3):830–838, March 2011a. doi: 10.1002/prot.22921. URL https://doi.org/ 10.1002/prot.22921. Elizabeth H Kellogg, Andrew Leaver-Fay, and David Baker. Role of conformational sampling in com- puting mutation-induced changes in protein structure and stability. Proteins: Structure, Function, and Bioinformatics, 79(3):830–838, 2011b. 18 Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017. Johannes Klicpera, Stefan Weißenberger, and Stephan G¨unnemann. Diffusion improves graph learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pages 13340–13352, 2019. M D Kumar, K A Bava, M Michael Gromiha, N Srinivasan Prabhu, Shandar Ahmad, Harinder Singh, and Akinori Sarai. Protherm and pronit: thermodynamic databases for proteins and protein–nucleic acid interactions. Nucleic Acids Research, 34(D1):D204–D206, 2006. Bo Li, Yung-Tsung Yang, John A Capra, and Mark B Gerstein. Predicting changes in protein thermodynamic stability upon point mutation with deep',\n",
       "  'Michael Gromiha, N Srinivasan Prabhu, Shandar Ahmad, Harinder Singh, and Akinori Sarai. Protherm and pronit: thermodynamic databases for proteins and protein–nucleic acid interactions. Nucleic Acids Research, 34(D1):D204–D206, 2006. Bo Li, Yung-Tsung Yang, John A Capra, and Mark B Gerstein. Predicting changes in protein thermodynamic stability upon point mutation with deep 3d convolutional neural networks. PLoS Computational Biology, 16(11):e1008291, 2020a. Guohao Li, Chen Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper gcns. In Proceedings of the International Conference on Learning Representations (ICLR), 2020b. Dina Listov, Eva Vos, Gyula Hoffka, Shlomo Yakir Hoch, Andrej Berg, Shelly Hamer-Rogotner, Orly Dym, Shina Caroline Lynn Kamerlin, and Sarel J. Fleishman. Complete computational design of high-efficiency kemp elimination enzymes. Nature, 643(8074):1421–1427, Jul 2025. ISSN 1476-4687. doi: 10.1038/ s41586-025-09136-2. URL https://doi.org/10.1038/s41586-025-09136-2. Joshua Meier, Roshan Rao, Raoul Verkuil, Jason Liu, Tom Sercu, Mihaly Varadi, Jinhe Lee, John Muschelli, Dima Kozakov, Andrej Sali, et al. Language models enable zero-shot prediction of the effects of mutations on protein function. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 29287–29303, 2021a. Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Language models enable zero-shot prediction of the effects of mutations on protein function. In M. Ran- zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 29287–29303. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/ f51338d736f95dd42427296047067694-Paper.pdf. Luca Montanucci, Emidio Capriotti, Yana Frank, Pier Luigi Martelli, and Rita Casadio. DDGun: an un- trained predictor of protein stability changes upon amino acid variants. Nucleic Acids Research, 50(W1): W222–W227, 2022. Roshan Rao, Joshua Meier, Tom Sercu, Saleh Manzoor, Wojciech Martini, Aditya Benites, Calvin Pan, Jesse Ingraham, Stephane Gobeil, Fatima Amanat, et al. Transformer protein language models are unsupervised structure learners. In International Conference on Learning Representations (ICLR), 2021. Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function emerge from scal- ing unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences (PNAS), 118(15):e2016239118, 2021. Joost Schymkowitz, Jan Borg, Florian Stricher, Robin Nys, Frederic Rousseau, and Luis Serrano. The foldx web server: an online force field. Nucleic Acids Research, 33(Web Server issue):W382–W388, July 2005a. doi: 10.1093/nar/gki387. URL https://doi.org/10.1093/nar/gki387. Joost Schymkowitz, Jesper Borg, Francois Stricher, Remy Nys, Frederic Rousseau, and Luis Serrano. The FoldX web server: an online force field. Nucleic Acids Research, 33(suppl 1):W382–W388, 2005b. 19 Jan Stourac, Jiri Dubrava, Milos Musil, David Bednar, Jiri Damborsky, and Zbynek Prokop. Fireprotdb: database of manually curated protein stability data. Nucleic Acids Research, 49(D1):D319–D324, 2021. Nobuhiko Tokuriki and Dan S Tawfik. Stability effects of mutations and protein evolvability. Current Opinion in Structural Biology, 19(5):596–604, 2009. doi: 10.1016/j.sbi.2009.08.003. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Process- ing Systems (NeurIPS), volume 30, 2017. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In',\n",
       "  'Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Process- ing Systems (NeurIPS), volume 30, 2017. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations (ICLR), 2018. Peiqi Yue, Zhen Li, and John Moult. Loss of protein structure stability as a major causative factor in monogenic disease. Journal of Molecular Biology, 353(2):459–473, October 2005. doi: 10.1016/j.jmb. 2005.08.020. Guangyu Zhou, Min Chen, Christopher J T Ju, Jie Xu, Haoyang Chen, and Zhiyong Lu. Mutation effect esti- mation on protein–protein interactions using deep contextualized representation learning. NAR Genomics and Bioinformatics, 2(2):lqaa015, 2020. Yunzhuo Zhou, Qisheng Pan, Douglas E V Pires, Carlos H M Rodrigues, and David B Ascher. Ddmut: predicting effects of mutations on protein stability using deep learning. Nucleic Acids Research, 51(W1): W122–W128, 06 2023. ISSN 0305-1048. doi: 10.1093/nar/gkad472. URL https://doi.org/10. 1093/nar/gkad472. 20'],\n",
       " ['Neural Image Abstraction Using Long Smoothing B-Splines DANIEL BERIO, Goldsmiths, University of London, United Kingdom MICHAEL STROH, University of Konstanz, Germany SYLVAIN CALINON, Idiap Research Institute, Switzerland FREDERIC FOL LEYMARIE, Goldsmiths, University of London, United Kingdom OLIVER DEUSSEN, University of Konstanz, Germany ARIEL SHAMIR, Reichman University, Israel Stylized space filling Stroke-based image abstraction Stylized image vectorization Calligrams and text stylization Fig. 1. Our method allows to optimize long and smooth B-Spline curves using DiffVG rasterization pipelines. Applications range from text stylization to image abstraction and vectorization. We integrate smoothing B-splines into a standard differentiable vector graph- ics (DiffVG) pipeline through linear mapping, and show how this can be used to generate smooth and arbitrarily long paths within image-based deep learning systems. We take advantage of derivative-based smoothing costs for parametric control of fidelity vs. simplicity tradeoffs, while also enabling stylization control in geometric and image spaces. The proposed pipeline is compatible with recent vector graphics generation and vector- ization methods. We demonstrate the versatility of our approach with four applications aimed at the generation of stylized vector graphics: stylized space-filling path generation, stroke-based image abstraction, closed-area image abstraction, and stylized text generation. CCS Concepts: • Computing methodologies →Non-photorealistic ren- dering; Rasterization; Parametric curve and surface models; Neural networks; • Applied computing →Fine arts. Additional Key Words and Phrases: Differentiable vector graphics, B-splines, Diffusion, CLIP, Long strokes ACM Reference Format: Daniel Berio, Michael Stroh, Sylvain Calinon, Frederic Fol Leymarie, Oliver Deussen, and Ariel Shamir. 2025. Neural Image Abstraction Using Long Authors’ Contact Information: Daniel Berio, Goldsmiths, University of London, Lon- don, United Kingdom, daniel.berio@gold.ac.uk; Michael Stroh, University of Konstanz, Konstanz, Germany, michael.stroh@uni-konstanz.de; Sylvain Calinon, Idiap Research Institute, Martigny, Switzerland, sylvain.calinon@idiap.ch; Frederic Fol Leymarie, Gold- smiths, University of London, London, United Kingdom, ffl@gold.ac.uk; Oliver Deussen, University of Konstanz, Konstanz, Germany, oliver.deussen@uni-konstanz.de; Ariel Shamir, Reichman University, Herzliya, Israel, arik@runi.ac.il. This work is licensed under a Creative Commons Attribution 4.0 International License. © 2025 Copyright held by the owner/author(s). ACM 1557-7368/2025/12-ART225 https://doi.org/10.1145/3763345 Smoothing B-Splines. ACM Trans. Graph. 44, 6, Article 225 (December 2025), 12 pages. https://doi.org/10.1145/3763345 1 Introduction he ability to produce long, smooth curves is cen- tral to a variety of design and artistic tasks. These include freehand drawing, sketching, calligraphy, typography, logo design as well as image abstrac- tions into compositions of organic, flowing, or blobby shapes. Our aim is to enable the gener- ation of vector graphic outputs that allow these types of designs, while taking advantage of recent advances in gradient-based image generation, stylization, and understanding. Developments in differentiable vector graphics (DiffVG) raster- ization have enabled gradient-based optimization methods that leverage complex image-space losses to drive image generation, stylization and abstraction methods. Most existing approaches rely on the method of Li et al. [2020], which implements differentiable rasterization for a large subset of elements of the Scalable Vector Graphics (SVG) standard, including piecewise cubic and quadratic Bézier curves. Most of these methods directly optimize Bézier curves, but even with additional smoothing penalties they do not provide guarantees of continuity across segments, which limits their ability to represent long, smooth and expressive strokes.',\n",
       "  'of elements of the Scalable Vector Graphics (SVG) standard, including piecewise cubic and quadratic Bézier curves. Most of these methods directly optimize Bézier curves, but even with additional smoothing penalties they do not provide guarantees of continuity across segments, which limits their ability to represent long, smooth and expressive strokes. Our work is based on two observations. First, alternative spline parametrizations such as B-spline [De Boor 2001] or Catmull-Rom [DeRose and Barsky 1988] provide inherent continuity constraints in their definition. Second, the conversion of such curves to Bézier curves is a linear transformation, making their integration into exist- ing DiffVG pipelines a matter of an additional matrix multiplication. Although these curve parameterizations are well established, to the ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. arXiv:2511.05360v1 [cs.GR] 7 Nov 2025 225:2 • Berio et al. best of our knowledge, their integration into DiffVG remains largely unexplored. In our work we focus on uniform B-splines for their simplicity, high-order continuity and analytic properties [Farin 2001]. This en- ables a straightforward implementation of derivative-based smooth- ing criteria that are well known in the fairing and motor-control/robo- tics domains, but most importantly support our goals of generating long stylized curves within a DiffVG pipeline. We define B-splines with control-polygons consisting of series of “key-points” and convert these to piecewise cubic Bézier curves for rendering. Since this transformation is linear and rendering is differ- entiable, gradients from image-space losses can be back-propagated to the key-points. We treat stroke width as a third curve dimension, where each curve control point can be assigned an independent stroke radius, enabling smooth variations similar to that seen in physical brush strokes [Fujioka and Kano 2007]. Allowing the stroke width to vanish also presents an effective way to alter the number of visible strokes required for an image abstraction. Our method op- erates with both open and closed curves, supporting the generation of closed and organic-looking areas. We present four different applications for our method: abstract space filling curves (Section 4.1), sketch-based stylization (Section 4.2), abstract image vectorization with color quantization (Section 4.3) as well as text stylization and calligram generation with a novel legibility cost (Section 4.4). We provide a practical implementation of smoothing B-splines that can be directly integrated into DiffVG pipelines and demonstrate how this enables long and expressive curves while maintaining flexible geometric and stylistic control. Working code and examples for our method are available at github. com/colormotor/calligraph. 2 Related work 2.1 Smooth and stylized curve generation Long and stylized strokes have been explored used in the litera- ture for applications including image stylization [Kaplan and Bosch 2005; Tong et al. 2025; Wong and Takahashi 2011], text-based styl- ization [Maharik et al. 2011], and fabrication [Liu et al. 2017; Yang et al. 2021]. To widen and enhance such applications, our method also enables the generation of long and smooth strokes through the use of neural-driven image-based costs. Smoothing is achieved by minimizing the squared magnitude of higher-order positional derivatives. In the motor control literature, it is well established that the kine- matics',\n",
       "  '2021]. To widen and enhance such applications, our method also enables the generation of long and smooth strokes through the use of neural-driven image-based costs. Smoothing is achieved by minimizing the squared magnitude of higher-order positional derivatives. In the motor control literature, it is well established that the kine- matics of hand and arm movements can be modeled by optimizing performance criteria [Flash and Hogan 1998]. The so-called mini- mum square derivative models have been successfully applied to handwriting and curved motion by minimizing third-order deriva- tives (jerk) [Flash and Hogan 1985] and fourth-order derivatives (snap) [Edelman and Flash 1987]. Similar minimum principles are widely employed for smooth motion control in drones [Mellinger and Kumar 2011; Ren and Kry 2019] and robots [Todorov 2004; Tou- ssaint 2017], as well as in statistics for smoothing noisy data [Eilers and Marx 1996; Reinsch 1967]. Similar principles of smooth motion and continuity have also been used for curve fairing, where a “fair” curve is typically one that exhibits a smooth variation of the curvature [Farin 2001]. In this context, jerk has been adopted as an approximation for curvature variation [Lu 2015; Meier and Nowacki 1987; Pottmann 1990], while snap serves as an approximation for transverse distributed load [Meier and Nowacki 1987]. In an extensive body of work, Egerstedt and Martin [2009] de- velop “dynamic splines” that formulate polynomial splines through optimal control of linear systems. Berio et al. [2017] use similar principles for the interactive generation of stylized paths similar to the ones seen in graffiti art and calligraphy with applications similar to ours. Kano et al. [2003] study the relations between dynamic splines and B-splines and in a collection of work, they develop an optimal formulation of B-splines [Kano et al. 2005] applied to gen- erate motion paths and curves similar to those found in Japanese calligraphy [Fujioka et al. 2006; Matsukida and Fujioka 2013]. Our approach is strongly inspired by the B-spline construction initially proposed by Kano et al. [2005], but we extend their formulation to support DiffVG and demonstrate its flexibility for generative and stylization settings. 2.2 DiffVG and applications In recent years, differentiable rendering has enabled the use of large pretrained vision and generative imaging models with 3D [Kato et al. 2020; Tewari et al. 2020; Worchel and Alexa 2023] and 2D [Li et al. 2020; Mihai and Hare 2021; Worchel and Alexa 2023] parametric primitives . We adopt the method of Li et al. [2020], which supports a large subset of the SVG standard and cubic curves with varying width profiles. Our method leverages DiffVG’s support for cubics with varying width profiles, a feature yet to be used comprehensively, likely due to limited support in mainstream vector graphics tools and standards. CLIP-driven graphics. One of the first applications of DiffVG to large-pretrained models has been through the use of the Contrastive Language–Image Pretraining (CLIP) model [Radford et al. 2021], a multimodal model that has been trained to share an embedding space between images and their textual descriptions. Frans et al. [2022] demonstrate that together with DiffVG, the model is able',\n",
       "  'to large-pretrained models has been through the use of the Contrastive Language–Image Pretraining (CLIP) model [Radford et al. 2021], a multimodal model that has been trained to share an embedding space between images and their textual descriptions. Frans et al. [2022] demonstrate that together with DiffVG, the model is able to generate vector images guided by a text caption or “prompt”. Ganz and Elad [2024] use an adversarial \"robustification\" method to fine-tune CLIP in order to enable gradients that are better aligned with human perception. Vinker et al. [2022] introduce the idea of using a loss on internal layers of CLIP to guide vector image abstraction. A similar approach, combined with DiffVG, has enabled the generation of stroke-based stylization methods [Schaldenbrand et al. 2023; Vinker et al. 2023; Xing et al. 2023]. Our method provides similar capacities, but we take advantage of the fine-tuned CLIPAG model of Ganz and Elad [2024] and support long smooth strokes, which was not possible with previous methods. Diffusion-driven graphics. In the context of 3D asset generation, Poole et al. [2023] pioneered the so-called Score Distillation Sam- pling (SDS), which enables gradient propagation from pre-trained diffusion models to parametric representations. While effective, the original method relies on high classifier-free guidance (CFG) scales, often resulting in over-saturation and lack of detail [Katzir ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. Neural Image Abstraction Using Long Smoothing B-Splines • 225:3 Fig. 2. Flow chart of our pipeline, all operations are differentiable. Fig. 3. Optimization procedure. From left to right: input image with an initial spline (quintic with multiplicity 3 on all keypoints) and subsequent optimization steps 10, 150, 300. et al. 2024]. Recent methods, including variational methods [Wang et al. 2023], DDIM inversion [Liang et al. 2024] and noise-free score distillation (NFSD) [Katzir et al. 2024], address these challenges, improving fidelity and control. Our method is compatible with all these techniques, and we specifically adopt the approach of Liang et al. [2024], which proved to be the most effective for us. In the context of 2D asset generation, Jain et al. [2023b] pioneered the use of SDS in conjunction with the DiffVG method of Li et al. [2020], demonstrating the expressive potential of diffusion for vector graphics generation. Iluz et al. [2023] use SDS for stylizing vector font outlines to resemble user-defined semantics. To name a few, variants of SDS have been used for prompt-based sketch generation [Xing et al. 2023] and animation [Gal et al. 2024], 2D vector graphics [Xing et al. 2024; Zhang et al. 2024] as well as 3D line art [Qu et al. 2024; Tojo et al. 2024]. None of these methods support the creation of long, smooth strokes aligned with our objectives, except for Tojo et al. [2024], who also use B-splines to produce long single- stroke outputs. We incorporate their proposed repulsion loss in our method. However, their work does not cover high-order derivative smoothing, relies on curve discretization, uses a custom CUDA renderer, and does not support variable-width strokes. 3 Method Our approach works as follows: we',\n",
       "  'also use B-splines to produce long single- stroke outputs. We incorporate their proposed repulsion loss in our method. However, their work does not cover high-order derivative smoothing, relies on curve discretization, uses a custom CUDA renderer, and does not support variable-width strokes. 3 Method Our approach works as follows: we specify one or more B-splines through a series of 2D or 3D keypoints, where the third dimension can be used to describe width variation along a stroke. The B-splines are converted to cubic piecewise Bézier curves that are then rendered in a differentiable manner with the method of Li et al. [2020] (see also Figure 2). Similarly to conventional DiffVG pipelines, this enables gradient optimization of the key-points with costs that depend on curve geometry as well as on the rendered version of the curves. Figure 3 shows the process: first, some initial points and an image are given; then, during optimization a spline gradually represents the input image more explicitly, while the stroke widths are jointly adapted. 3.1 Uniform B-splines We use normalized uniform or “cardinal” B-splines that have uni- formly spaced integer knots [De Boor 2001], which simplifies com- putations and proves successful in our applications. A B-spline of degree 𝑝and order 𝑘= 𝑝+ 1 is a linear combination 𝒙(𝑢) = 𝑛−1 ∑︁ 𝑖=0 c𝑖𝑁𝑘(𝑢−𝑡𝑖) where 𝑛control-points 𝑪= [c0, c1, . . . c𝑛−1] and shifted bases 𝑁𝑘 are associated with a non-decreasing sequence of 𝑚= 𝑛+ 𝑘knots. In our formulation we keep these fixed to 𝒕= [𝑡0, . . . ,𝑡𝑚−1] = [−𝑝, . . . , 0, . . . ,𝑛−𝑘 | {z } 𝑡𝑝,...,𝑡𝑚−𝑘 , . . .𝑛]. The spline is defined by sampling 𝑢in the interval [𝑡𝑘−1,𝑡𝑚−𝑘]. Increasing order derivatives 𝒙(𝑑) of a B-spline are easily computed as weighted combinations of lower order B-splines. We refer the reader to the supplement for details on the basis functions construction, but these are readily available in many modern scientific computation packages [Virtanen et al. 2020]. The number of curves and control points is predefined, so that basis functions and knot sequences can be precomputed and remain fixed during optimization. 3.2 Spline construction B-splines are approximating curves, and both periodicity and clamp- ing to endpoints require the repetition of either knots or control points. This is typically achieved with repeated knots, but we follow Fujioka et al. [2006] and use repeated control points. This maintains strict uniformity while enabling adaptive smoothing of corner-like features and simplifying integral computations, which is advanta- geous for our use-case. Instead of directly specifying control points, we let a user initially specify a spline through a series of 𝑀key- points 𝑸= 𝒒1, . . . , 𝒒𝑀and optimize these rather than the spline control points directly. The key-points are automatically adapted into a series of control points 𝑪depending on the curve’s desired clamped or periodic behavior. For a clamped (open) spline the control points are given by the key-points 𝑸padded the first and last key-point repeated 𝑘−1 times. This results in a parametric motion that begins and ends with a rest. For periodic closed',\n",
       "  'of control points 𝑪depending on the curve’s desired clamped or periodic behavior. For a clamped (open) spline the control points are given by the key-points 𝑸padded the first and last key-point repeated 𝑘−1 times. This results in a parametric motion that begins and ends with a rest. For periodic closed splines we construct 𝑪by appending the first 𝑘−1 keypoints to the initially specified key-point sequence 𝑸. Key-points may optionally be repeated to create sharp corners, as each repetition initially reduces the continuity of the curve by one degree [Farin 2001]. This strategy is useful to produce additional degrees of freedom for the subsequent optimization, where the cor- ners can be adaptively smoothed depending on the desired amount of smoothing. 3.3 Smoothing B-splines B-splines of order 𝑘are by definition 𝐶𝑘−2-continuous, but more im- portantly their construction facilitates the formulation of smoothing criteria since they allow closed form computation of derivatives and ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. 225:4 • Berio et al. integrals. In our method, we adopt a smoothing cost based on the squared magnitude of the curve derivatives, which is standard in the smoothing literature and is also known for its utility in curve fairing [Pottmann 1990] and for modeling human arm movements [Todorov and Jordan 1998]. These methods typically trade off smoothness with a geometric accuracy term, but in our work we consider a variety of image-space objectives instead of geometry and define a smoothing cost: L𝑑 smooth = 1 𝑇 ∫𝑡𝑚−𝑘 𝑡𝑘−1 ∥𝒙(𝑑) (𝑢)∥2d𝑢= 1 𝑇𝒄⊤¯𝑮𝒄 (1) where𝑇= 𝑡𝑚−𝑘−𝑡𝑘−1 and 𝒄is a vector that concatenates all control points of the spline. The integral can be calculated exactly by setting ¯𝑮to a block Gram-matrix constructed from the inner products of the basis function derivatives [Fujioka and Kano 2007; Vermeulen et al. 1992], resulting in the standard spline smoothing criterion. Alternatively, a finite difference approximation of ¯𝑮results in the penalized-spline method of Eilers and Marx [1996]. Both methods have similar run-time performance because the matrix is precom- puted for each stroke, and we refer the reader to the supplement for derivations. Most of our examples use quintic splines with a smoothing cost L3 smooth on the third positional derivative (jerk). We do so on the basis that “minimum jerk” is a known criterion that has been used to model hand and arm movements [Flash and Hogan 1985; Todorov and Jordan 1998] as well as an approximant for curvature variation in curve fairing [Lu 2015]. Nevertheless, our method generalizes to different curve and smoothing orders (Fig. 4). 3.4 Conversion to Bézier and rendering Our goal is to integrate smoothing B-splines into a DiffVG pipeline by taking advantage of the linear relationship between B-splines and Bézier curves. B-splines can be converted exactly to piecewise Bézier curves of the same degree. To do so we use the method of Romani and Sabin [2004], which reduces to a matrix multiplication between the flattened spline control points 𝒄and a block transformation matrix 𝑺. Our method also supports smoothing costs on higher-order posi- tional derivatives such as jerk (third derivative)',\n",
       "  'of the same degree. To do so we use the method of Romani and Sabin [2004], which reduces to a matrix multiplication between the flattened spline control points 𝒄and a block transformation matrix 𝑺. Our method also supports smoothing costs on higher-order posi- tional derivatives such as jerk (third derivative) and snap (fourth derivative), which require polynomial curves of degree greater than three. Although native rendering of such higher-degree curves is not supported in DiffVG and remains a challenge, we observe that reducing the degree of B-splines to three introduces negligible geo- metric error (less than 0.3% of the curve’s bounding box diagonal in all our experiments), making the optimization of higher-degree B-splines practical for image-based error calculations. We perform a degree reduction of Bézier curves using the multi- reduction method of Sunwoo [2005], which involves a second block transformation matrix ¯𝑹. As a result, the control points for a cubic piecewise Bézier curve compatible with DiffVG are computed from the (flattened) control points 𝒄with the linear map ¯𝑹¯𝑺𝒄. We refer the reader to the work of Romani and Sabin [2004] and Sunwoo [2005] for details; we include in the supplement details and matrices for quintic Bézier and their reduction to cubic. DiffVG rendering and optimization. The conversion procedure results in a sequence of Bézier control points ∈IR3 , where the third dimension represents the stroke radius. Control points and associ- ated stroke and fill colors are all treated as differentiable parameters to be optimized. Rendering the scene results in an image I, which is differentiable with respect to all the underlying parameters. 4 Applications The proposed B-spline construction, smoothing and conversion to Bézier enables the optimization of long, expressive and optionally periodic curves, which would be challenging to produce with cur- rently known methods leveraging DiffVG. All the results presented hereafter are produced using a combined cost: L = LI + LG (2) consisting of an image-space term, LI, and a geometric term, LG. We construct each term as a combination of losses depending on the application objective. LI relies on differentiable rasterization, which allows gradients to propagate from raster-based objectives to the geometry parameters. LG leverages the properties of B-splines to enable smoothing, stylization objectives, and constraints while preserving continuity. We denote the relative weights of any loss L◦as 𝜆◦, e.g. the weight of a smoothing loss on the third derivative is denoted as 𝜆smooth. If not specified, the weights are assumed to be 1. When also optimizing stroke widths, we clip these to a minimum and maximum value at each iteration. We generate strokes using the Adam optimizer and use a cosine annealing schedule on the learning rates. We run our experiments on a single NVIDIA GeForce RTX 3060 with 12 Gb of memory. We run most of the presented applications for 300 steps, which approximately takes between 30 and 60 seconds on our system. One exception is using diffusion-guidance, which takes approximately 0.6 to 1.0 second per step depending on the method used, leading to an optimization time of up to 6 minutes. Fig. 4. Comparison of different',\n",
       "  'presented applications for 300 steps, which approximately takes between 30 and 60 seconds on our system. One exception is using diffusion-guidance, which takes approximately 0.6 to 1.0 second per step depending on the method used, leading to an optimization time of up to 6 minutes. Fig. 4. Comparison of different spline degrees 𝑝(rows), smoothing deriv- ative orders 𝑑and smoothing weight 𝜆smooth (columns). In each row, we let the smooting derivative to 𝑝−1. We quantify smoothness using the dimensionless jerk measure [Hogan and Sternad 2009]. Lower is smoother. We use the stylized area fill method in Section 4.1 using the style image in Fig. 6, left. ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. Neural Image Abstraction Using Long Smoothing B-Splines • 225:5 Fig. 5. Text combining areas generated with our stylized area filling method. Each letter is generated separately. Fig. 6. Examples of stylized area filling for a letter “S”. The images on the lower left are used to guide stylization. Fig. 7. Left, weighted Voronoi samples (black) for a bitmap of the letter ’A’ and an open TSP path connecting the points (blue). Middle, initial quintic B- spline with key-points given by the Voronoi samples overlaid on the bitmap area with 50% opacity. Right, result of an optimization with LG = L3 smooth and LI for the 50% opacity bitmap. Decreasing opacity results in sparser and thinner strokes. 4.1 Area fillings and pattern generation As a baseline for our method we demonstrate how our pipeline can be used to create pattern fills of solid regions. It illustrates also how our approach can be flexibly used to control stylization while maintaining smoothness (Fig. 5 and 6). Initialization. Stochastic gradient descent is well known to be sensitive to initialization due to its susceptibility to local minima. We find good points using an initialization strategy based on so- called weighted Voronoi stippling [Secord 2002]. For simplicity, we adopt this method for different applications presented in this paper. The input can be an arbitrary bitmap (Fig. 7, Left) or a saliency map (Fig. 13). To create a single stroke, we use a TSP route connecting the points in an open or looping path. This method is known in the literature as “TSP art” [Kaplan and Bosch 2005] (Fig. 7, Left). For open paths, we select the left-topmost point and the bottom- rightmost as initial and final points, respectively. Image coverage loss. We find that setting LI as a multiscale mean squared error (MSE) loss works particularly well to fill an area or silhouette defined as an image. This loss is computed between the target and the rendered image, with each step corresponding to a progressively reduced scale and blurred version of the image. This approach is similar to the shape-based losses used by Iluz et al. (a) (b) (c) Fig. 8. Stylized coverage of a letter “S” using the image on the left as a target. (a) Optimization using Bézier curves. (b) Optimization using quintic splines and smoothing on jerk with 𝜆smooth = 1. (c). Same procedure with 𝜆smooth =',\n",
       "  'losses used by Iluz et al. (a) (b) (c) Fig. 8. Stylized coverage of a letter “S” using the image on the left as a target. (a) Optimization using Bézier curves. (b) Optimization using quintic splines and smoothing on jerk with 𝜆smooth = 1. (c). Same procedure with 𝜆smooth = 10. Fig. 9. Examples combining image coverage with a patch-wise loss on CLIP features derived from an example image (top left) and using the same initialization from Fig. 7. From the left, the first two examples use a quintic spline with a smoothing loss LG = L3 smooth (jerk). For comparison, the right example uses a Catmull-Rom spline only enforcing 𝐶1 continuity. Allowing zero stroke width results in the appearance of multiple strokes, but the optimization is still performed on a single curve (left, dotted cyan). [2023] and Tojo et al. [2024], but lower scales encourage alignment with broader intensity regions and faster convergence, while higher scales promote a more accurate silhouette reconstruction. Reducing the opacity of the target image directly decreases the density of curves used to cover it (Fig. 7-right), allowing control over the visual result. Bounding box loss. For some of our optimization procedures, it is useful to extend LG with a bounding box loss that keeps curve key-points within the bounding box of a given image: Lbox = ∑︁ 𝑖 1⊤\\x02 𝜑\\x00𝒃min −𝒑𝑖 \\x01 + 𝜑\\x00𝒑𝑖−𝒃max \\x01\\x03 where Lbox > 0 only if key-points fall outside of the bounding box 𝒃min, 𝒃max and where 𝜑can be either a Softplus or a ReLU function applied element-wise to the vectors. Image-space semantic-driven stylization. Together with geometry- based stylization costs, we can add a semantic stylization term Lstyle to the image-space loss LI, which enables stylization based on a text prompt or features extracted from an example image ( Fig. 8 , 6 and 9). We apply the technique proposed by Kwon and Ye [2022] for semantic-driven image stylization and use a patch-wise directional loss between the encoded features of an example image and the encoded features of the rendered curves. We use the augmented CLIPAG [Ganz and Elad 2024] ViT-B/32 transformer architecture as we find it to be efficient while working well for our use vector stylization use-case. ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. 225:6 • Berio et al. (a) (b) (c) Fig. 10. Diffusion driven stroke abstraction with ControlNet and IP-adapter conditioning. (a) a variable width abstraction of “Spock”. (b) Allowing a single strokes to reach zero width in regions results in an effective strategy for automatically determining the number of strokes for a multi-stroke abstraction. (c) combining the diffusion cost with a stylization term that favors horizontal and vertical orientations. Fig. 11. Rendering stroke abstractions with a spray-like brush. The quintic B-splines together with smoothing on jerk produce smooth motions that tend to slow down where curvature is higher. This is a characteristic feature of human hand motions [Viviani, Paolo and Flash, Tamar 1995] and results in a lower deposition of paint particles where speed is higher. 4.2 Single-stroke image abstraction',\n",
       "  'B-splines together with smoothing on jerk produce smooth motions that tend to slow down where curvature is higher. This is a characteristic feature of human hand motions [Viviani, Paolo and Flash, Tamar 1995] and results in a lower deposition of paint particles where speed is higher. 4.2 Single-stroke image abstraction Most existing DiffVG-based methods that work with diffusion mod- els rely on variants of Score Distillation Sampling (SDS) together with a text caption to guide the generation of parametric vector primitives. We follow a similar approach but enable image-conditi- oned stylization by integrating ControlNet [Zhang et al. 2023] with Canny edge detection and IP-Adapter [Ye et al. 2023] into the diffu- sion pipeline. ControlNet helps to preserve structural cues from the input image, while IP-Adapter encourages the strokes to align with its global appearance and style (Fig. 10 and 11). In our experiments, we find that using a generic text prompt such as “A black and white drawing” for stroke-based outputs is sufficient to generate recognizable abstractions and stylizations of an input image. For a given condition 𝑦, the gradient of the SDS-like loss with respect to the optimizated parameters 𝜃has the form: ∇𝜃LSDS = E𝑡 \\x14 𝜔(𝑡) \\x00𝝐𝜙(𝑥𝑡,𝑡,𝑦) −𝝐\\x01 𝜕𝑔(𝜃) 𝜕𝜃 \\x15 , (3) where 𝝐𝜙(𝑥𝑡,𝑡,𝑦) is the predicted denoising direction for a latent 𝑥𝑡at time step 𝑡, 𝝐is the noise predicted by the model and 𝜔(𝑡) is a weighting function dependent on the time-step. We employ the time-step schedule annealing procedure proposed by Liang et al. [2024] and use their Interval Score Matching (ISM) variant of SDS, which helps convergence in our experiments and enables a standard classifier-free guidance of 7.5. It is known that for diffusion models, higher time steps during denoising typically produce coarser features, while lower time steps Fig. 12. From left to right: varying the minimum time-step (100, 300, 500, 700) for 300 timesteps of the ISM [Liang et al. 2024] variant of SDS and a single quintic stroke. The cyan line emphasizes the centerline of the stroke, which reaches zero width in certain regions. Fig. 13. Left, initialization with a saliency map computed from the nor- malized logits of the last layer of the OneFormer panoptic segmentation model [Jain et al. 2023a]. Right, stroke optimization using ISM with diffusion conditioned on the edge map, using a minimum time step of 400 and with an additional stylization loss 𝜆style guided by the same image as Fig. 8. yield finer details [Hwang et al. 2023]. Given our goal of producing single stroke image abstractions, the curves lack sufficient degrees of freedom to capture these finer details, so we limit the time steps in the denoising process to a minimum of 500 (Fig. 12). With a similar motivation, we find that with diffusion-guided stroke abstraction it is useful to initialize the strokes with a multiplicity > 1 (we use 3 with quintic splines in our examples). Using a higher multiplicity results in smoother strokes, where fewer details are captured. 4.3 Area-based image abstraction Our method allows for the generation of smooth closed areas, and we observe that this',\n",
       "  'to initialize the strokes with a multiplicity > 1 (we use 3 with quintic splines in our examples). Using a higher multiplicity results in smoother strokes, where fewer details are captured. 4.3 Area-based image abstraction Our method allows for the generation of smooth closed areas, and we observe that this is useful to generate image abstractions similar to what can be seen in certain designs consisting of overlapping smooth regions and a limited color palette. Examples include psych- edelic designs, album covers, screen-printed graphics, or street-art inspired fashion and graphic designs. We are interested in generating outputs that aim to be printed or fabricated as collages with a limited number of regions and colors. To guide stylization, we use filled areas instead of strokes and set LI to a variant of the CLIP-driven geometric cost described by Vinker et al. [2022] LCLIP = ∑︁ 𝑙 CLIP𝑙(ˆI) −CLIP𝑙(I𝜃) 1 , (4) using the 𝐿1 norm instead of 𝐿2 and omitting the semantic term originally proposed by the authors. We use layers 2 and 3 together with the CLIPAG [Ganz and Elad 2024] architecture. We use CLIP as opposed to diffusion because we find this to be significantly faster, while being effective for this kind of stylization task. Repulsion loss. For applications using closed curves, we adopt the repulsion method for 3D wire fabrication proposed by Tojo ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. Neural Image Abstraction Using Long Smoothing B-Splines • 225:7 Fig. 14. Quantized color vectorizations using an additional image-driven stylization term Lstyle. The palette is extracted from the style image. Fig. 15. Increasing 𝜆style weight for an abstract vectorization of Bach. From left to right 𝜆style = 0, 𝜆style = 0.06 , 𝜆style = 0.1 et al. [2024] to compute a geometric loss Lrepul, penalizing self- intersections and overlaps based on a tangent-point energy kernel for a set of sampled points along the spline. For this application, we compute the loss for each area separately, thus allowing overlaps and intersections among different areas. Optimization with quantized coloring. Jang et al. [2017] use the Gumbel-Softmax trick to make discrete choices differentiable during training. We apply the same idea to assign colors to image regions, using soft selections from a fixed color palette that can be optimized with backpropagation. To progressively transit from soft to discrete assignments during the training process, we anneal the Gumbel- Softmax temperature using an exponential schedule. Given a set of 𝐾palette colors organized as a matrix 𝑽∈IR𝐾×3, we optimize the logits per area ℓ𝑖∈R𝐾using a soft assignment 𝒂𝑖= softmax \\x12ℓ𝑖+ g𝑖 𝜏 \\x13 with 𝒈𝑖∼Gumbel(0, 𝛽)𝐾, where 𝛽is a scale parameter that we empirically set to 0.15 to avoid excessive noise during optimization [Huijben et al. 2023] and 𝜏is a temperature parameter that we anneal during optimization. We use these soft colors computed as v𝑖= 𝒂⊤ 𝑖𝑽during the optimization. At the same time, for visualization, we obtain hard assignments by taking the argmax over the optimized logits and selecting the corresponding palette color. To encourage a balanced use of all the specified',\n",
       "  'that we anneal during optimization. We use these soft colors computed as v𝑖= 𝒂⊤ 𝑖𝑽during the optimization. At the same time, for visualization, we obtain hard assignments by taking the argmax over the optimized logits and selecting the corresponding palette color. To encourage a balanced use of all the specified palette colors, we add a regularization term: 𝜆𝐾 E𝑖[𝒂𝑖] −𝐾−11 2 to LI, which penalizes deviations from a uniform color assignment, encouraging a balanced use of the palette. Figures 14 and 15 show some results. Area initialization and optimization. We initialize a user-defined number of areas using weighted Voronoi sampling on a saliency map of the input image and create an initial series of closed curves with keypoints given by the vertices of each resulting Voronoi regions. Each curve is then assigned random initial logit and the curves are sorted by increasing saliency of the covered area. Optimization proceeds with the inclusion of the repulsion loss in LG, which keeps the area outlines from intersecting. 4.4 Text stylization In line with the smooth curve image abstractions, we aim to generate text abstractions made of smooth curves that fit inside a target area. Examples of this approach can be seen in posters, graphic designs, as well as in “calligrams”: renditions of text that is arranged to fit a specific silhouette, such as those seen in the methods of Xu and Kaplan [2007] and Zou et al. [2016] (c.f. Figure 17). Our pipeline results in a simple way to generate calligrams, such as “blobby” texts (Fig. 16) and abstract monospace fonts (Fig. 18). We tackle text stylization with the tools we have covered so far and start with a bitmap image ˆI representing the desired silhouette and an initial text layout rendered as a second image Itxt. We uni- formly sample the glyph outlines and produce key-point sequences used in optimization. The optimization deforms the outlines based on a loss that balances silhouette coverage, outline smoothness, and repulsion between outline points. This procedure alone smooths and fits the outlines into the target area, but this may compromise legibility (Figure 18c). To preserve legibility, we introduce a perceptual loss based on the features of a pretrained vision encoder, which we use to compute the feature-space distance between the rendered deformed image I and the original layout Itxt. We find that using the last-layer [CLS] token as feature of the TrOCR model [Li et al. 2023] and calculating a loss based on the 𝐿1-norm of the embeddings produce robust results for this application (Figure 18). The placement of glyph can be manual or automatic. In the auto- matic case, we optimize a similarity transform per glyph to maxi- mize silhouette coverage while avoiding overlaps and maintaining a readable text layout. We first offset each glyph by a user-specified amount to encourage padding around the text. At each optimization step, we render both a morphologically opened version of the silhou- ette and glyphs into two images using white with 50% opacity on a black background. We minimize a loss that combines (i) a coverage term LI (Section',\n",
       "  'a user-specified amount to encourage padding around the text. At each optimization step, we render both a morphologically opened version of the silhou- ette and glyphs into two images using white with 50% opacity on a black background. We minimize a loss that combines (i) a coverage term LI (Section 4.1), (ii) an overlap cost given by Í ReLU(𝑣−0.5) for each pixel intensity 𝑣∈[0, 1] of the rendered image and (iii) (a) (b) (c) (d) Fig. 16. Automatic calligram production for a silhouette generated with the prompt “Silhouette of a BUNNY“. (a) initial text layout rendered with 50% opacity and overlayed on the silhouette. (b) intermediate step of the layout optimization displaying an image area that increases the overlap cost. (c) Sampled glyphs placed according to the layout. (d) Result of the optimization. ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. 225:8 • Berio et al. (a) (b) Fig. 17. Calligram generation: comparison of (a) an example from Zou et al. [2016] for a camel silhouette and (b) two runs of our method on the same silhouette with automatic initialization and two different fonts. (f) (a) (b) (c) (d) (e) Fig. 18. Monospace font generation. (a) A letter “R” (quintic B-splines with jerk cost) adapted to a triangle, using 𝜆repul = 666, 𝜆txt = 6.6 and 𝜆smooth = 200.0. (b) Setting 𝜆repul = 0 (no repulsion), still results in a readable letter but (c) removing the legibility loss does not. (d) B-splines with legibility but no smoothing. (e) Catmull-Rom to enforce tangent continuity with legibility loss (for comparison). (f) Combining glyphs optimized to fit a triangle, a square and a circle. Fig. 19. More calligrams generated with our system. The seagull silhouette is generated using the prompt “Silhouette of a SEAGULL”. an alignment cost Í ∥𝜃∥that penalizes the absolute turning an- gles 𝜃between consecutive glyph center-points and maintains text ordering. We note that image generation models such as DALL-E 3 [Betker et al. 2023] are particularly effective at generating sil- houettes with a prompt, which finally results in a fully automatic calligram generation pipeline. 5 Discussion In our example applications, we have seen how a B-spline reparame- trization can be used to generate long and expressive strokes and curves in a DiffVG pipeline. B-splines enforce high-order continu- ity by design, which enables analytic smoothing losses that help producing more regular geometry when combined with different stylization losses. This offers a considerable advantage compared to using only Bézier curves or parametrizations with lower order continuity, especially for applications like the ones demonstrated in this paper. Qualitative examples of this can be seen in examples Fig. 20. Stroke abstraction of Thelonious Monk. Left, using a single stroke and Voronoi with TSP initialzation. Middle, using multiple strokes with multiple key-points along vertical lines. Right, using facial features extracted with MediaPipe [Lugaresi et al. 2019]. Fig. 21. Smooth speed and acceleration of a quntic spline covering a num- ber “5” and optimized using the method of Section 4.1. With appropriate resampling the path kinematics can be safely tracked with a robot.',\n",
       "  'along vertical lines. Right, using facial features extracted with MediaPipe [Lugaresi et al. 2019]. Fig. 21. Smooth speed and acceleration of a quntic spline covering a num- ber “5” and optimized using the method of Section 4.1. With appropriate resampling the path kinematics can be safely tracked with a robot. Fig. 22. Our method produces smooth kinematics that facilitate reproduc- tion with a robot, and the varying width can be used to control brush pressure. Left and center, the robot reproducing portraits. Right, the robot reproducing a stylized area fill. such as Figure 8 and Figure 18. For conciseness, we used a simi- lar Voronoi-based initialization strategy in most of our examples. However, our method performs well with different initializations (Fig. 20), which can serve as an additional design parameter to be explored by users. One common challenge in stroke-based abstraction pipelines is controlling the trade-off between visual fidelity and geometric sim- plicity. Previous methods typically address this by pre-determining the number of curves [Vinker et al. 2022] or by integrating a learned component into the optimization loop [Vinker et al. 2023]. We find that our use of smoothing, combined with optimizable stroke width, allows this trade-off to be controlled parametrically and with the number of strokes emerging from the optimization. This results in a solution that is significantly simpler than previous methods. We investigate the utility of our representations and different loss terms in different examples of our applications. In Figure 18 we perform a small qualitative ablation showing the effectiveness ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. Neural Image Abstraction Using Long Smoothing B-Splines • 225:9 of the proposed legibility loss (Figure 18c) as well as the benefits of B-splines and smoothing compared to Catmull-Rom splines (Fig- ure 18e), which only enforce 𝐶1 continuity. In Figure 8a we can observe that directly optimizing Bézier curves effectively captures features of the example style image. However, higher degrees of freedom produce results that capture finer details at the expense of a clear stroke structure. Although this additional detail may be de- sirable in certain applications, it is not suitable for the applications considered in our work. Interestingly, the computational overhead of the proposed B- spline to Bézier matrix conversion is lower than the one for the additional optimization parameters required for an equivalent multi- Bézier curve. We tested performance with a simple comparison where we cover an area by optimizing the 290 key-points of a single open cubic B-spline. We compared this to a similar setup directly optimizing the corresponding 874 Bézier control points. On our hardware setup, the Bézier case is 4.5 times slower. This shows that the additional cost of the proposed matrix conversion is negligible and suggests that our method is an efficient way to enforce output continuity in DiffVG settings. Robotic reproduction. Optimizing splines with degree greater than three results in smooth acceleration profiles (Fig. 21). This enables a safe reproduction of the resulting trajectory kinematics with an articulated robot arm (Fig. 22), without requiring an intermediate reparameterization step. We tested this',\n",
       "  'way to enforce output continuity in DiffVG settings. Robotic reproduction. Optimizing splines with degree greater than three results in smooth acceleration profiles (Fig. 21). This enables a safe reproduction of the resulting trajectory kinematics with an articulated robot arm (Fig. 22), without requiring an intermediate reparameterization step. We tested this by reproducing the trajec- tories using a 7-axis Franka robot equipped with a brush. We first transformed the control points to a desired workspace coordinate system, treating the stroke widths as perpendicular distances to the drawing plane. We then sampled the trajectories at a resolution that produced a maximum speed and accelerations within the robot’s mechanical limits. The inverse kinematics for the resulting trajecto- ries are then computed with an iterative linear quadratic regulator (iLQR) [Li and Todorov 2004]. 6 Conclusions and future work We have presented a framework for integrating high-order B-splines into DiffVG pipelines together with minimum-square derivative- based smoothing costs. We have explored different applications and demonstrated how this enables the generation of long, smooth, and stylized strokes through a combination of geometric and image- space loss functions. While the combination of losses allows for a large variety of creative outputs, a practical challenge is the necessity to weigh different losses to achieve the desired result, which, given the iterative optimization procedure, can be slow and tedious. Although our formulation draws on a large body of existing work on B-splines, an effective use of this tool together with DiffVG is novel, and we expect it to be a valuable tool for the community. We used uniform B-splines because of their simplicity and effectiveness for our use cases. However, exploring non-uniform parameteriza- tions, such as NURBs, presents an interesting direction for further research, as it may unlock additional flexibility and control for styl- ized outputs. Acknowledgments This work was funded by the EACVA (Embodied Agents in Contem- porary Visual Art) Project, led by Goldsmiths (UKRI/AHRC grant AH/X002241/1) and the University of Konstanz (grant 508324734, Deutsche Forschungsgemeinschaft/DFG). Special thanks to Guil- laume Clivaz (Idiap Research Institute) for the technical support and useful discussions. A B-Spline details A B-spline (or basis-spline) or order 𝑘is a piecewise polynomial curve of degree𝑝= 𝑘−1 defined by a linear combination of𝑛weights or control points c0, c1, . . . c𝑛−1 and a non-decrasing sequence of 𝑚= 𝑛+ 𝑘knots (or breakpoints) 𝑡0,𝑡1,𝑡2, . . . ,𝑡𝑚−1. 𝒙(𝑢) = 𝑛−1 ∑︁ 𝑖=0 c𝑖𝐵𝑖,𝑘(𝑢) Each basis function 𝐵𝑖,𝑘defines 𝑘polynomial segments spanning 𝑘+ 1 knots 𝑡𝑖,𝑡𝑖+1, . . . ,𝑡𝑖+𝑘and is positive in the half-open domain [𝑡𝑖,𝑡𝑖+ 𝑘). The knots between 𝑡𝑝and 𝑡𝑚−𝑘(not included) are called “internal” or “interior” knots. From here: For 𝑛control points we have 𝑛+ 𝑘knots and 𝑛−𝑘interior knots. B-spline bases can be defined through the “Cox-de Boor” recur- sion starting from order 1 (degree 0): 𝐵𝑖,1(𝑢) = ( 1 if 𝑡𝑖≤𝑢< 𝑡𝑖+1 0 otherwise And with 𝐵𝑖,𝑘(𝑢) = 𝑢−𝑡𝑖 𝑡𝑖+𝑘−1 −𝑡𝑖 𝐵𝑖,𝑘−1(𝑢) + 𝑡𝑖+𝑘−𝑢 𝑡𝑖+𝑘−𝑡𝑖+1 𝐵𝑖+1,𝑘−1(𝑢) The number of control points 𝑛, order 𝑘and number of knots 𝑚 are related by 𝑛+ 𝑘−𝑚= 0. For nonrepeating knot sequences, the curve will be 𝐶𝑘−2 continuously differentiable. A.1',\n",
       "  '= ( 1 if 𝑡𝑖≤𝑢< 𝑡𝑖+1 0 otherwise And with 𝐵𝑖,𝑘(𝑢) = 𝑢−𝑡𝑖 𝑡𝑖+𝑘−1 −𝑡𝑖 𝐵𝑖,𝑘−1(𝑢) + 𝑡𝑖+𝑘−𝑢 𝑡𝑖+𝑘−𝑡𝑖+1 𝐵𝑖+1,𝑘−1(𝑢) The number of control points 𝑛, order 𝑘and number of knots 𝑚 are related by 𝑛+ 𝑘−𝑚= 0. For nonrepeating knot sequences, the curve will be 𝐶𝑘−2 continuously differentiable. A.1 Derivatives The derivative of a B-spline basis function of order 𝑘is given by d d𝑢𝐵𝑖,𝑘(𝑢) = 𝐵′ 𝑖,𝑘(𝑢) = 𝑘−1 𝑡𝑖+𝑘−1 −𝑡𝑖 𝐵𝑖,𝑘−1(𝑢) − 𝑘−1 𝑡𝑖+𝑘−𝑡𝑖+1 𝐵𝑖+1,𝑘−1(𝑢). It is a linear combination of all the derivatives of the basis function. As a result, the derivative of a B-spline is equivalent to a B-spline of order 𝑘−1 with a new set of control points given by weighted differences of pairs of consecutive control points. A.2 Cardinal B-splines A cardinal B-spline (not to be confused with cardinal/Catmull-Rom splines) is a “normalized uniform B-spline”. It has uniformly spaced knots, with 𝑡𝑖+1 −𝑡𝑖= ℎ(uniform) with ℎ= 1 (normalized) so the knots are all integers (Fig. 23). Uniformity and normalization simplify the computations of a B-spline as all basis functions are translated versions of the same basis function that we denote as 𝑁𝑘(𝑢). We then have 𝒙(𝑢) = 𝑛−1 ∑︁ 𝑖=0 c𝑖𝑁𝑘(𝑢−𝑡𝑖) ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. 225:10 • Berio et al. and the B-spline derivatives simplify to d d𝑢𝑁𝑘(𝑢) = 𝑁′ 𝑘(𝑢) = 𝑁𝑘−1(𝑢) −𝑁𝑘−1(𝑢−1) so ¤𝒙(𝑢) = 𝑛−1 ∑︁ 𝑖=0 c𝑖(𝑁𝑘−1(𝑢−𝑡𝑖) −𝑁𝑘−1(𝑢−𝑡𝑖−1)) A.3 Smoothing term The smoothing term can be computed exactly and is considerably simplified for the case of cardinal B-splines [Schumaker 1981]. While different approaches exist to calculate this kind of integral [de Boor et al. 1976; Vermeulen et al. 1992] to calculate this kind of integral, we follow Fujioka and Kano [2007] and Fujioka et al. [2017] to have L𝑑 smooth = ∫∞ −∞ 𝐷(𝑢) d𝑢− ∫𝑡𝑘−1 −∞ 𝐷(𝑢) d𝑢− ∫∞ 𝑡𝑛 𝐷(𝑢) d𝑢 with 𝐷(𝑢) = ∥𝒙(𝑑) (𝑢)∥2 This can be computed explicitly by constructing a Gramian 𝑮 with: 𝐺𝑖,𝑗= \\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2 \\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3 ∫𝑘 0 𝑁(𝑑) 𝑖,𝑗d𝑢− ∫𝑝−𝑖 0 𝑁(𝑑) 𝑖,𝑗d𝑢 if 𝑖< 𝑝and 𝑗< 𝑝 ∫𝑘 0 𝑁(𝑑) 𝑖,𝑗d𝑢− ∫𝑝−𝑖 0 𝑁(𝑑) 𝑛+𝑝−𝑖,𝑛+𝑝−𝑗d𝑢 if 𝑖≥𝑛and 𝑗≥𝑛 ∫𝑘 0 𝑁(𝑑) 𝑖,𝑗d𝑢 otherwise and 𝑁(𝑑) 𝑖,𝑗 = 𝑁(𝑑) 𝑘 (𝑢)𝑁(𝑑) 𝑘 (𝑢−𝑗+ 𝑖) . Then each 𝐺𝑖,𝑗can be computed exactly using quadrature [Ver- meulen et al. 1992]. If we let 𝒄∈IR𝑛𝐷be a vector that concatenates 𝑛control points, each of dimensions 𝐷we have L𝑑 smooth = 𝒄⊤¯𝑮𝒄, ¯𝑮= 𝑮⊗𝑰𝐷 where ⊗is the Kroenecker product and 𝑰𝐷is the identity matrix of dimensions 𝐷. 2 0 2 4 6 8 0.0 0.2 0.4 0.6 0.8 1.0 Degree 3 4 2 0 2 4 6 8 u 0.0 0.2 0.4 0.6 0.8 1.0 Degree 4 Fig. 23. B-Splines and their bases with degrees 3 and 4. P-splines. A similar procedure can be efficiently approximated with discretization of the derivative cost, using penalized-splines (P-splines) as described by Eilers and Marx [1996]. To do this, we can simply use 𝑮= 𝑫⊤ (𝑑)𝑫(𝑑) with 𝑫(𝑑) a matrix representing the finite difference operator of order 𝑑. The advantage of this method is the',\n",
       "  'procedure can be efficiently approximated with discretization of the derivative cost, using penalized-splines (P-splines) as described by Eilers and Marx [1996]. To do this, we can simply use 𝑮= 𝑫⊤ (𝑑)𝑫(𝑑) with 𝑫(𝑑) a matrix representing the finite difference operator of order 𝑑. The advantage of this method is the simplicity of implementation and the possibility of achieving similar smoothing results. We can arbitrarily combine the degree of discrete differences with the degree of the curve. We expose both methods for completeness and to enable applications where the integral cost may be necessary (e.g., planning and robotics). A.4 Conversion to Bézier With the method of Romani and Sabin [2004], converting the 𝑝+ 1 control points of a quintic B-spline of degree 𝑝to single Bézier segment of the same degree, can be done with a (𝑝+ 1) × (𝑝+ 1) matrix that we denote as 𝑺𝑝. To convert all the control points of a B-spline we stack multiple shifted and overlapping copies of 𝑺𝑝into a larger matrix 𝑺, by shifting each copy by 𝑝rows and 1 column. For a quintic spline this can be visualized as: • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • The blocks for a quintic spline are given by: 𝑺5 = 1 120 \\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0 1 26 66 26 1 0 0 16 66 36 2 0 0 8 60 48 4 0 0 4 48 60 8 0 0 2 36 66 16 0 0 1 26 66 26 1 \\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb . The block matrix ¯𝑺used to compute the Bézier control points from the flattend B-spline control points 𝒄is given by the Kroenecker product 𝑺⊗𝑰𝐷. A.5 Degree reduction With the method of Sunwoo [2005], reducing a Bézier curve of degree 𝑝to one of degree 𝑞can be done with a (𝑞+ 1) × (𝑝+ 1) matrix that we denote as 𝑹𝑝,𝑞. To reduce the degree of all the control points of a Bézier chain we stack multiple shifted and overlapping copies of 𝑹𝑝,𝑞into a larger matrix 𝑹by shifting each copy by 𝑝rows and 𝑞columns. For a reduction from quintic to cubic this can be visualized as: ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. Neural Image Abstraction Using Long Smoothing B-Splines • 225:11 • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • The blocks of the quintic to cubic reduction matrix are given by 𝑹5,3 = \\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0 1 0 0 0 0 0 −2 3 5 3 0 0 0 0 0 0 0 0 5 3 −2',\n",
       "  '• • • • • • • • • • • • • • The blocks of the quintic to cubic reduction matrix are given by 𝑹5,3 = \\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0 1 0 0 0 0 0 −2 3 5 3 0 0 0 0 0 0 0 0 5 3 −2 3 0 0 0 0 0 1 \\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb The block matrix ¯𝑹used to compute the reduced Bézier control points from is given by the Kroenecker product 𝑹⊗𝑰𝐷. References Daniel Berio, Sylvain Calinon, and Frederic Fol Leymarie. 2017. Generating Calligraphic Trajectories with Model Predictive Control. In Proceedings of Graphics Interface. Canadian Human-Computer Communications Society, Edmonton, Canada. James Betker, Gabriel Goh, Li Jing, Aditya Ramesh, et al. 2023. Improving Image Generation with Better Captions. Technical Report. OpenAI. https://cdn.openai. com/papers/dall-e-3.pdf Carl De Boor. 2001. A Practical Guide to Splines (1 ed.). Springer, New York, NY. Carl de Boor, Tom Lyche, and Larry L Schumaker. 1976. On Calculating with B-splines II. Integration. Numerische Methoden der Approximationstheorie/Numerical Methods of Approximation Theory: Vortragsauszüge der Tagung über numerische Methoden der Approximationstheorie vom 25. bis 31. Mai 1975 im Mathematischen Forschungsinstitut Oberwolfach (Schwarzwald) (1976), 123–146. Tony D. DeRose and Brian A. Barsky. 1988. Geometric Continuity, Shape Parameters, and Geometric Constructions for Catmull-Rom Splines. ACM Transactions on Graphics 7, 1 (Jan. 1988), 1–41. doi:10.1145/42188.42265 Shimon Edelman and Tamar Flash. 1987. A Model of Handwriting. Biological Cybernetics 57, 1-2 (1987), 25–36. Magnus Egerstedt and Clyde Martin. 2009. Control Theoretic Splines: Optimal Control, Statistics, and Path Planning. Princeton University Press, Princeton. doi:10.1515/ 9781400833870 Paul H. C. Eilers and Brian D. Marx. 1996. Flexible Smoothing with B-splines and Penalties. Statist. Sci. 11, 2 (May 1996). doi:10.1214/ss/1038425655 Gerald E. Farin. 2001. Curves and Surfaces for CAGD: A Practical Guide (5th ed ed.). Morgan Kaufmann, San Francisco, CA. T Flash and N Hogan. 1985. The Coordination of Arm Movements: An Experimentally Confirmed Mathematical Model. The Journal of Neuroscience 5, 7 (July 1985), 1688– 1703. doi:10.1523/JNEUROSCI.05-07-01688.1985 Tamar Flash and Neville Hogan. 1998. Optimization Principles in Motor Control. In The Handbook of Brain Theory and Neural Networks. MIT Press, Cambridge, MA, USA, 682–685. Kevin Frans, Lisa Soros, and Olaf Witkowski. 2022. CLIPDraw: Exploring Text-to- Drawing Synthesis through Language-Image Encoders. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 5207–5218. Hiroyuki Fujioka and Hiroyuki Kano. 2007. Constructing Character Font Models from Measured Human Handwriting Motion. In 2007 American Control Conference. IEEE, New York, NY, USA, 1467–1472. doi:10.1109/ACC.2007.4282784 H. Fujioka, H. Kano, H. Nakata, and H. Shinoda. 2006. Constructing and Reconstruct- ing Characters, Words, and Sentences by Synthesizing Writing Motions. IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 36, 4 (July 2006), 661–670. doi:10.1109/TSMCA.2005.851344 Hiroyuki Fujioka, Wenli Zhu, Akinori Hidaka, and Hiroyuki Kano. 2017. Recon- structing Dynamic Font-Based Chinese Characters Using Support Vector Machine. In 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 2408–2413. doi:10.1109/SMC.2017.8122983 Rinon Gal, Yael Vinker, Yuval Alaluf, Amit Bermano, Daniel Cohen-Or, Ariel Shamir, and Gal Chechik. 2024. Breathing Life into Sketches Using',\n",
       "  'Akinori Hidaka, and Hiroyuki Kano. 2017. Recon- structing Dynamic Font-Based Chinese Characters Using Support Vector Machine. In 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC). 2408–2413. doi:10.1109/SMC.2017.8122983 Rinon Gal, Yael Vinker, Yuval Alaluf, Amit Bermano, Daniel Cohen-Or, Ariel Shamir, and Gal Chechik. 2024. Breathing Life into Sketches Using Text-to-Video Pri- ors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 4325–4336. Roy Ganz and Michael Elad. 2024. CLIPAG: Towards Generator-Free Text-to-Image Gen- eration. In 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, Waikoloa, HI, USA, 3831–3841. doi:10.1109/WACV57701.2024.00380 Neville Hogan and Dagmar Sternad. 2009. Sensitivity of smoothness measures to movement duration, amplitude, and arrests. J. Mot. Behav. 41, 6 (Nov. 2009), 529– 534. Iris A. M. Huijben, Wouter Kool, Max B. Paulus, and Ruud J. G. van Sloun. 2023. A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in Machine Learning . IEEE Transactions on Pattern Analysis & Machine Intelligence 45, 02 (Feb. 2023), 1353–1371. doi:10.1109/TPAMI.2022.3157042 Juno Hwang, Yong-Hyun Park, and Junghyo Jo. 2023. Resolution Chromatography of Diffusion Models. arXiv:2401.10247 [cs] doi:10.48550/arXiv.2401.10247 Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, and Ariel Shamir. 2023. Word-as-Image for Semantic Typography. ACM Transactions on Graphics 42, 4, Article 151 (July 2023). doi:10.1145/3592123 Ajay Jain, Amber Xie, and Pieter Abbeel. 2023b. VectorFusion: Text-to-SVG by Abstract- ing Pixel-Based Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1911–1920. Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. 2023a. OneFormer: One Transformer To Rule Universal Image Segmen- tation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2989–2998. Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical Reparametrization with Gumbel-Softmax. In Proceedings International Conference on Learning Representations (ICLR). https://openreview.net/pdf?id=rkE3y85ee Hiroyuki Kano, Magnus Egerstedt, Hiroaki Nakata, and Clyde F. Martin. 2003. B-Splines and Control Theory. Appl. Math. Comput. 145, 2 (2003), 263–288. doi:10.1016/S0096- 3003(02)00486-1 Hiroyuki Kano, Hiroaki Nakata, and Clyde F. Martin. 2005. Optimal Curve Fitting and Smoothing Using Normalized Uniform B-splines: A Tool for Studying Complex Systems. Appl. Math. Comput. 169, 1 (Oct. 2005), 96–128. doi:10.1016/j.amc.2004. 10.034 Craig S Kaplan and Robert Bosch. 2005. TSP Art. In Renaissance Banff: Mathematics, Music, Art, Culture. 301–308. Hiroharu Kato, Deniz Beker, Mihai Morariu, Takahiro Ando, Toru Matsuoka, Wadim Kehl, and Adrien Gaidon. 2020. Differentiable Rendering: A Survey. arXiv:2006.12057 [cs] doi:10.48550/arXiv.2006.12057 Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. 2024. Noise-Free Score Distillation. In The Twelfth International Conference on Learning Representations. Gihyun Kwon and Jong Chul Ye. 2022. CLIPstyler: Image Style Transfer with a Single Text Condition. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, New Orleans, LA, USA, 18041–18050. doi:10. 1109/CVPR52688.2022.01753 Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. 2023. TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. In AAAI 2023. Tzu-Mao Li, Michal Lukáč, Michaël Gharbi, and Jonathan Ragan-Kelley. 2020. Differen- tiable Vector Graphics Rasterization for Editing and Learning. ACM Transactions on Graphics 39, 6 (Dec. 2020),',\n",
       "  'Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. 2023. TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. In AAAI 2023. Tzu-Mao Li, Michal Lukáč, Michaël Gharbi, and Jonathan Ragan-Kelley. 2020. Differen- tiable Vector Graphics Rasterization for Editing and Learning. ACM Transactions on Graphics 39, 6 (Dec. 2020), 1–15. doi:10.1145/3414685.3417871 W. Li and E. Todorov. 2004. Iterative Linear Quadratic Regulator Design for Nonlin- ear Biological Movement Systems. In Proc. Intl Conf. on Informatics in Control, Automation and Robotics (ICINCO). 222–229. Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Ying-Cong Chen. 2024. LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Chenxi Liu, Jessica Hodgins, and James McCann. 2017. Whole-cloth quilting pat- terns from photographs. In Proceedings of the Symposium on Non-Photorealistic Animation and Rendering (Los Angeles, California) (NPAR ’17). Association for Computing Machinery, New York, NY, USA, Article 7, 8 pages. doi:10.1145/3092919. 3092925 Lizheng Lu. 2015. A Note on Curvature Variation Minimizing Cubic Hermite Inter- polants. Appl. Math. Comput. 259 (May 2015), 596–599. doi:10.1016/j.amc.2014.11. 113 Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Yong, Juhyun Lee, Wan- Teh Chang, Wei Hua, Manfred Georg, and Matthias Grundmann. 2019. Medi- aPipe: A Framework for Perceiving and Processing Reality. In Third Workshop on Computer Vision for AR/VR at IEEE Computer Vision and Pattern Recognition (CVPR) 2019. https://mixedreality.cs.cornell.edu/s/NewTitle_May1_MediaPipe_ CVPR_CV4ARVR_Workshop_2019.pdf Ron Maharik, Mikhail Bessmeltsev, Alla Sheffer, Ariel Shamir, and Nathan Carr. 2011. Digital micrography. ACM Transactions on Graphics 30, 4, Article 100 (July 2011), 12 pages. doi:10.1145/2010324.1964995 Hirotsugu Matsukida and Hiroyuki Fujioka. 2013. Modeling and Reshaping Handwritten Characters Based on Dynamic Font Model. In 2013 International Joint Conference on Awareness Science and Technology & Ubi-Media Computing (iCAST 2013 & UMEDIA 2013). IEEE, Aizuwakamatsu, Japan, 344–350. doi:10.1109/ICAwST.2013. 6765463 ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025. 225:12 • Berio et al. H. Meier and H. Nowacki. 1987. Interpolating Curves with Gradual Changes in Curva- ture. Computer Aided Geometric Design 4, 4 (Dec. 1987), 297–305. doi:10.1016/0167- 8396(87)90004-5 Daniel Mellinger and Vijay Kumar. 2011. Minimum Snap Trajectory Generation and Control for Quadrotors. In 2011 IEEE International Conference on Robotics and Automation. 2520–2525. doi:10.1109/ICRA.2011.5980409 Daniela Mihai and Jonathon Hare. 2021. Differentiable Drawing and Sketching. arXiv:2103.16194 [cs] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Text- to-3D Using 2D Diffusion. In The Eleventh International Conference on Learning Representations. H. Pottmann. 1990. Smooth Curves under Tension. Computer-Aided Design 22, 4 (May 1990), 241–245. doi:10.1016/0010-4485(90)90053-F Zhiyu Qu, Lan Yang, Honggang Zhang, Tao Xiang, Kaiyue Pang, and Yi-Zhe Song. 2024. Wired Perspectives: Multi-View Wire Art Embraces Generative AI. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, Seattle, WA, USA, 6149–6158. doi:10.1109/CVPR52733.2024.00588 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning.',\n",
       "  'Seattle, WA, USA, 6149–6158. doi:10.1109/CVPR52733.2024.00588 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning. PMLR, 8748–8763. Christian H. Reinsch. 1967. Smoothing by Spline Functions. Numer. Math. 10, 3 (Oct. 1967), 177–183. doi:10.1007/BF02162161 Kejia Ren and Paul G. Kry. 2019. Single Stroke Aerial Robot Light Paint- ing. In Proceedings of the 8th ACM/Eurographics Expressive Symposium on Computational Aesthetics and Sketch Based Interfaces and Modeling and Non-Photorealistic Animation and Rendering (Expressive ’19). Eurographics As- sociation, Goslar, DEU, 61–67. doi:10.2312/exp.20191077 L Romani and M.A Sabin. 2004. The Conversion Matrix between Uniform B-spline and Bézier Representations. Computer Aided Geometric Design 21, 6 (July 2004), 549–560. doi:10.1016/j.cagd.2004.04.002 Peter Schaldenbrand, James McCann, and Jean Oh. 2023. FRIDA: A Collaborative Robot Painter with a Differentiable, Real2Sim2Real Planning Environment. In 2023 IEEE International Conference on Robotics and Automation (ICRA). 11712–11718. doi:10.1109/ICRA48891.2023.10160702 Larry L. Schumaker. 1981. Spline Functions: Basic Theory. Adrian Secord. 2002. Weighted Voronoi Stippling. In Proceedings of the 2nd International Symposium on Non-photorealistic Animation and Rendering. ACM, Annecy France, 37–43. doi:10.1145/508530.508537 Hasik Sunwoo. 2005. Matrix Representation for Multi-Degree Reduction of Bézier Curves. Computer Aided Geometric Design 22, 3 (March 2005), 261–273. doi:10. 1016/j.cagd.2004.12.002 A. Tewari, O. Fried, J. Thies, V. Sitzmann, S. Lombardi, K. Sunkavalli, R. Martin-Brualla, T. Simon, J. Saragih, M. Nießner, R. Pandey, S. Fanello, G. Wetzstein, J.-Y. Zhu, C. Theobalt, M. Agrawala, E. Shechtman, D. B Goldman, and M. Zollhöfer. 2020. State of the Art on Neural Rendering. Computer Graphics Forum 39, 2 (2020), 701–727. doi:10.1111/cgf.14022 E. Todorov. 2004. Optimality Principles in Sensorimotor Control. Nature Neuroscience 7, 9 (2004), 907–915. Emanuel Todorov and Michael I. Jordan. 1998. Smoothness Maximization Along a Predefined Path Accurately Predicts the Speed Profiles of Complex Arm Movements. Journal of Neurophysiology 80, 2 (Aug. 1998), 696–714. doi:10.1152/jn.1998.80.2.696 Kenji Tojo, Ariel Shamir, Bernd Bickel, and Nobuyuki Umetani. 2024. Fab- ricable 3D Wire Art. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers ’24. ACM, Denver CO USA, 1–11. doi:10.1145/3641519.3657453 Zhifang Tong, Bolei Zuo, Xiaoxia Yang, Shengjun Liu, and Xinru Liu. 2025. Continuous- Line Image Stylization Based on Hilbert Curve. Computer Graphics Forum (2025). doi:10.1111/cgf.70169 M. Toussaint. 2017. A Tutorial on Newton Methods for Constrained Trajectory Opti- mization and Relations to SLAM, Gaussian Process Smoothing, Optimal Control, and Probabilistic Inference. In Geometric and Numerical Foundations of Movements. Springer International Publishing, Cham, 361–392. Alan H Vermeulen, Richard H Bartels, and Glenn R Heppler. 1992. Integrating Products of B-splines. SIAM journal on scientific and statistical computing 13, 4 (1992), 1025– 1038. Yael Vinker, Yuval Alaluf, Daniel Cohen-Or, and Ariel Shamir. 2023. CLI- Pascene: Scene Sketching with Different Types and Levels of Abstraction. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Paris, France, 4123–4133. doi:10.1109/ICCV51070.2023.00383 Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. 2022. CLIPasso: Semantically-Aware Object Sketching. ACM Transactions on Graphics 41,',\n",
       "  'Different Types and Levels of Abstraction. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Paris, France, 4123–4133. doi:10.1109/ICCV51070.2023.00383 Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Roman Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. 2022. CLIPasso: Semantically-Aware Object Sketching. ACM Transactions on Graphics 41, 4, Article 86 (July 2022). doi:10.1145/3528223.3530068 Pauli Virtanen et al. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261–272. doi:10.1038/s41592-019-0686-2 Viviani, Paolo and Flash, Tamar. 1995. Minimum-Jerk, Two-Third Power Law, and Isochrony, Converging Approaches to Motor Planning. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan LI, Hang Su, and Jun Zhu. 2023. ProlificDreamer: High-fidelity and Diverse Text-to-3D Generation with Vari- ational Score Distillation. In Advances in Neural Information Processing Systems, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 8406–8441. Fernando J. Wong and Shigeo Takahashi. 2011. A Graph-based Approach to Continuous Line Illustrations with Variable Levels of Detail. Computer Graphics Forum 30, 7 (2011), 1931–1939. arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467- 8659.2011.02040.x doi:10.1111/j.1467-8659.2011.02040.x Markus Worchel and Marc Alexa. 2023. Differentiable Rendering of Parametric Ge- ometry. ACM Transactions on Graphics 42, 6, Article 232 (Dec. 2023), 18 pages. doi:10.1145/3618387 XiMing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian Yu, and Dong Xu. 2023. DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models. In Thirty-seventh Conference on Neural Information Processing Systems. https: //openreview.net/forum?id=CY1xatvEQj Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. 2024. SVGDreamer: Text Guided SVG Generation with Diffusion Model. (June 2024), 4546–4555. Jie Xu and Craig S. Kaplan. 2007. Calligraphic Packing. In Proceedings of Graphics Interface 2007 on - GI ’07. ACM Press, Montreal, Canada, 43. doi:10.1145/ 1268517.1268527 Zhijin Yang, Pengfei Xu, Hongbo Fu, and Hui Huang. 2021. WireRoom: model-guided explorative design of abstract wire art. ACM Transactions on Graphics 40, 4, Article 128 (July 2021), 13 pages. doi:10.1145/3450626.3459796 Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. IP-adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models. arXiv:2308.06721 [cs.CV] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding Conditional Control to Text-to-Image Diffusion Models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, Paris, France, 3813–3824. doi:10.1109/ICCV51070. 2023.00355 Peiying Zhang, Nanxuan Zhao, and Jing Liao. 2024. Text-to-Vector Generation with Neural Path Representation. ACM Transactions on Graphics 43, 4 (July 2024), 1–13. doi:10.1145/3658204 Changqing Zou, Junjie Cao, Warunika Ranaweera, Ibraheem Alhashim, Ping Tan, Alla Sheffer, and Hao Zhang. 2016. Legible Compact Calligrams. ACM Transactions on Graphics 35, 4 (July 2016), 1–12. doi:10.1145/2897824.2925887 ACM Trans. Graph., Vol. 44, No. 6, Article 225. Publication date: December 2025.'],\n",
       " ['A Hybrid Deep Learning based Carbon Price Forecasting Framework with Structural Breakpoints Detection and Signal Denoising Runsheng Rena,1, Jing Lia,1, Yanxiu Lia, Shixun Huanga,∗, Jun Shena, Wanqing Lia, John Lea, Sheng Wangb aSchool of Computing and Information Technology, University of Wollongong, Wollongong, Australia bSchool of Computer Science, Wuhan University, Wuhan, China Abstract Accurately forecasting carbon prices is essential for informed energy market decision-making, guiding sustainable energy planning, and supporting effective decarbonization strategies. How- ever, it remains challenging due to structural breaks and high-frequency noise caused by frequent policy interventions and market shocks. Existing studies, including the most recent baseline ap- proaches, have attempted to incorporate breakpoints but often treat denoising and modeling as separate processes and lack systematic evaluation across advanced deep learning architectures, limiting the robustness and the generalization capability. To address these gaps, this paper pro- poses a comprehensive hybrid framework that integrates structural break detection (Bai–Perron, ICSS, and PELT algorithms), wavelet signal denoising, and three state-of-the-art deep learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot prices from 2007 to 2024 and exogenous features such as energy prices and policy indicators, the framework constructs univariate and multivariate datasets for comparative evaluation. Experimental results demonstrate that our proposed PELT-WT-TCN achieves the highest prediction accuracy, reduc- ing forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by 70.55% in RMSE and 74.42% in MAE compared to the original LSTM without decomposition from the same baseline study. These findings underscore the value of integrating structural awareness and multiscale decom- position into deep learning architectures to enhance accuracy and interpretability in carbon price forecasting and other nonstationary financial time series. Keywords: Carbon price forecasting, European Union Emissions Trading System, structural breakpoints detection, wavelet transform, deep learning ∗Corresponding author. Email address: shixunh@uow.edu.au (Shixun Huang) 1These authors contributed equally as co-first authors. arXiv:2511.04988v1 [cs.LG] 7 Nov 2025 1. Introduction Background. Against the backdrop of increasingly severe global climate change, carbon mar- kets have become a vital economic tool for controlling carbon emissions. Among them, the European Union Emissions Trading System (EU ETS) is the most mature and influential carbon trading framework in the world (European Commission, 2020). Within this system, European Union Allowances (EUA), the de facto tradable permits for greenhouse gas emissions, serve as a market indicator. Their prices (i.e., the \"EU carbon price\") (Koch et al., 2014) not only re- flect supply and demand dynamics but also convey changes in macroeconomic conditions and policy signals. Therefore, accurately forecasting EUA carbon prices is crucial for policy for- mulation, enabling hybrid data-driven and physical modeling approaches, supporting intelligent energy management, guiding decarbonization roadmaps, and addressing both energy system and societal impacts. However, the dynamic nature of the carbon market makes forecasting particularly complex. Unlike traditional financial assets, carbon prices are heavily influenced by frequent policy shocks, such as cap adjustments, energy price volatility, and geopolitical disruptions (Koch et al., 2014). These external factors often result in structural breaks and high-frequency noise (Lin and Zhang, 2022), making carbon prices highly',\n",
       "  'makes forecasting particularly complex. Unlike traditional financial assets, carbon prices are heavily influenced by frequent policy shocks, such as cap adjustments, energy price volatility, and geopolitical disruptions (Koch et al., 2014). These external factors often result in structural breaks and high-frequency noise (Lin and Zhang, 2022), making carbon prices highly nonlinear and nonstationary, which poses serious challenges to existing modeling and forecasting techniques. Existing Approaches and their Limitations. Although numerous studies have attempted to forecast EUA carbon prices (Huang et al., 2021; Lin and Zhang, 2022), most models still rely on linear econometric approaches or fail to fully account for structural changes and noise in the carbon price series. For example, models such as ARIMA (Box et al., 1976), GARCH (Bollerslev, 1986) have been widely applied. More recently, deep learning frameworks, includ- ing LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), and carbon price spe- cific approaches (Zhang and Wen, 2022) have also emerged. While these models demonstrate strong predictive performance during stable periods, they often suffer from underfitting, lagged responses, and poor generalization when confronted with major institutional changes or macroe- conomic disruptions, making them inadequate for real-world forecasting tasks. While some recent works attempt to address these challenges, important limitations still re- main. For instance, Liu et al. (2012) applied wavelet analysis to carbon allowance price dynam- ics, showing that time and frequency methods can capture structural variations in carbon price series. Building on such approaches, later studies (Lin and Zhang, 2022) have developed hybrid models that integrate structural breakpoint detection (e.g., Bai–Perron and ICSS algorithms) and wavelet based denoising with deep learning architectures such as LSTM, which significantly im- proved prediction accuracy. However, these studies typically employ only a single deep learning architecture (LSTM) without comparative evaluation of alternative models (e.g., GRU, TCN (Bai et al., 2018)), limiting the robustness assessment. On the other hand, breakpoint detection is conducted solely on carbon price series, neglecting macroeconomic or policy driven external features, which limits interpretability. Moreover, most of these methods rely on Bai–Perron or ICSS, which are computationally intensive and less flexible for large-scale datasets. To over- come this, our study adopts the PELT algorithm, which offers linear computational complexity and can detect multiple change points efficiently, making it particularly suitable for incorporating multivariate external factors alongside carbon price series. Further, despite employing wavelet denoising, they do not systematically explore multiscale decomposition, potentially missing im- portant frequency domain dynamics. 2 Our goals and Innovations. To address the aforementioned issues, our proposed framework integrates PELT breakpoints detection, wavelet signal decomposition, comparative evaluation of multiple deep learning models, and the incorporation of multisource external features. This integrated design significantly enhances the robustness, accuracy, and interpretability of carbon price forecasting by: • Integrating methods into a unified pipeline: aligning training data with stable market regimes through breakpoint detection (Killick et al., 2012; Bai and Perron, 2003; Inclan and Tiao, 1994), applying wavelet analysis to highlight key dynamics in carbon price data (Liu et al., 2012), and linking breaks to specific policy or economic events for clearer interpretation (Lin and Zhang, 2022).',\n",
       "  'with stable market regimes through breakpoint detection (Killick et al., 2012; Bai and Perron, 2003; Inclan and Tiao, 1994), applying wavelet analysis to highlight key dynamics in carbon price data (Liu et al., 2012), and linking breaks to specific policy or economic events for clearer interpretation (Lin and Zhang, 2022). • Comparative evaluation of multiple deep learning architectures: implementing LSTM (Hochreiter and Schmidhuber, 1997), GRU (Cho et al., 2014), and TCN (Bai et al., 2018) to systematically assess robustness and generalizability across different scenarios. • Incorporating external features: expanding the feature space with commodity prices, exchange rates, and policy signals to capture macroeconomic and policy-driven influences on carbon price dynamics. This study also builds upon a real EUA carbon market dataset, incorporating multiple external variables such as coal and natural gas prices (Lin and Zhang, 2022), policy events (Fan, 2025), and broader financial indicators (e.g., exchange rates) that have been considered in recent deep learning approaches (Zhao and Xu, 2023). A new multifactor enhanced dataset is constructed to improve the feature richness and realism of the training data. Contributions. The main contributions of this study are summarized as follows: • We propose a comprehensive forecasting framework that tightly couples structural break detection, wavelet denoising, and deep learning modeling in a unified pipeline. Unlike prior modular approaches that treat these components independently, we leverage detected breakpoints to guide the segmentation and wavelet decomposition of the time series, en- abling the learning models to focus on cleaner, regime consistent data. This integration improves robustness to structural changes and enhances forecasting accuracy under high market volatility, directly addressing the limitations of baseline methods. • We enrich the feature set by incorporating commodity prices, exchange rates, and policy signals, thereby expanding input diversity and enabling the models to better capture the economic and policy-driven dynamics underlying carbon price fluctuations. • We implement and evaluate multiple deep learning models—PELT-WT-LSTM (both uni- variate and multivariate variants), PELT-WT-GRU, and PELT-WT-TCN—and conduct a systematic comparison against the baseline BP&ICSS-WT-LSTM. This allows us to assess the effectiveness of alternative architectures beyond traditional LSTM models. • Our experiments demonstrate that incorporating breakpoint aware decomposition signifi- cantly improves forecasting accuracy and robustness under volatile market conditions. Our best-performing method, PELT-WT-TCN, achieves a MAE of 1.1855 and an RMSE of 1.5866. This performance indicates an 18.63% reduction in MAE and a 22.35% reduction in RMSE respectively, compared to the strongest baseline model, and a 70.55% reduction in RMSE and a 74.42% reduction in MAE respectively, compared to the weakest baseline. 3 Structure of the Remainder of this Paper. Section 2: Related work reviews previous studies and defines our approach, including structural breakpoint detection, time series modeling, and model research; Section 3: Methodology describes the research methods, including breakpoint detection, wavelet denoising, and model architecture design; Section 4: Experiments present the results along with their implications and limitations; Section 5: Conclusion summarizes this work and outlines future directions. 2. Related Work Signal Decomposition and Deep Learning Models. Several recent studies (Zhang et al., 2024, 2023; Wang et al., 2023) have explored hybrid models that combine',\n",
       "  'design; Section 4: Experiments present the results along with their implications and limitations; Section 5: Conclusion summarizes this work and outlines future directions. 2. Related Work Signal Decomposition and Deep Learning Models. Several recent studies (Zhang et al., 2024, 2023; Wang et al., 2023) have explored hybrid models that combine signal decomposition tech- niques with machine learning or deep learning architectures. For instance, a deep learning based framework including multisource features was proposed to improve prediction performance in carbon markets (Zhang and Wen, 2022). A multifrequency combined model was also introduced to capture both long-term and short-term patterns in price dynamics (Duan et al., 2025). In ad- dition, quantile regression with feature selection has been applied to improve accuracy under uncertain market conditions (Pang et al., 2023). These methods generally aim to denoise input data and capture nonlinear structures, often using tools such as EMD (Lorentz, 1938), CEEM- DAN (Torres et al., 2011), or wavelet transforms (Mallat, 1989). While such approaches have shown promise in improving accuracy, most do not explicitly address the role of structural break- points, or only incorporate them in a limited fashion (Zhang and Wen, 2022; Duan et al., 2025). Furthermore, few studies (Torres et al., 2011; Shahid et al., 2020; Zhang and Wen, 2022) conduct systematic comparisons of deep learning models under standardized conditions. It is therefore difficult to assess their generalizability, robustness and computational cost. Model Comparisons and Advanced Neural Architectures. Although the application of deep learning in carbon price forecasting has rapidly expanded in recent years, systematic compar- isons of different models under standardized conditions remain scarce. This lack of comparative studies limits a comprehensive evaluation of these methods in terms of generalization ability, robustness, and computational cost. To address this gap, several studies have explored the issue from various perspectives. One study combined GARCH and LSTM models to handle the high volatility of the EU carbon market, demonstrating the potential of hybrid modeling in multi- scale forecasting (Huang et al., 2021). Another study incorporated attention mechanisms into a BiLSTM network to improve its ability to model degradation trends, thereby highlighting how structural modifications can impact model performance (Guo et al., 2023). Ensemble deep learn- ing methods based on high-frequency trading data were evaluated to emphasize the coupling between data granularity and model compatibility (Zhao and Xu, 2023). Additionally, prediction accuracy has been examined from the perspectives of model integration (Chen et al., 2021) and regional heterogeneity (Zhao et al., 2023). An integrated CEEMDAN and TCN-LSTM frame- work has also been proposed to enhance adaptability and accuracy under nonlinear volatility (Cai et al., 2025). Model Interpretability and Performance Optimization. In the area of model interpretabil- ity and decision support, SHAP and LIME techniques have been applied to improve the trans- parency and practical applicability of AI models in carbon market forecasting (Olawumi and Oladapo, 2025; Lei et al., 2024). Comparative validation has been conducted within an SSA- LSTM framework to provide empirical support for methodological evaluation (Wang et al., 2022). Furthermore, LightGBM combined with Bayesian optimization has been used to as- sess error convergence across',\n",
       "  'AI models in carbon market forecasting (Olawumi and Oladapo, 2025; Lei et al., 2024). Comparative validation has been conducted within an SSA- LSTM framework to provide empirical support for methodological evaluation (Wang et al., 2022). Furthermore, LightGBM combined with Bayesian optimization has been used to as- sess error convergence across multiparameter spaces, emphasizing the importance of structural 4 tuning and performance optimization (Deng et al., 2024). Collectively, these studies underscore the critical need to establish standardized evaluation benchmarks and cross model performance comparison systems to support the sustainable development of intelligent forecasting in carbon markets. Breakpoint Detection and Carbon Price Forecasting. The baseline paper (Lin and Zhang, 2022) proposed a forecasting framework that integrates structural breakpoint detection with deep learning and highlights the value of capturing policy changes to improve the prediction of carbon prices. While this provides a useful foundation for hybrid modeling approaches that account for market dynamics, the structural breakpoint detection methods adopted are relatively dated and more suitable for identifying linear structural changes. This limits their ability to capture more complex or nonlinear regime shifts that are often present in volatile markets such as carbon trad- ing. Building on this line of research, our study optimizes the breakpoint detection method to- gether with wavelet based signal decomposition. In addition, we systematically evaluate several deep learning models using the preprocessed data (univariate, multivariate), including standard LSTM without structural features, univariate LSTM(Hochreiter and Schmidhuber, 1997), mul- tivariate LSTM (Karim et al., 2019), GRU(Shahid et al., 2020), and TCN(Chang et al., 2024). All models are trained and tested under consistent conditions to enable a fair comparison across different input configurations. 3. Our Methodology Figure 1: The flowchart of the proposed framework. This framework outlines a carbon price forecasting process com- bining structural break detection, wavelet denoising, and sequence models (LSTM, GRU, TCN) for univariate and mul- tivariate analysis. This study formulates the task of carbon price forecasting as a multivariate, one-step regres- sion problem, aiming to predict the carbon allowance price at the next time point based on histor- ical carbon price data. As illustrated in Figure 1, the overall prediction framework encompasses key stages such as data construction, preprocessing, feature extraction, and model forecasting, forming a systematic and generalizable approach to carbon price prediction. First, the study con- structs the raw dataset based on actual carbon market data, and in the preprocessing stage, the carbon price series undergoes structural break detection and wavelet denoising to generate input features that are more stable and trend oriented. Specifically, Sections 3.1.1 and 3.1.2 present the theoretical foundations and implementation of the Bai-Perron+ICSS and PELT algorithms, which identify potential breakpoints in the time series from different perspectives to enhance the robustness and interpretability of subsequent modeling. Section 3.2 explains the application 5 of wavelet transform to separate different frequency components, remove high-frequency noise, and retain the main trend signal, thereby improving the quality of extracted features. Based on this, the denoised carbon price series is processed with time lags to construct input variables that reflect temporal dependencies, which are then integrated with structural break',\n",
       "  'transform to separate different frequency components, remove high-frequency noise, and retain the main trend signal, thereby improving the quality of extracted features. Based on this, the denoised carbon price series is processed with time lags to construct input variables that reflect temporal dependencies, which are then integrated with structural break information to form the final modeling feature set. Subsequently, Section 3.3.1 introduces typical deep learn- ing models such as LSTM, GRU, and TCN, detailing their structural principles and suitability for nonlinear time series prediction, and demonstrating their respective advantages for carbon price forecasting tasks. Furthermore, Section 3.4 compares the detection performance of differ- ent structural break detection methods, discusses the rationale for model selection, andvisualizes the overall prediction process and the connections between each stage. Through the integration of these steps, the proposed framework effectively achieves the transformation from raw data to structured and denoised features, combines key breakpoints identification with deep learning model training, and ultimately outputs more accurate and interpretable carbon price predictions. 3.1. Underlying Models 3.1.1. Bai Perron + ICSS Bai-Perron Structural Break Test. The Bai–Perron test (Bai and Perron, 2003) is a classical method for detecting multiple structural breakpoints in time series data. It partitions the series yt into m + 1 regimes by minimizing the residual sum of squares (RSS). Formally, the model is expressed as a piecewise linear regression: yt = x⊤ t βj + ut, t = τ j−1 + 1, . . . , τj, j = 1, . . . , m + 1 (1) where • yt: the dependent variable (e.g., carbon price) at time t. • xt: a vector of explanatory variables (regressors) at time t. • βj: the coefficient vector specific to regime j. • ut: the error term at time t. • τj: the j-th structural break point. • m: the number of structural breaks in the series. The breakpoints {τ1, . . . , τm} are estimated by minimizing the overall residual sum of squares: S T = min {τ1,...,τm} m+1 X j=1 τ j X t=τ j−1+1 \\x10 yt −x⊤ t ˆβj \\x112 (2) To evaluate significance, the S upF test statistic is constructed: SupF = ⌊T−ηT⌋ max k=⌈1+ηT⌉ S 0 −S k bσ2 (3) where S 0 is the RSS under the null hypothesis (no break), S k is the RSS under the alternative (break at k), and ˆσ2 is the variance estimate. 6 ICSS Algorithm. The Iterative Cumulative Sum of Squares (ICSS) algorithm (Inclan and Tiao, 1994) detects structural changes in the variance of the same time series yt. The observed series can be expressed as: yt = µ + et, t = 1, 2, . . . , T (4) where µ is the mean and et is the residual term. To test for variance shifts, the cumulative variance statistic is defined as: Ck = k X i=1 z2 i , zt = yt −¯y1:t q 1 t + 1 T S y , t = 1, . . . , T (5) where ¯y1:t is the mean of the first t observations and',\n",
       "  'variance shifts, the cumulative variance statistic is defined as: Ck = k X i=1 z2 i , zt = yt −¯y1:t q 1 t + 1 T S y , t = 1, . . . , T (5) where ¯y1:t is the mean of the first t observations and S y is the sample standard deviation. The normalized deviation is: Dk = Ck CT −k T , D0 = DT = 0, (6) and the test statistic is: IT = sup \\x12 q T 2 |Dk| \\x13 . (7) If IT exceeds a critical threshold, a variance break is identified. Unified Output. Both Bai–Perron (mean/trend changes) and ICSS (variance shifts) produce breakpoints {τ j}, which are encoded into regime labels rt. These regime labels serve as structured inputs for subsequent forecasting models. 3.1.2. PELT Multiple breakpoints Detection. The Pruned Exact Linear Time (PELT) algorithm (Killick et al., 2012) detects multiple breakpoints in yt by minimizing the cost function: m+1 X i=1 C(yτi−1+1:τi) + βm, (8) where C(·) is the segment cost (e.g., negative log-likelihood), β is the penalty, and {τ1, . . . , τm} are breakpoints. Recursive Formulation. Let F(t) denote the optimal segmentation cost up to time t: F(t) = min s<t n F(s) + C(ys+1:t) + β o . (9) This recursion prunes suboptimal candidates, achieving O(n) complexity in both time and space. Unified Output. The resulting breakpoints {τ j} are also encoded into regime labels rt, ensuring consistency with Bai–Perron and ICSS. This unified representation allows fair comparison of forecasting models under different breakpoint detection strategies. 7 3.2. Wavelet Transform Wavelet transform (WT) is a time–frequency analysis tool that projects a signal f(t) onto a family of dilated and translated basis functions, thereby revealing localised spectral information. Owing to its multiresolution capability, WT has become a standard procedure for decomposing nonstationary series into an approximation component (low frequency, denoised) and a detail component (high-frequency) (Mallat, 1989). Figure 2 illustrates the multilevel wavelet decom- position pipeline adopted in this study, where the original signal is first decomposed into an approximation component (A1) and a detail component (D1). The approximation component is then recursively decomposed into further approximation (A2, A3, ...) and detail components (D2, D3, ...) at subsequent levels. Figure 2: Flowchart of wavelet-transform decomposition. Nested subspaces. Vj ⊂Vj+1, j ∈Z (10) where • Vj: approximation subspace at resolution 2 j; • j: scale index. Limit properties. [ j∈Z Vj = L2(R), \\\\ j∈Z Vj = {0} (11) where • L2(R): space of square-integrable functions; • Vj: approximation subspace. Scaling function and orthonormal basis. φ j,n(x) = 2j/2 φ(2 jx −n), {φ j,n(x)}n∈Z (12) where • φ(x): scaling function (father wavelet); • φj,n(x): scaling basis at scale j and translation n; • j: scale index (dilation); • n: translation index (shift). 8 Orthogonal expansion. Aj f(x) = X n∈Z ⟨f, φj,n⟩φj,n(x) (13) where • Aj f: projection of f onto subspace V j (approximation); • ⟨f, φj,n⟩: approximation coefficient; • φj,n(x): scaling basis function. Convolution + downsampling (pyramid algorithm). ⟨f, φj,n⟩= X k∈Z h(2n −k) ⟨f, φj−1,k⟩ (14)',\n",
       "  'translation index (shift). 8 Orthogonal expansion. Aj f(x) = X n∈Z ⟨f, φj,n⟩φj,n(x) (13) where • Aj f: projection of f onto subspace V j (approximation); • ⟨f, φj,n⟩: approximation coefficient; • φj,n(x): scaling basis function. Convolution + downsampling (pyramid algorithm). ⟨f, φj,n⟩= X k∈Z h(2n −k) ⟨f, φj−1,k⟩ (14) where • h(n): low-pass filter associated with the scaling function; • f: original signal; • φj−1,k: scaling basis at the previous level. Frequency-domain definition of the wavelet. ˆψ(ω) = G \\x10 ω 2 \\x11 ˆφ \\x10 ω 2 \\x11 , G(ω) = e−iω H(ω + π) (15) where • ψ(x): wavelet function (mother wavelet); • ˆψ(ω): Fourier transform of the wavelet function; • φ(x): scaling function; • H(ω): low-pass filter in the Fourier domain; • G(ω): high-pass filter in the Fourier domain. Detail signal expansion. Dj f = \\x10 ⟨f, ψj,n⟩ \\x11 n∈Z (16) where • Dj f: detail component at scale j; • ψj,n(x): wavelet basis at scale j, translation n; • ⟨f, ψj,n⟩: detail coefficient. 9 Complete wavelet decomposition. f = AJ f + ∞ X j=J Dj f (17) where • f: original signal; • AJ f: approximation component at the coarsest scale J; • Dj f: detail components at scales j ≥J. 3.3. Sequence Forecasting Models 3.3.1. Long Short-Term Memory (LSTM) Long Short-Term Memory (LSTM). Long short-term memory (LSTM) network is a gated variant of the recurrent neural network (RNN) originally proposed by Hochreiter and Schmidhu- ber (1997). By introducing gating mechanisms, LSTM alleviates the vanishing-gradient problem that plagues conventional RNNs, thereby retaining long range dependencies in time series. It has become the de facto backbone for sequential modelling across finance, speech and language domains. Figure 3 illustrates the architecture of the peephole-free LSTM cell employed in our study, which takes the unified input zt = [˜yt, ut, et]. Figure 3: Architecture of a peephole-free LSTM cell with the unified input zt = [˜yt, ut, et]. Following Sections 3.1 and 3.2, we construct a unified input vector zt = \\uf8ee\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0 ˜yt ut et \\uf8f9\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb∈Rd, where ˜yt denotes the wavelet-denoised carbon price (from Sec. 3.2), ut ∈Rp represents exoge- nous variables, and et ∈Rm+1 is the one-hot encoded regime label from breakpoint detection (Sec. 3.1). At each time step t, the LSTM cell receives this input zt, together with the previous hidden state ht−1 ∈Rh and the previous cell state Ct−1 ∈Rh. Three gates—forget, input, and output—jointly regulate the information flow: 10 ft = σ(Wf zt + U f ht−1 + b f ), (18) it = σ(Wizt + Uiht−1 + bi), (19) ˜Ct = tanh(Wazt + Uaht−1 + ba), (20) where • ft: the forget gate activation at time t, controlling how much of the previous memory to retain. • it: the input gate activation at time t, controlling how much new information to write to the cell state. • ˜Ct: the candidate cell state (candidate activation), computed using a tanh activation. • zt: the unified input vector, consisting of the wavelet-denoised carbon price ˜yt, the exoge- nous variables ut, and the one-hot regime label et. • ht−1:',\n",
       "  'controlling how much new information to write to the cell state. • ˜Ct: the candidate cell state (candidate activation), computed using a tanh activation. • zt: the unified input vector, consisting of the wavelet-denoised carbon price ˜yt, the exoge- nous variables ut, and the one-hot regime label et. • ht−1: the hidden state from the previous time step. • W, U: weight matrices for input and hidden state, respectively, specific to each gate. • b: the bias term corresponding to each gate. • σ(·): the logistic sigmoid activation function, producing values in the range (0, 1). • tanh(·): the hyperbolic tangent function, producing values in the range (−1, 1). Cell-state update. The forget gate ft⊙ct−1 determines how much historical information to retain, while it ⊙˜Ct writes the current candidate content into the memory cell. The cell state is updated as Ct = σ(ft ⊙Ct−1 + it ⊙˜Ct), (21) where • Ct: the cell state at time t, representing the long-term memory of the LSTM unit. • Ct−1: the previous cell state (from time t −1). • ft: the forget gate value, controlling how much of ct−1 is retained. • it: the input gate value, controlling how much new information is written into the cell. • ˜Ct: the candidate cell state (or activation), proposed as new content. • ⊙: element-wise (Hadamard) multiplication. 11 Hidden-state output. Finally, the output gate selects a portion of the updated cell state as the hidden representation: ot = σ(Wozt + Uoht−1 + bo), (22) ht = ot ⊙tanh(Ct), (23) where • ot: the output gate at time t, determining how much of the internal cell state is exposed to the output. • Wo, Uo: weight matrices for the current input zt and previous hidden state ht−1 in the output gate. • bo: the bias vector for the output gate. • ht: the hidden state (i.e., the actual output of the LSTM cell at time t). • Ct: the updated internal cell state. • ⊙: element-wise (Hadamard) multiplication. • tanh(Ct): the nonlinear transformation of the cell state to produce the output. 3.3.2. Gated Recurrent Unit (GRU) The Gated Recurrent Unit (GRU) is a streamlined alternative to the LSTM, retaining only two gates—update and reset—and discarding the explicit memory cell. Despite its compactness, GRU still mitigates vanishing gradients and has shown competitive performance in sequence modelling tasks, such as pandemic trajectory forecasting (Shahid et al., 2020). At time step t, given the unified input vector zt ∈Rd (defined in Sec. 3.3.1) and the previous hidden state ht−1 ∈Rh, the gates are computed as z(g) t = σ(Wzzt + Uzht−1 + bz), (24) ρt = σ(Wρzt + Uρht−1 + bρ), (25) where • z(g) t : the update gate at time t, controlling how much of the past hidden state is preserved. • ρt: the reset gate at time t, determining how much past information to forget. • zt: the unified input vector at time t, consisting of ˜yt, ut, and et. • ht−1: the hidden state from the previous time step. • Wz, Wρ: weight matrices for input-to-gate connections. •',\n",
       "  'preserved. • ρt: the reset gate at time t, determining how much past information to forget. • zt: the unified input vector at time t, consisting of ˜yt, ut, and et. • ht−1: the hidden state from the previous time step. • Wz, Wρ: weight matrices for input-to-gate connections. • Uz, Uρ: weight matrices for hidden-to-gate connections. • bz, bρ: bias vectors for the two gates. • σ(·): the logistic sigmoid activation function. 12 Candidate activation. The reset gate modulates how much past information enters the candi- date activation: ˜ht = tanh(Wzt + U(ρt ⊙ht−1) + bh), (26) where • ˜ht: the candidate activation at time t, representing the new content to be potentially added to the hidden state. • W, U: weight matrices for input-to-hidden and hidden-to-hidden connections. • zt: the unified input vector at time t, consisting of ˜yt, ut, and et. • ht−1: the previous hidden state. • ρt: the reset gate at time t, controlling the contribution of past memory. • bh: bias vector for the candidate activation. • ⊙: element-wise (Hadamard) multiplication. • tanh(·): hyperbolic tangent activation function. Hidden-state interpolation. Finally, the update gate linearly interpolates between the previ- ous state and the candidate activation: ht = (1 −z(g) t ) ⊙ht−1 + z(g) t ⊙˜ht, (27) where • ht: the final hidden state (and output) of the GRU cell at time t. • ht−1: the previous hidden state from time t −1. • z(g) t : the update gate, determining how much of the new candidate state ˜ht is used. • ˜ht: the candidate activation computed in Eq. (26). • (1 −z(g) t ) ⊙ht−1: the retained part of the previous hidden state. • z(g) t ⊙˜ht: the newly added information. • ⊙: element-wise (Hadamard) multiplication. Intuitively, z(g) t controls the trade-off between retaining past information and overwriting it with new content, while ρt determines how much historical context contributes to the candidate ac- tivation. Owing to this gated interpolation, GRU achieves comparable accuracy to LSTM with fewer parameters, which explains its adoption in recent time series studies. 13 3.3.3. Temporal Convolutional Network (TCN) Figure 4: The structure of the TCN model. Temporal Convolutional Network (TCN). The temporal convolutional network (TCN) adopted in this work, originally introduced for grating nanomeasurement by Chang et al. (2024), com- prises an input layer, N stacked residual blocks, a fully connected projection, and an output node. At time step t, the TCN receives a windowed input matrix Xt = [zt−T+1, . . . , zt] ∈RT×d, where each zτ is the unified input vector defined in Sec. 3.3.1, consisting of the wavelet-denoised carbon price ˜yτ, the exogenous variables uτ, and the regime one-hot label eτ. This design en- ables the TCN to capture dependencies across a receptive field of length T, in contrast to re- current models which process one time step at a time. The overall dataflow is illustrated in Figure 4 (Chang et al., 2024). Each residual block performs: 1. Dilated causal convolution (twice): a dilation factor d enlarges the receptive field expo- nentially while preserving temporal causality;',\n",
       "  'T, in contrast to re- current models which process one time step at a time. The overall dataflow is illustrated in Figure 4 (Chang et al., 2024). Each residual block performs: 1. Dilated causal convolution (twice): a dilation factor d enlarges the receptive field expo- nentially while preserving temporal causality; 2. Weight normalisation (two layers): stabilises activation scale and accelerates conver- gence; 3. Crop (shear): aligns sequence length after dilation and clips gradients to avert numerical overflow; 4. ReLU activation: injects nonlinearity after each convolution; 14 5. Dropout: randomly masks neurons to curb overfitting. An additional 1×1 convolution on the shortcut branch matches channel dimensions before summation. Finally, a dense layer maps the hidden representation to a scalar prediction delivered by the output node. This cascade allows the TCN to model both short-range and long-range dependencies efficiently and stably. 3.4. Comparison of related models Comparison of Structural Mutation Tests. The multiple structural mutation test method pro- posed by Bai and Perron (2003) is used to analyze carbon market time series with significant institutional changes. The core advantage of this method is that it can detect structural mutation points at multiple unknown locations simultaneously, and allows different regression models to be applied in each segment. This offers a powerful tool for revealing the dynamic relationship between carbon prices and macroeconomic or energy policy regimes. Although the Bai–Perron method performs well in detecting institutional shifts, it relies on segmented Ordinary Least Squares (OLS) regression, which imposes several limitations. First, it assumes linearity within each segment, making it less suitable for capturing nonlinear or jump type dynamics. Second, the algorithm has high computational complexity, particularly when the sample size is large or when many breakpoints exist. In contrast, the ICSS (Iterative Cumulative Sum of Squares) method (Inclan and Tiao, 1994) is specifically designed to detect variance changes in time series. It does not require parametric model specification within segments and is particularly useful for identifying abrupt shifts in volatility. However, ICSS assumes the mean of the series is constant and is more sensitive to outliers or serial correlation in the residuals, which may affect its accuracy. Compared with both BP and ICSS, the PELT (Pruned Exact Linear Time) method (Killick et al., 2012) is better suited for large-scale and high-frequency time series. It features a linear time complexity O(n) and employs pruning strategies to significantly reduce redundant search paths, thus maintaining high computational efficiency while preserving detection accuracy. PELT was chosen because breakpoints were measured for each column of data. Comparison of Sequence Forecasting Models. Carbon market prices are influenced by mul- tiple factors such as energy prices, policy interventions, supply and demand fluctuations, and seasonality. This results in significant nonlinearity and long-term and short-term dependencies in the price series. Effective forecasting requires models that can capture such complex tempo- ral dynamics. The Gated Recurrent Unit (GRU) is a recurrent neural network variant designed to retain long-term dependencies via update and reset gates, which helps mitigate the vanishing gradient problem (Shahid et al., 2020). In contrast, the Temporal Convolutional Network (TCN) employs dilated causal convolutions',\n",
       "  'can capture such complex tempo- ral dynamics. The Gated Recurrent Unit (GRU) is a recurrent neural network variant designed to retain long-term dependencies via update and reset gates, which helps mitigate the vanishing gradient problem (Shahid et al., 2020). In contrast, the Temporal Convolutional Network (TCN) employs dilated causal convolutions and residual connections to model long range dependencies with high efficiency and stability (Bai et al., 2018). While GRU is well-suited for sequential processing, its inherent sequential nature limits training speed. TCN, by contrast, supports par- allel computation and demonstrates faster convergence, making it advantageous for large scale or multistep forecasting tasks (Bai et al., 2018) (Lim et al., 2021). Empirical studies have shown that TCN can outperform GRU in many time series forecasting benchmarks, particularly in energy and financial domains (Lim et al., 2021). As carbon prices exhibit both market and policy driven volatility, a comparative evaluation of GRU and TCN provides valuable guidance in selecting robust modeling architectures for practical forecasting applications. 15 4. Experiment This section presents a comprehensive experimental analysis structured into four main parts. Section 4.1 introduces the dataset, outlines its statistical characteristics, and identifies structural breakpoints that signal major shifts in carbon pricing patterns. It also explains the data pre- processing technique using Wavelet Transform (WT) for denoising. Section 4.2 compares the performance of deep learning models, i.e., BP&ICSS-WT-LSTM, PELT-WT-LSTM, PELT-WT- GRU, and PELT-WT-TCN, under various configurations, highlighting their predictive behaviors. Section 4.3 provides a quantitative performance evaluation using metrics such as MAE, RMSE, MAPE and R². Section 4.4 further analyzes the scalability of the models and the distribution of residuals and errors to evaluate models’ robustness and generalizability. 4.1. Dataset Analysis and Preprocessing Data Description. The daily spot trading price data of EU Allowances (EUA) from September 10, 2007, to June 4, 2024, totaling 6,113 samples, was used for the empirical analysis in this study. This dataset was obtained from the financial market data and information service platform (investing.com). The primary reason for selecting this dataset is its ability to capture the dynamic evolution of the EU carbon trading market across multiple policy phases (European Commission, 2024). Moreover, the extended time span provides a richer set of training samples for machine learning models, which helps enhance their predictive performance and generalization capability. Key Impact Feature Analysis. Figure 5 presents the Pearson correlation coefficients between carbon prices and selected explanatory features. The results indicate that policy indicators, coal and natural gas prices, as well as stock market indices (e.g., Euro Stoxx 50) are strongly and positively correlated with carbon prices, highlighting their role as major drivers. In contrast, fea- tures such as the EUR/USD exchange rate and economic policy uncertainty indices show weaker or even negative correlations. These findings suggest that energy market fundamentals and pol- icy dynamics exert the strongest influence on carbon price movements, whereas macroeconomic and financial uncertainty indicators play a more limited role. The correlation analysis provides a basis for feature selection in subsequent forecasting models. Figure 5: Key drivers of carbon price 16 Figure 6: Relative importance of input features for',\n",
       "  'icy dynamics exert the strongest influence on carbon price movements, whereas macroeconomic and financial uncertainty indicators play a more limited role. The correlation analysis provides a basis for feature selection in subsequent forecasting models. Figure 5: Key drivers of carbon price 16 Figure 6: Relative importance of input features for carbon price prediction This study aims to screen multiple external features to identify the key drivers most closely associated with carbon price fluctuations. Following the approach of Geurts et al. (2006), we employed the Extremely Randomized Trees (ET) method to evaluate the importance of input features. As shown in Figure 6, the results of the ET evaluation indicate that the policy features have the highest importance score, exceeding 0.5, which is significantly higher than the other characteristics. Among the remaining features, energy-related features such as coal and natural gas prices, as well as financial indicators like the Euro Stoxx 50 index and EUR/USD exchange rate, also exhibit non-negligible contributions. In contrast, macroeconomic uncertainty indices and geopolitical risk show very limited explanatory power, with importance scores close to zero. This analysis highlights that policy factors overwhelmingly dominate the drivers of carbon price dynamics, while energy markets and selected financial indicators play secondary roles. Figure 7: Carbon price time series with detected structural breakpoints by feature Breakpoint Analysis. As shown in Figure 7, the carbon price time series can be divided into three distinct phases. The first phase, from September 2007 to the end of 2012, is characterized by relatively small price fluctuations, with carbon prices being significantly influenced by policy and coal price disturbances. The second phase, from 2013 to 2019, reflects a relatively stable market, though structural shifts still occurred due to localized impacts from energy prices and policy signals. The third phase, from 2020 to 2024, is marked by a rapid increase and high volatil- ity in carbon prices, mainly driven by the combined influence of natural gas prices, electricity market dynamics, and major policy announcements. In Figure 7, the solid black line represents the carbon price, while the dashed lines indicate structural breakpoints associated with various 17 feature features. Notably, breakpoints for Epex Spot Germany, Euro Stoxx 50, TTF natural gas, and policy features appear densely after 2021, clearly highlighting the nonlinear, dynamic, and highly uncertain nature of carbon prices. Around 2008, due to underdeveloped market mechanisms (EcoAct, 2022), carbon prices dropped rapidly to near zero, indicating a clear structural break. The carbon price series exhibits positive skewness and high kurtosis, further demonstrating its significant nonnormality. To en- hance the model’s ability to learn from the carbon price series and improve prediction accuracy, the dataset is divided into training and testing sets. Common split strategies include “80%/20%” or “90%/10%.” This study adopts the “80%/20%” split, with 80% of the data used for model training and 20% for model testing, in order to evaluate the model’s predictive robustness. Based on the above test results, we also attempt to interpret the structural breakpoints ob- served in carbon prices. In the first phase (September 2007 – end of 2012), the carbon market was',\n",
       "  'used for model training and 20% for model testing, in order to evaluate the model’s predictive robustness. Based on the above test results, we also attempt to interpret the structural breakpoints ob- served in carbon prices. In the first phase (September 2007 – end of 2012), the carbon market was still in its exploratory stage. Issues such as excessive allocation of allowances and frequent economic fluctuations led to generally low and stable carbon prices, with significant influence from policy missteps and coal price disturbances. (1) In 2007, toward the end of Phase I of the EU ETS (2005–2007), carbon prices plummeted from around €1.5/ton to nearly €0/ton due to the surplus of allowances and the inability to carry over unused allowances into the next phase. (2) In January 2008, Phase II of the EU ETS officially began. Due to a reduction in the new round of allowance allocations and the partial acceptance of international credits (such as CDM), car- bon prices quickly rebounded from nearly €0/ton to around €20/ton. (3) In September 2008, the global financial crisis severely impacted industrial production in Europe, causing a sharp drop in the demand for carbon allowances. As a result, carbon prices fell rapidly from around €20/ton to approximately €8/ton. (4) In December 2009, the United Nations Climate Change Conference in Copenhagen failed to reach a legally binding global emissions reduction agreement, under- mining market confidence in future carbon prices and causing them to fall from around €15/ton to €12/ton. (5) In the middle of 2012, the ongoing Eurozone debt crisis continued to suppress industrial activity. Combined with the persistent issue of allowance oversupply, carbon prices dropped to approximately €6.76/ton in June 2012. Entering the second phase (2013–2019), as reform measures. Such as the Market Stabil- ity Reserve (MSR) were gradually implemented, market confidence began to recover (European Commission, 2009), and carbon prices rose significantly. However, localized disruptions caused by energy price fluctuations and policy adjustments still persisted. (1) In April 2013, the Eu- ropean Parliament rejected the “backloading” proposal aimed at temporarily removing surplus allowances from the market. This triggered disappointment among market participants, leading to a carbon price drop from €7.10/ton to €2.75/ton. (2) In July 2015, the EU officially adopted the proposal to establish the Market Stability Reserve (MSR) to absorb surplus allowances and improve price flexibility. Although there was little short-term change in carbon prices, market confidence was strengthened, laying the groundwork for future price increases. (3) Throughout 2018, with the adoption of the reform after 2020 package—which included accelerating the an- nual reduction of allowances (raising the Linear Reduction Factor to 2.2%) and tightening the free allocation mechanism—carbon prices surged from €7/ton at the beginning of the year to €25/ton by end of the year, marking an increase of over 250%. In the third phase (2020–2024), carbon prices exhibited a combination of rapid increases and high volatility. This period was driven by multiple factors, including the natural gas crisis, the Russia-Ukraine conflict, and new climate policies such as the “Fit for 55” package. The carbon market entered a new stage characterized',\n",
       "  'In the third phase (2020–2024), carbon prices exhibited a combination of rapid increases and high volatility. This period was driven by multiple factors, including the natural gas crisis, the Russia-Ukraine conflict, and new climate policies such as the “Fit for 55” package. The carbon market entered a new stage characterized by both policy driven momentum and structural changes in the energy sector (Fan, 2025). (1) In July 2021, the European Union proposed the “Fit 18 for 55” climate policy package, raising the 2030 emissions reduction target from 40% to 62% (compared to 2005 levels). This led to a surge in carbon prices from €33/ton at the beginning of the year to €60/ton (European Parliament, 2022). (2) From February to August 2022, the Russia- Ukraine war caused a spike in natural gas prices, prompting a shift back to coal fired power generation and increasing demand for carbon allowances. As a result, carbon prices reached a historical peak of €97/ton in August 2022. (3) On February 21, 2023, the EU ETS carbon price surpassed €100/ton for the first time, reaching €101/ton. This was driven by a combination of policy expectations, energy shortages, and growing participation from financial institutions. (4) From January to February 2024, due to milder winter temperatures, lower electricity demand, and ample renewable energy supply, carbon prices fell from €84/ton to €52/ton, marking the lowest level in 31 months. Wavelet Data Denoising for Carbon Price Series. In the original time series, there can exist some notable noises, which can interfere with the ability of machine learning models to learn the intrinsic features of the data. Therefore, it is necessary to remove noise from the original carbon price series before modeling. To address this, this study applies Wavelet Transform (WT) to process the carbon price time series, decomposing it into approximation components (low frequency trends) and detail components (high-frequency noise). We retain only the approxima- tion component as input to the model, thereby achieving the goal of denoising. A single level decomposition is chosen because excessive decomposition may lead to information loss, which could negatively affect the model’s predictive accuracy. As shown in Figure 8, the processed carbon price curve (in orange) is noticeably smoother compared to the original curve (in blue), effectively eliminating abrupt changes and spikes caused by high-frequency fluctuations, while preserving the main trend of price movements. One key motivation for using WT in this study is to introduce structural breakpoint information on top of the denoised series, in order to explore whether such information can further improve the accuracy of carbon price prediction models under a cleaner signal background. Figure 8: Denoised by Wavelet transformer 4.2. Comparison of Deep Learning Models This study evaluates several deep learning models for carbon price forecasting. The com- pared methods are as follows: • BP&ICSS-WT-LSTM: Baseline approach that combines Bai–Perron and ICSS structural break detection with wavelet-based LSTM modeling (Lin and Zhang, 2022). • PELT-WT-LSTM (Univariate): LSTM model with PELT structural break detection and wavelet transform applied to a single input feature (carbon price). 19 • PELT-WT-LSTM (Multivariate): Extension of the above model',\n",
       "  '• BP&ICSS-WT-LSTM: Baseline approach that combines Bai–Perron and ICSS structural break detection with wavelet-based LSTM modeling (Lin and Zhang, 2022). • PELT-WT-LSTM (Univariate): LSTM model with PELT structural break detection and wavelet transform applied to a single input feature (carbon price). 19 • PELT-WT-LSTM (Multivariate): Extension of the above model incorporating multiple external influencing factors. • PELT-WT-GRU: GRU-based model with PELT structural break detection and wavelet transform. • PELT-WT-TCN: Temporal Convolutional Network (TCN) model with PELT structural break detection and wavelet transform. To ensure consistency and comparability in the training process of deep learning models for car- bon price forecasting, this study adopts a unified training configuration for all models. The Adam optimizer is employed during training due to its adaptive learning rate mechanism, which is well- suited for handling nonstationary time series data. The initial learning rate is set to 0.001, with momentum parameters 1 and 2 set to 0.9 and 0.999, respectively, to balance training speed and stability. The models are trained with a batch size of 64 for a maximum of 50 epochs. An early stopping mechanism is introduced to prevent overfitting, where training is halted if the validation mean squared error (MSE) does not improve for 10 consecutive epochs. Additionally, 10% of the training data is allocated as a validation set to continuously monitor model performance. A slid- ing window approach is used to construct the input data, with each input window consisting of 30 time steps and a stride of 1, ensuring sufficient extraction and modeling of temporal features. Both LSTM and GRU models were built with two hidden layers, each containing 128 units, and a dropout rate of 0.2 was applied between layers to mitigate overfitting. The TCN model was constructed with four residual blocks, each having 64 channels. BP&ICSS-WT-LSTM Performance. Following the approach of Lin and Zhang (Lin and Zhang, 2022), Figure 9 presents the prediction results of the BP&ICSS-WT-LSTM model, reproduced from the baseline study using our updated carbon price dataset. In this configuration, structural breakpoints were detected using a combination of the Bai & Perron and ICSS methods (Lin and Zhang, 2022). As a result, the model remains exposed to high-frequency noise and abrupt fluctuations in the time series. Figure 9: Comparison of actual and predicted carbon prices using the BP&ICSS-WT-LSTM model Although the inclusion of breakpoint information helps the model capture shifts in price regimes, BP&ICSS-WT-LSTM is only able to approximate the overall trend of carbon prices. The model performs poorly around local peaks and troughs, exhibiting amplitude compression, a phenomenon indicating a degree of underfitting. This limitation is particularly evident during periods of high volatility (e.g., around time points 400 and 900), where prediction errors increase significantly. 20 PELT-WT-LSTM (Univariate vs. Multivariate). Figures 10 and Figures 11 reveal the effect of multivariate inputs. The PELT-WT-LSTM(univariate) shows lagged response and error fluc- tuations. The multivariate version better aligns with actual values in several segments (e.g., step 500 to 900), suggesting enhanced dynamic awareness and forecasting insights. Figure 10: Performance of the PELT-WT-LSTM(uni) model in forecasting carbon prices Figure 11: Comparison of actual',\n",
       "  'multivariate inputs. The PELT-WT-LSTM(univariate) shows lagged response and error fluc- tuations. The multivariate version better aligns with actual values in several segments (e.g., step 500 to 900), suggesting enhanced dynamic awareness and forecasting insights. Figure 10: Performance of the PELT-WT-LSTM(uni) model in forecasting carbon prices Figure 11: Comparison of actual and PELT-WT-LSTM(multi) PELT-WT-GRU Performance. As shown in Figure 12, the GRU model offers structural sim- plicity with solid fitting capabilities. Prediction trends closely match actual values, especially during price peaks and drops (e.g., step 600–1000). GRU demonstrates a balance between accu- racy and training efficiency. Figure 12: Comparison of actual and PELT-WT-GRU predicted carbon prices PELT-WT-TCN Performance. Figure 13 indicates that TCN outperforms other models. Its 21 predictions closely follow real values even during sharp fluctuations, thanks to its one dimen- sional convolution architecture that excels in capturing local temporal features and modelling nonlinearities. Figure 13: PELT-WT-TCN model prediction performance for carbon price forecasting 4.3. Quantitative Performance Evaluation The experiment employed five key performance metrics to quantify model effectiveness, in- cluding Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Mean Absolute Per- centage Error (MAPE), the coefficient of determination (R²), and training time. MAE measures the average deviation between predicted and actual values, with smaller values indicating higher prediction accuracy. RMSE emphasizes large errors through squared differences and reflects the overall dispersion of the prediction errors. MAPE expresses the prediction error as a percentage, making it easier to compare across models or datasets. R² evaluates how well the model explains the variance of the target variable, with values closer to 1 indicating better fit. Training time reflects the computational efficiency of each model. Table 1, Figure 14 and Figure 15 present five key performance metrics—Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), R² (coefficient of determination), and training time—for various deep learning models used in time series forecasting. The results reveal clear differences in accuracy and computational efficiency among the models: Table 1: Performance comparison for carbon price prediction Model MAE RMSE MAPE (%) R2 BP&ICSS-WT-LSTM 4.6345 5.3878 5.8731 0.8712 PELT-WT-LSTM (uni) 2.3627 2.7488 3.0582 0.9664 PELT-WT-LSTM (multi) 1.8192 2.2967 2.3267 0.9765 PELT-WT-GRU 1.3308 1.6987 1.7401 0.9872 PELT-WT-TCN 1.1855 1.5866 1.6451 0.9888 22 Figure 14: Performance comparison of multiple models in carbon price forecasting Figure 15: Comparison of model performance metrics PELT-WT-TCN. The TCN model delivers the best overall performance, achieving the lowest er- ror metrics (MAE = 1.1855, RMSE = 1.5866, MAPE = 1.6451%), indicating minimal deviation between predicted and actual values. It also achieves the highest R² score of 0.9888, meaning it explains nearly all variance in the data. However, this comes at the cost of the longest training time (48.7 seconds), likely due to its deeper architecture or complex convolutional operations. PELT-WT-GRU. The GRU model strikes a strong balance between performance and speed. With MAE = 1.3308, RMSE = 1.6987, MAPE = 1.7401%, it ranks closely behind TCN in accuracy. Importantly, it has the fastest training time (19.6 seconds), making it highly suitable for time sensitive applications. Its R² of 0.9872 also',\n",
       "  'The GRU model strikes a strong balance between performance and speed. With MAE = 1.3308, RMSE = 1.6987, MAPE = 1.7401%, it ranks closely behind TCN in accuracy. Importantly, it has the fastest training time (19.6 seconds), making it highly suitable for time sensitive applications. Its R² of 0.9872 also confirms excellent predictive reliability. PELT-WT-LSTM(Multivariate). The PELT-WT-LSTM(Multivariate) model, which incorpo- rates multiple input features, performs well in terms of prediction accuracy (MAE = 1.8192, RMSE = 2.2967, MAPE = 2.3267%) with a respectable R² of 0.9765. The training time is moderate (24.9 seconds), making it a viable option when balancing accuracy and computational cost. PELT-WT-LSTM(Univariate). This model uses only a single input feature and underperforms compared to its multivariate counterpart. With MAE = 2.3627, RMSE = 2.7488, MAPE = 3.0582%, and R² = 0.9664, it shows limited predictive power. The training time is around 24.7 seconds, similar to the multivariate version. 23 BP&ICSS-WT-LSTM. The BP&ICSS-WT-LSTM configuration performs the worst overall, with high error rates (MAE = 4.6345, RMSE = 5.3878, MAPE = 5.8731%) and the lowest R² score (0.8712), suggesting a poor fit to the data. Despite its moderate training time (24.5 seconds), its low accuracy renders it unsuitable for practical applications. While PELT-WT-TCN achieves the highest accuracy, it requires longer training time. PELT- WT-GRU provides a approximately optimal balance between speed and performance, making it ideal for use in real time. PELT-WT-LSTM(Multi) also performs well with multiple features. In contrast, PELT-WT-LSTM(Uni) and BP&ICSS-WT-LSTM models fall short, particularly the latter, which is not recommended for deployment due to its poor performance. 4.4. Scalability and Residual Distribution Analysis Figure 16: Model training times Figure 16 provides a detailed comparison of the training times required by different models under identical training conditions, highlighting significant differences in computational resource consumption. Overall, the PELT-WT-GRU model stands out with the fastest training time of just 14.3 seconds, making it particularly advantageous in scenarios that are sensitive to time or constrained by resources. In contrast, the BP&ICSS-WT-LSTM, PELT-WT-LSTM(univariate), and PELT-WT-LSTM(Multivariate) show very similar training durations, 26.1, 26.4, and 26.2 seconds respectively, indicating that the PELT-WT-LSTM architecture maintains relatively stable training efficiency regardless of input dimensionality. Although their training times are moderate, these LSTM-based models are slightly less efficient than PELT-WT-GRU. On the other hand, the PELT-WT-TCN model requires the longest training time, reaching 52.5 seconds, nearly 3.5 times that of PELT-WT-GRU. This extended duration is primarily due to the TCN’s deep convolutional structure and multiple stacked layers, which make the training process more computationally intensive and complex. 24 Figure 17: Residual distributions across time Figure 17 illustrates the temporal distribution of residuals (actual minus predicted values) for five deep learning models. The PELT-WT-TCN model demonstrates relatively constrained residual variation, with values fluctuating closely around zero, which may reflect its strength in capturing long range temporal dependencies through dilated causal convolutions (Bai et al., 2018). At the other end of the spectrum, the BP&ICSS-WT-LSTM exhibits broader and more irregular residual patterns, with multiple spikes exceeding ±10, potentially indicating heightened sensitivity to input volatility or sequence length. PELT-WT-GRU and PELT-WT-LSTM',\n",
       "  'its strength in capturing long range temporal dependencies through dilated causal convolutions (Bai et al., 2018). At the other end of the spectrum, the BP&ICSS-WT-LSTM exhibits broader and more irregular residual patterns, with multiple spikes exceeding ±10, potentially indicating heightened sensitivity to input volatility or sequence length. PELT-WT-GRU and PELT-WT-LSTM (Mul- tivariate) present moderate residual variability, while PELT-WT-LSTM (Univariate) maintains a more symmetric error pattern with smaller amplitude, although an increased concentration of negative deviations is observed in the latter part of the series. These findings highlight the extent to which architectural design shapes residual dynamics and suggest that residual stability may serve as a practical proxy for model generalization in time series forecasting. Figure 18: Distribution of residuals density Figure 18 illustrates the residual distributions of different models. Among all the methods, the PELT-WT-TCN model exhibits the most concentrated and symmetric density around zero, indicating both low residual variance and minimal bias. The PELT-WT-GRU and PELT-WT- LSTM(Multivariate) models demonstrate moderately peaked curves, with PELT-WT-GRU show- ing slight right skewness suggestive of occasional overestimation. In addition, the PELT-WT- LSTM(Univariate) distribution is broader and shifted rightward, reflecting a greater frequency of positive errors. The BP&ICSS-WT-LSTM displays the widest and most asymmetric distribu- tion, characterized by a pronounced right tail, which implies a tendency toward persistent over 25 prediction. The distributional characteristics of residuals provide an additional layer of model evaluation beyond time domain analysis (Lawrance and Lewis, 2018), revealing that PELT-WT- TCN and PELT-WT-GRU are more consistent and less prone to systematic error compared to the BP&ICSS-WT-LSTM. 5. Conclusion This study proposed a hybrid forecasting framework that integrates structural breakpoint de- tection, wavelet denoising, and advanced deep learning models to enhance the prediction of EU carbon prices. Breakpoint detection using PELT enables the model to capture regime shifts caused by policy changes and market shocks, while wavelet transform reduces high-frequency noise and improves stability. Together, these preprocessing steps provide denoised and inputs with regime awareness for subsequent forecasting. Experimental results show that incorporat- ing structural and denoised information significantly improves predictive accuracy. Among the tested models, PELT-WT-TCN achieves the lowest error metrics, while PELT-WT-GRU demon- strates a favorable balance between efficiency and performance, making it more suitable for real time applications. The comparison confirms that combining structural awareness with multiscale learning is essential for improving robustness and interpretability in carbon price forecasting. Overall, the findings contribute to the growing literature on hybrid modeling for financial and energy markets, demonstrating the benefits of integrating advanced time series decomposition with deep learning. Future research may focus on improving model interpretability and extend- ing the dataset with more indicators related to policy and macroeconomic factors. Declaration of Competing Interest The authors declare that they have no known competing financial interests or personal rela- tionships that could have appeared to influence the work reported in this paper. References Bai, J., Perron, P., 2003. Computation and analysis of multiple structural change models. Journal of Applied Econometrics 18, 1–22. doi:10.1002/jae.659. Bai, S., Kolter, J.Z., Koltun, V., 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.',\n",
       "  'have appeared to influence the work reported in this paper. References Bai, J., Perron, P., 2003. Computation and analysis of multiple structural change models. Journal of Applied Econometrics 18, 1–22. doi:10.1002/jae.659. Bai, S., Kolter, J.Z., Koltun, V., 2018. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271 . Bollerslev, T., 1986. Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics 31, 307–327. doi:10.1016/0304-4076(86)90063-1. Box, G.E.P., Jenkins, G.M., Reinsel, G.C., Ljung, G.M., 1976. Time Series Analysis: Forecast- ing and Control. 5th ed., John Wiley & Sons. Cai, X., Yuan, B., Wu, C., 2025. An integrated ceemdan and tcn-lstm deep learning framework for forecasting. International Review of Financial Analysis 98, 103879. doi:10.1016/j. irfa.2024.103879. Chang, L., Li, Z., Zhou, B., Xiu, G., Guo, Y., 2024. Research on grating nano-measurement algorithm based on tcn. Measurement Science and Technology 36, 015031. URL: https: //doi.org/10.1088/1361-6501/ad889a, doi:10.1088/1361-6501/ad889a. 26 Chen, Y., Li, X., Wang, X., 2021. A hybrid model for carbon price forecasting. Applied Energy 285, 116485. doi:10.1016/j.apenergy.2021.116485. Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Ben- gio, Y., 2014. Learning phrase representations using rnn encoder–decoder for statistical ma- chine translation, in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics. pp. 1724–1734. doi:10.3115/v1/D14-1179. Deng, S., Su, J., Zhu, Y., Yu, Y., Xiao, C., 2024. Forecasting carbon price trends based on an interpretable light gradient boosting machine and bayesian optimization. Expert Systems with Applications 242, 122502. doi:10.1016/j.eswa.2023.122502. Duan, Y., Fan, Y., Wang, X., Liu, K., Zhang, X., 2025. Dynamic prediction of carbon prices based on the multi-frequency combined model. PeerJ Computer Science 11, e2827. doi:10. 7717/peerj-cs.2827. EcoAct, 2022. Key messages from 2022 state of the eu ets report. https://eco-act.com/ blog/state-of-the-eu-ets-2022/. Accessed 22 May 2025. European Commission, 2009. Un climate change conference (cop15). https://climate.ec.europa.eu/news-your-voice/events/ un-climate-change-conference-cop15-2009-12-07_en. Accessed 22 May 2025. European Commission, 2020. The eu emissions trading system (eu ets). https://climate. ec.europa.eu/eu-action/eu-emissions-trading-system-eu-ets_en. Accessed 22 May 2025. European Commission, 2024. 2024 Carbon Market Report: a stable and well-functioning mar- ket, driving emissions from power and industry installations to a historic reduction of 16.5%. https://tinyurl.com/4m75hstt. Accessed 03 March 2025. European Parliament, 2022. Revision of the EU ETS market stability reserve. Technical Report. European Parliament. URL: https://www.europarl.europa.eu/RegData/etudes/ BRIE/2022/698896/EPRS_BRI%282022%29698896_EN.pdf. accessed 22 May 2025. Fan, J., 2025. The external impact of eu climate policy: political responses to the eu’s car- bon border adjustment mechanism. Climatic Change URL: https://doi.org/10.1007/ s10784-025-09667-z, doi:10.1007/s10784-025-09667-z. accessed 8 May 2025. Geurts, P., Ernst, D., Wehenkel, L., 2006. Extremely randomized trees. Machine Learn- ing 63, 3–42. URL: https://doi.org/10.1007/s10994-006-6226-1, doi:10.1007/ s10994-006-6226-1. Guo, J., Liu, M., Luo, P., Chen, X., Yu, H., Wei, X., 2023. Attention-based bilstm for the degradation trend prediction of lithium battery. Energy Reports 9, 655–664. doi:10.1016/j. egyr.2023.03.056. Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Computation 9, 1735– 1780. 27 Huang, Y., Dai, X., Wang, Q., Zhou, D., 2021. A hybrid model for carbon price forecasting using garch and long short-term memory network. Applied Energy 298, 116485. doi:10.1016/j. apenergy.2021.116485. Inclan, C., Tiao, G.C., 1994. Use of cumulative sums of squares for retrospective detection of',\n",
       "  'Neural Computation 9, 1735– 1780. 27 Huang, Y., Dai, X., Wang, Q., Zhou, D., 2021. A hybrid model for carbon price forecasting using garch and long short-term memory network. Applied Energy 298, 116485. doi:10.1016/j. apenergy.2021.116485. Inclan, C., Tiao, G.C., 1994. Use of cumulative sums of squares for retrospective detection of changes of variance. Journal of the American Statistical Association 89, 913–923. doi:10. 2307/2290916. Karim, F., Majumdar, S., Darabi, H., Harford, S., 2019. Multivariate lstm-fcns for time series classification. Neural networks 116, 237–245. Killick, R., Fearnhead, P., Eckley, I., 2012. Optimal detection of changepoints with a linear computational cost. Journal of the Royal Statistical Society: Series B 74, 493–508. Koch, N., Fuss, S., Grosjean, G., Edenhofer, O., 2014. Causes of the eu ets price drop: Recession, cdm, renewable policies or a bit of everything?—new evidence. Energy Pol- icy 73, 676–685. URL: https://www.sciencedirect.com/science/article/pii/ S0301421514003966, doi:https://doi.org/10.1016/j.enpol.2014.06.024. Lawrance, A.J., Lewis, P.A.W., 2018. Modelling and residual analysis of nonlinear autoregres- sive time series in exponential variables. Journal of the Royal Statistical Society: Series B (Methodological) 47, 165–183. doi:10.1111/j.2517-6161.1985.tb01344.x. Lei, H., Xue, M., Liu, H., Ye, J., 2024. Unveiling the driving patterns of carbon prices through an explainable machine learning framework: Evidence from chinese emission trading schemes. Journal of Cleaner Production 438, 140697. doi:10.1016/j.jclepro.2024.140697. Lim, B., Arik, S.O., Loeff, N., Pfister, T., 2021. Temporal fusion transformers for interpretable multi-horizon time series forecasting. International Journal of Forecasting 37, 1748–1764. Lin, B., Zhang, C., 2022. Forecasting carbon price in the european carbon market: The role of structural changes. Process Safety and Environmental Protection 166, 341–354. URL: https: //doi.org/10.1016/j.psep.2022.08.011, doi:10.1016/j.psep.2022.08.011. Liu, X., Chen, C., Zhao, L., et al., 2012. Wavelet analysis of carbon-allowance price dynamics. Energy Economics 34, 1028–1034. doi:10.1016/j.eneco.2011.09.002. Lorentz, H.A., 1938. The limitations of the lorentz model of the electron. Proceedings of the Royal Society of London. Series A, Mathematical, Physical, and Engineering Sciences 167, 148–169. URL: https://doi.org/10.1098/rspa.1938.0124, doi:10.1098/rspa. 1938.0124. Mallat, S.G., 1989. A theory for multiresolution signal decomposition: The wavelet repre- sentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 11, 674–693. doi:10.1109/34.192463. Olawumi, M.A., Oladapo, B.I., 2025. Ai-driven predictive models for sustainability. Journal of Environmental Management 373, 123472. doi:10.1016/j.jenvman.2024.123472. Pang, T., Tan, K., Fan, C., 2023. Carbon price forecasting with quantile regression and feature selection. URL: https://arxiv.org/abs/2305.03224, arXiv:2305.03224. 28 Shahid, F., Zameer, A., Muneeb, M., 2020. Predictions for covid-19 with deep learning models of lstm, gru and bi-lstm. Chaos, Solitons & Fractals 140, 110212. doi:10.1016/j.chaos. 2020.110212. Torres, M.E., Colominas, M.A., Schlotthauer, G., Flandrin, P., 2011. A complete ensemble empirical mode decomposition with adaptive noise, in: 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE. pp. 4144–4147. URL: https: //doi.org/10.1109/ICASSP.2011.5947265, doi:10.1109/ICASSP.2011.5947265. Wang, F., Jiang, J., Shu, J., 2022. Carbon trading price forecasting: based on improved deep learning method. Procedia Computer Science 214, 845–850. Wang, Y., Wang, Z., Wang, X., Kang, X., 2023. Multi-step-ahead and interval carbon price fore- casting using transformer-based hybrid model. Environmental Science and Pollution Research 30, 95692–95719. doi:10.1007/s11356-023-28135-3. Zhang, F., Wen, N., 2022. Carbon price forecasting: a novel deep learning approach. Envi- ronmental Science and Pollution Research 29, 54782–54795. URL: https://doi.org/10. 1007/s11356-022-19713-x,',\n",
       "  'Wang, Z., Wang, X., Kang, X., 2023. Multi-step-ahead and interval carbon price fore- casting using transformer-based hybrid model. Environmental Science and Pollution Research 30, 95692–95719. doi:10.1007/s11356-023-28135-3. Zhang, F., Wen, N., 2022. Carbon price forecasting: a novel deep learning approach. Envi- ronmental Science and Pollution Research 29, 54782–54795. URL: https://doi.org/10. 1007/s11356-022-19713-x, doi:10.1007/s11356-022-19713-x. Zhang, K., Yang, X., Wang, T., Thé, J., Tan, Z., Yu, H., 2023. Multi-step carbon price fore- casting using a hybrid model based on multivariate decomposition strategy and deep learning algorithms. Journal of Cleaner Production 405, 136959. doi:10.1016/j.jclepro.2023. 136959. Zhang, Z., Liu, X., Zhang, X., Yang, Z., Yao, J., 2024. Carbon price forecasting using optimized sliding window empirical wavelet transform and gated recurrent unit network to mitigate data leakage. Energies 17, 4358. doi:10.3390/en17174358. Zhao, Y., Liu, L., Wang, A., Liu, M., 2023. A novel deep learning based forecasting model for carbon emissions trading: A comparative analysis of regional markets. Solar Energy 262, 111863. doi:10.1016/j.solener.2023.111863. Zhao, Y., Xu, J., 2023. Hybrid deep learning approaches for high-frequency carbon price forecasting. https://doi.org/10.2139/ssrn.5020501. doi:10.2139/ssrn.5020501. sSRN preprint. 29'],\n",
       " ['Early Alzheimer’s Disease Detection from Retinal OCT Images: A UK Biobank Study 1st Yasemin Turkan Department of Computer Engineering Isik University Istanbul, Turkey 2nd F. Boray Tek Department of Artificial Intelligence and Data Engineering Istanbul Technical University Istanbul, Turkey 3rd M. Serdar Nazlı Department of Artificial Intelligence and Data Engineering Istanbul Technical University Istanbul, Turkey 4th ¨Oyk¨u Eren Department of Artificial Intelligence and Data Engineering Istanbul Technical University Istanbul, Turkey Abstract—Alterations in retinal layer thickness, measurable using Optical Coherence Tomography (OCT), have been associ- ated with neurodegenerative diseases such as Alzheimer’s disease (AD). While previous studies have mainly focused on segmented layer thickness measurements, this study explored the direct classification of OCT B-scan images for the early detection of AD. To our knowledge, this is the first application of deep learning to raw OCT B-scans for AD prediction in the literature. Unlike conventional medical image classification tasks, early detection is more challenging than diagnosis because imaging precedes clinical diagnosis by several years. We fine-tuned and evaluated multiple pretrained models, including ImageNet-based networks and the OCT-specific RETFound transformer, using subject- level cross-validation datasets matched for age, sex, and imaging instances from the UK Biobank cohort. To reduce overfitting in this small, high-dimensional dataset, both standard and OCT- specific augmentation techniques were applied, along with a year- weighted loss function that prioritized cases diagnosed within four years of imaging. ResNet-34 produced the most stable results, achieving an AUC of 0.62 in the 4-year cohort. Although below the threshold for clinical application, our explainability analyses confirmed localized structural differences in the central macular subfield between the AD and control groups. These find- ings provide a baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches. Index Terms—Alzheimer’s Disease, ImageNet, Transfer Learn- ing, UK Biobank, Deep Learning, Augmentation I. INTRODUCTION Alzheimer’s disease (AD) is an irreversible and progressive brain disorder characterized by a decline in cognitive function and is the most prevalent type of dementia. Currently, there is no known cure, but it is marked by a significant reduction in brain size (neurodegeneration) caused by the accumulation of proteins (amyloid-beta and tau) in neurons [1]. As the retina and brain originate from the same neural tube, the eyes are of- ten regarded as extensions of the brain [2]. Postmortem studies This study was supported by Scientific and Technological Research Council of Turkey (TUBITAK) under the Grant Number 122E509. The authors thank to TUBITAK for their supports. have shown that amyloid-beta and tau proteins accumulate in the retinas of individuals with AD [3]. High-resolution visual imaging technologies, such as optical coherence tomography (OCT), have recently been proposed to examine the structural and vascular changes in the retinas of patients with AD. In contrast to current gold-standard diagnostic tools, such as positron emission tomography (PET), cerebrospinal fluid (CSF) analysis, and genetic screening, OCT offers a non-invasive, rapid, and cost-efficient imaging modal- ity that is already widely available in clinical ophthalmology settings. Incorporating such retinal imaging biomarkers into clinical workflows could',\n",
       "  'with AD. In contrast to current gold-standard diagnostic tools, such as positron emission tomography (PET), cerebrospinal fluid (CSF) analysis, and genetic screening, OCT offers a non-invasive, rapid, and cost-efficient imaging modal- ity that is already widely available in clinical ophthalmology settings. Incorporating such retinal imaging biomarkers into clinical workflows could help identify at-risk individuals for further neurological testing, thus enabling earlier intervention strategies and improving patient outcomes in the future. Existing studies have shown statistical associations between retinal layer thickness and AD. However, to our knowledge, this is the first study to apply deep learning to raw OCT B-scan images from the UK Biobank for early AD prediction. This represents a significant methodological advance over prior studies, which primarily relied on retinal fundus images [4], [5] or quantitative summary measurements relying on a layer segmentation procedure [6] rather than on raw, high-resolution structural scans. Our study introduces an end-to-end deep learning frame- work that includes anatomically guided pre-processing, mul- tichannel input design, and rigorous cross-validation tailored for OCT data. We evaluate multiple state-of-the-art models, in- cluding CNNs and vision transformers, to assess the feasibility of learning subtle retinal biomarkers from small, temporally distant datasets. The dataset, model, and methodology are explained in Section II. Section III describes the experiments and presents the results of this study. Our discussion and conclusions are presented in Sections IV and V, respectively. II. METHODS We adopted a standardized deep learning pipeline in align- ment with the Deep Learning Flow framework tailored for arXiv:2511.05106v1 [cs.CV] 7 Nov 2025 TABLE I: Summary of Cohorts with Dementia and AD Counts and Percentages Dataset Cohort AD N N % UK Biobank cohort 502,386 3955 0.79% OCT-Scanned cohorts 85,704 539 0.65% High Quality Cohorts with OCT scans 43,934 223 0.51% 4-Year dataset cohorts (Dataset 1) 49 19 39% 4-Year dataset scans (Dataset 1) 58 28 48% 4-Year dataset cohorts (Dataset 2) 45 19 42% 4-Year dataset scans (Dataset 2) 55 28 51% AD/MCI diagnosis using OCT-OCTA [7]. This framework emphasizes the best practices for the curation of retinal OCT datasets, model training, and evaluation protocols. This modu- lar structure emphasizes the reproducibility and fairness of the framework in developing deep learning models for ophthalmic imaging. A. Dataset Curation Inclusion and Exclusion: The UK Biobank database in- cludes 502,386 participants, of whom 85,704 underwent OCT tomography scans at instances 0 and 1 using the same OCT device, Topcon 3D OCT 1000 Mk 2 [8]. Until July 2023, 3,955 patients were diagnosed with Alzheimer’s disease (AD). However, only 539 patients with AD underwent corresponding OCT scans. Low-quality scans were excluded based on the cri- teria used by Patel et al. [6], [9]. After applying the exclusion criteria, 43,934 participants remained in the dataset, with only 223 of them diagnosed with Alzheimer’s disease (AD). The dataset included patients who were diagnosed over a 12-year period. Due to the slow progression of AD, more significant changes are typically observed in patients at least three years prior to diagnosis. Since only 9 patients with AD had reached the three-year mark in our dataset, a four-year threshold',\n",
       "  'The dataset included patients who were diagnosed over a 12-year period. Due to the slow progression of AD, more significant changes are typically observed in patients at least three years prior to diagnosis. Since only 9 patients with AD had reached the three-year mark in our dataset, a four-year threshold was selected to ensure a larger and more representative sample size (19 patients with AD). Table I summarizes the number of cohorts in the full dataset versus AD at each data-exclusion iteration. Both eyes of participants in the AD cohort were included in the study if they were eligible for the study. However, to prevent data leakage during model development, both eyes from a single patient were assigned exclusively to the training or validation set. Age, sex, and instance matched individuals from the non-AD group were randomly selected to form a balanced control group. Image Pre-process and Feature Extraction: In the UK Biobank, FDS bulk files contain the entire, 16-bit raw OCT volume for both the left and right eyes, providing the complete dynamic range and unprocessed scan data necessary for cus- tom image processing or resegmentation. In contrast, the FDA bulk files are an 8-bit downsampled representation derived directly from FDS volumes; they include four segmentation layers but do not permit recovery of the original 16-bit voxel intensities. The differences between the FDA and FDS image formats are shown in Fig. 1 (a) and (b). In this study, we used Fig. 1: Overview of the preprocessing pipeline and retinal layer annotations in OCT B-scans. (a) Original grayscale OCT scan with inner and outer retinal boundaries. (b) Retinal layer segmentation with 11 color-coded contours representing the anatomical boundaries. (c) Alternate view of the same segmented B-scan for visualization consistency. (d) Pixel-wise retinal layer mask used as input for the second channel of the model. The legend on the right maps each color to a specific retinal layer, from the inner limiting membrane (ILM) to the outer boundary of the retinal pigment epithelium (OB RPE). This multichannel representation encodes both intensity and anatomical structure, providing richer input for deep learning models. Images used with permission from the UK Biobank under Application Number 82266. the OCTExplorer software, which utilizes the Iowa Reference Algorithms (Retinal Image Analysis Lab, Iowa Institute for Biomedical Imaging, Iowa City, IA) [10], [11] for the seg- mentation of the fds files (Fig. 1 (c)). This study focused on ImageNet-pretrained 2D models. Therefore, the middle slice (B-scans) of the raw image vol- umes (128 × 512 × 650) was selected as the input. The OCT image was rectified using the bottom contour, as shown in Fig. 1 (c). The areas above the top contour and below the bottom contour were removed to eliminate noise at the top and bottom of the scans. An additional layer-masked image was generated (Fig. 1 (d) ). Finally, all pre-processed images of size 512 × 650 were cropped to 512 × 512. To adapt the single-channel OCT images to the three- channel input requirements of the pretrained models, we constructed a composite 3-channel representation for each',\n",
       "  'scans. An additional layer-masked image was generated (Fig. 1 (d) ). Finally, all pre-processed images of size 512 × 650 were cropped to 512 × 512. To adapt the single-channel OCT images to the three- channel input requirements of the pretrained models, we constructed a composite 3-channel representation for each sample, as shown in Fig. 2. Specifically, the original grayscale OCT B-scan was assigned to the first channel of the network. The second channel contained a layer-masked version of the image, where each retinal layer was selectively enhanced to highlight the structural regions of interest [12]. The third channel consisted of a binary image encoding the retinal layer contours, providing explicit anatomical boundaries as auxiliary input. This multichannel representation was designed to enrich the input with both intensity-based and structural information. This facilitates improved feature extraction in downstream convolutional-based architectures. Fig. 2: Composite RGB representation of a single OCT B- scan used as model input. (a) First channel: original grayscale OCT image. (b) The second channel: the layer-masked version, where the retinal layers are selectively enhanced to highlight the structural features. (c) Third channel: binary retinal layer contours providing anatomical boundary information. Images used with permission from the UK Biobank under Application Number 82266. B. Training Augmentation: Deep learning models are prone to over- fitting, particularly when they are trained on small datasets. To mitigate this issue and improve generalization, we applied extensive augmentation strategies to artificially expand the training set and introduce greater variability, thereby enhanc- ing the robustness of the model and reducing the risk of overfitting. During training, the augmentation techniques were randomly selected and applied one at a time. Custom image augmentations, such as flipping and affine transformations (excluding shear and rotation in rectified images because of the nature of these images), were applied to all channels. In addition, OCT-specific augmentations such as occlusion, contrast adjustment, artificial vascular patterns, and noise addition were employed only in the first channel of the model. Training Models and Parameters: To investigate the effec- tiveness of deep learning for OCT-based classification, we employed a pretrained convolutional neural network (CNN) and transformer architecture. Specifically, we used ResNet (depths of 18,34,50, and 101) and VGG (depths of 8 and 11). In addition to these standard architectures, we also tested a domain-specific RETFound [13], a vision transformer pre- trained on a large dataset (over a million) of OCT and fundus images. RETFound has demonstrated strong performance in retinal imaging tasks [14] and was therefore included for com- parative evaluation in our Alzheimer’s disease classification framework. We implemented two distinct training strategies: RETFound-S, trained using three identical mid-scan OCT images as input, and RETFound-C, trained end-to-end using the composite 3-channel representation described in Fig. 2. We also developed and trained a custom convolutional model to serve as a baseline and explored architectural varia- tions tailored to the specific characteristics of OCT data. All models were trained using a batch size of 4, which was selected to maintain stable gradient updates while accommo- dating the constraints of small dataset size. We experimented with a range of learning',\n",
       "  'as a baseline and explored architectural varia- tions tailored to the specific characteristics of OCT data. All models were trained using a batch size of 4, which was selected to maintain stable gradient updates while accommo- dating the constraints of small dataset size. We experimented with a range of learning rates (0.001, 0.0001, and 0.000027) to explore the optimal convergence behavior across different model architectures. The AdamW optimizer was used for optimization in all experiments. We employed extensive image augmentation techniques (as detailed in the previous section) to address the risk of overfitting. To adapt the pretrained backbones for binary classification under low-data conditions, we appended a lightweight classi- fication head consisting of a fully connected layer (from the feature dimension to 64 units), Layer Normalization, followed by a ReLU activation, dropout (p = 0.4), and a final linear layer projecting to two output classes. This minimal regular- ization structure was selected to balance the expressiveness and overfitting control. We employed a year-weighted loss function [14], assigning higher importance to samples temporally closer to the clinical diagnosis of Alzheimer’s disease to recognize disease progres- sion dynamics. Due to the extensive data augmentation and regularization techniques applied to mitigate overfitting, the training and val- idation accuracy exhibited considerable fluctuations, making it difficult to identify an appropriate stopping epoch for the model. To stabilize the training and improve convergence, we trained the models for 100 epochs and then applied Stochastic Weight Averaging (SWA) [15] starting from epoch 80. The source code supporting the findings of this study will be released upon publication in the GitHub repository: https://github.com/OCTALZ-Project/UKBiobankAD C. Validation Comparing Results: We employed a nested cross-validation strategy [16] for a robust and unbiased performance estima- tion. The data were split into five outer folds, each of which served as a held-out test set. Within the remaining training data, three-fold inner cross-validation was used to tune the hyperparameters. The selected model was evaluated using the corresponding test fold. The predictions from the five outer folds were pooled to compute a single AUC per run. This procedure was repeated five times with different random splits, yielding five AUC values per model (5 runs × 5 outer folds × 3 inner folds). We applied this procedure separately to ResNet (18, 34, 50, and 101), VGG (8 and 11), RetFound (S and C), and the CNN baseline models. For each architecture, we calculated the mean AUC across five runs and selected the top-performing variant within each family. These best variants were then compared using a calibrated paired t-test [17], with adjusted degrees of freedom to account for dependence on repeated cross-validation. Explainability: We chose the Grad-CAM method to explain our best-performing model. Instead of using the standard image overlay format, we used 10 OCT-specific layers and the central subfield of the macula. We applied a 0.8 threshold to the Grad-CAM results to generate a highly focused explain- ability image. The contours of the OCT scan were overlaid on this image. To evaluate the class-level performance, we used the Intersection Over Union (IoU), Dice Score, and Filling Ratio on',\n",
       "  'subfield of the macula. We applied a 0.8 threshold to the Grad-CAM results to generate a highly focused explain- ability image. The contours of the OCT scan were overlaid on this image. To evaluate the class-level performance, we used the Intersection Over Union (IoU), Dice Score, and Filling Ratio on each layer, as detailed in the study by Nazli et al. [14]. Finally, we added the center subfield region of the OCT b- scans, where the overlaid GradCAM results were highlighted. Running with another sample set: We generated another training dataset with different age, sex, and instance-matching CNs while keeping the AD population the same. For the best- performing model, the same experiments were performed to determine whether the results were consistent with different datasets. Ablation Studies: The model trained with only one of the channels was replicated for all three channels to determine the impact of the segmentation and contour information on the training results. III. RESULTS We evaluated multiple deep learning models using nested cross-validation, 5 outer folds, and 3 inner folds to assess the classification performance in predicting Alzheimer’s disease (AD) using retinal OCT scans acquired 4 years before diag- nosis. Table II lists the top AUC results obtained for each model. The highest performing model was ResNet-34, which achieved a mean AUC of 0.624 ± 0.060 across five nested cross-validation runs. This model was used as a reference point for pairwise comparisons with alternative architectures. VGG-11 (mAUC = 0.581±0.017) and RETFound-C (mAUC = 0.540 ± 0.037) showed reduced performance relative to ResNet-34, but the differences were not statistically significant (corrected paired t-tests p = 0.1458 and p = 0.0845). The custom CNN model (mAUC = 0.519±0.026) also performed worse than ResNet-34, with significance under the standard t-test (p = 0.0266), but not after correction (p = 0.0364). In contrast, RETFound-S (mAUC = 0.459 ± 0.068) was significantly worse than ResNet-34 under both tests (standard p = 0.0043; corrected p = 0.0202). We further validated the ResNet architecture on an inde- pendent sample set (Dataset 2), which achieved a very similar performance (mAUC = 0.652 ± 0.058), This supports the robustness of the model. Finally, ablation experiments were conducted by rerunning ResNet-34 with reduced feature inputs. When trained with masked images only (ResNet1, mAUC = 0.452 ± 0.042), OCT images replicated to three channels (ResNet2, mAUC = 0.561 ± 0.061), or layer contour inputs (ResNet3, mAUC = 0.529 ± 0.071), The performance decreased significantly compared to that of the full multichannel representation. These results confirm that combining raw OCT, masked, and contour information provides the strongest feature representation for early AD prediction. To investigate the classification decision of the model and ensure that it learns clinically relevant features, we generated and analyzed saliency maps using GradCAM. The results are shown in Fig. 3. First, we aggregated the top 5% most salient pixels from all test images for each class to identify the most consistently important regions for classification (Fig. 3(a)). Furthermore, we analyzed individual cases to connect (a) Aggregated Top 5% Saliency Regions (b) Saliency Maps for Individual Test',\n",
       "  'shown in Fig. 3. First, we aggregated the top 5% most salient pixels from all test images for each class to identify the most consistently important regions for classification (Fig. 3(a)). Furthermore, we analyzed individual cases to connect (a) Aggregated Top 5% Saliency Regions (b) Saliency Maps for Individual Test Samples Fig. 3: Model interpretability analysis. (a) The aggregated top 5% most salient pixels for the AD (left, red/yellow) and CN (right, blue/green) classes highlight the model’s consistent focus on distinct anatomical regions. (b) Individual examples show model attention for True Positives (TP), True Negatives (TN), False Negatives (FN), and False Positives (FP). Images used with permission from the UK Biobank under Application Number 82266. the model’s attention to its performance on specific examples (Fig. 3(b)). The saliency analysis in Table III shows that the model gave minimal attention to the RNFL (Filling Ratio < 6% for AD) and instead focused on the central macular region (34.9% Filling Ratio for AD), especially the BMEIS and IS/OSJ layers. IV. DISCUSSION This study presents the first end-to-end deep learning framework for predicting Alzheimer’s disease (AD) from UK Biobank Optical Coherence Tomography (OCT) B-scans up to four years before diagnosis. Our best-performing model, ResNet-34, achieved a mean AUC (mAUC) of 0.624 ± 0.060 on a rigorously selected age, sex, and instance-matched AD and control sample. Earlier studies using data from the UK Biobank predicted early AD from fundus images. Tian et al [4]. achieved an accu- racy of 0.824 using fundus images with vessel segmentation. Wisely et al. [18] obtained an AUC of 0.625 using only OCTA images and 0.681 with GC-IPL projection maps. However, when they used quantitative data, including age and sex, their TABLE II: Model accuracies on 4-Year dataset Model mAUC f1-score Precision Sensitivity Specificity t-test corrected t-test p Value p Value ResNet 0.624 ± 0.060 0.552 ± 0.135 0.583 ± 0.086 0.680 ± 0.018 0.486 ± 0.159 VGG 0.581 ± 0.017 0.527 ± 0.064 0.568 ± 0.055 0.633 ± 0.062 0.500 ± 0.076 0.1354 0.1458 RETFound-C 0.540 ± 0.037 0.524 ± 0.061 0.557 ± 0.039 0.607 ± 0.037 0.507 ± 0.081 0.0728 0.0845 Custom CNN model 0.519 ± 0.026 0.490 ± 0.039 0.553 ± 0.028 0.600 ± 0.041 0.464 ± 0.051 *0.0266 *0.0364 RETFound-S 0.459 ± 0.068 0.468 ± 0.072 0.461 ± 0.046 0.446 ± 0.122 0.479 ± 0.117 *0.0043 *0.0202 ResNet1 0.452 ± 0.042 0.424 ± 0.018 0.463 ± 0.021 0.520 ± 0.038 0.407 ± 0.020 *0.0133 *0.0208 ResNet2 0.561 ± 0.061 0.523 ± 0.095 0.531 ± 0.090 0.527 ± 0.086 0.536 ± 0.107 *0.0464 0.0577 ResNet3 0.529 ± 0.071 0.498 ± 0.091 0.517 ± 0.081 0.533 ± 0.071 0.500 ± 0.104 0.0643 0.0759 ResNet4 0.652 ± 0.058 0.613 ± 0.037 0.604 ± 0.051 0.593 ± 0.094 0.614 ± 0.030 0.6815 0.6773 Note: ResNet1: the ablation test run with masked images replicated to 3 channels. ResNet2: ablation test run with OCT images replicated into three channels. ResNet3: The ablation test was performed with layer contours replicated into three channels. ResNet4: the ResNet model trained with the dataset 2. Corrected p-values',\n",
       "  '0.6815 0.6773 Note: ResNet1: the ablation test run with masked images replicated to 3 channels. ResNet2: ablation test run with OCT images replicated into three channels. ResNet3: The ablation test was performed with layer contours replicated into three channels. ResNet4: the ResNet model trained with the dataset 2. Corrected p-values were computed using a calibrated paired t-test to account for dependence induced by repeated cross-validation [17]. TABLE III: Quantitative Overlap of Saliency Maps with Retinal Layers Layer IoU Dice Fill (%) CN AD CN AD CN AD RNFL-GCL .020 .013 .039 .025 10.6 5.6 GCL-IPL .038 .017 .070 .033 15.8 7.4 IPL-INL .048 .028 .089 .054 15.7 8.6 INL-OPL .052 .021 .097 .041 22.7 10.7 OPL-HFL .067 .034 .124 .065 23.7 12.6 BMEIS .164 .103 .273 .177 26.3 18.6 IS/OSJ .022 .015 .042 .029 16.8 13.4 IB OPR .022 .017 .042 .033 15.7 14.9 IB RPE .019 .015 .036 .029 11.6 10.9 OB RPE .016 .015 .030 .028 9.5 9.9 Macula .249 .192 .379 .304 41.3 34.9 Note: ”Fill (%)” is the Filling Ratio. The filling Ratio is defined as |Saliency ∩Layer|/|Layer|. The bold values indicate the highest overlap for each class within the specific layers. model achieved an AUC of 0.96. When all the information (image and quantitative) was combined, the AUC was 0.809. Chua et al [19]. obtained an AUC of 0.82 when using a GC-IPL projection map dataset; however, their performance was reduced to 0.76 when trained with age-matched subjects. These results show that age and sex can be strong confounding factors in AD prediction models. There is no OCTA image dataset in the UK Biobank; therefore, these studies [18], [19] used private OCTA image datasets. Our goal was to study only the structural retinal changes related to AD. As a result, our model performance was moderate, but it likely reflects AD- related structural features in the retina more directly, as it was not influenced by demographic confounders. When comparing the architectures, ResNet-34 outperformed both VGG-11 and OCT-pretrained transformer models. VGG- 11 achieved a mAUC of 0.581 ± 0.017, which was not significantly different from that of ResNet-34 after correction. RETFound-C reached an mAUC of 0.540 ± 0.037, which was not significantly different. In contrast, RETFound-S performed considerably worse, with an mAUC of 0.459 ± 0.068, and the difference relative to ResNet-34 remained statistically significant even after correction for multiple comparisons. These findings suggest that in low-sample settings, convolu- tional networks may provide more stable representations than transformer-based models, despite being pre-trained on OCT data. To further test robustness, we repeated the analysis using the same AD cohort and a randomly matched control group. Performance was preserved (AUC = 0.652 ± 0.058), indicating that the results were stable against variations in the control selection. Ablation experiments further emphasized the importance of the three-channel input strategy. When ResNet was trained using only masked images, only replicated grayscale OCT, or only retinal layer contours, performance dropped substantially compared to the full multichannel input. This demonstrates that the combination of raw OCT intensity, layer-specific masking, and anatomical boundary (contour) information pro- vides',\n",
       "  'importance of the three-channel input strategy. When ResNet was trained using only masked images, only replicated grayscale OCT, or only retinal layer contours, performance dropped substantially compared to the full multichannel input. This demonstrates that the combination of raw OCT intensity, layer-specific masking, and anatomical boundary (contour) information pro- vides the most informative feature representations. Explainability analysis showed that the model allocated very little attention to the RNFL (Filling Ratio <6% for AD), despite its frequent use as a biomarker in previous studies. Instead, attention was concentrated on the central macular region (34.9% for AD), with the highest overlap observed in the BMEIS and IS/OSJ layers. This indicates that the network relies on features within the photoreceptor and RPE complex, highlighting regions that may act as early biomarkers of AD. These results further support the potential of retinal OCT combined with AI for noninvasive and scalable early AD risk prediction. Despite these promising results, this study has several lim- itations. The dataset size, particularly in the 4-year diagnostic group, constrained the statistical analyses and may have lim- ited the generalizability of the findings to broader populations. One of the main contributing factors to the modest prediction accuracy was the study’s dependency on a single dataset, which limited the model’s ability to generalize and increased the risk of overfitting. Although our nested cross-validation framework was designed to mitigate overfitting and provide a robust internal estimate of model performance, external validation remains a critical next step. However, to the best of our knowledge, no publicly available OCT datasets exist that include both retinal imaging and longitudinal Alzheimer’s disease outcomes, making such validation infeasible. Future collaborations with clinical centers or research initiatives will be essential to acquire independent cohorts for further vali- dation. Establishing shared datasets and benchmarks in this domain would greatly enhance reproducibility and compara- bility across studies and facilitate the translation of OCT-based biomarkers into clinical use. Additionally, while nested cross- validation mitigates overfitting, further improvements could be achieved through ensembling, multimodal integration (e.g., OCT angiography, cognitive testing), or temporal modeling of longitudinal scans. Finally, clinical validation against gold- standard biomarkers (e.g., amyloid PET and CSF tau) is necessary to establish OCT’s role in preclinical AD screening. V. CONCLUSION This study presents the first deep learning framework ap- plied to UK Biobank retinal OCT data for early prediction of Alzheimer’s disease up to four years before diagnosis. We proposed an anatomically guided preprocessing pipeline with multichannel OCT input, hybrid augmentation, and nested cross-validation. ResNet-34 achieved the best performance, with a mean AUC of 0.624, and robustness was confirmed with a re-matched control dataset. Ablation experiments showed that combining raw, masked, and contour inputs improved performance, while saliency maps highlighted the macular BMEIS and IS/OSJ layers. Our findings provide a repro- ducible baseline for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches. ACKNOWLEDGEMENTS This study was conducted using the UK Biobank Resource under Application Number 82266. Computational resources were provided by the Turkish National High-Performance',\n",
       "  'for OCT-based AD prediction, highlight the challenges of detecting subtle retinal biomarkers years before AD diagnosis, and point to the need for larger datasets and multimodal approaches. ACKNOWLEDGEMENTS This study was conducted using the UK Biobank Resource under Application Number 82266. Computational resources were provided by the Turkish National High-Performance Computing Center (UHeM, Project Number 1017802024). This study was also supported by the Scientific and Techno- logical Research Council of Turkey (T ¨UB˙ITAK) under Grant Number 122E509. REFERENCES [1] WHO, “World Health Organization: Dementia Fact Sheet,” pp. 1–5, 2022. [Online]. Available: https://www.who.int/news-room/fact-sheets/ detail/dementia [2] A. Grzybowski and P. Barboni, The Eye as a Window to the Brain OCT and Imaging in Central Nervous System Diseases. Springer, 2020. [3] A. London, I. Benhar, and M. Schwartz, “The retina as a window to the brain - From eye research to CNS disorders,” Nature Reviews Neurology, vol. 9, no. 1, pp. 44–53, jan 2013. [4] J. Tian, G. Smith, H. Guo, B. Liu, Z. Pan, Z. Wang, S. Xiong, and R. Fang, “Modular machine learning for Alzheimer’s disease classification from retinal vasculature,” Scientific Reports —, vol. 11, p. 238, 2021. [Online]. Available: https://doi.org/10.1038/s41598-020-80312-2 [5] N. Yousefzadeh, C. Tran, A. Ramirez-Zamora, J. Chen, R. Fang, and M. T. Thai, “Neuron-level explainable ai for alzheimer’s disease assessment from fundus images,” Scientific Reports, vol. 14, 12 2024. [6] F. C. van der Heide, A. Khawaja, T. T. Berendschot, T. J. Littlejohns, E. Ku´zma, R. Luben, P. J. Patel, P. J. Foster, G. Bertelsen, T. von Hanno, B. Johnsen, H. Schirmer, S. C. Rebouc¸as, L. Grasset, C. Delcourt, C. Helmer, and C. D. Stehouwer, “Associations of inner retinal layers with risk of incident dementia: An individual participant data analysis of four prospective cohort studies,” Alzheimer’s and Dementia, vol. 20, pp. 211–220, 1 2024. [7] Y. Turkan, F. B. Tek, F. Arpaci, O. Arslan, D. Toslak, M. Bulut, and A. Yaman, “Automated diagnosis of alzheimer’s disease using oct and octa: A systematic review,” IEEE Access, vol. 12, pp. 104 031–104 051, 2024. [8] S. Y. L. Chua, D. Thomas et al., “Cohort profile: Design and methods in the eye and vision consortium of UK Biobank,” BMJ Open, vol. 9, no. 2, pp. 1–13, 2019. [9] P. J. Patel, P. J. Foster, C. M. Grossi, P. A. Keane, F. Ko, A. Lotery, T. Peto, C. A. Reisman, N. G. Strouthidis, and Q. Yang, “Spectral-domain optical coherence tomography imaging in 67 321 adults: Associations with macular thickness in the uk biobank study,” Ophthalmology, vol. 123, pp. 829–840, 2016. [Online]. Available: http://dx.doi.org/10.1016/j.ophtha.2015.11.009 [10] M. K. Garvin, M. D. Abr`amoff, X. Wu, S. R. Russell, T. L. Burns, and M. Sonka, “Automated 3-d intraretinal layer segmentation of macular spectral-domain optical coherence tomography images,” IEEE Transac- tions on Medical Imaging, vol. 28, pp. 1436–1447, 9 2009. [11] M. D. Abramoff, M. K. Garvin, and M. Sonka, “Retinal imaging and image analysis,” pp. 169–208, 2010. [12] ¨Oyk¨u Eren, F. B. Tek, and Y. Turkan, “Segmentation based classification of retinal diseases in oct images.” Institute of Electrical and Electronics Engineers Inc., 2024, pp. 890–895. [13] Y. Zhou,',\n",
       "  '2009. [11] M. D. Abramoff, M. K. Garvin, and M. Sonka, “Retinal imaging and image analysis,” pp. 169–208, 2010. [12] ¨Oyk¨u Eren, F. B. Tek, and Y. Turkan, “Segmentation based classification of retinal diseases in oct images.” Institute of Electrical and Electronics Engineers Inc., 2024, pp. 890–895. [13] Y. Zhou, M. A. Chia, and other, “A foundation model for generalizable disease detection from retinal images,” Nature, vol. 622, pp. 156–163, 10 2023. [14] M. S. Nazlı, Y. Turkan, F. B. Tek, D. Toslak, M. Bulut, F. Arpacı, and M. C. ¨Ocal, “Retinal disease diagnosis in oct scans using a foundational model,” vol. 15618 LNCS. Springer Science and Business Media Deutschland GmbH, 2025, pp. 208–220. [15] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. G. Wilson, “Averaging weights leads to wider optima and better generalization,” arxiv, 2 2019. [Online]. Available: http://arxiv.org/abs/1803.05407 [16] Y. Zhong, P. Chalise, and J. He, “Nested cross-validation with ensemble feature selection and classification model for high-dimensional biolog- ical data,” Communications in Statistics: Simulation and Computation, vol. 52, pp. 110–125, 2023. [17] R. R. Bouckaert, “Choosing between two learning algorithms based on calibrated tests,” in Proceedings of the Twentieth International Conference on Machine Learning (ICML), 2003. [18] C. E. Wisely, D. Wang, R. Henao, D. S. Grewal, A. C. Thompson, C. B. Robbins, S. P. Yoon, S. Soundararajan, B. W. Polascik, J. R. Burke, A. Liu, L. Carin, and S. Fekrat, “Convolutional neural network to identify symptomatic Alzheimer’s disease using multimodal retinal imaging,” British Journal of Ophthalmology, vol. 106, no. 3, pp. 388– 395, 2022. [19] J. Chua, C. Li, F. Antochi, E. Toma, D. Wong, B. Tan, G. Garh¨ofer, S. Hilal, A. Popa-Cherecheanu, C. L. H. Chen, and L. Schmetterer, “Utilizing deep learning to predict alzheimer’s disease and mild cogni- tive impairment with optical coherence tomography,” Alzheimer’s and Dementia: Diagnosis, Assessment and Disease Monitoring, vol. 17, 1 2025.'],\n",
       " ['DL101 Neural Network Outputs and Loss Functions Fernando Berzal berzal@acm.org Department of Computer Science and Artificial Intelligence, University of Granada, Spain I. Activation Functions 1 II. Loss Functions 2 III. Statistical Justification 3 A. Maximum Likelihood Estimation 3 1. Regression and Mean Squared Error (MSE) 4 2. Regression and Mean Absolute Error (MAE) 5 3. Binary Classification and Binary Cross-Entropy 6 4. Multi-Class Classification and Categorical Cross-Entropy 6 5. Summary 7 B. Generalized Linear Models 7 1. Regression: Gaussian GLM 9 2. Regression: Laplace GLM 9 3. Binary Classification: Bernoulli GLM 9 4. Multi-Class Classification: Multinomial GLM 10 5. Summary 10 IV. Additional Situations 10 A. Binary Classification with Bipolar Encoding 10 B. Regression of Positive Values 11 C. Fat Tails 13 V. Conclusion 15 A. Activation functions 15 1. Logistic function 15 2. Hyperbolic tangent 15 3. Softmax function 16 4. Softplus function 17 5. Swish function 17 6. ReLU function 18 B. Loss Functions 19 References 22 I keep saying the sexy job in the next ten years will be statisticians. People think I’m joking, but who would’ve guessed that com- puter engineers would’ve been the sexy job of the 1990s? The ability to take data -to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it- that’s going to be a hugely important skill in the next decades... Because now we really do have essentially free and ubiquitous data. So the complimentary scarce factor is the ability to understand that data and extract value from it. Hal R. Varian The McKinsey Quarterly, January 2009 Choosing the right loss function is absolutely central to building effective machine learning models. In the case of deep learning models (1, 3, 4, 6, 17), the output layer of the neural network can be viewed as a generalized linear model. Given its inputs (x1..xn), the output of the final layer of the neural network is typically computed as: y = f n X i=1 wixi ! = f(⃗w · ⃗x) = f(w⊤x) where f is the activation function and (w1..wn) are the output neuron parameters, a.k.a. weights. It is also common that a neuron incorporates a bias b, so that its output can be computed as f(⃗w · ⃗x + b). Assuming an extra input x0 = 1 with its corresponding weight w0 = b allows us to absorb the bias into the above expression, thus avoiding clutter. I. ACTIVATION FUNCTIONS The output of an artificial neuron is often just a linear combination of its inputs and its parameters or weights. For such neurons, it is suitable to define its net input (a.k.a. pre-activation) as z = ⃗w · ⃗x. Such scalar input is transformed by the neuron activation function f to ob- tain the neuron output y = f(z). Table I summarizes some common activation functions, which are often non- linear so that a multilayer neural network can act as an universal approximator (11, 15, 21, 28, 34): • The linear or identity function is used mainly in the output layer for regression tasks',\n",
       "  'neuron output y = f(z). Table I summarizes some common activation functions, which are often non- linear so that a multilayer neural network can act as an universal approximator (11, 15, 21, 28, 34): • The linear or identity function is used mainly in the output layer for regression tasks where the goal is to predict a continuous numerical value. It is also used within specialized network architectures, such as convolutional networks, but in hidden lay- ers it should also be combined with non-linear ac- tivation functions (since you could always replace multiple linear layers with a single linear layer, thus loosing the universal approximation properties of multi-layered networks with non-linear activation functions). • The logistic function (σ) is a sigmoid function (i.e. S-shaped) that squashes it input into the [0, 1] in- terval, making it suitable for output layers in bi- nary classification problems, when the output can be interpreted as a probability. arXiv:2511.05131v1 [cs.LG] 7 Nov 2025 2 Activation function Range Derivative Linear (identity) f(z) = z (−∞, ∞) f ′(z) = 1 Logistic (standard sigmoid) f(z) = σ(z) = 1 1 + e−z (0, 1) f ′(z) = f(z)(1 −f(z)) Hyperbolic tangent (tanh) f(z) = tanh(z) = ez −e−z ez + e−z (−1, 1) f ′(z) = 1 −f 2(z) Softmax fi(⃗z) = ezi PK j=1 ezj [0,1] ∂yi ∂xj = ∂fi(⃗z) ∂xj = yi(δij −yj) Softplus f(z) = ln(1 + ez) [0, ∞) f ′(z) = ez 1 + ez = 1 1 + e−z = σ(z) ReLU f(z) = max{0, z} [0, ∞) f ′(z) = H(z) = ( 1 if z > 0 0 if z ≤0 Leaky ReLU f(z) = ( z if z > 0 αz if z ≤0 (−∞, ∞) f ′(z) = ( 1 if z > 0 α if z ≤0 Heaviside step (LTU) f(z) = H(z) = ( 1 if z > 0 0 if z ≤0 {0, 1} f ′(z) = δ(z) = ( ∞ if z = 0 0 if z ̸= 0 Table I Common activation functions in neural networks: The activation function determines the output of a neuron, trans- forming the weighted sum of its inputs into an output signal. For additional activation functions and their variants, check Appendix A. • The hyperbolic tangent (tanh) function is similar to the sigmoid but maps the input to the [−1, 1]. It is zero-centered, which often aids in faster conver- gence during training when compared to the logistic function. It is often used in conjunction with bipo- lar encoding −1, 1. The hyperbolic tangent and the logistic function are related sigmoid squashing functions: tanh(z) = 2σ(2z) −1. • The softmax function is typically used in the output layer for multi-class classification problems with K classes. It converts a vector of numbers into a prob- ability distribution, where the sum of the probabil- ities is equal to 1.1 • The softplus activation function avoids the main drawback sigmoid functions (logistic and tanh), whose gradients go to zero when their net inputs have large positive or large negative values. For',\n",
       "  'vector of numbers into a prob- ability distribution, where the sum of the probabil- ities is equal to 1.1 • The softplus activation function avoids the main drawback sigmoid functions (logistic and tanh), whose gradients go to zero when their net inputs have large positive or large negative values. For large positive inputs z ≫0, f(z) ≈ln(ez) = z, so that its gradient is not zero. For large negative inputs z ≪0, ez ≈0 and f(z) ≈ln(1) = 0, i.e. approaches 0 but is always positive. 1 The derivative of the softmax function can be expressed in terms of the Kronecker delta (δij) and it results in a Jacobian matrix because softmax is a vector-valued function. The Kronecker delta is δij = 1 when i = j and δij = 0 when i ̸= j. The derivative of the i-th component of the softmax output, yi, with respect to the j-th component of the input, xj, is yi(δij −yj). • Rectified linear units (ReLU) are a popular choice for hidden layers in deep learning models. The ReLU function introduces a single non-linearity and is computationally efficient. The softplus func- tion is a smooth, differentiable approximation of the ReLU function. At z = 0, the ReLU func- tion has a sharp corner (non-differentiable point), while the softplus function transitions smoothly from near-zero to linear. • The Leaky ReLU is a variant of the ReLU function that attempts to solve the “dying ReLU” problem by allowing a small, non-zero gradient for negative inputs (typically α = 0.01). • The Heaviside step function was used as the acti- vation function in the linear threshold units used by McCulloch and Pitts (32). However, it is not used in modern artificial neural networks because its derivative is the Dirac delta function, zero ev- erywhere except at z = 0, where it is infinitely large. Therefore, it is not suitable for gradient- based learning algorithms. II. LOSS FUNCTIONS We can now return to our original problem, that of choosing the right loss function for our particular prob- lem. Let us consider different tasks and justify which loss functions seem to be the most natural fit: 3 • In regression problems, we predict continuous values, so the loss must measure the distance be- tween predicted and true values. Mean Squared Error (MSE) or Mean Absolute Error (MAE) di- rectly quantify this distance, making them natural choices: MSE = 1 n n X i=1 (ti −yi)2 MAE = 1 n n X i=1 |ti −yi| where n is the number of data points, ti is the ac- tual (target) value, and yi is the predicted value. MSE calculates the average of the squared differ- ences between the predicted values and the actual values. MSE penalizes large errors more heavily be- cause of the square term, which can be useful when you want your model to be especially sensitive to outliers or large deviations. MAE treats all errors linearly, so it’s more robust / less sensitive to outliers. MAE would be a better choice when you care about median-like behavior rather',\n",
       "  'cause of the square term, which can be useful when you want your model to be especially sensitive to outliers or large deviations. MAE treats all errors linearly, so it’s more robust / less sensitive to outliers. MAE would be a better choice when you care about median-like behavior rather than mean-like behavior. MAE is also more intuitive to understand as it represents the average error in the same units as the target variable. • Binary classification is about predicting the probabilities of two possible outcomes (i.e. two classes). Binary cross-entropy is the most princi- pled way to compare predicted probabilities with the true labels: H = −1 n n X i=1 (ti · log(yi) + (1 −ti) · log(1 −yi)) where n is the number of observations, ti is the ac- tual class label (0 or 1), and yi is the predicted probability of the i-th observation belonging to class 1. This loss function is applied when the model out- puts a probability between 0 and 1. Cross-entropy measures the “distance” between the predicted probability distribution and the true distribution (0 or 1). It quantifies how close the predicted prob- ability is to the actual class label (0 or 1) for each example. A lower binary cross-entropy value in- dicates a more accurate model. The logarithmic component of the formula heavily penalizes confi- dent but incorrect predictionss, which encourages the model to output well-calibrated probabilities. • In multi-class classification problems, with K different classes (K > 2), the goal is to pre- dict a probability distribution across K classes (of- ten via the softmax function). Categorical cross- entropy measures how well the predicted distribu- tion matches the true one-hot encoded label: H = −1 n n X i=0 K X j=0 tij · log(yij) where K is the number of classes, n is the number of observations, tij is a binary indicator (0 or 1) if class label j is the correct classification for observation i, and yij is the predicted probability of observation i belonging to class j. The categorical cross-entropy ensures the model not only predicts the correct class but also as- signs high probability to it relative to other classes. Similar to its binary counterpart, a lower categori- cal cross-entropy value signifies a better-performing model. As we will see, the above choices of loss functions are far from arbitrary. They are not only natural, but they are also the statistically justified choice for training clas- sification models because minimizing those loss functions is equivalent to maximizing the likelihood of the model predictions. III. STATISTICAL JUSTIFICATION The formal justification for why mean squared error (MSE) or mean absolute error (MAE), binary cross- entropy (BCE), and categorical cross-entropy (CCE) are the natural loss/error functions for their respective prob- lems comes from the principle of maximum likelihood estimation (MLE), a method for estimating the param- eters of an assumed probability distribution, given some observed data. In short, choosing a loss function is equiv- alent to assuming a specific probability distribution for your data. It turns out that these common loss func-',\n",
       "  'from the principle of maximum likelihood estimation (MLE), a method for estimating the param- eters of an assumed probability distribution, given some observed data. In short, choosing a loss function is equiv- alent to assuming a specific probability distribution for your data. It turns out that these common loss func- tions are precisely the negative log-likelihoods for stan- dard probability distributions. A. Maximum Likelihood Estimation The MLE principle states that we should choose model parameters that maximize the probability (or likelihood) of observing the training data, i.e. the most likely model that could have produced the observed data. Minimizing the negative log-likelihood (NLL) of the data is the same as maximizing their likelihood. The likelihood function measures how ”likely” the ob- served training data are for different values of the model parameters. Assuming independent and identically dis- tributed data (i.i.d.), the likelihood function is the prod- uct of the probabilities of each individual observation, given the model parameters: L(θ|x1, x2, ..., xn) = P(x1|θ) · P(x2|θ) · ... · P(xn|θ) 4 L(θ|x1, x2, ..., xn) = n Y i=1 P(xi|θ) where L(θ|x1, ..., xn) is the likelihood of the parameter θ given the data x1, ..., xn. Multiplying many small probabilities can lead to nu- merical underflow problems (i.e. a rounding error when floating-point numbers near zero are rounded to zero). It is then common to work with the natural logarithm of the likelihood function, called the log-likelihood function. Taking the log turns the product into a sum: log L(θ) = n X i=1 log P(xi|θ) Since the logarithm is a monotonically increasing func- tion, maximizing the log-likelihood is the same as max- imizing the likelihood. Therefore, to find the value of the distribution parameters that maximizes the log- likelihood function of the observed data, you can take the derivative of the log-likelihood function with respect to the model parameters, set it to zero, and solve for the parameters. When closed analytical solutions cannot be found, numerical optimization algorithms are used. MLE is widely used because its estimates have sev- eral desirable properties: consistency (as the sample size grows, the MLE gets closer to the true value of the pa- rameters), asymptotic normality (with a large enough sample size, the distribution of the MLE is approximately normal), efficiency (for large samples, the MLE has the smallest possible variance among all unbiased estima- tors), and invariance (if you apply a function to a param- eter, the MLE of the transformed parameter is simply the function applied to the original MLE). The choice of different underlying probability distribu- tions for the observed data leads naturally to different loss functions. Obviously, MLE relies on the assumption that the chosen probability distribution should be a good fit for the data. If that is not the case, your MLE esti- mates will be inaccurate. Another key assumption is that the data must be independent and identically distributed (i.i.d.), i.e. each data point is drawn from the same un- derlying distribution and is independent of the others, a requirement that might not always hold in practice. 1. Regression',\n",
       "  'your MLE esti- mates will be inaccurate. Another key assumption is that the data must be independent and identically distributed (i.i.d.), i.e. each data point is drawn from the same un- derlying distribution and is independent of the others, a requirement that might not always hold in practice. 1. Regression and Mean Squared Error (MSE) For regression, we want to predict a continuous scalar value y from the input vector ⃗x. We can assume that the target variable y is generated from our model prediction f(x) plus some Gaussian (normal) noise, ϵ. y = f(x) + ϵ where ϵ ∼N(0, σ2) The noise ϵ is assumed to have a mean of 0 and a constant variance σ2. This is equivalent to saying the conditional probability of y given x is a Gaussian distribution centered at our model prediction: p(y|x) = N(y|f(x), σ2) = 1 σ √ 2π e−1 2( y−f(x) σ ) 2 To find the best model parameters, we maximize the likelihood of observing our entire dataset of n i.i.d. points or, equivalently, its log-likelihood: L(f) = n Y i=1 p(yi|xi) = n Y i=1 N(yi|f(xi), σ2) log L(f) = n X i=1 log p(yi|xi) = n X i=1 log N(yi|f(xi), σ2) Given our assumption of Gaussian noise: log L(f) = n X i=1 log \\x12 1 σ √ 2π \\x13 −1 2 \\x12yi −f(xi) σ \\x132 log L(f) = n X i=1 log \\x12 1 σ √ 2π \\x13 − n X i=1 1 2σ2 (yi −f(xi))2 log L(f) = n log \\x12 1 σ √ 2π \\x13 − 1 2σ2 n X i=1 (yi −f(xi))2 log L(f) = −n 2 log \\x002πσ2\\x01 − 1 2σ2 n X i=1 (yi −f(xi))2 The first term is independent of the model predictions f(xi), so to maximize the log-likelihood, we must mini- mize the second term, whose leading factor 1/2σ2 is also independent of the model predictions. So we just have to minimize n X i=1 (yi −f(xi))2 which is precisely the sum of squared errors (SSE). There- fore, minimizing the mean squared error MSE (just SSE divided by the constant n) is equivalent to performing maximum likelihood estimation under the assumption of Gaussian noise. In other words, MSE is the negative log-likelihood under Gaussian noise. Bayes optimality refers to a decision-making princi- ple that minimizes the expected loss, representing the best possible performance for a given task. A model that achieves this lowest possible error rate is called the Bayes optimal classifier (for classification) or Bayes optimal pre- dictor (for regression). It’s a theoretical benchmark that sets the performance ceiling; no other model can do bet- ter on average. 5 In a regression setting, the Bayes optimal predic- tor, f ∗(x), is the function that minimizes this expected squared error. It achieves this by always predicting the conditional expectation of the output given the input: f ∗(x) = E[y|x] Why is the conditional expectation the best possible prediction? Imagine you have a fixed input x. There might be a range of possible y values associated with it, each with a certain probability.',\n",
       "  'this by always predicting the conditional expectation of the output given the input: f ∗(x) = E[y|x] Why is the conditional expectation the best possible prediction? Imagine you have a fixed input x. There might be a range of possible y values associated with it, each with a certain probability. If you pick any prediction ˆy, the expected squared error at that point is E[(y − ˆy)2|x]. This expression is minimized when ˆy is exactly the average of all possible y values, which is the conditional mean E[y|x]. From a more formal point of view, we can prove that the conditional mean minimizes the squared loss. Let Y be a random variable with conditional distribution given X = x. For any predictor ˆy ∈R, consider the conditional risk, i.e. the expected loss for a specific decision, given a particular observation: R2(ˆy) = E \\x02 (Y −ˆy)2|X = x \\x03 R2(ˆy) = E[Y 2|x] −2ˆy E[Y |x] + ˆy2 In Bayesian decision theory, the conditional risk is used to evaluate how good a decision-making rule is for each possible data point you might see. The rule that mini- mizes the conditional risk for every possible observation is called the Bayes estimator or Bayes decision rule. So we differentiate with respect to the prediction ˆy: d dˆy R2(ˆy) = −2 E[Y |x] + 2ˆy. Setting the derivative to zero, we obtain that the condi- tional expectation the best possible prediction: ˆy∗= E[Y |x]. Given that the second derivative is 2 > 0, so ˆy∗is the unique minimizer of the MSE. MSE risk is minimized by the conditional mean and learning with the MSE loss function estimates E[Y |X]. Therefore, minimizing the MSE is equivalent to finding the Bayes optimal predictor. The remaining error, known as the Bayes error, is caused by the inherent noise or randomness in the data itself. Under the assumption of Gaussian noise, minimizing the negative log-likelihood is equivalent to minimizing the mean squared error. Since minimizing MSE gives us the Bayes optimal predictor, it follows that minimizing the NLL under Gaussian noise also leads to the Bayes optimal solution. 2. Regression and Mean Absolute Error (MAE) If instead of starting from a regression model with ad- ditive Gaussian noise, we assume additive Laplace noise: y = f(x) + ε, ε ∼Laplace(0, b), with a Laplace probability density p(ε) = Laplace(0, b) = 1 2be−|ε| b The conditional probability of y given x is a Laplace distribution centered at our model prediction: p(y|x) = 1 2be−|y−f(x)| b , The log-likelihood of observing our entire dataset of i.i.d. examples is now log L(f) = −n log(2b) −1 b n X i=1 |yi −f(xi)| Maximizing L(f) is equivalent to minimizing n X i=1 |yi −ˆyi|, i.e., the empirical sum of absolute errors. Therefore, MAE is the negative log-likelihood under Laplace noise. Using Bayesian decision theory, we can check that the conditional median minimizes the absolute loss. For any ˆy ∈R, the conditional risk is now R1(ˆy) = E[|Y −ˆy| | X = x] = Z |y −ˆy| dFY |x(y). The conditional risk function',\n",
       "  'MAE is the negative log-likelihood under Laplace noise. Using Bayesian decision theory, we can check that the conditional median minimizes the absolute loss. For any ˆy ∈R, the conditional risk is now R1(ˆy) = E[|Y −ˆy| | X = x] = Z |y −ˆy| dFY |x(y). The conditional risk function is R1(ˆy), the mean abso- lute error (MAE), and our goal is to find the prediction ˆy that minimizes it. The problem is that the absolute value function, |z|, is not differentiable at z = 0. Since we cannot just take the standard derivative of R1(ˆy) with respect to ˆy and set it to zero, we find the subgradient2 of the risk function R1(ˆy). We split the risk integral into two parts, based on where y −ˆy is positive or negative: R1(ˆy) = Z ˆy −∞ (ˆy −y) dFY |x(y) + Z ∞ ˆy (y −ˆy) dFY |x(y) Now, we can differentiate with respect to ˆy using the Leibniz rule for differentiating under the integral sign: ∂R1(ˆy) = Z ˆy −∞ 1 dFY |x(y) − Z ∞ ˆy 1 dFY |x(y) The integrals of the probability distribution function are, by definition, probabilities: Z ˆy −∞ dFY |x(y) = P(Y ≤ˆy | X = x) = FY |x(ˆy) 2 A subgradient is a way to generalize the concept of a derivative (or gradient) to functions that aren’t smooth or differentiable everywhere. For a convex function, instead of a single tangent line at a point, you might have a whole set of lines that lie below the function. Any of these lines’ slopes is a subgradient. A sub- gradient allows us to optimize a function even when the original function is not perfectly differentiable. 6 Z ∞ ˆy dFY |x(y) = P(Y > ˆy | X = x) = 1 −FY |x(ˆy) Substituting these back we obtain a subgradient with respect to ˆy is ∂R1(ˆy) = FY |x(ˆy) −(1 −FY |x(ˆy)) = 2FY |x(ˆy) −1, where FY |x is the conditional cdf. Any ˆy with 0 ∈∂R1(a) satisfies FY |x(a) = 1 2, i.e., ˆy is a conditional median, the point where the cumulative dis- tribution function (CDF) equals 0.5. Thus, minimizing the expected absolute loss yields the conditional median. 3. Binary Classification and Binary Cross-Entropy For binary classification problems, the target y is ei- ther 0 or 1, y ∈{0, 1}. We can assume that the outcome follows a Bernoulli distribution. The model, with a sig- moid output ˆy, predicts the probability of the outcome being 1. The classifier outputs ˆy ∈(0, 1): P(y = 1|x) = ˆy P(y = 0|x) = 1 −ˆy Both expressions can be combined into a more succinct single expression, the Bernoulli likelihood for a single ob- servation: P(y|x) = ˆyy(1 −ˆy)1−y Assuming i.i.d. examples, as always, the likelihood for the whole training dataset is: L = n Y i=1 P(yi|xi) = n Y i=1 ˆyyi i (1 −ˆyi)1−yi Taking the logarithm gives the log-Likelihood: log L = n X i=1 [yi log(ˆyi) + (1 −yi) log(1 −ˆyi)] Since our goal is maximizing this log-likelihood, we can also',\n",
       "  'likelihood for the whole training dataset is: L = n Y i=1 P(yi|xi) = n Y i=1 ˆyyi i (1 −ˆyi)1−yi Taking the logarithm gives the log-Likelihood: log L = n X i=1 [yi log(ˆyi) + (1 −yi) log(1 −ˆyi)] Since our goal is maximizing this log-likelihood, we can also minimize the negative of this quantity. The averaged (per example) negative log-likelihood is precisely the bi- nary cross-entropy: BCE = −1 n n X i=1 [yi log(ˆyi) + (1 −yi) log(1 −ˆyi)] Over the training examples, minimizing the average BCE is equivalent to the maximum likelihood estimate for a Bernoulli model: minimizing the binary cross- entropy is identical to maximizing the log-likelihood of the data under the assumption of a Bernoulli distribu- tion. True probabilities minimize the expected log loss (i.e. the binary cross-entropy). Given x, for any candidate ˆy ∈(0, 1), the conditional risk is RBCE(ˆy) = E[−Y log ˆy −(1 −Y ) log(1 −ˆy) | X = x] RBCE(ˆy) = −y log ˆy −(1 −y) log(1 −ˆy), We minimize such conditional risk by differentiation: d dˆy RBCE(ˆy) = −y ˆy + 1 −y 1 −ˆy Setting the derivative to zero: −y ˆy + 1 −y 1 −ˆy = 0 ⇒ (1 −y)ˆy = y(1 −ˆy) ⇒ ˆy = y The second derivative is d2 dˆy2 RBCE(ˆy) = y ˆy2 + 1 −y (1 −ˆy)2 > 0 shows that the conditional risk is strictly convex, so ˆy = y is its unique minimum. Therefore, the log loss is a strictly proper scoring rule: it is minimized by the true conditional probability. 4. Multi-Class Classification and Categorical Cross-Entropy For multi-class classification, the target y belongs to one of K classes. We can now assume that the outcome follows a multinomial distribution, also known as cate- gorical, multinoulli, or generalized Bernoulli distribution. The true label yi is represented as by one-hot encoded vector and the model softmax output ˆyi is a probability distribution, i.e. a vector of probabilities for each of the K classes: ˆyi = [pi1, pi2, ..., piK] The probability of observing the one-hot vector yi is: P(yi|xi) = K Y k=1 pyik ik Since yik is 1 only for the true class and 0 otherwise, this product simply picks the predicted probability for the correct class. The likelihood for the whole training dataset is: L = n Y i=1 K Y k=1 pyik ik Taking the logarithm gives the log-likelihood: log L = n X i=1 K X k=1 yik log(pik) 7 Maximixing this log-likelihood is equivalent to mini- mizing its negative, the negative log-likelihood. In this case, the negative log-likelihood is just the categorical cross-Entropy loss: CCE = − n X i=1 K X k=1 yik log(pik) Therefore, minimizing the categorical cross-entropy is formally equivalent to maximizing the log-likelihood of the data under the assumption of a multinomial distri- bution: the categorical cross-entropy is the multinomial negative log-likelihood. Let us now check how the true distribution minimizes the expected cross-entropy. Given x, let y = (y1, . . . , yK) be the true class distribution and ˆy any',\n",
       "  'of the data under the assumption of a multinomial distri- bution: the categorical cross-entropy is the multinomial negative log-likelihood. Let us now check how the true distribution minimizes the expected cross-entropy. Given x, let y = (y1, . . . , yK) be the true class distribution and ˆy any candidate model output. The conditional risk is now RCCE(ˆy) = E \" − K X k=1 Yk log ˆyk | X = x # = − K X k=1 yk log ˆyk. This is the cross-entropy H(y, ˆy), since − X k yk log ˆyk = − X k yk log yk + X k yk log yk ˆyk H(y, ˆy) = H(y) + KL(y∥ˆy) Given that H(y) is constant with respect to ˆy and KL(y∥ˆy) ≥0 with equality if and only if ˆy = y, the unique minimizer of the conditional risk is ˆy = y. There- fore, categorical cross-entropy is a strictly proper scoring rule: its expected value is uniquely minimized when the predicted probability distribution exactly matches the true (or empirical) probability distribution. 5. Summary We have seen that choosing loss functions is not ar- bitrary. Common loss functions arise as the negative log-likelihood of natural probabilistic models and yield Bayes-optimal estimators under common noise assump- tions (see Table II). • For regression problems, MSE is justified by Gaus- sian MLE and Bayes-optimality for the mean; whereas MAE is the result of Laplace MLE and Bayes-optimality for the median. • For binary and multi-class classification, cross- entropy is the negative log-likelihood of Bernoulli and multinomial models, respectively, and is a strictly proper scoring rule, uniquely minimized by the true conditional probabilities. Properness en- sures calibrated probability estimation. With respect to their robustness: • MSE is optimal under homoscedastic Gaussian noise and targets the mean. It is sensitive to out- liers because Gaussian tails penalize large devia- tions quadratically. • MAE is optimal under Laplace noise and targets the median. Heavier tails yield linear penalties and robustness to outliers. • Cross-entropy arises from discrete likelihoods. Its strict convexity in predicted probabilities penal- izes overconfident misclassifications and aligns with maximum likelihood estimation for categorical out- comes. The loss function used in training deep learning models is the negative log-likelihood of the chosen distribution. That is the reason why the “right” loss function depends on the prediction task. B. Generalized Linear Models The output layer of many deep neural networks can be seen as a generalized linear model (GLM), where the activation function of the output neurons corresponds to the inverse of a canonical link function and the error (or loss) function is derived from the probability distribution assumed by the GLM. This connection provides a statis- tical foundation for the design of neural network output layers for various machine learning tasks, since the loss functions discussed (MSE, MAE, cross-entropy) are the negative log-likelihoods of GLMs under different assump- tions about the distribution of the response variable. Generalized linear models (GLMs) extend the princi- ples of linear regression to response variables that are not normally distributed. A GLM is composed of three',\n",
       "  'since the loss functions discussed (MSE, MAE, cross-entropy) are the negative log-likelihoods of GLMs under different assump- tions about the distribution of the response variable. Generalized linear models (GLMs) extend the princi- ples of linear regression to response variables that are not normally distributed. A GLM is composed of three key elements: • A random component: The probability distribution of the response variable Y , taken from the expo- nential family, a broad class of distributions that includes the Gaussian, Bernoulli, binomial, multi- nomial, exponential, gamma, beta, chi-squared, ge- ometric, Dirichlet, Whishart, and Poisson distribu- tions. • A systematic component: A linear predictor, η = Xβ, which is a linear combination of the input fea- tures. The model parameters β would be the out- put layer weights w in a deep learning context. • A link function: A function g(·) that connects the expected value of the random component (e.g. the mean of Y ) to the systematic component, i.e. the linear predictor (e.g. g(µ) = η). A distribution is in the exponential family if its proba- bility mass/density function can be written in the form: p(y|η) = h(y)eηT (y)−A(η) 8 Problem type Output activation Loss function Distribution MLE estimate Regression Linear MSE Gaussian Mean Regression Linear MAE Laplace Median Regression Linear log Pareto Mode Classification: Binary Logistic sigmoid BCE Bernoulli Conditional probability Classification: Binary (bipolar) Hyperbolic tangent BCE Bernoulli Conditional probability Classification: Multi-class Softmax CCE Multinomial Class probability distribution Table II Common loss functions and their MLE justification. where y is the outcome variable, η (eta) is the natural pa- rameter of the distribution, T(y) is the sufficient statistic (often, just y), h(y) > 0 is the base measure, and A(η) is the log-partition of the particular probability distribu- tion. Canonical link functions are a specific type of link func- tion that have desirable mathematical properties, often leading to more stable and efficient model fitting. Each probability distribution in the exponential family has a unique canonical link function. In a generalized linear model (GLM), the canonical link function, g(.), is the specific function that maps the expected value of the out- come, µ = E[Y ], to the natural/canonical parameter, η, i.e. η = g(µ). When η(θ) = θ, the exponential fam- ily is said to be in canonical form. In the special case that η(θ) = θ and T(y) = y, the distribution is in the natural exponential family (NEF), a special case of the exponential family. This choice is “canonical” or natural because it directly links the model linear predictor to the distribution nat- ural parameter. Given the canonical link function, an alternative, equivalent form for members of the exponen- tial family of probability distributions is p(y|η, s) = 1 sh \\x10y s \\x11 g(η)e ηy s taking into account that, when f(x) is a normalized den- sity function, 1 sf \\x00 y s \\x01 is also a normalized density func- tion. Therefore, the function g(η) can be interpreted as the coefficient that ensures that the distribution is normal- ized: g(η) s.t. Z p(y|η, s)dy = 1 which can',\n",
       "  'account that, when f(x) is a normalized den- sity function, 1 sf \\x00 y s \\x01 is also a normalized density func- tion. Therefore, the function g(η) can be interpreted as the coefficient that ensures that the distribution is normal- ized: g(η) s.t. Z p(y|η, s)dy = 1 which can be used to compute the conditional expecta- tion of y, E[y] E[y|η, s] = Z p(y|η, s)ydy by computing the gradient of the above equality with respect to the natural parameter η: ∇η Z p(y|η, s)dy = ∇η1 = 0 ∇ηg(η) Z 1 sh \\x10y s \\x11 g(η)e ηy s dy = 0 ∇ηg(η) Z 1 sh \\x10y s \\x11 e ηy s dy + g(η) Z 1 sh \\x10y s \\x11 e ηy s \\x10y s \\x11 dy = 0 ∇ηg(η) g(η) Z 1 sh \\x10y s \\x11 g(η)e ηy s dy + 1 s Z 1 sh \\x10y s \\x11 g(η)e ηy s ydy = 0 ∇ηg(η) g(η) Z p(y|η, s)dy + 1 s Z p(y|η, s)ydy = 0 ∇ηg(η) g(η) · 1 + 1 sE[y|η, s] = 0 so E[y|η, s] = −s∇ηg(η) g(η) = −s d dη log g(η) The log likelihood for this model, in terms of η, is log p(y|η) = n X i=1 log \\x141 sh \\x10yi s \\x11 g(ηi)e ηiyi s \\x15 log p(y|η) = n X i=1 h −log s + log h \\x10yi s \\x11 + log g(ηi) + ηiyi s i Assuming that all examples share a common scale pa- rameter, s, and h(yi/s) are independent of i and the log h(yi/s) term can be considered constant. The deriva- tive of the log likelihood with respect to the model pa- rameters is then ∇w log p(y|η) = n X i=1 \\x14 d dηi log g(ηi) + yi s \\x15 dηi dyi dyi dzi ∇wzi 9 where zi = wixi. Given our expression for the conditional mean of y d dηi log g(ηi) = −1 sE[yi|η] = −ˆyi s Therefore ∇w log p(y|η) = n X i=1 1 s [yi −ˆyi] ψ′( ˆyi)f ′(zi)xi where ˆyi = f(zi), f ′(zi) = dyi dzi , ψ′( ˆyi) = dηi dyi , and xi = ∇wzi = ∇wwixi. That expression can be simplified if we choose a par- ticular link function f−1(ˆy) = ψ(ˆy). In other words, the activation function used in the output layer of a neu- ral network is the inverse of the canonical link function, g−1(η). Given that z = f −1(ˆy), z = ψ(ˆy). Since f ′(ψ)ψ′(ˆy) = 1, f ′(z)ψ′(ˆy) = 1. Therefore, the gradient of the log likelihood reduces to ∇w log p(y|η) = 1 s n X i=1 [yi −ˆyi] xi It comes as no surprise that the derivative of the log likelihood with respect to the parameter vector, used by gradient-descent algorithms to train neural networks, al- ways results in contributions to the error function of the form [yi −ˆyi] xi when the natural pairs of activation and loss functions are chosen (linear/MSE, sigmoid/BCE, softmax/CCE). The formal connection between the loss functions derived from maximum likelihood',\n",
       "  'parameter vector, used by gradient-descent algorithms to train neural networks, al- ways results in contributions to the error function of the form [yi −ˆyi] xi when the natural pairs of activation and loss functions are chosen (linear/MSE, sigmoid/BCE, softmax/CCE). The formal connection between the loss functions derived from maximum likelihood estimation and canonical link functions lies in the exponential fam- ily of distributions. 1. Regression: Gaussian GLM In a regression problem, the goal is to predict a con- tinuous value. Linear regression is a GLM with Gaussian errors and the identity link. MSE is its natural loss. From a GLM perspective: • Random component: The response variable is as- sumed to follow a Gaussian (normal) distribution, Y ∼N(µ, σ2). • Canonical link function: For a Gaussian distribu- tion, the expected value is its mean, so the natural parameter is η = E[Y ] = µ. The canonical link function connects the expected value to the nat- ural parameter, so, for the Gaussian distribution, the canonical link function is the function g(µ) = η. Since both η and µ are the same, the canonical link function is the identity function g(µ) = µ. This means the expected value of the response is directly modeled by the linear predictor: µ = Xβ. As shown previously, assuming a Gaussian distribu- tion for the data leads directly to the MSE loss func- tion via maximum likelihood estimation: minimizing the MSE is mathematically equivalent to maximizing the log- likelihood of the data under a Gaussian distribution. The canonical link for the Gaussian distribution is the identity function. The inverse link, g−1(η), which dic- tates the neural network output layer activation, is also the identity function (i.e., a linear layer). This formally connects the linear output layer and MSE loss function to the canonical link of the underlying Gaussian probability assumption. In summary, a neural network with a linear output layer trained with MSE is performing a form of non-linear regression, where the hidden layers learn a complex trans- formation of the input features, and the output layer acts as a simple linear regression on these learned features. 2. Regression: Laplace GLM Linear regression in terms of MAE is a GLM with Laplace errors and the identity link. The GLM goal is minimizing the average absolute difference between pre- dicted and actual values rather than the more common mean squared error: • Random component: The response variable is assumed to follow a Laplace distribution, Y ∼ Laplace(µ, b), with mean µ and scale parameter b. • Canonical link function: For a Laplace distribu- tion, the expected value is its mean, so its natural parameter is η = E[Y ] = µ. For the Laplace dis- tribution, the canonical link function is the func- tion g(µ) = η. Since both η and µ are the same, the canonical link function is the identity function g(µ) = µ. This means the expected value of the re- sponse is directly modeled by the linear predictor: µ = Xβ. Assuming a Laplace distribution for the data leads directly to the MAE loss',\n",
       "  'both η and µ are the same, the canonical link function is the identity function g(µ) = µ. This means the expected value of the re- sponse is directly modeled by the linear predictor: µ = Xβ. Assuming a Laplace distribution for the data leads directly to the MAE loss function via maximum likeli- hood estimation: minimizing the MAE is mathematically equivalent to maximizing the log-likelihood of the data under a Laplace distribution. As in the Gaussian case, since the canonical link is the identity function, the inverse link, g−1(η), which dictates the neural network output layer activation, is also the identity function (i.e., a linear layer). This formally con- nects the linear output layer and MAE loss function to the canonical link of the underlying Laplace probability assumption. 3. Binary Classification: Bernoulli GLM For binary classification, the objective is to predict one of two possible classes. Logistic regression is a GLM 10 with Bernoulli distribution and logit link. Binary cross- entropy is its natural loss. From a GLM perspective: • Random component: The response variable is as- sumed to follow a Bernoulli distribution, Y ∼ Bernoulli(p). The expected value of a Bernoulli variable is its probability of success, so µ = E[Y ] = p. • Canonical link function: The canonical link func- tion for the Bernoulli distribution is the logit func- tion: η = logit(p) = log \\x10 p 1−p \\x11 , where p is the prob- ability of success (i.e., the probability of y = 1). The logit transforms probabilities into log odds, given that an event odds is just the ratio of the probability of the event occurring to the probabil- ity of not occurring, p/(1 −p). As shown in the previous Section, assuming a Bernoulli distribution leads directly to the binary cross-entropy loss function (also known as log loss): Minimizing the binary cross-entropy is equivalent to maximizing the log- likelihood of the data under a Bernoulli distribution. The inverse link, g−1(η), gives us the required activa- tion function: p = logit−1(η) = 1 1 + e−η The inverse of the logit function is the logistic (or stan- dard sigmoid) function, which maps the linear predic- tor to a probability between 0 and 1. This formally connects the sigmoid activation function and the binary cross-entropy loss to the canonical link of the Bernoulli distribution. A neural network with a single sigmoid output unit trained with binary cross-entropy loss is essentially learn- ing a logistic regression model on a transformed feature space created by its hidden layers. 4. Multi-Class Classification: Multinomial GLM In multi-class classification, the task is to predict one of several mutually exclusive classes. Multinomial logistic regression, also known as softmax regression, is a GLM with multinomial distribution and generalized logit link. Categorical cross-entropy is its natural loss. • Random component: The response variable is as- sumed to follow a multinomial/categorical distri- bution, Y ∼Multinomial(1, p). Its expected value is the vector of class probabilities, µ = E[Y ] = [p1, . . . , pK]. • Canonical link function: The canonical link con- nects the vector',\n",
       "  'loss. • Random component: The response variable is as- sumed to follow a multinomial/categorical distri- bution, Y ∼Multinomial(1, p). Its expected value is the vector of class probabilities, µ = E[Y ] = [p1, . . . , pK]. • Canonical link function: The canonical link con- nects the vector of probabilities to the natural pa- rameters. For K classes, there are K −1 natural parameters, ηk = log \\x10 pk pK \\x11 . This is the generalized logit function. As shown before, assuming a categorical/multinomial distribution leads directly to the categorical cross- entropy loss function: Minimizing the categorical cross- entropy loss is equivalent to maximizing the log- likelihood of the data under a categorical/multinomial distribution. The inverse of the generalized logit link function, g−1(η), which maps the linear outputs back to proba- bilities, is the softmax function, which takes a vector of arbitrary real numbers and transforms them into a prob- ability distribution over the problem classes: pk = eηk PK j=1 eηj A neural network with a softmax output layer trained with categorical cross-entropy loss is effectively a multi- nomial logistic regression classifier (also known as soft- max classifier) that operates on the high-level features extracted by the preceding layers of the network. 5. Summary The choice of activation function (linear, sigmoid, soft- max) is the inverse of the canonical link function for the assumed data distribution, and the choice of loss func- tion (MSE, cross-entropy) is the negative log-likelihood of that same distribution. This theoretical foundation provides a cohesive, statistically-grounded framework for designing neural network output layers. IV. ADDITIONAL SITUATIONS A. Binary Classification with Bipolar Encoding The loss function for binary classification problems was obtained for logistic activation functions, where the logis- tic output estimates the probability of the example be- longing to the positive class, p(y = 1 | x). Training data was then coded so that y = 1 indicated that the example belonged to the positive class, whereas y = 0 indicated that that example belonged to the negative class. However, a bipolar encoding of output classes is also common in practice: y = +1 for the positive class, y = −1 for the negative class. In that case, we are just scaling and shifting our binary output: ybipolar = 2y −1 so that y = ybipolar + 1 2 The bipolar loss function can be derived from the bi- nary cross-entropy log likelihood: log L = n X i=1 [yi log(ˆyi) + (1 −yi) log(1 −ˆyi)] 11 Problem type GLM GLM distribution Output activation Link function Loss function Regression Linear regression Gaussian Linear Identity MSE Regression Linear regression Laplace Linear Identity MAE Binary classification Logistic regression Bernoulli Logistic sigmoid Logit BCE Binary classification (bipolar) Logistic regression Bernoulli Hyperbolic tangent Logit BCE Multi-class classification Softmax regression Multinomial Softmax Generalized logit CCE Table III Common loss functions for different problem types and their GLM justification. just by applying the above transformation both to the target variable yi and the model estimation ˆyi. log L = n X i=1 yi + 1 2 log \\x12 ˆyi + 1',\n",
       "  'Multinomial Softmax Generalized logit CCE Table III Common loss functions for different problem types and their GLM justification. just by applying the above transformation both to the target variable yi and the model estimation ˆyi. log L = n X i=1 yi + 1 2 log \\x12 ˆyi + 1 2 \\x13 + \\x12 1 −yi + 1 2 \\x13 log \\x12 1 −ˆyi + 1 2 \\x13 log L = 1 2 n X i=1 (yi + 1) log (ˆyi + 1) + (1 −yi) log (1 −ˆy) −n log 2 The last term n log 2 does not affect the log likelihood maximization, since it is independent of the model pa- rameters, so it can be dropped. The activation function that matches the above BCE- like minimization problem can be obtained by applying the linear transformation to the logistic function, which is the activation function of choice for binary classification problems: y(z) = 2σ(z) −1 = 2 1 + e−z −1 = 1 −e−z 1 + e−z = ez/2 ez/2 1 −e−z 1 + e−z = ez/2 −e−z/2 ez/2 + e−z/2 = tanh(z/2) Therefore, the hyperbolic tangent activation function is the suitable activation function to be used for solv- ing binary classification problems using bipolar encoding when we want to minimize the binary cross-entropy (i.e. the negative log-likelihood of the data under the assump- tion of a Bernoulli distribution). B. Regression of Positive Values In many real-world applications, regression models should always return a positive value for predicting prices, counts, durations, or intensities (e.g. waiting times, insurance claims, rainfall amounts, energy con- sumption...). The choice of activation/loss pair depends on the distribution of the target variable and the con- straints you want to enforce. Let us consider several al- ternatives: • The quick and dirty solution is resorting to con- ventional regression loss functions. Since they do not enforce positivity, you can constraint the out- put using an activation function that ensures that the network output is in the correct domain. ReLU and softplus ensure non-negative values. ReLU can be seen as a piecewise linear approxima- tion to the log link role (the canonical link for a Poisson likelihood). ReLU enforces non-negativity, but grows linearly instead of exponentially, which might prevent numerical problems. Even though ReLU is not the exact MLE link, it is a computa- tionally efficient surrogate that enforces the posi- tivity constraint. The softplus activation function also ensures pos- itivity. Softplus is a smooth approximation to ReLU. Softplus behaves like ez for large negative z (small positive outputs) and like z for large pos- itive z (linear growth, like ReLU). It also has a stronger tie to likelihood theory. For distributions requiring strictly positive parameters (e.g. variance in Gaussian, scale in Gamma, rate in Poisson), the canonical MLE link is the log link (θ = ez). Soft- plus can be loosely interpreted as a numerically sta- ble approximation to the exponential link used in MLE, avoiding overflow problems while still enforc- ing positivity. • A popular strategy is training the network to pre- dict log ˆy instead',\n",
       "  'MLE link is the log link (θ = ez). Soft- plus can be loosely interpreted as a numerically sta- ble approximation to the exponential link used in MLE, avoiding overflow problems while still enforc- ing positivity. • A popular strategy is training the network to pre- dict log ˆy instead of ˆy. You can use the MSE loss function on log y vs. log ˆy. The log transformation naturally enforces positivity: the final output is eˆν, where ν is the model output. It also makes the model target multiplicative errors, since y = f(x)·ϵ becomes log y = log f(x) + ϵ. In practice, therefore, a model is trained to predict ˆν = log ˆy and the final prediction is ˆy = eˆν. Optionally, you can perform bias correction: If er- rors on log scale are approximately Gaussian with 12 variance σ2, an unbiased mean estimate is ˆµ = eˆν+σ2/2, where the variance σ2 itself can be esti- mated from residuals. Since the output of the model is transformed us- ing an exponential function, it does not need to be constrained and a simple linear output layer can be used with the logarithmic transformation. • The Gamma distribution is a distribution of the exponential family set of probability distributions that generalizes the exponential, Erlang, and chi- squared distributions. p(x) = λα Γ(α)xα−1e−λx where α is a shape parameter and λ = 1/θ is a rate parameter (an equivalent parameterization uses the α shape and θ scale parameters). Its mean is µ = α/λ = αθ and its variance is σ2 = α/λ2 = αθ2. The log likelihood of the Gamma distribution (for a single example) is log p(x) = α log λ −log Γ(α) + (α −1) log x −λx Minimizing the negative log likelihood of the Gamma distribution, a model can learn to output the two positive parameters of the Gamma distribu- tion using softplus activation functions (to ensure positivity). For stability, a small constant ε ≈10−3 is added to the softplus output (to avoid zero pa- rameters). The model can then be used to predict values (using the mean of the distribution µ = α/λ) or sample from the distribution to provide uncer- tainty estimates. We could directly predict the mean ˆy = ˆµ by repa- rameterizing α = λµ: p(x) = xλµ−1λλµe−λx Γ(λµ) and maximizing the following log likelihood: log L = n X i=1 (λ ˆyi log λ + (λ ˆyi −1) log yi −λyi −log Γ(λ ˆyi)) = n X i=1 (λ ˆyi(log λ + log yi) −log yi −λyi −log Γ(λ ˆyi)) A simpler approach for mean-only Gamma regres- sion resorts to the deviance3 of the Gamma distri- 3 Deviance is a measure of how well a proposed statistical model fits the data compared to a ”perfect” model. This perfect model, known as the saturated model, has one parameter for every data point, meaning it fits the data perfectly but has no predictive power. Deviance is derived from the likelihood ratio test, since ratio of the likelihoods of the two models gives us an idea of',\n",
       "  '”perfect” model. This perfect model, known as the saturated model, has one parameter for every data point, meaning it fits the data perfectly but has no predictive power. Deviance is derived from the likelihood ratio test, since ratio of the likelihoods of the two models gives us an idea of how much worse our proposed model is compared to the perfect one: Λ = Lp/Ls. Deviance is formally defined as -2 times the log-likelihood ratio: D = −2(Lp −Ls). bution as loss function: LΓ = 2 \\x12y −ˆy ˆy −log y ˆy \\x13 Unlike MSE and MAE, which penalizes absolute differences, Gamma deviance focuses on relative er- rors. It is invariant to scaling of the target vari- able. Its first term is the percent error relative to the prediction and the logarithmic term makes the loss function asymmetric (penalizes underpre- dictions much more heavily than overpredictions). As a result, the loss grows very quickly as your pre- diction approaches zero for a positive true value. For the output layer, we can enforce a positive out- put using a positive activation function, such as softplus or exp (ez), adding a small positive value ε ≈10−6 to avoid zero outputs. The Gamma GLM canonical link is the inverse link (η = 1/µ). However, divisions by zero are prob- lematic in computers, so a log link is more stable. In practice, you predict ν = log µ) and then output µ = eν. • If your prediction target is a count (0, 1, 2...), i.e. the number of events, you can resort to a Poisson distribution: p(k) = λke−λ k! where the positive real number λ ≥0 is equal to the expected value of the distribution (and also its variance). log p(k) = k log λ −λ −log k! In Poisson regression, the dependent variable y is an observed count that follows the Poisson distribu- tion whose rate λ is determined as λ = ew⊤x = eˆy using a linear activation function. Therefore, the log likelihood of our model is log L = n X i=1 \\x00yi ˆyi −e ˆ yi −log yi! \\x01 An alternative loss function can be derived from the Poisson distribution calculating the deviance between the true count (y) and the predicted rate (ˆy): LP oisson = 2 (ˆy −y log ˆy) Since the Poisson loss function includes a log ˆy term, undefined for non-positive numbers, the model output must always be positive. An expo- nential is the canonical choice (the inverse link in the underlying GLM model). The softplus smooth approximation of the ReLU function might also be used. It should also be noted that the target vari- able should not be scaled nor normalized, since it must represent raw counts. 13 • Tweedie distributions (23) unify Gaussian (p = 0), Poisson (p = 1), Gamma (p = 2), compound Pois- son–Gamma, and inverse Gaussian (p = 3) distri- butions. For positive continuous data with mass near zero, use p ∈(1, 2], where p = 2 corresponds to pure Gamma-like behavior. The loss per exam- ple is also derived from',\n",
       "  '0), Poisson (p = 1), Gamma (p = 2), compound Pois- son–Gamma, and inverse Gaussian (p = 3) distri- butions. For positive continuous data with mass near zero, use p ∈(1, 2], where p = 2 corresponds to pure Gamma-like behavior. The loss per exam- ple is also derived from the distribution deviance: LT weedie = 2 1 −p \\x12 yˆy1−p −y2−p 2 −p \\x13 where the parameter p can be tuned to match variance-mean relationship. Gamma deviance aligns with variance scaling like σ2 ∝µ2, log-MSE aligns with constant variance on the log scale, and Tweedie covers intermediate power laws (σ2 ∝µp). As activation function for the output layer, you could use either softplus (softplus(z)+ε), exp (ez), or even a square (z2 + ε). Softplus is stable and smooth, exp strongly enforces positivity but can explode, and a square is less common because its gradients vanish near zero. However, a log link is usually recommended: just predict ν = log µ) using a linear output layer and then output µ = eν. MSE or MAE can be used in conjunction with ReLU or softplus activation functions when your data is roughly symmetric around the mean after a log transform. The logarithmic transformation handles skewed distributions (common in positive-only data) and can be used with log-normal-like heteroscedasticity, multiplicative noise, or many orders of magnitude in the output. The Gamma or Tweedie loss functions are statistically sound and suit- able for skewed positive values. The Poisson loss can be used for predicting counts. The log link matches the multiplicative noise and skew of positive-value out- comes and should be the default choice for implementing Gamma/Poisson/Tweedie regression. C. Fat Tails The tails of the exponential family of probability distri- butions, which includes the Gaussian, Laplace, and Pois- son distributions, are characterized by their tendency to decay exponentially, i.e. faster than polynomially, mak- ing them light-tailed distributions. Distributions that are not in the exponential family often exhibit heavy tails, where the decay is slower than exponential, such as power-law decay. These distributions, like the Stu- dent’s t-distribution, Cauchy, and Pareto distributions, assign a much higher probability to extreme values. The Gaussian assumption, which justifies the use of MSE, penalizes large errors more heavily because of the square error term and leads to models model that are especially sensitive to outliers or large deviations. The Laplace assumption lead to the use of MAE and is more robust to outliers, a better choice when you care about median-like behavior rather than mean-like behavior. The use of fat-tailed distributions can make models more robust to the presence of outliers. The exponential family works well when data is well- behaved (light-tailed, symmetric), outliers are rare and not influential, and our main concern the average-case er- ror in classification/regression. Fat-tailed distributions, with power-law instead of exponentially-decaying tails, are the natural choice when residuals/errors are heavy- tailed, extreme deviations matter and should not be washed out by excessive penalties, and robustness to out- liers is sought without discarding tail information. For example, let us assume a double symmetric Pareto distribution, with a',\n",
       "  'with power-law instead of exponentially-decaying tails, are the natural choice when residuals/errors are heavy- tailed, extreme deviations matter and should not be washed out by excessive penalties, and robustness to out- liers is sought without discarding tail information. For example, let us assume a double symmetric Pareto distribution, with a sharp probability peak at x = 0 and symmetric long probability tails derived from the Pareto distribution: p(x) = α 2 1 (1 + |x|)α+1 where α is a shape parameter, known as the tail in- dex, that controls the heaviness of the distribution tails. The double Pareto distribution exhibits power-law tails, a defining feature of scale-invariant properties (observed in many natural and economic phenomena). In a regression problem, the target variable y is gen- erated from our model prediction ˆy = f(x) plus some fat-tailed (double Pareto) noise, ϵ: y = f(x) + ϵ where ϵ ∼DoublePareto(α) The noise ϵ is assumed to have a mean of 0 (the center of our symmetric double Pareto distribution). This is equivalent to saying the likelihood of observing y given ˆy is: p(y|ˆy) = α 2 1 (1 + |y −ˆy|)α+1 Assuming a flat prior over ˆy, the Bayesian-optimal es- timator is the maximum a posteriori (MAP) estimate:4 ˆyMAP = arg max ˆy p(y|ˆy) Taking logarithms: log p(y|ˆy) = −(α + 1) log(1 + |y −ˆy|) + log (α/2) Assuming that α can be considered constant, the MAP estimate minimizes the logarithmic loss log L = log(1 + |y −ˆy|) 4 MAP (maximum a posteriori) estimation and MLE (maximum likelihood estimation) are both statistical methods to find the most likely parameter values, but MAP includes prior knowl- edge, while MLE considers only the observed data. The MAP estimate is the mode of the posterior distribution. MAP finds the maximum of the posterior probability by combining the like- lihood function with a prior distribution, while MLE finds the maximum of the likelihood function alone. MAP is a Bayesian approach, while MLE is a frequentist approach. MLE is a special case of MAP when the prior is uniform. 14 The double Pareto loss behaves like MSE near zero, like MAE for moderate errors, and becomes even more robust in the tails due to its logarithmic growth, which resembles cross-entropy. Comparing its gradient with respect to the model output (against MSE and MAE) shows how double Pareto naturally down-weights outliers: • MSE: Linear gradient (large for big errors). d dˆy LMSE = 2(ˆy −y) • MAE: Constant gradient (more robust). d dˆy LMAE = sign(ˆy −y) • Log: Decaying gradient (robust and smooth). d dˆy Llog = sign(ˆy −y) 1 + |ˆy −y| The double Pareto likelihood is symmetric and uni- modal. Under a flat prior, the MAP estimate coincides with the mode of the likelihood. This mirrors the Gaus- sian case, where the conditional mean is optimal under MSE, and the Laplace case, where the conditional me- dian is optimal under MAE, but here, the conditional mode is optimal under double Pareto noise.5 Let Y be a random variable with conditional distribu- tion given X =',\n",
       "  'mirrors the Gaus- sian case, where the conditional mean is optimal under MSE, and the Laplace case, where the conditional me- dian is optimal under MAE, but here, the conditional mode is optimal under double Pareto noise.5 Let Y be a random variable with conditional distribu- tion given X = x. For any predictor ˆy ∈R, consider the conditional risk, i.e. the expected loss for a specific decision, given a particular observation: Rlog(ˆy) = E[log(1 + |Y −ˆy|)|X = x] The Bayes estimator, or Bayes decision rule, minimizes the conditional risk for every possible observation: d dˆy Rlog(ˆy) = sign(E[Y |x] −ˆy) 1 + |E[Y |x] −ˆy| Setting the derivative to zero, we obtain that the condi- tional expectation the best possible prediction: sign(E[Y |x] −ˆy) 1 + |E[Y |x] −ˆy| = 0 sign(E[Y |x] −ˆy) = 0 5 In a symmetric unimodal double Pareto distribution, the mode is the point of maximum density. The median, for a symmet- ric distribution centered at 0, is also 0. In asymmetric double Pareto distribution, with tails decaying at different rates, the mode, median, and mean are all distinct. The mean exists only when the tail index α > 1 but is undefined when α ≤1 because the integral diverges due to heavy tails. When extreme values dominate, the mean becomes unstable, and the MAP (mode) or median are preferred estimators. E[Y |x] −ˆy |E[Y |x] −ˆy| = 0 ˆy∗= E[Y |x]. Logarithmic risk is minimized by the expectation and learning with the logarithmic loss function estimates E[Y |X]. The Gaussian distribution underestimates tail risk and the mean is sensitive to outliers. The Laplace distribu- tion captures moderate tails and the median is somewhat more robust to outliers. The double Pareto distribution explicitly models fat tails and is extremely robust to out- liers, since the mode ignores tail mass. Let us now model ˆy = f(x) to find the optimal form of f. If we use a linear output layer f(x) = w⊤x = ⃗w · ⃗x, is this form optimal under the double Pareto likelihood? The double Pareto loss is convex in w⊤x. The true con- ditional expectation E[y|x] is linear in x (E[y|x] = ˆy = w⊤x) and, therefore, the Bayesian-optimal estimator un- der symmetric noise is also linear. A linear output layer is the optimal solution because the symmetry and convexity of the double Pareto likelihood preserve the optimality of linear predictors. Since we are exploring loss–activation pairs, we can sketch how a double Pareto-inspired loss could also be paired with a non-saturating activation function, given that premature saturation would hide tail information. Potential candidates, for classification settings, would in- clude: • Softsign (similar to a sigmoid, but with polynomial tails consistent with Pareto-like decay): f(z) = 1 1 + |z| • Arctan (bounded, but with slower saturation than a sigmoid): f(z) = arctan(z) Such pairs would be robust alternatives for heavy- tailed distributions: the loss functions discounts extreme deviations without ignoring them, whereas the activa- tion allows gradients to flow even for large latent values (avoiding sigmoid vanishing gradient in the',\n",
       "  'Arctan (bounded, but with slower saturation than a sigmoid): f(z) = arctan(z) Such pairs would be robust alternatives for heavy- tailed distributions: the loss functions discounts extreme deviations without ignoring them, whereas the activa- tion allows gradients to flow even for large latent values (avoiding sigmoid vanishing gradient in the tails). Fat tails beat exponential ones in situations with heavy-tailed leptokurtic distributions, which include fi- nancial and risk modeling, robust regression with out- liers (e.g. spikes due to sensor faults), extreme event predictions (e.g. insurance claims, natural disasters, or network traffic spikes), and privacy-preserving or adver- sarial robust learning (i.e. when adversaries inject large perturbations). 15 V. CONCLUSION Error (or loss) functions used for training deep learn- ing models measure the discrepancy between the model’s predictions and the true values. The choice of an ap- propriate error function is crucial for training a neural network effectively. From a statistical point of view, the “right” loss func- tion is simply the negative log-likelihood of the GLM cor- responding to the assumed distribution of the response. That is the reason why MSE, MAE, BCE, and CCE are not just convenient, they are statistically principled. The same idea of negative log-likelihood minimization can be used to obtain a logarithmic loss function for regression under fat-tailed distributions, which should be the loss of choice for extreme value regression. In classification problems, cross-entropy losses are the negative log-likelihoods of exponential family distribu- tions (Bernoulli, multinomial) and are strictly proper scoring rules, uniquely minimized by the true probabili- ties. The activation function in the network output layer ensures that the network outputs lie in the correct do- main (e.g. real values, probabilities, positive rates). The most suitable activation function can be justified as the inverse canonical link function of a GLM-like likelihood model (for exponential distributions) or the Bayesian- optimal estimator under the assumed noise distribution (for fat-tailed distributions). Appendix A: Activation functions 1. Logistic function The logistic or standard sigmoid function is defined as σ(x) = 1 1 + e−x The derivative of the logistic function has a remarkably simple and useful form: σ′(x) = d dxσ(x) = σ(x) · (1 −σ(x)) This form is highly efficient for computation, especially in neural networks where the output σ(x) is often already calculated during the forward pass. The derivative is a bell-shaped curve with a maximum value of 0.25 at x = 0, where σ(0) = 0.5 (σ(x)(1 −σ(x)) is maximized when σ(x) = 1 −σ(x), or σ(x) = 0.5). As the input |x| gets very large (positive or negative), σ(x) approaches 1 or 0, making the derivative σ(x)(1 −σ(x)) approach 0, which contributes to the vanishing gradient problem. Step-by-step derivation: • Start with the logistic function: σ(x) = 1 1 + e−x = (1 + e−x)−1 • Apply the chain rule: u = 1 + e−x, so σ(x) = u−1. d dxσ(x) = d du(u−1) · d dx(1 + e−x) • Calculate the derivatives: d du(u−1) = −1 · u−2 = −u−2 d dx(1 + e−x) = 0 + d dx(e−x) = e−x · (−1) = −e−x •',\n",
       "  'the chain rule: u = 1 + e−x, so σ(x) = u−1. d dxσ(x) = d du(u−1) · d dx(1 + e−x) • Calculate the derivatives: d du(u−1) = −1 · u−2 = −u−2 d dx(1 + e−x) = 0 + d dx(e−x) = e−x · (−1) = −e−x • Substitute and simplify: d dxσ(x) = (−u−2) · (−e−x) = e−x u2 Substitute u = 1 + e−x back into the equation: σ′(x) = e−x (1 + e−x)2 • Express in terms of σ(x): σ′(x) = e−x 1 + e−x · 1 1 + e−x e−x 1 + e−x = 1 + e−x −1 1 + e−x = 1 + e−x 1 + e−x − 1 1 + e−x = 1 −σ(x) σ′(x) = (1 −σ(x)) · σ(x) 2. Hyperbolic tangent The hyperbolic tangent function is defined as tanh(x) = ex −e−x ex + e−x We can relate the hyperbolic tangent to the logistic function as follows: tanh(x) = ex −e−x ex + e−x = ex ex 1 −e−2x 1 + e−2x = 2 −1 −e−2x 1 + e−2x = 2 1 + e−2x −1 + e−2x 1 + e−2x = 2 1 + e−2x −1 = 2σ(2x) −1 16 From the above relationship, we could also express the logistic function in terms of the hyperbolic tangent: σ(x) = 1 2 \\x10 tanh \\x10x 2 \\x11 + 1 \\x11 The derivative of the hyperbolic tangent function, tanh(x), is the hyperbolic secant squared function, sech2(x): d dx tanh(x) = sech2(x) The derivative can be found by first expressing tanh(x) in terms of sinh(x) and cosh(x), and then applying the quotient rule and the fundamental hyperbolic identity: • Definition of the hyperbolic tangent and the deriva- tives of the hyperbolic sine and cosine functions: tanh(x) = sinh(x) cosh(x) d dx sinh(x) = cosh(x) d dx cosh(x) = sinh(x) • Applying the quotient rule, d dx \\x00 u v \\x01 = u′v−uv′ v2 , where u = sinh(x) and v = cosh(x): d dx tanh(x) = d dx \\x12 sinh(x) cosh(x) \\x13 d dx tanh(x) = (cosh(x))(cosh(x)) −(sinh(x))(sinh(x)) (cosh(x))2 d dx tanh(x) = cosh2(x) −sinh2(x) cosh2(x) • Given the fundamental hyperbolic identity, cosh2(x) −sinh2(x) = 1: d dx tanh(x) = 1 cosh2(x) = sech2(x) since the hyperbolic secant function is defined as sech(x) = 1 cosh(x). The derivative of tanh(x) can be also be expressed in terms of tanh(x) itself using the fundamental hyper- bolic identity, the hyperbolic Pythagorean identity relat- ing cosh(x) and sinh(x): cosh2(x) −sinh2(x) = 1 Dividing every term in this identity by cosh2(x), you get the desired relationship: cosh2(x) cosh2(x) −sinh2(x) cosh2(x) = 1 cosh2(x) 1 − \\x12 sinh(x) cosh(x) \\x132 = \\x12 1 cosh(x) \\x132 1 −tanh2(x) = sech2(x) Since d dx(tanh(x)) = sech2(x), we can substitute the expression above: d dx tanh(x) = 1 −tanh2(x) 3. Softmax function The softmax function, often used as an activation func- tion in the output layer of a neural network for multi-class classification problems, is defined as: yi = softmaxi(⃗x) = exi PK k=1 exk where yi is the i-th element of the',\n",
       "  'd dx tanh(x) = 1 −tanh2(x) 3. Softmax function The softmax function, often used as an activation func- tion in the output layer of a neural network for multi-class classification problems, is defined as: yi = softmaxi(⃗x) = exi PK k=1 exk where yi is the i-th element of the output vector ⃗y, xi is the i-th element of the weighted input vector ⃗x, and the denominator is the sum over all K classes. The derivative of the softmax function is most easily expressed in terms of the Kronecker delta (δij), and it results in a matrix of values (a Jacobian matrix) because softmax is a vector-valued function. The derivative of the i-th component of the softmax output, yi, with respect to the j-th component of the input, xj, is ∂yi ∂xj = yi(δij −yj) where yi and yj are the outputs of the softmax function, whereas δij is the Kronecker delta (1 if i = j and 0 if i ̸= j). In the full derivation of the softmax derivative, we must consider two separate cases: when i = j and when i ̸= j. • The derivative ∂yi ∂xi with respect to the same index (i = j): ∂ ∂xi (exi) = exi ∂ ∂xi N X k=1 exk ! = exi (only the k = i term in the sum depends on xi) 17 ∂yi ∂xi = (exi)(PN k=1 exk) −(exi)(exi) (PN k=1 exk)2 ∂yi ∂xi = exi PN k=1 exk PN k=1 exk −exi PN k=1 exk ! ∂yi ∂xi = yi 1 − exi PN k=1 exk ! = yi(1 −yi) • The derivative ∂yi ∂xj with respect to a different index (i ̸= j): ∂ ∂xj (exi) = 0 ∂ ∂xj N X k=1 exk ! = exj ∂yi ∂xj = (0)(PN k=1 exk) −(exi)(exj) (PN k=1 exk)2 ∂yi ∂xj = − exiexj (PN k=1 exk)(PN k=1 exk) ∂yi ∂xj = − exi PN k=1 exk ! exj PN k=1 exk ! ∂yi ∂xj = −yiyj Combining both cases: ∂yi ∂xj = ( yi(1 −yi) if i = j −yiyj if i ̸= j which can be written compactly using the Kronecker delta (δij) as ∂yi ∂xj = yi(δij −yj) 4. Softplus function The softplus function (12), or SmoothReLU function, is defined as f(x) = ln (1 + ex) For large negative x, it is roughly ln 1, just above 0. For large positive x, it is roughly ln(ex) ,just above x. The derivative of the softplus function is the logistic function (the standard sigmoid function). Given the softplus function, f(x) = ln(1 + ex), the softplus derivative can be derived using the chain rule and simplified into the standard sigmoid function: • Apply the chain rule d du[ln(u)] = 1 u · du dx, where u = 1 + ex: f ′(x) = 1 1 + ex · d dx(1 + ex) f ′(x) = 1 1 + ex · (0 + ex) f ′(x) = ex 1 + ex • The form ex 1+ex is a common representation of the sigmoid function, σ(x). To',\n",
       "  'u = 1 + ex: f ′(x) = 1 1 + ex · d dx(1 + ex) f ′(x) = 1 1 + ex · (0 + ex) f ′(x) = ex 1 + ex • The form ex 1+ex is a common representation of the sigmoid function, σ(x). To show the most common form of the sigmoid, 1 1+e−x , just multiply both nu- merator and denominator by e−x: f ′(x) = ex 1 + ex · e−x e−x = ex · e−x e−x(1 + ex) = 1 e−x + 1 Therefore, the derivative of softplus is: d dx(softplus(x)) = σ(x) = 1 1 + e−x 5. Swish function The swish family function (13, 20, 35) is defined as swishβ(x) = xσ(βx) = x 1 + e−βx where β can be constant or trained with the network parameters. The swish family was designed to smoothly interpolate between a linear function and the ReLU function: • When β = 0, the function is linear: swish0(x) = x/2 • When β = 1, the function is the standard sigmoid or logistic: swish1(x) = σ(x) 18 • When β →∞, the function converges to the ReLU: swish∞(x) = lim β→∞ x 1 + e−βx = ReLU(x) since for x > 0, e−βx → 0 and, therefore, swish∞(x) →x, whereas, for x < 0, e−βx →∞ and swish∞(x) →0. The derivative of the swish function is given by swish′ β(x) = σ(βx) + βx · σ′(βx) = σ(βx) + βx · σ(βx)(1 −σ(βx)) = βx · σ(βx) + σ(βx)(1 −βx · σ(βx)) = β · swishβ(x) + σ(βx)(1 −β · swishβ(x)) 6. ReLU function The rectifier or ReLU (rectified linear unit) activation function is defined as the non-negative part of its argu- ment, i.e., the ramp function: ReLU(x) = x+ = max{0, x} = ( x if x > 0 0 if x ≤0 The ReLU is analogous to half-wave rectification in electrical engineering and one of the most common acti- vation functions for artificial neural networks. Many variants of the ReLU function have been pro- posed in the literature: • Leaky ReLU (31) is a piecewise-linear variant that allows a small, positive gradient when the unit is inactive (α typically between 0.01 and 0.3): LReLU(x) = ( x if x > 0 αx if x ≤0 • Parametric ReLU (18) makes the leaky ReLU α a learnable parameter. For α ≤1: PReLU(x) = max{x, αx} • Concatenated ReLU (37) preserves both positive and negative inputs by returning two values: CReLU(x) = [ReLU(x), ReLU(−x)] Apart from the softplus function, or SmoothReLU, ad- ditional smooth approximations to the rectifier function include: • Exponential linear units, ELUs, which smoothly al- low negative values (8): ELU(x) = ( x if x > 0 α(ex −1) if x ≤0 ELU′(x) = ( 1 if x > 0 αex if x ≤0 ELU can be viewed as a smoothed version of a shifted ReLU (SReLU), which has the form f(x) = max{−α, x}. • Gaussian-error linear units, GELUs (20): GELU(x) = xΦ(x) GELU ′(x) = xΦ′(x) + Φ(x) where Φ(x)',\n",
       "  'ELU′(x) = ( 1 if x > 0 αex if x ≤0 ELU can be viewed as a smoothed version of a shifted ReLU (SReLU), which has the form f(x) = max{−α, x}. • Gaussian-error linear units, GELUs (20): GELU(x) = xΦ(x) GELU ′(x) = xΦ′(x) + Φ(x) where Φ(x) is the cumulative distribution function of the standard normal distribution N(0, 1). The swish function, or SiLU (sigmoid linear unit), is similar, also with a bump with negative derivative to the left of x = 0, and computationally cheaper. • The mish function (33) was obtained by experi- menting with functions similar to swish and ex- hibits a self-regularizing behavior attributed to a ∆(x) term in its first derivative: mish(x) = x tanh(softplus(x)) mish’(x) = ∆(x)swish1(x) + mish(x) x where ∆(x) = sech2(softplus(x)). • Squareplus (2) is an algebraic softplus-like function squareplus(x) = x + √ x2 + b 2 squareplus′(x) = 1 2 \\x12 x √ x2 + b + 1 \\x13 where the b ≥0 hyperparameter determines the extent of the curved region near x = 0. Similar to softplus, squareplus can be computed using only algebraic functions, making it suitable for compu- tational efficiency and numerical stability. • Extended exponential linear units, DELUs, are smoother within the neighborhood of zero and sharper for larger values (7): DELU(x) = ( x if x > xc (eαx −1)/β if x ≤xc DELU′(x) = ( 1 if x > xc (α/β)eαx if x ≤xc whose hyperparameters are typically set as α = 1, β = 2, xc = 1.25643. The value of xc ≥0 results from imposing the continuity constraint for the chosen values of α and β: xc = (eαxc −1)/β 19 Appendix B: Loss Functions Common loss functions are described in this Appendix, grouped by category. Well-known loss functions for clas- sification problems (and matching probability distribu- tions) include the following: • Cross-entropy loss (a.k.a. log loss): Standard for probabilistic classification, with special cases for bi- nary (binary cross-entropy, BCE) and multi-class (categorical cross-entropy) classification. LCE = H(y∥ˆy) = − X k yk log ˆyk • Focal loss (29, 30): Down-weights easy examples, focuses on hard ones (common in imbalanced clas- sification). Common for image segmentation. LF L = − X k yk(1 −ˆyk)γ log ˆyk Focal loss adds a modulating factor (1−ˆyk)γ to the cross entropy loss, with a tunable focusing param- eter γ ≥0. • Hinge loss (9): Used in support vector machines (SVMs), with a geometrical justification. Margin- based, with a linear penalty when the margin is violated. For a binary classification problem using bipolar encoding for the target y ({−1, +1}) and a classifier score ˆy: LHL = max{0, 1 −yˆy} The squared hinge loss is a variant of the hinge loss with squared penalty (quadratic penalty when the margin is violated): LSHL = (max{0, 1 −yˆy})2 For multi-class classification problems, the Crammer-Singer (10) and Weston-Watkins (41) loss functions can be used: LCS = max{0, 1 + max k̸=y {ˆyy −ˆyk}} LW W = X k̸=y max{0, 1 + ˆyyˆyk} • Kullback–Leibler divergence, or KL loss',\n",
       "  'penalty when the margin is violated): LSHL = (max{0, 1 −yˆy})2 For multi-class classification problems, the Crammer-Singer (10) and Weston-Watkins (41) loss functions can be used: LCS = max{0, 1 + max k̸=y {ˆyy −ˆyk}} LW W = X k̸=y max{0, 1 + ˆyyˆyk} • Kullback–Leibler divergence, or KL loss (26), also known as the relative entropy of y with respect to ˆy: LKL = DKL(y∥ˆy) = X k yk log yk ˆyk The KL divergence is an f-divergence measure be- tween true and predicted distributions. Since y is usually represented using one-hot encod- ing, it simplifies to the cross-entropy loss, given that DKL(y∥ˆy) = H(y∥ˆy) −H(y). • Hellinger divergence, a.k.a. Hellinger distance (19): Another f-divergence that quantifies the similarity between two probability distributions: LHD = DH(y∥ˆy) = 1 √ 2 sX k \\x10√yk − p ˆyk \\x112 which is directly related to the Euclidean norm of the difference of the square root vectors. Some- times the factor 1/ √ 2 is omitted, in which case the Hellinger distance ranges from zero to the square root of two. • Total variation distance, a.k.a. statistical distance, statistical difference,or variational distance: An- other f-divergence that provides a statistical dis- tance between probability distributions. LT V D = DT V D(y∥ˆy) = X k |yk −ˆyk| i.e. half of the L1 distance between the probability functions on discrete domains. The three f-divergences (36) above are related: DT V D(y∥ˆy) ≤ r 1 2DKL(y∥ˆy) D2 H(y∥ˆy) ≤DT V D(y∥ˆy) ≤ √ 2DH(y∥ˆy) i.e. inequalities that follow immediately from the relationship between the 1-norm and the 2-norm. • Jensen–Shannon divergence, also known as infor- mation radius (IRad) or total divergence to the av- erage: Yet another f-divergence, a symmetrized and smoothed version Kullback–Leibler divergence that always has a finite value. LJS = DJS(y∥ˆy) = DKL(y∥m) + DKL(ˆy∥m) 2 where m = (y + ˆy)/2 is a mixture of y and ˆy (i.e. the pointwise mean of y and ˆy probabilities). The square root of the Jensen–Shannon divergence is a metric often referred to as Jensen–Shannon dis- tance, p DJS(y∥ˆy). • Wasserstein Distance (24, 40), a.k.a. the Earth mover’s distance or the optimal transport distance: Another similarity metric between two probability distributions. It can be interpreted as the mini- mum energy cost of moving and transforming a pile of dirt in the shape of one probability distribution to the shape of the other distribution. The cost is calculated as the product of the amount of proba- bility mass being moved and the distance it is being moved. δ0 = 0 20 δk+1 = δk + yk −ˆyk LW1 = W1(y, ˆy) = X k |δk| In general, for any Minkowski distance: LWp = Wp(y, ˆy) = X k ||δk||p !1/p • Bhattacharyya distance (5): Not a metric, actually, since it does not satisfy the triange inequality. LB = DB(y, ˆy) = −log BC(y, ˆy) where BC(y, ˆy) = X k p yk ˆyk is the Bhattacharyya coefficient for discrete proba- bility distributions. • Renyi divergence, or α-divergence (36): A spec- trum of divergence measures generalize the KL di-',\n",
       "  'since it does not satisfy the triange inequality. LB = DB(y, ˆy) = −log BC(y, ˆy) where BC(y, ˆy) = X k p yk ˆyk is the Bhattacharyya coefficient for discrete proba- bility distributions. • Renyi divergence, or α-divergence (36): A spec- trum of divergence measures generalize the KL di- vergence. Its order α controls how sensitive you want to be to rare differences. Lα = Dα(y∥ˆy) = 1 α −1 log X k yα k ˆyα−1 k For special values, we can define the R´enyi diver- gence by taking a limit: – As α →0, D0 measures how much ˆy supports y, checking whether ˆy assigns probability to all points where y is nonzero. D0(y∥ˆy) = −log X k ˆy · 1yk>0 where 1yk>0 is an indicator function that is 1 if yk > 0, and 0 otherwise. – As α →1, the R´enyi divergence converges to the Kullback–Leibler (KL) divergence: D1(y∥ˆy) = DKL(y∥ˆy) = X k yk log yk ˆyk – As α →∞, D∞becomes the max-divergence, focusing only on the largest discrepancy be- tween the two distributions: D∞(y∥ˆy) = log max k yk ˆyk – When α = 1/2, D1/2 is twice the Bhat- tacharyya divergence: D2(y∥ˆy) = 2DB(y∥ˆy) = −2 log p yk ˆyk – When α = 2, D2 is the logarithm of the ex- pected ratio of probabilities: D2(y∥ˆy) = log \\x12 yk yk ˆyk \\x13 = log yk ˆyk The R´enyi divergence is indeed a divergence, i.e. it is greater than or equal to zero, and zero only when both distributions are the same. For any given pair of distributions, the R´enyi divergence is nondecreasing as a function of its order α. For regression problems, a wide variety of loss func- tions have been proposed: • Mean Squared Error, MSE (16): Highly sensitive to outliers, but smooth and differentiable. Equiv- alent to MLE under Gaussian noise with constant variance. LMSE = 1 n X i (yi −ˆyi)2 ∂LMSE ∂ˆyi = −2(yi −ˆyi) • Mean Absolute Error, MAE (27), a.k.a. L1 Loss: More robust to outliers, but non-differentiable at 0. MLE under Laplace (double exponential) noise, leads to median regression. LMAE = 1 n X i |yi −ˆyi| ∂LMAE ∂ˆyi = −sign(yi −ˆyi) • Huber loss (22): Quadratic near 0, linear for large residuals. Balances MSE for small errors and MAE for large errors. Robust, differentiable, widely used in robust regression. LHuber = ( 1 2(y −ˆy)2 |y −ˆy| ≤δ δ(|y −ˆy| −1 2δ) |y −ˆy| > δ ∂LHuber ∂ˆyi = ( −(yi −ˆyi) |yi −ˆyi| ≤δ −δ sign(yi −ˆyi) |yi −ˆyi| > δ • Log-cosh loss: Smooth approximation to MAE, less harsh than MSE, with bounded gradient, shares similarities with Huber loss but is infinitely differ- entiable. LLCL = X i log cosh(yi −ˆyi) ∂LLCL ∂ˆyi = −tanh(yi −ˆyi) 21 Loss function Small errors Large errors Derivative ∂L/∂ˆyi MSE ϵ2 ϵ2 ϵ MAE ϵ ϵ 1 Huber ϵ2 ϵ 1 Log-cosh ϵ2 ϵ 1 Hinge 0 ϵ 1 Logarithmic log ϵ log ϵ 1/ϵ Cauchy log ϵ2 log ϵ2 1/ϵ Student log ϵ2 log ϵ2',\n",
       "  '∂ˆyi = −tanh(yi −ˆyi) 21 Loss function Small errors Large errors Derivative ∂L/∂ˆyi MSE ϵ2 ϵ2 ϵ MAE ϵ ϵ 1 Huber ϵ2 ϵ 1 Log-cosh ϵ2 ϵ 1 Hinge 0 ϵ 1 Logarithmic log ϵ log ϵ 1/ϵ Cauchy log ϵ2 log ϵ2 1/ϵ Student log ϵ2 log ϵ2 1/ϵ Fair ϵ ϵ 1 Tukey ϵ2 1 0 Pinball/quantile ϵ ϵ 1 Table IV Regression loss functions and their asymptotic behavior with small and large errors. • Hinge loss for regression, a.k.a. ϵ-insensitive loss (39): Robust to small noise, focuses on large devi- ations. Support Vector Regression (SVR) general- izes hinge loss to continuous targets. Unlike MSE and MAE, SVR ignores small noise: errors smaller than the ϵ margin of tolerance are ignored (SVR has a dead zone of zero penalty, which makes it less sensitive to tiny fluctuations). Errors larger than ϵ are penalized linearly, as in MAE. LSV R = Lϵ = max(0, |y −ˆy| −ϵ) ∂LSV R ∂ˆy = ( 0 |y −ˆy| ≤ϵ −sign(y −ˆy) |y −ˆy| > ϵ The hinge loss can be justified geometrically, con- necting regression to margin-based learning: SVR finds a function such that most data points lie within an ϵ-tube, while keeping the function “flat”. However, the hinge loss is not a proper scoring rule (the forecasted distribution does not match the dis- tribution of the observation, i.e. the true distribu- tion). Variations of the hinge loss include: - ϵ-insensitive squared loss (MSE-like, with quadratic penalty outside the tube): L(2) ϵ = \\x00max{0, |y −ˆy| −ϵ} \\x012 - Huberized ϵ-insensitive loss (Huber-like, more nu- merically stable than sharp ϵ-insensitive): Flat in- side the tube, quadratic near the boundary, linear far out. Lϵ,δ = \\uf8f1 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f3 0 |y −ˆy| ≤ϵ 1 2δ(|y −ˆy| −ϵ)2 ϵ < |y −ˆy| ≤ϵ + δ |y −ˆy| −ϵ −δ 2 |y −ˆy| > ϵ + δ - Pinball (quantile) loss in SVR: Useful for predict- ing intervals. If you set ϵ = 0 and choose asymmet- ric penalties, SVR becomes quantile regression. Lτ = ( τ(y −ˆy) y ≥ˆy (1 −τ)(ˆy −y) y < ˆy • Logarithmic loss for regression (i.e. Pareto loss): Heavy-tailed, robust to outliers, decaying gradient. MLE under double Pareto noise. In probabilistic regression, the log score is a proper scoring rule, ensuring calibrated predictive distributions. Llog = X i log (1 + |yi −ˆyi|) ∂Llog ∂ˆyi = sign(ˆyi −yi) 1 + |ˆyi −yi| • Cauchy loss: Heavy-tailed, robust to outliers. MLE under Cauchy noise. LCauchy = X i log \\x12 1 + (yi −ˆyi)2 c2 \\x13 ∂LCauchy ∂ˆyi = − 2(yi −ˆyi) c2 + (yi −ˆyi)2 • Student-t loss: Heavy-tailed, robust to outliers. MLE under Student-t noise. LStudent = X i ν + 1 2 log \\x12 1 + (yi −ˆyi)2 νσ2 \\x13 ∂LStudent ∂ˆyi = −(ν + 1)(yi −ˆyi) (yi −ˆyi)2 + σ2ν 22 • Fair loss (14): Smooth, large residuals with bounded influence, less aggressive than Cauchy. LF air = c2 \\x12|y −ˆy| c −log \\x12 1 + |y −ˆy| c \\x13\\x13 ∂Lfair ∂ˆy = − y −ˆy',\n",
       "  '−ˆyi)2 νσ2 \\x13 ∂LStudent ∂ˆyi = −(ν + 1)(yi −ˆyi) (yi −ˆyi)2 + σ2ν 22 • Fair loss (14): Smooth, large residuals with bounded influence, less aggressive than Cauchy. LF air = c2 \\x12|y −ˆy| c −log \\x12 1 + |y −ˆy| c \\x13\\x13 ∂Lfair ∂ˆy = − y −ˆy 1 + |y −ˆy|/c • Tukey’s biweight loss (38): Bounded influence, completely ignores very large residuals beyond theshold c. Extremely robust, but non-convex. Used for outlier rejection. LT ukey = \\uf8f1 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f3 c2 6 \" 1 − \\x12 1 − \\x10 y−ˆy c \\x112\\x133# |y −ˆy| ≤c c2 6 |y −ˆy| > c For small error values, Tukey’s biweight loss func- tion can be closely approximated by a simple quadratic polynomial, behaving much like the stan- dard squared error loss: LT ukey ≈ϵ2/2 −ϵ4/2c2 from the Taylor series expansion of the loss func- tion around ϵ = 0. • Pinball loss, a.k.a. quantile loss (25): For quan- tile regression, asymmetric penalty depending on quantile τ. MLE under asymmetric Laplace dis- tribution computes conditional quantiles instead of mean. Useful for heteroskedastic or skewed data. Lpinball = X i max{τ(yi −ˆyi), (1 −τ)(yi −ˆyi)} The function gets its name from its characteristic V shape, which is asymmetrical when τ is not 0.5: for underprediction (yi ≥ˆyi), the loss is weighted by τ; for overprediction (yi < ˆyi), the loss is weighted by 1−τ. This weighting encourages the model to make predictions that are more likely to be above (τ > 0.5) or below (τ < 0.5) the actual value, depending on the chosen quantile. References [1] Aggarwal, C. C. Neural Networks and Deep Learning: A Textbook, 2nd edition. Springer, 2023. [2] Barron, J. T. Squareplus: A Softplus-Like Algebraic Rectifier, 2021. [3] Berzal, F. Redes Neuronales & Deep Learning - Vol- umen I: Entrenamiento de redes neuronales artificiales [Neural Networks and Deep Learning - Volume I: Train- ing Artificial Neural Networks, in Spanish]. Amazon KDP, Granada, Spain, 2019. [4] Berzal, F. Redes Neuronales & Deep Learning - Vol- umen II: Regularizaci´on, optimizaci´on & arquitecturas especializadas [Neural Networks and Deep Learning - Volume II: Regularization, Optimization, and Special- ized Architectures, in Spanish]. Amazon KDP, Granada, Spain, 2019. [5] Bhattacharyya, A. K. On a Measure of Divergence between Two Multinomial Populations. Sankhy¯a 7, 4 (1946), 401–406. [6] Bishop, C. M., and Bishop, H. Deep Learning: Foun- dations and Concepts. Springer, 2024. [7] C¸atalbas¸, B., and Morg¨ul, ¨O. Deep learning with extendeD exponential linear unit (DELU). Neural Com- puting and Applications 35, 30 (2023), 22705–22724. [8] Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast and Accurate Deep Network Learning by Expo- nential Linear Units (ELUs), 2016. [9] Cortes, C., and Vapnik, V. Support-vector networks. Machine learning 20, 3 (1995), 273–297. [10] Crammer, K., and Singer, Y. On the Algorithmic Implementation of Multiclass Kernel-based Vector Ma- chines. Journal of Machine Learning Research 2 (Mar. 2002), 265–292. [11] Cybenko, G. Approximation by superpositions of a sig- moidal function. Mathematics of Control, Signals and Systems 2, 4 (1989), 303–314. [12] Dugas, C., Bengio, Y., B´elisle, F., Nadeau, C.,',\n",
       "  'Y. On the Algorithmic Implementation of Multiclass Kernel-based Vector Ma- chines. Journal of Machine Learning Research 2 (Mar. 2002), 265–292. [11] Cybenko, G. Approximation by superpositions of a sig- moidal function. Mathematics of Control, Signals and Systems 2, 4 (1989), 303–314. [12] Dugas, C., Bengio, Y., B´elisle, F., Nadeau, C., and Garcia, R. Incorporating second-order functional knowledge for better option pricing. In Advances in Neural Information Processing Systems (2000), T. Leen, T. Dietterich, and V. Tresp, Eds., vol. 13, MIT Press. [13] Elfwing, S., Uchibe, E., and Doya, K. Sigmoid- weighted linear units for neural network function approx- imation in reinforcement learning, 2017. [14] Fair, R. C. On the robust estimation of econometric models. Annals of Economic and Social Measurement 3, 4 (1974), 667–677. [15] Funahashi, K.-I. On the approximate realization of con- tinuous mappings by neural networks. Neural Networks 2, 3 (1989), 183–192. [16] Gauss, C. F. Theoria motus corporum coelestium in sectionibus conicis solem ambientium [Theory of the Mo- tion of Heavenly Bodies Moving about the Sun in Conic Sections, in Latin]. Friedrich Perthes & Johann Heinrich Besser, 1809. [17] Goodfellow, I., Bengio, Y., and Courville, A. Deep Learning. MIT Press, 2016. [18] He, K., Zhang, X., Ren, S., and Sun, J. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, 2015. [19] Hellinger, E. Neue Begr¨undung der Theorie quadratis- cher Formen von unendlichvielen Ver¨anderlichen [New foundation of the theory of quadratic forms of infinitely many variables, in German]. Journal f¨ur die reine und angewandte Mathematik [Journal of Pure and Applied Mathematics] 136 (1909), 210–271. [20] Hendrycks, D., and Gimpel, K. Gaussian Error Lin- ear Units (GELUs), 2016. [21] Hornik, K., Stinchcombe, M., and White, H. Multi- layer feedforward networks are universal approximators. Neural Networks 2, 5 (1989), 359–366. [22] Huber, P. J. Robust Estimation of a Location Parame- ter. The Annals of Mathematical Statistics 35, 1 (1964), 73 – 101. 23 [23] Jørgensen, B. Exponential dispersion models. Journal of the Royal Statistical Society: Series B (Methodological) 49, 2 (1987), 127–145. [24] Kantorovich, L. V. On the translocation of masses. In Doklady Akademii Nauk USSR (NS) (1942), vol. 37, pp. 199–201. [25] Koenker, R., and Bassett Jr, G. Regression quan- tiles. Econometrica: Journal of the Econometric Society 46, 1 (1978), 33–50. [26] Kullback, S., and Leibler, R. A. On information and sufficiency. Annals of Mathematical Statistics 22, 1 (1951), 79–86. [27] Laplace, P.-S. M´emoire sur la Probabilit´e des Causes par les ´Ev´enements [Memoir on the Probability of Causes from Events, in French]. M´emoires de l’Acad´emie Royale des Sciences de Paris 6 (1774), 621–656. [28] Leshno, M., Lin, V. Y., Pinkus, A., and Schocken, S. Multilayer feedforward networks with a nonpolyno- mial activation function can approximate any function. Neural Networks 6, 6 (1993), 861–867. [29] Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll´ar, P. Focal Loss for Dense Object Detection. In Proceedings of the IEEE International Conference on Computer Vision (2017), pp. 2980–2988. [30] Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll´ar, P. Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis and Machine',\n",
       "  'R., He, K., and Doll´ar, P. Focal Loss for Dense Object Detection. In Proceedings of the IEEE International Conference on Computer Vision (2017), pp. 2980–2988. [30] Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll´ar, P. Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis and Machine In- telligence 42, 2 (2020), 318–327. [31] Maas, A. L., Hannun, A. Y., and Ng, A. Y. Rectifier nonlinearities improve neural network acoustic models. In ICML Workshop on Deep Learning for Audio, Speech and Language Processing (2013), Atlanta, GA. [32] McCulloch, W. S., and Pitts, W. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics 5, 4 (1943), 115–133. [33] Misra, D. Mish: A Self Regularized Non-Monotonic Ac- tivation Function. In 31st British Machine Vision Virtual Conference (2020), British Machine Vision Association. [34] Mont´ufar, G., Pascanu, R., Cho, K., and Bengio, Y. On the number of linear regions of deep neural net- works. In Proceedings of the 28th International Confer- ence on Neural Information Processing Systems - Volume 2 (Cambridge, MA, USA, 2014), NIPS’14, MIT Press, p. 2924–2932. [35] Ramachandran, P., Zoph, B., and Le, Q. V. Search- ing for activation functions, 2017. [36] R´enyi, A. On measures of entropy and information. In Proceedings of the 4th Berkeley Symposium on Mathe- matical Statistics and Probability (1961), vol. 4, Univer- sity of California Press, pp. 547–562. [37] Shang, W., Sohn, K., Almeida, D., and Lee, H. Understanding and improving convolutional neural net- works via concatenated rectified linear units. In Pro- ceedings of The 33rd International Conference on Ma- chine Learning (New York, New York, USA, 20–22 Jun 2016), M. F. Balcan and K. Q. Weinberger, Eds., vol. 48 of Proceedings of Machine Learning Research, PMLR, pp. 2217–2225. [38] Tukey, J. W. Exploratory Data Analysis. Addison- Wesley, 1977. [39] Vapnik, V. N. The Nature of Statistical Learning The- ory. Springer, 1995. [40] Vaserstein, L. N. Markov processes over denumerable products of spaces describing large systems of automata [in Russian]. Problemy Peredaci Informacii 5, 3 (1969), 64 – 72. [41] Weston, J., Watkins, C., et al. Support Vec- tor Machines for Multi-Class Pattern Recognition. In ESANN’1999 European Symposium on Artificial Neural Networks (1999), pp. 219–224.'],\n",
       " ['1 DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning Tharindu Fernando, Member, IEEE, Clinton Fookes, Senior Member, IEEE, and Sridha Sridharan, Life Senior Member, IEEE. Abstract—Rapid advances in generative AI have led to in- creasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high- quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm em- powers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dy- namic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive eval- uations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios. Index Terms—Deepfake Detection, Semi-Fragile Watermarks, Multi-Agent Adversarial Reinforcement Learning, Digital Foren- sics I. INTRODUCTION Generative AI has rapidly transformed the way we create and edit multimedia, making it faster and easier than ever to produce high-quality images, audio, and video. While these innovations unlock exciting possibilities, they also introduce serious risks. Among the most concerning threats are image and video deepfakes, highly realistic, AI-generated images and videos that mimic real persons, posing significant challenges, not only for their role in spreading misinformation and disin- formation at an alarming pace but also for their implications on privacy, consent, security, and societal trust [1], [2], [3], [4], [5]. To combat the rapid spread of deepfakes, watermarking has emerged as a technique that allows users to validate the authenticity and integrity of the multimedia they con- sume. A robust watermarking technique should be resilient to T. Fernando, C. Fookes, and S. Sridharan are with The Signal Processing, Artificial Intelligence and Vision Technologies (SAIVT), Queensland Univer- sity of Technology, Australia. Adaptive Watermark Attacks Learned Image Compression Attacks Hair Colour and Expression Changed Diﬀusion Attacks Hair Colour Changed Face Swap Fig. 1. Watermarking and watermark retrieval performance of proposed DeepForgeSeal framework. The visualisations include low-resolution and high-resolution bonafide images (row 1), numerous learned attacks against watermarking (row 2), and watermarked images that have undergone semantic- altering manipulations (row 3). distortions such as JPEG compression, cropping, and colour changes to keep the embedded watermark intact. While being resilient to such incidental distortions, a good watermark- ing technique should demonstrate fragility towards content- altering modifications, such as face-swapping-type malicious edits that alter the semantic-level meaning of the image. Recently, deep learning',\n",
       "  'distortions such as JPEG compression, cropping, and colour changes to keep the embedded watermark intact. While being resilient to such incidental distortions, a good watermark- ing technique should demonstrate fragility towards content- altering modifications, such as face-swapping-type malicious edits that alter the semantic-level meaning of the image. Recently, deep learning techniques [6], [7], [8], [9], [10] have made significant progress towards achieving this paradoxical combination of resilience and fragility. For instance, SepMark [8] introduced an encoder–decoder architecture featuring a unified encoder for watermark embedding and two distinct decoders designed to recover the watermark under varying robustness constraints, thereby enabling selective resilience against different levels of adversarial distortion. However, most existing deep watermarking techniques for deepfake detection rely on a fixed, pre-defined watermark location for extraction, arXiv:2511.04949v1 [cs.CV] 7 Nov 2025 2 which makes them vulnerable, as perpetrators can easily detect and tamper the watermark once its position is known. In addition, these methods often lack robustness against benign image manipulations such as brightness adjustments or heavy compression, leading to a high rate of false positives in deep- fake detection. Furthermore, current supervised and adversarial learning approaches provide only limited fidelity in addressing the resilience–fragility paradox. Most recently, OmniGuard [9] introduced a more flexible approach for image manipulation detection and localisation by incorporating a degradation-aware tamper extraction network designed for blind, robust copyright protection and tamper localization. However, the framework embeds watermarks directly in the pixel space, which makes them more suscep- tible to attacks. Even minor changes in image orientation or scale can significantly alter pixel values, making pixel- based systems highly sensitive to trivial modifications despite the semantic content remaining unchanged. Consequently, the OmniGuard framework is less effective in detecting deepfakes, where the primary objective is to identify only malicious edits that alter the semantic-level meaning of the image. In contrast to existing methods, we embed watermarks in the latent space rather than the pixel space, leveraging semantic-level manipulations of the image’s representation. Since the latent space encodes high-level meaning, the wa- termark becomes tightly coupled with the image’s semantics. Any malicious attempt to alter the image’s meaning disrupts this coupling, effectively breaking the watermark and revealing tampering. Most importantly, through a learnable watermark embedder network, we identify less perceptually salient direc- tions to embed information without significantly altering the human-perceived semantics of the input image. We propose to leverage a spherical latent space for the embedding of the proposed watermark, which offers normalised operations for the embedding of our watermark, preventing the watermark embedder from drifting arbitrarily far from the original input representation, and making significant changes. Embedding the watermark in this space offers two key advantages: (1) inherent resilience to benign transformations, such as resizing, compression, or brightness adjustments, that do not significantly alter semantic features, and (2) heightened sensitivity to malicious manipulations that change the semantic meaning of the image, such as identity swaps or expression alterations in deepfakes. By disentangling the watermark from the pixel domain and leveraging semantic representations, our approach aligns watermarking with the very nature of generative models, making it both resilient and semantically',\n",
       "  'sensitivity to malicious manipulations that change the semantic meaning of the image, such as identity swaps or expression alterations in deepfakes. By disentangling the watermark from the pixel domain and leveraging semantic representations, our approach aligns watermarking with the very nature of generative models, making it both resilient and semantically aware. We propose a novel Multi-agent Adversarial Reinforcement Learning (MAARL) paradigm for learning watermarks that are both resilient to benign manipulations and fragile to semantic alterations, enabling robust deepfake detection. The framework is structured as a multi-objective game between two agents: the watermarking agent (i.e., the watermark embedder, extractor), and the attacker. The watermarking agents aim to embed latent space watermarks that survive traditional image transforma- tions but fail under semantic changes. The attacker, in turn, seeks to break the watermark by maximizing extractor loss. It can dynamically adjust the strength and type of attacks, rang- ing from conventional (e.g., compression, resizing) to semantic (e.g., facial attribute editing via StyleGAN), and combine them into complex, evolving curricula. This adversarial setup forces the watermarking agent to learn a sophisticated strategy that balances robustness and fragility, avoiding both overly weak and excessively strong watermarking schemes, allowing us to arrive at the right combination for this paradoxical objective. The result is an architecture that achieves unprecedented levels of resilience across bonafide transformations and greater levels of fragility towards malicious manipulations (See Fig. 1). The main technical contributions of this paper, through which we introduce the proposed DeepForgeSeal framework, can be summarised as follows: 1) We introduce a novel deep watermarking framework for deepfake detection that operates in the high-dimensional semantic space of input images. 2) We design a learnable watermarking agent with explicit control over message encoding and extraction, achieving semantic stealth. 3) We propose a new MAARL (Multi Agent Adversar- ial Reinforcement Learning) paradigm that enables the agent to learn a strategy balancing watermark resilience and fragility. 4) We develop a reward function that guides the watermark attacker to discover failure regions in the latent space and image manipulations that induce significant latent shifts, allowing it to devise a structured curriculum of attacks. II. RELATED WORK A. Watermarking Approaches for Deepfake Detection Passive deepfake detection refers to techniques that analyse the media without requiring any prior knowledge of the generation process or embedding of security features such as watermarks. These approaches have demonstrated remarkable performance over the past decade, primarily by identifying artefacts introduced during the deepfake generation process, such as unsynchronised lip movements [11], irregular facial landmarks [12], or physiological inconsistencies like abnormal blood oxygen concentration [13]. However, their effectiveness is inherently tied to the presence of such artefacts, which vary depending on the generation technique used. Consequently, the generalisability of these detectors to novel or unseen deepfake generation methods remains limited. Moreover, the rapid evolution of deepfake technologies poses a significant challenge to maintaining detection robustness, raising concerns about the long-term reliability of passive detection strategies. Such limitations have led to the development of proactive deepfake detection approaches, with watermarking being a prominent example. This technique embeds invisible signals into benign images',\n",
       "  'the rapid evolution of deepfake technologies poses a significant challenge to maintaining detection robustness, raising concerns about the long-term reliability of passive detection strategies. Such limitations have led to the development of proactive deepfake detection approaches, with watermarking being a prominent example. This technique embeds invisible signals into benign images before any manipulation occurs. These signals act as verification markers, enabling systems to actively determine whether the content has been tampered with based on the presence and integrity of the embedded watermark. One of the earliest works in watermarking based deepfake detection can be attributed to [14], which embeds a robust, 3 invisible watermark into facial images before any manipulation occurs. These watermarks are resilient to various deepfake transformations and image processing operations. The embed- ded watermark acts as a unique identifier, allowing the origin of a manipulated image to be traced. The works [15], [16] have also leveraged the concept of robust watermarking as a line of defense against deepfakes. Specifically, in contrast to [14], which embeds identity-linked tags using a learned network which is resilient to deepfake transformations, [15], [16] introduce training-free watermarking mechanisms based on facial landmarks. For example, instead of relying on learned embeddings, the authors of [16] propose to transform structural facial representations into binary perceptual watermarks, en- abling robust and imperceptible embedding without extensive model training. Despite these advances, robust watermarking- based deepfake detection approaches require validating the embedded watermark against a known source or identity, which can be logistically complex in real-world applications, such as in social media platforms. Each image or video must be checked to ensure the watermark matches a trusted source identity, which assumes access to and maintenance of such a database. Semi-fragile watermarks, which leverage both the fragility and robustness of watermarking, simplify the detection process by serving as self-contained integrity checks. Specifically, if the watermark is broken or unreadable, it directly signals tampering, without needing to compare against a source, making them more efficient and scalable for deployment in environments where rapid and autonomous verification is critical. One of the early works in semi-fragile watermarks for deepfake detection can be attributed to FaceGuard [17], which encodes a semi-fragile identity-specific watermark. Along similar lines, Zhao et al. [6] proposed an identity- based semi-fragile watermarking approach that is sensitive to face swap manipulations while remaining resilient to benign image transformations. Despite its advances, the reliance on identity-specific watermarks for each individual makes these techniques less appealing for real-world applications in which the true identity of the individuals is not available or cannot be readily extracted. WaterLo [18] addresses these limitations by introducing a localised, fixed semi-fragile watermark that disappear in manipulated regions, allowing not only detection but also precise localisation of tampered areas. Following this approach, the authors of FaceSigns [19] have introduced a semi-fragile watermarking framework that embeds a 128-bit secret message directly into the image pixels. Most recently, the EditGuard [10] introduces a novel ap- proach for decoupling the training process from specific tam- pering types. Traditional watermarking systems often require retraining or fine-tuning for each manipulation scenario. In contrast,',\n",
       "  'introduced a semi-fragile watermarking framework that embeds a 128-bit secret message directly into the image pixels. Most recently, the EditGuard [10] introduces a novel ap- proach for decoupling the training process from specific tam- pering types. Traditional watermarking systems often require retraining or fine-tuning for each manipulation scenario. In contrast, EditGuard’s image-to-image steganography frame- work generalises across diverse tampering methods, including face swaps, background replacements, and AI-generated edits, without needing retraining. OmniGuard [9] further enhances this approach by introducing a hybrid forensic framework that combines proactive embedding of the semi-fragile watermark with passive blind extraction. In a different line of work, the authors of [7] leverage the mathematical properties of fractals to generate watermark patterns through a parameter- driven pipeline. Moreover, watermarks are embedded using an entry-to-patch strategy, where each watermark matrix entry is mapped to a specific image patch, enabling precise localisation of manipulated regions. Though significant strides have been made, the majority of existing semi-fragile watermarking approaches still rely on the pixel space for embedding, rather than leveraging the latent space, which could offer greater flexibility and robustness. Furthermore, current adversarial training pipelines used when training the watermarking frame- works are often simplistic and lack the sophistication needed to simulate complex attack curricula. As a result, the resilience of watermarking techniques under realistic and increasingly so- phisticated adversarial conditions remains limited. In contrast, by leveraging a spherical latent space, our framework ensures more structured and disentangled representations, which en- hances watermark embedding consistency, robustness, and ma- nipulation sensitivity. Additionally, the integration of MAARL enables the system to simulate and defend against complex, curriculum-based adversarial attacks, significantly improving the robustness and generalisability of the watermarking strat- egy across diverse manipulation scenarios. B. Multi Agent Adversarial Reinforcement Learning In multi-agent learning setting the relationships among agents can be categorised into corporative, competitive, and both [20], while most of the prior works [21], [22], [23], [24], [25] in Multi Agent Reinforcement Learning (MARL) have focused on cooperative tasks, in which the cumulative reward is maximized as a group. Among the limited number of work extending traditional MARL into adversarial settings, researchers have introduced adversarial elements, either as competing agents or perturbations, to enhance the robustness and generalisation capabilities of the overall algorithm. For example, in [26] the authors have shown the potential of learn- ing a robust generalise policy using Multi Agent Adversarial Reinforcement Learning (MAARL), where as in the authors of [27] have used MAARL to avoid overfitting. In a similar line of work, Bukharin et. al [28] address the lack robustness and sensitivity to environment changes in MARL through adversarial training. Specifically, the authors of [28] introduce adversarial regularization to enforce Lipschitz continuity in policies, improving robustness against noisy observations. Most recently, Yuan et. al [29] have proposed an approach based on evolutionary learning to enhance robustness in message-passing for improving the communication efficiency of agents. While adversarial training has shown promise in both MARL and watermarking independently, to the best of our knowledge, none of the prior works have investigated the integration of MAARL into watermarking. Through empirical evaluations we',\n",
       "  'on evolutionary learning to enhance robustness in message-passing for improving the communication efficiency of agents. While adversarial training has shown promise in both MARL and watermarking independently, to the best of our knowledge, none of the prior works have investigated the integration of MAARL into watermarking. Through empirical evaluations we demonstrate that MAARL offers a compelling framework to train watermarking agents that can dynami- cally adapt to image manipulations, potentially leading to watermarking systems that are both robust and semi-fragile, providing resistant to benign transformations yet sensitive to malicious tampering. 4 III. METHODS In this section, we discuss our proposed approach. We first provide an overview of the main components that constitute our DeepForgeSeal framework in Sec. III-A. Our watermark- ing agent, watermark attacker, and watermark extraction and deepfake detection processes are discussed in Secs. III-B, III-C, and III-D, respectively. Finally, implementation details of the framework are discussed in Sec. III-E. A. Overview Given an input image x, we define a watermarking agent θ that learns the distribution of the semantic features f(x), projects these features into a spherical latent space S, inserts a watermark wx in the latent space ensuring that it does not significantly alter the semantic meaning of x, and decodes the new image xw with the watermark from the spherical latent space back to the image space. The objective of the watermarking agent is to identify the region in S where it could embed the watermark without significantly altering the human-perceived meaning of x. Similarly, it should learn to maintain an optimal balance between the resilience of the embedded watermark towards benign operations, such as JPEG compression, cropping, and color adjustments, and its fragility to malicious edits, such as face swapping and facial attribute editing, that alter the semantic content of the image. The objective of the watermark attacker agent, η, is to gen- erate an adversarial image xa from the watermarked image xw, i.e., η(xw) →xa, by applying transformations that degrade or remove the embedded watermark. To achieve this, the attacker can choose from a range of image manipulations, including benign operations and malicious edits. These manipulations may also be combined, and for each type, the agent can control the associated parameters to vary the strength of the attack. The watermark extractor, δ, is responsible for retrieving the embedded watermark from a given image x′. Since the watermarking agent θ dynamically adjusts both the location and content of the watermark, the extractor must co-adapt and learn in tandem with the watermarking agent to ensure reliable extraction. In our framework, the success or failure of watermark extraction serves as a heuristic for deepfake detection. Specifically, if the extractor δ fails to recover a valid watermark from the image x′, the image is flagged as a potential deepfake. Our approach leverages the learned con- sistency between the watermarking and extraction processes as an indirect signal of authenticity, enabling the detection of semantic-level manipulations without relying on a fixed watermark pattern. These modules, and the flow of information between them, are illustrated in Fig. 2. B. Watermarking Agent The watermarking',\n",
       "  'approach leverages the learned con- sistency between the watermarking and extraction processes as an indirect signal of authenticity, enabling the detection of semantic-level manipulations without relying on a fixed watermark pattern. These modules, and the flow of information between them, are illustrated in Fig. 2. B. Watermarking Agent The watermarking agent is responsible for encoding and embedding the watermark into the image. Its architecture is designed to operate within the semantic latent space of a pre- trained CLIP model, ensuring that the embedding process is guided by high-level image representations. The procedure comprises two key stages: (i) directional embedding in the latent sphere, and (ii) perturbative watermark embedding. These stages are discussed in detail in the following sub- sections. 1) Directional Embedding in Latent Sphere: To extract semantically meaningful information from the input image, x, our watermarking agent, θ, operates in the feature space of the CLIP image encoder [30], which is denoted as ECLIP. Specifically, the feature vector f(x) = ECLIP(x) ∈Rζ is L2-normalized, such that ∥f(x)∥2 = 1, constraining it to the surface of a ζ-dimensional hypersphere. The watermarking agent has explicit control over the orthonormal embedding directions that it leverages to embed the message (i.e., wa- termark), M. However, allowing the watermarking agent to freely modify both the embedded message and the latent space directions makes it infeasible for the watermark extractor to recover the message without access to either the original image x or the watermark M. To address the challenge of extracting dynamically embedded messages, we propose deriving the la- tent space directions through a learnable direction generator, µ, while relying on a shared secret key K. Specifically, the embedding directions are deterministically derived from K, which may be a password-like string. The key is first projected into the CLIP text embedding space [30] to obtain a feature vector f(K). The direction generator network µ then uses this feature to produce a canonical set of L orthonormal direction vectors D = {d1, d2, . . . , dL}. Formally, D = µ(f(K)), where di ∈Rd, ∥di∥2 = 1 (1) Since D depends only on K, it remains consistent across all images. However, the directions are still learnable, as the weights ω of µ are optimised during training. For each bit mi ∈{0, 1} of a dynamically generated message M, the objective is to modify the image x to produce x′ such that the projection of its new feature vector f(x′) onto each direction di matches a target value, ⟨f(x′), di⟩≈ptarget i = ( ξ1 if mi = 1 ξ0 if mi = 0 (2) where ⟨·, ·⟩denotes the dot product, and ξ1, ξ0 are prede- fined projection magnitudes that encode binary values. 2) Perturbative Watermark Embedding: Once the mi ∈ {0, 1} of the message M have been identified, the water- marking agent employs a MLP layer, ϕ, that receives semantic features, f(x), of x and the watermark, M, and synthesizes a perturbation map p over ζ-dimensional feature vector f(x) such that: q = ϕ(x, M). (3) A decoder π then reconstructs the watermarked image x′ from',\n",
       "  'been identified, the water- marking agent employs a MLP layer, ϕ, that receives semantic features, f(x), of x and the watermark, M, and synthesizes a perturbation map p over ζ-dimensional feature vector f(x) such that: q = ϕ(x, M). (3) A decoder π then reconstructs the watermarked image x′ from the perturbed features: x′ = π(q). (4) The watermarking agent’s objective is to minimize a com- posite loss function LW that balances imperceptibility (Lclip), embedding accuracy (Ldir), and the conditional fragility ob- jective (Lext). Formally, this can be written as: LW = αLclip + βLdir + γLext, (5) 5 Watermarker Attacker CLIP Image Feature Extraction Watermark Extractor (𝞭) Direction generator CLIP Text Feature Extraction “Secret Key” Watermarked Image (𝒙w) Original Image (𝒙) Benign manipulations • Crop • Color Jitter • Affine Transform • Compression Malicious manipulations • Change Hair Color • Change Expression • Change Age Attack Curriculum MLP Attack Parameters Watermarking Agent (𝛳) Attacker Agent (η) D={d1,…,dL}. D={d1,…,dL} D={d1,…,dL} Extracted Directional Vectors Learned Directional Vectors Bit Error Rate Benign / Tampered Deepfake Detector Attacked Image (𝒙a) f(𝒙) Fig. 2. Method Overview: Given an input image x, the watermarking agent θ embeds a watermark wx into the spherical latent space S derived from semantic (CLIP Image) features f(x), producing a watermarked image xw. An attacker η generates an adversarial image xa to disrupt watermark integrity. η uses both benign edits (e.g., JPEG compression, cropping) and semantic-altering manipulations (e.g., face swaps) when generating its attack curriculum. The extractor δ attempts to recover wx from any image x′; failure to extract a valid watermark flags x′ as a potential tampered image, leveraging watermark consistency as a proxy for semantic authenticity. where, Lclip = ∥f(x) −f(x′)∥2 2 (6) Ldir = CrossEntropy(M, M ′), (7) Lext = −BER(M, M ′), (8) where M ′ is the recovered message obtained by decoding the projection vector P = {⟨f(x′), di⟩}L i=1 and BER(., .) is Bit Error Rate. See Sec. III-D for details. C. Watermark Attacker The objective of the watermark attacker, η, is to learn an optimal policy to destroy the embedded watermark. One of our key innovations is its ability to compose complex attacks using a predefined set of benign/traditional image transformations and malicious/generative transformations, effectively learning a dynamic attack curriculum which includes both type and intensity of each attack. The following subsections illustrate: (i) the process of composing a combinatorial attack, (ii) how the watermark attacker agent is trained, and (iii) how it is incentivized to identify attack curricula that lead to substantial semantic drifts and are closer to historically known failure regions in the latent space, where the watermark extractor fails to recover the watermark. 1) Composing a Combinatorial Attack: Let the set of available attacks, including the benign and malicious trans- formation, be denoted as A = {a1, . . . , al}. The watermark attacker agent, η, receives the CLIP features, f(x′), of the watermarked image, x′, as input and outputs two vectors: logits for attack selection and normalized strength parameters τ as: (logits, τ) = η(f(x′)). (9) These logits are passed',\n",
       "  'denoted as A = {a1, . . . , al}. The watermark attacker agent, η, receives the CLIP features, f(x′), of the watermarked image, x′, as input and outputs two vectors: logits for attack selection and normalized strength parameters τ as: (logits, τ) = η(f(x′)). (9) These logits are passed through a sigmoid function (denoted by σ to convert them into probabilities: B = σ(logits) ∈[0, 1]L. (10) Then, for each probability, bl , we sample a binary action, ak, from a Bernoulli distribution as: al ∼Bernoulli(Pl) ∀l ∈{1, ..., L} (11) The result is a binary action vector a ∈{0, 1}L in which each element is independently sampled, creating a combina- torial attack strategy. Simultaneously, the strength vector τ ∈[0, 1]L determines the intensity for each attack. Each normalized value τl is mapped to a meaningful parameter, paraml, in the selected attack type, al, and within a predefined range [minl, maxl] as: paraml = minl + (maxl −minl) · τl. (12) 2) Learning an Attack Curriculum: We train our watermark attacker agent using reinforcement learning to learn an optimal policy ψ that maximizes the likelihood of watermark extraction failure, thereby ensuring the success of the attack. 6 Specifically, the primary reward R for the watermark attacker agent can be formulated as the failure of the Watermark Extractor, measured by the Binary Cross-Entropy (BCE) loss between the extracted message ˜ M and the original message M as: Rfailure = BCE( ˜ M, M). (13) A strong and adaptive attack policy would drive the water- marker to learn more resilient and sophisticated watermarking strategies. In contrast, the simplistic objective in Eq. 13, which focused solely on extractor failure, lacks the distinction required to drive meaningful improvements. As such, an advanced reward signal formulation is required to incentivize the attacker to pursue diverse and exploratory policies that provide richer adversarial signals. 3) Promoting Semantic Drift and Targeted Shifts Toward Known Failure Regions: We propose to augment the re- ward with bonuses to encourage exploration and exploitation. Specifically, we define a curiosity reward which is propor- tional to the squared Euclidean distance between the CLIP features of the image before and after the attack. This could be written as: Rcuriosity = ∆∥f(x′) −f(a(x′))∥2, (14) where ∆is a scaling factor. This reward incentivizes the attacker to discover attacks that cause the most significant semantic disruption, which are often the most challenging for a watermark to survive. In addition, we introduce a proximity reward that encour- ages the attacker to perturb the image toward regions of the latent space known to challenge the watermark extractor. In particular, we introduce a memory buffer, J, that stores the la- tent representations, f(a(x′)), of images that have historically caused failures in watermark extraction. Then, the potential, ρ, of a new attacked image a(x′′) can be defined as its proximity to its closest feature vector stored in the failure memory. Now the attacker’s proximity reward can be defined as inversely proportional to the distance of its attacked image from the closest failure memory embedding. Formally, this could be written as: ρ(a(x′′))',\n",
       "  'attacked image a(x′′) can be defined as its proximity to its closest feature vector stored in the failure memory. Now the attacker’s proximity reward can be defined as inversely proportional to the distance of its attacked image from the closest failure memory embedding. Formally, this could be written as: ρ(a(x′′)) = min fj∈J ∥f(a(x′′)) −fj∥2, Rproximity = ν ρ(a(x′′)) + ϵ, (15) where ν is a scaling factor and ϵ is a small constant to ensure numerical stability. Now, the final loss function of the watermark attacker agent can be written as: Lη = −Ea∼ϕ h Rfailure + Rproximity + Rcuriosity + o L X l=1 al i −rH(P) (16) where PL l=1 al is a small regularization term penalizing the number of active actions, H(P) is the entropy of the action distribution P, and promotes exploration in the policy ϕ, and o and r are hyperparameters. It should be noted that Rcuriosity encourages goal-directed exploration while H(P) is used to encourage policy-level exploration such that the agent maintains diversity in action selection, serving complementary purposes in the learning process. D. Watermark Extraction and Deepfake Detection The watermark extractor, δ, is responsible for recovering the message M from a potentially corrupted image x′′. Specifi- cally, given the input x′′, it first extracts CLIP features fx′′, and independently generates the direction vectors, D, from the secret key, K, using the direction generator, µ. Then it produces the projection of fx′′ onto each of the direction vector di ∈D as: p′′ i = ⟨fx′′, di⟩. (17) The resulting vector of projections P ′′ = {p′′ 1, ..., p′′ L} is passed through an MLP, which outputs the logits for the predicted message, M ′. In the proposed framework, the fragility of the watermark serves as the deepfake detection mechanism. Even when the direction generator, µ, the integrity of an input image can be verified at inference time by computing the Bit Error Rate (BER) between the extracted message M ′ and the known orig- inal message M. Since the same trained direction generator µ and secret key K are used during both watermark embedding and extraction, the resulting direction vectors should remain consistent. A high BER indicates potential generative manip- ulation, enabling us to leverage the fragility of the watermark as a deepfake detection signal. Formally, this could be written as: fDEEPFAKE(x′′) = ( True, if BER(M ′, M) > λ False, otherwise (18) where λ is a predefined classification threshold. E. Implementation Details Implementation of this framework is completed using Py- Torch. All the images are resized to a resolution of 224 × 224. For both watermarking agent and attacker agent, Adam [31] optimiser with a learning rate of 1e−4 is used for optimisation. The model is trained for 50 epochs with a batch size of 16 on an NVIDIA A100 GPU. For consistency in comparison, similar to baseline methods, the length of the watermark bits is set to 512 bits. The classification threshold, λ is evaluated experimentally and set to 0.8. The architecture details of the watermarking agent, watermark attacker, direction generator,',\n",
       "  'batch size of 16 on an NVIDIA A100 GPU. For consistency in comparison, similar to baseline methods, the length of the watermark bits is set to 512 bits. The classification threshold, λ is evaluated experimentally and set to 0.8. The architecture details of the watermarking agent, watermark attacker, direction generator, and watermark extractor are provided below. The Direction Generator (µ) receives the 512-D CLIP-Text feature of the secret message. This is processed by a two- layer MLP: a linear layer of 256 dimensions followed by ReLU, and a second linear layer (R256 →R512). The output is L2-normalized along the feature dimension. The watermarker operates in the CLIP feature space. It combines the original CLIP image feature (512 dimensions) and the directional vectors (512 dimensions) into a 1024-D vector. This is processed by a linear layer with 512 hidden 7 units and ReLU activation, and outputs a 512-D feature-space perturbation using Tanh activation, which is then passed to the integrated Image Decoder. The decoder, π, first maps this vector to a higher-dimensional space using a single FC layer (R512 →R4096), reshaping the output to 256 × 4 × 4. This tensor is sequentially upsampled through five transposed convolutional layers, using ReLU (except the final layer’s Tanh) and a stride of 2 (except for the final layer’s stride of 4) to reach the final 3 × 224 × 224 image resolution. The watermark extractor (δ) recovers the message by first projecting the 512-D CLIP features onto a 128-D latent space. This vector is then passed through a two-layer MLP: a linear layer (R128 →R256) with ReLU, followed by a final linear layer (R256 →R512) to output the message logits. The watermark attacker (η) uses a shared two-layer MLP backbone, first layer with 256 hidden units and ReLU acti- vation, and the next layer with 128 hidden units and ReLU activation. This backbone outputs two vectors: a vector of logits for attack selection, and a vector of normalized strength parameters. The logits are passed through a Sigmoid function to yield a probability vector, B. For each probability bl, a binary action al is sampled from a Bernoulli(Pl) distribution, resulting in a binary action vector a ∈{0, 1}L that defines the combinatorial attack strategy. The strength vector τ determines the intensity for each attack. IV. EXPERIMENTS In this section, we present the results of our experiments designed to evaluate and compare the effectiveness of the proposed DeepForgeSeal framework against existing state- of-the-art methods. We begin by detailing the datasets used in our evaluations (Sec. IV-A), followed by a description of the evaluation metrics employed to assess model perfor- mance (Sec. IV-B). The main experimental results, where we benchmark our method against current state-of-the-art approaches, are provided in Sec. IV-C. Additionally, ablation studies conducted to validate the contributions of the learnable directional embedding strategy, adversarial learning pipeline, and novel reward function are discussed in Sec. IV-D. Finally, we analyse the time complexity of the DeepForgeSeal model in Sec. IV-E. A. Datasets Following prior works [6], we use Flickr-Faces-HQ (FFHQ) [32] dataset for training our model, and CelebA',\n",
       "  'contributions of the learnable directional embedding strategy, adversarial learning pipeline, and novel reward function are discussed in Sec. IV-D. Finally, we analyse the time complexity of the DeepForgeSeal model in Sec. IV-E. A. Datasets Following prior works [6], we use Flickr-Faces-HQ (FFHQ) [32] dataset for training our model, and CelebA [33] and CelebA-HQ [34] datasets for evaluating the model and demon- strating its generalisability. The FFHQ dataset contains 70,000 images at a resolution of 1024 × 1024 pixels. In accordance with the dataset authors’ guidelines, the first 60,000 images are designated for training, while the remaining 10,000 are reserved for validation. We follow this protocol and use the training subset of FFHQ to train our model. CelebA comprises 10,000 distinct identities, with each identity represented by 20 images at a 128x128 resolution. CelebA-HQ is a high-quality version of the CelebA dataset, consisting of 30,000 images at a resolution of 1024 × 1024 pixels. Specifically, we followed the official test split provided by the CelebA and CelebA-HQ dataset authors to enable direct comparisons. B. Evaluation Metics We evaluate our method from two key perspectives. First, we assess the visual fidelity of the watermarked images com- pared to the original images using two widely adopted metrics: Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). PSNR provides a quantitative measure of the pixel-level differences, where higher values indicate less distortion and better preservation of image quality. SSIM, on the other hand, evaluates perceptual similarity by considering factors such as luminance, contrast, and structural information, offering a more human-aligned assessment of image quality. Together, these metrics help us determine how effectively our method maintains the integrity of the original image while embedding the watermark. To evaluate the performance of deepfake detection, fol- lowing prior works [6], we compute image-level Accuracy (ACC) and F1-Score. Accuracy reflects the overall proportion of correctly classified images, encompassing both correctly identified real and fake samples. The F1-Score, which balances precision and recall, is particularly informative in capturing the trade-off between false alarms (incorrectly classifying real images as fake) and miss detections (failing to identify fake images). Together, these metrics provide a reliable measure of the system’s ability to detect deepfakes while minimizing classification errors. C. Comparisons with Existing State-of-the-art Methods In this section, we report results for the proposed model and compare with the existing state-of-the-art methods considering the visual quality of the watermarking (See Sec. IV-C1), deep- fake detection performance (See Sec. IV-C2), and watermark’s resilience and fragility (See Sec. IV-C3). 1) Visual Quality of Watermarking Approaches: As base- lines we use state-of-the-art methods, including FaceSigns [19], EditGard [10], and Zhao et. al [6]. When choosing our baselines, we ensured that a variety of different deep learning approaches, including adversarial trained models [19], dual-purpose watermarking methods for tamper detection and copyright protection [10], and identity-entangled watermark- ing methods [6], are compared, enabling a comprehensive comparison. TABLE I VISUAL FIDELITY OF DIFFERENT WATERMARKING TECHNIQUES. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD. Model Quality Metrics PSNR (↑) SSIM (↑) FaceSigns [19] 32.03 0.92 Zhao et. al [6] 38.32 0.94 EditGard',\n",
       "  'detection and copyright protection [10], and identity-entangled watermark- ing methods [6], are compared, enabling a comprehensive comparison. TABLE I VISUAL FIDELITY OF DIFFERENT WATERMARKING TECHNIQUES. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD. Model Quality Metrics PSNR (↑) SSIM (↑) FaceSigns [19] 32.03 0.92 Zhao et. al [6] 38.32 0.94 EditGard [10] 45.07 0.96 DeepForgeSeal (ours) 48.39 0.97 Quantitative comparisons for the CelebA-HQ dataset are presented in Tab. I. When comparing the proposed DeepForge- Seal with the previous state-of-the-art methods, we observe that our method has the highest PSNR and SSIM, demonstrat- ing our framework’s ability to embed watermarks stealthily, preserving the image’s visual quality. We attribute this to 8 operating within the high-dimensional semantic space of the input images, which enables subtle yet effective modifications. In addition to the above quantitative comparisons, in Fig. 3 we present a qualitative comparison between FaceSigns [19], EditGard [10], and the proposed DeepForgeSeal method for embedding watermarks in two sample images from the CelebA dataset. Figure 3 supports the findings from our quantitative comparisons, showing that our watermarked images maintain high visual realism by accurately preserving the original textures, hue, and lighting. In contrast, both FaceSigns and EditGuard noticeably alter the image hue, with FaceSigns additionally introducing visible artefacts in the facial region. These results highlight the effectiveness of our proposed framework, which embeds watermarks in the latent space rather than directly in the pixel space, enabling more natural and less intrusive modifications. 2) Deepfake Detection Performance: We compare the pro- posed DeepForgeSeal method with both passive and proac- tive deepfake detection methods. Following [6], we employ CelebA and CelebA-HQ datasets to represent low and high- resolution real image cases and generate deepfakes using a variety of deepfake generation methods, including AttGAN, StarGAN, InfoSwap and SimSwap, CIAGAN, and DeepPri- vacy. These methods allow us to evaluate the performance of the proposed deepfake detection framework against a wide range of malicious manipulations, including identity swap- ping and face anonymization techniques. In our experimental pipeline, for passive deepfake detection methods, we use non-watermarked images from the CelebA and CelebA-HQ datasets as real images. The above deepfake generation tech- niques are then applied to create their fake counterparts. For proactive deepfake detection methods, including the proposed DeepForgeSeal approach, we first watermark the images from the CelebA and CelebA-HQ datasets, and then generate the corresponding fake images using the same deepfake generation methods. Following [6], as baselines for passive deepfake detection we use the state-of-the-art BTS [35], CD [36], Bonettini et. al [37], PF [38], RFM [39], SBI [40], and CBO-DD [41]. In addition, the state-of-the-art FaceSigns [19], EditGard [10], and Zhao et al. [6], which are proactive deepfake detection methods, have also been used for comprehensiveness. Tab. II summarises the comparison results for deepfake detection in CelebA and CelebA-HQ datasets. It is evident that passive deepfake detection methods generally struggle to generalise across different deepfake generation techniques. Among the approaches evaluated, only methods such as SBI [40] and CBO-DD [41] consistently demonstrate strong performance across all considered generation methods. In contrast, most other passive detection techniques perform well against',\n",
       "  'datasets. It is evident that passive deepfake detection methods generally struggle to generalise across different deepfake generation techniques. Among the approaches evaluated, only methods such as SBI [40] and CBO-DD [41] consistently demonstrate strong performance across all considered generation methods. In contrast, most other passive detection techniques perform well against a limited subset of generation methods but show significant performance degradation when applied to others. This limitation stems from their training approach, which focuses on identifying artefacts specific to the generation techniques used during training. Consequently, these methods tend to perform poorly when tested on deepfakes produced by unfamiliar or diverse generation techniques. When comparing the results of the proactive detection methods FaceSigns [19], Zhao et. al [6], and EditGard [10] with the proposed DeepForgeSeal method, we see that our method has been able to achieve significant detection accuracy across all considered generation methods. Although state-of- the-art methods such as Zhao et. al [6], and EditGard [10] have been able to achieve impressive performance against deepfake generation methods such as AttGAN and DeepPri- vacy, their performance degrade when tested against Deep- Privacy and StarGAN2 generation methods. In contrast, our proposed DeepForgeSeal method consistently demonstrates superior detection performance across all evaluated deepfake generation techniques. We attribute this to the learnable wa- termarking strategy embedded within the framework, which enables explicit control over message encoding and extraction. Additionally, the adversarial reinforcement learning pipeline allows the watermarking agent to actively observe and respond to malicious attacks, dynamically adapting its watermarking strategy. Together, these components contribute to DeepForge- Seal’s strong generalisation capability across a wide range of deepfake generation methods. To further demonstrate the strong generalisation capabilities of the proposed method, we conducted an additional evaluation involving a series of facial attribute manipulations and face reenactments using watermarked images. For this experiment, we employed several state-of-the-art generative AI (GenAI) video synthesis tools, including OpenAI SORA, Gemini Veo 3, StyleMask, and Hyper Reenact, and tested the deepfake detection performance of our DeepForgeSeal approach us- ing these manipulated images. It is important to note that the DeepForgeSeal model was not exposed to these types of manipulations during training. Fig 4 presents qualitative visualisations, highlighting the faces our method successfully identified as fake. For video-level decisions, we computed the average of frame-level detection scores. As anticipated, our model effectively detected manipulations generated by the latest GenAI video synthesis tools, demonstrating its robustness and ability to provide comprehensive protection against malicious deepfakes. For additional visual examples, please refer to the supplementary material. 3) Watermark Resilience and Fragility Against Attacks: In this section, we compare the resilience of our Deep- ForgeSeal watermarking methods with the current state-of- the-art methods under different attacks against watermarks. For comprehensiveness, we consider both traditional benign attacks, such as cropping, compression, and color changes, as well as the learnable benign attacks proposed by Zhao et. al [42] and Lukas et. al [43]. In addition, to quantitatively assess the fragility against malicious image manipulations, we generate attacks using state-of-the-art GAN-based face swapping and reenactment techniques, including SimSwap [44], UniFace [45], FaceDancer [46], and HyperReenact [47]. The evaluation',\n",
       "  'as the learnable benign attacks proposed by Zhao et. al [42] and Lukas et. al [43]. In addition, to quantitatively assess the fragility against malicious image manipulations, we generate attacks using state-of-the-art GAN-based face swapping and reenactment techniques, including SimSwap [44], UniFace [45], FaceDancer [46], and HyperReenact [47]. The evaluation results are presented in Tab. III. As baselines, we use the state-of-the-art FaceSigns [19] and EditGard [10] semi-fragile watermarking methods. Following prior works [19], [10], we measure the Bit Recovery Accuracy (BRA) as the evaluation metric, where we expect a higher BRA against benign and lower BRA against malicious transforms to achieve the goal of semi-fragile watermarking. As reported 9 Original Watermarked Attacked DeepForgeSeal(Ours) Face Signs Edit Gard PSNR: 46.21 dB PSNR: 32.89 dB PSNR: 35.07 dB Bit Acc: 100% Bit Acc: 62.50% Bit Acc: 60.94% JPEG compression, Noise, Crop, Jitter, Affine transform Original Watermarked Attacked PSNR: 48.58 dB PSNR: 33.01 dB PSNR: 36.63 dB Bit Acc: 100% Bit Acc: 31.25% Bit Acc: 48.44% JPEG compression, Noise, Jitter, Affine transform Fig. 3. Qualitative Results of Visual Quality: A comparison between the existing state-of-the-art models, FaceSigns [19], EditGard [10], and the proposed DeepForgeSeal method for watermarking two sample images from the CelebA dataset [33]. TABLE II ACCURACY (↑) AND F1 SCORES (↑) OF DIFFERENT METHODS’ DEEPFAKE DETECTION RESULTS. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD. Detection methods Low Resolutions (CelebA) High Resolutions (CelebA-HQ) AttGAN CIAGAN DeepPrivacy InfoSwap SimSwap StarGAN2 AttGAN CIAGAN DeepPrivacy InfoSwap SimSwap StarGAN2 BTS [35] 0.86/0.87 0.51/0.66 0.50/0.66 0.49/0.66 0.51/0.67 0.53/0.67 0.86/0.87 0.50/0.66 0.50/0.66 0.49/0.65 0.50/0.66 0.55/0.68 CD [36] 0.88/0.86 0.51/0.03 0.51/0.01 0.54/0.17 0.51/0.01 0.78/0.71 0.81/0.77 0.51/0.04 0.52/0.07 0.52/0.07 0.52/0.06 0.84/0.81 Bonettini et. al [37] 0.59/0.69 0.62/0.62 0.58/0.68 0.57/0.64 0.60/0.71 0.63/0.63 0.53/0.68 0.65/0.62 0.56/0.69 0.51/0.66 0.55/0.69 0.49/0.65 PF [38] 0.76/0.79 0.51/0.66 0.52/0.65 0.57/0.68 0.54/0.67 0.99/0.98 0.75/0.79 0.51/0.66 0.55/0.68 0.56/0.69 0.54/0.68 0.98/0.97 RFM [39] 0.50/0.67 0.51/0.67 0.51/0.67 0.50/0.67 0.51/0.67 0.50/0.67 0.50/0.67 0.50/0.67 0.51/0.67 0.50/0.67 0.50/0.67 0.50/0.67 SBI [40] 0.79/0.82 0.77/0.8 0.78/0.82 0.77/0.81 0.78/0.81 0.72/0.75 0.80/0.80 0.72/0.78 0.78/0.80 0.76/0.77 0.83/0.84 0.69/0.70 CBO-DD [41] 0.84/0.84 0.91/0.93 0.86/0.87 0.79/0.85 0.84/0.88 0.85/0.85 0.83/0.86 0.92/0.93 0.86/0.87 0.80/0.85 0.89/0.91 0.82/0.84 FaceSigns [19] 0.91/0.93 0.83/0.84 0.91/0.95 0.93/0.95 0.95/0.95 0.96/0.98 0.87/0.88 0.92/0.93 0.80/0.83 0.91/0.91 0.82/0.82 0.84/0.87 Zhao et. al [6] 0.94/0.94 0.87/0.86 0.98/0.98 0.98/0.98 0.97/0.98 0.82/0.84 0.94/0.94 0.85/0.82 0.99/0.98 0.99/0.99 0.98/0.98 0.85/0.87 EditGard [10] 0.96/0.97 0.88/0.88 0.96/0.98 0.94/0.97 0.97/0.98 0.91/0.93 0.95/0.96 0.87/0.91 0.97/0.99 0.99/0.99 0.98/0.99 0.93/0.95 DeepForgeSeal (Ours) 0.96/0.98 0.92/0.95 0.99/0.99 0.98/0.99 0.98/0.98 0.99/0.99 0.96/0.98 0.95/0.96 0.99/0.99 0.99/0.99 0.99/0.99 0.98/0.97 TABLE III BIT RECOVERY ACCURACY (BRA) OF BASELINE FACESIGNS [19] AND EDITGARD [10] WATERMARKING METHODS AND PROPOSED DEEPFORGESEAL METHOD UNDER BENIGN AND MALICIOUS TRANSFORMATIONS. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD. Method BRA (%) - Benign Transforms (↑) BRA (%) - Malicious Transforms (↓) JPG Noise Crop Jitter Affine Zhao et. al [42] Lukas et. al [43] SimSwap UniFace FaceDancer HyperReenact FaceSigns [19] 0.87 0.73 0.72 0.91 0.96 0.87 0.88 0.52 0.49 0.62 0.51 EditGard [10] 0.95 0.78 0.96 0.96 0.99 0.92 0.95 0.48 0.49 0.44 0.50 DeepForgeSeal (ours) 1.00 0.98 0.98 0.99 1.00 0.98 0.99 0.24 0.11 0.10 0.06 10 OpenAI - SORA Style Mask FaceDancer Google Veo 3',\n",
       "  'HyperReenact FaceSigns [19] 0.87 0.73 0.72 0.91 0.96 0.87 0.88 0.52 0.49 0.62 0.51 EditGard [10] 0.95 0.78 0.96 0.96 0.99 0.92 0.95 0.48 0.49 0.44 0.50 DeepForgeSeal (ours) 1.00 0.98 0.98 0.99 1.00 0.98 0.99 0.24 0.11 0.10 0.06 10 OpenAI - SORA Style Mask FaceDancer Google Veo 3 Hyper Reenact OpenAI - SORA Fig. 4. Qualitative results of our DeepForgeSeal model when tested on videos generated by completely unseen image and video deepfake generation techniques (OpenAI SORA, Gemini Veo 3, StyleMask, and Hyper Reenact). We visualise the sample images showing the detected face, along with the original (bonafide) watermarked image before the manipulation (provided in bottom left). For additional visualisations, please refer to the supplementary material. in Tab. III, we observe that both baseline models perform poorly against benign image manipulations, demonstrating that these watermarking techniques are too fragile to withstand the benign image transformations that users can apply in the real world. At the same time, both baseline techniques demonstrate less fragility towards the malicious, content- altering manipulations that the considered face swapping and reenactment techniques have generated. These evaluations clearly demonstrate the limitations of the existing state-of- the-art watermarking techniques and the superiority of the DeepForgeSeal paradigm that enables us to learn a strategy that optimally balances watermark resilience and fragility. D. Ablation Evaluations We conducted a series of ablation studies to systematically analyse the impact of the individual innovations that our DeepForgeSeal framework proposes. Several design choices contribute to the robustness of our model: i) the proposed learnable directional embedding strategy that uses a latent sphere; ii) the proposed adversarial learning pipeline with composed attacks with a learnable attack curriculum; and iii) the novel reward function that promotes semantic drift and targeted shifts toward known failure regions. All ablation experiments were conducted on the CelebA-HQ dataset, and for testing the ablation models, we used the validation set of CelebA-HQ. As evaluation metrics, we use PSNR and average BRA against benign and malicious manipulations. 1) Effects of Learnable Directional Embedding in Latent Sphere: To study the effect of the proposed learnable di- rectional embedding strategy that uses a latent sphere, we generated two ablation variants of the proposed DeepForge- Seal model: i) DeepForgeSeal - w/o [D] - a model with naive latent embedding in which we embed the message directly in the latent space without directional encoding, and ii) DeepForgeSeal - FD - a model with fixed directional embeddings. Results of this comparison are shown in Tab. IV. We observe 11 TABLE IV EFFECT OF LEARNABLE DIRECTIONAL EMBEDDING IN LATENT SPHERE. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD. Model PSNR (↑) BRA - Benign (↑) BRA - Malicious (↓) DeepForgeSeal - w/o [D] 48.43 0.78 0.42 DeepForgeSeal - FD 46.58 0.86 0.19 DeepForgeSeal 48.12 0.99 0.12 that the learnable directional embedding strategy is an integral part of the proposed framework. Specifically, we observe that the naive latent embedding approach, where the message is directly embedded into the latent space, leads to a reduction in PSNR, distorts the natural image characteristics, and dimin- ishes the watermark’s resilience against benign',\n",
       "  'learnable directional embedding strategy is an integral part of the proposed framework. Specifically, we observe that the naive latent embedding approach, where the message is directly embedded into the latent space, leads to a reduction in PSNR, distorts the natural image characteristics, and dimin- ishes the watermark’s resilience against benign image transfor- mations. The introduction of directional embedding enhances the robustness of the watermark (see the row corresponding to DeepForgeSeal - FD). Notably, the experimental results demonstrate that the learnable directional embedding strategy, which encodes the message within a latent sphere, achieves the most favorable balance between resilience and fragility, while preserving image fidelity. These experiments confirm the utility of our learnable directional embedding procedure. 2) Effects of Composing a Combinatorial Attack and Learn- ing an Attack Curriculum: The effectiveness of the proposed adversarial learning pipeline with composed attacks with a learnable attack curriculum is evaluated in this experiment. We generated eight ablation variants of the proposed Deep- ForgeSeal model: i) DeepForgeSeal - w/o η - a model without a watermark attacker agent, ii) DeepForgeSeal - w/o A - a model in which one fixed attack type (no composition or learning of an attack curriculum) is applied, iii) DeepForgeSeal - |A| - a model in which multiple attacks are applied in a fixed sequence, iv) DeepForgeSeal - ˜ A - a model in which multiple random combinations of attacks applied, v) Deep- ForgeSeal - a1, τ ↑- a model in which a single attack type with gradually increasing strength is applied, vi) DeepForgeSeal - A→- a model which progressively introduces more complex combinations of attacks, vii) DeepForgeSeal - a1, τ - a model with single attack type with learnable strength parameter, viii) DeepForgeSeal - [A] - a model with a predefined attack curriculum without adaptation to agent performance. TABLE V EFFECTS OF COMPOSING A COMBINATORIAL ATTACK AND LEARNING AN ATTACK CURRICULUM. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD. Model PSNR (↑) BRA - Benign (↑) BRA - Malicious (↓) DeepForgeSeal - w/o η 32.11 0.61 0.59 DeepForgeSeal - w/o A 41.58 0.72 0.55 DeepForgeSeal - |A| 46.20 0.82 0.37 DeepForgeSeal - ˜ A 47.15 0.87 0.31 DeepForgeSeal - a1, τ ↑ 47.12 0.76 0.42 DeepForgeSeal - A→ 48.09 0.92 0.22 DeepForgeSeal - a1, τ 48.05 0.79 0.38 DeepForgeSeal - [A] 47.11 0.89 0.28 DeepForgeSeal 48.12 0.99 0.12 From the results in Tab. V, we can confirm that there are benefits to incorporating both combinatorial attacks with a learnable attack curriculum as an adversary in the proposed framework, which is evidenced by the rows in Tab. V corre- sponding to models DeepForgeSeal - A→, and DeepForgeSeal - a1, τ. Most importantly, when the attacker agent has the knowledge of the current vulnerabilities of the victim model (i.e. watermarking agent), it helps the attacker dynamically adapt its strategy and generate a more complex curriculum of attacks, in turn helping the victim to understand the vul- nerabilities of its watermarking strategy and rectifying those. We believe this is the reason for the poor performance of the DeepForgeSeal - [A] model, in which a predefined attack',\n",
       "  'dynamically adapt its strategy and generate a more complex curriculum of attacks, in turn helping the victim to understand the vul- nerabilities of its watermarking strategy and rectifying those. We believe this is the reason for the poor performance of the DeepForgeSeal - [A] model, in which a predefined attack curriculum was used without adaptation to the victim’s perfor- mance. Furthermore, rows corresponding to DeepForgeSeal - |A|, and DeepForgeSeal - ˜ A confirm the importance of using a combinatorial attack in which a learnable agent determines the attack sequence. Therefore, using this experiment, we can confirm the necessity of both combinatorial attacks and learning an attack curriculum to enhance the robustness of the watermark, while simultaneously preserving image fidelity. 3) Effects of Promoting Semantic Drift and Targeted Shifts Toward Known Failure Regions: To better establish the con- tributions of the proposed reward function that promotes semantic drift and targeted shifts toward known failure regions, we conducted a further ablation experiment. Specifically, three ablation variants of the proposed model were generated: i) DeepForgeSeal - w/o [Rproximity, Rcuriosity] - proposed model without both curiosity reward and proximity reward, ii) Deep- ForgeSeal - w/o Rproximity - proposed model with curiosity reward but without proximity reward, iii) DeepForgeSeal - w/o Rcuriosity - proposed model without curiosity reward but with proximity reward. TABLE VI EFFECTS OF THE PROMOTING SEMANTIC DRIFT AND TARGETED SHIFTS TOWARD KNOWN FAILURE REGIONS. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD. Model PSNR (↑) BRA - Benign (↑) BRA - Malicious (↓) DeepForgeSeal - w/o [Rproximity, Rcuriosity] 48.10 0.82 0.35 DeepForgeSeal - w/o Rproximity 48.12 0.91 0.24 DeepForgeSeal - w/o Rcuriosity 48.08 0.94 0.21 DeepForgeSeal 48.12 0.99 0.12 The results presented in Tab. VI confirm the need for the proposed innovative reward function. In particular, the ablation model without both the multi-head retrieval mechanism and the dynamic masking procedure (see the row corresponding to DeepForgeSeal - w/o [Rproximity, Rcuriosity] in Tab. VI) struggles to achieve a good watermark robustness. This is because the attacker agent has learned a sub-optimal attacking policy, marking the watermarking agent unprepared for real-world attacks against watermarks. Comparing rows corresponding to DeepForgeSeal - w/o Rproximity and DeepForgeSeal - w/o Rcuriosity models in Tab. VI, we can confirm that proximity and curiosity rewards complement each other. By leveraging underexplored regions of the latent space alongside known failure zones encountered during watermark retrieval, the attacker agent can formulate a sophisticated attack policy. This, in turn, drives the watermarker agent to refine and strengthen its watermarking strategy. These observations strongly validate the importance of the proposed reward function that promotes semantic drift and targeted shifts toward known failure regions E. Time Complexity We perform a detailed time complexity analysis using two state-of-the-art watermarking techniques: FaceSigns [19] and 12 EditGard [10]. These methods were selected due to the public availability of their implementations. Using a single NVIDIA A100 GPU, we measured the average time required to generate 100 image-level predictions, including preprocessing, feature extraction, watermark extraction, and deepfake classifications. The results of this analysis are summarized in Table VII. Model Total Params (M) Runtime',\n",
       "  'selected due to the public availability of their implementations. Using a single NVIDIA A100 GPU, we measured the average time required to generate 100 image-level predictions, including preprocessing, feature extraction, watermark extraction, and deepfake classifications. The results of this analysis are summarized in Table VII. Model Total Params (M) Runtime (in Sec) FaceSigns [19] 7.58 7.1205 EditGard [10] 9.32 8.6201 DeepForgeSeal (Ours) 8.27 7.8535 TABLE VII TIME COMPLEXITY ANALYSIS: THE PARAMETER COUNT IN MILLIONS AND THE TIME TAKEN TO GENERATE 100 IMAGE-LEVEL PREDICTIONS USING A SINGLE NVIDIA A100 GPU V. CURRENT LIMITATIONS AND FUTURE DIRECTIONS We observe two limitations of DeepForgeSeal: (i) The pro- posed system has been developed for watermarking image data and has not yet been tested for multimodal data or other modal- ities such as audio data. Given the growing prevalence of mul- timodal deepfakes, it is crucial to assess the applicability of the proposed learnable watermarking strategy, particularly its use of high-dimensional semantic spaces, multi-agent adversarial reinforcement learning, and innovative reward functions, for other data types. Future research could also focus on extending this framework to support multimodal watermarking. One promising direction would be to design a more sophisticated multi-agent architecture that facilitates collaboration among watermarking agents. Such a system could more effectively navigate and exploit the multimodal latent space, enhancing semantic stealth and robustness across diverse modalities. (ii) While our evaluations (Tab. VII) indicate that the proposed model exhibits computational complexity comparable to ex- isting state-of-the-art watermarking systems, it has not been tested on resource-constrained environments such as edge devices (e.g., smartphones). The reliance on high-dimensional semantic embeddings and multi-agent interactions increases the computational demands, potentially limiting scalability and real-time performance. To address these limitations, future research could explore model compression techniques such as pruning, quantization, or knowledge distillation. These strate- gies may help reduce inference time and memory footprint while preserving detection accuracy, thereby improving the feasibility of deploying the DeepForgeSeal model in edge scenarios. VI. CONCLUSION This paper presented DeepForgeSeal, a Latent Space-Driven Semi-Fragile Watermarking using Multi-Agent Adversarial Reinforcement Learning (MAARL) for accurate detection of face deepfakes. We demonstrate that the proposed learnable watermarking agent achieves enhanced robustness and seman- tic stealth through a directional embedding strategy in the latent space. The MAARL paradigm enabled the agent to dy- namically balance resilience and fragility by interacting with a curriculum of benign and adversarial manipulations introduced by an attacker agent. To intensify this adversarial interaction, we designed reward functions that encouraged semantic drift and targeted perturbations toward known failure regions. This incentivised the attacker agent to develop sophisticated attack strategies, which in turn drove the watermarking agent to refine and strengthen its embedding approach, resulting in a more resilient and adaptive watermarking system. Exten- sive experiments were conducted on two public benchmarks: CelebA and CelebA-HQ, which demonstrated the ability of the proposed framework to outperform the current state-of-the-art algorithms by significant margins. ACKNOWLEDGMENT The research was supported by the Australian Government through the Office of National Intelligence Postdoctoral Grant awarded to the primary author under Project NIPG-2024-022. REFERENCES [1] T. Hunter, “Princess catherine cancer video spawns',\n",
       "  'demonstrated the ability of the proposed framework to outperform the current state-of-the-art algorithms by significant margins. ACKNOWLEDGMENT The research was supported by the Australian Government through the Office of National Intelligence Postdoctoral Grant awarded to the primary author under Project NIPG-2024-022. REFERENCES [1] T. Hunter, “Princess catherine cancer video spawns fresh round of ai conspiracies,” 2024, accessed on 31 09, 2025. [Online]. Available: https://www.washingtonpost.com/technology/2024/ 03/27/kate-middleton-video-cancer-ai/ [2] W. Galston, “Is seeing still believing? the deepfake challenge to truth in politics,” 2020, accessed on 09 23, 2025. [Online]. Available: https://www.brookings.edu/articles/ is-seeing-still-believing-the-deepfake-challenge-to-truth-in-politics/ [3] N. S. Agenc, “Nsa, u.s. federal agencies advise on deepfake threats,” 2023, accessed on 08 23, 2025. [Online]. Available: https://www. nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/ Article/3523329/nsa-us-federal-agencies-advise-on-deepfake-threats/ [4] T. Fernando, D. Priyasad, S. Sridharan, A. Ross, and C. Fookes, “Face deepfakes–a comprehensive review,” arXiv preprint arXiv:2502.09812, 2025. [5] M. Mustak, J. Salminen, M. M¨antym¨aki, A. Rahman, and Y. K. Dwivedi, “Deepfakes: Deceptions, mitigations, and opportunities,” Journal of Business Research, vol. 154, p. 113368, 2023. [6] Y. Zhao, B. Liu, M. Ding, B. Liu, T. Zhu, and X. Yu, “Proactive deepfake defence via identity watermarking,” in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2023, pp. 4602– 4611. [7] T. Wang, H. Cheng, M.-H. Liu, and M. Kankanhalli, “Fractalforensics: Proactive deepfake detection and localization via fractal watermarks,” arXiv preprint arXiv:2504.09451, 2025. [8] X. Wu, X. Liao, and B. Ou, “Sepmark: Deep separable watermarking for unified source tracing and deepfake detection,” in Proceedings of the 31st ACM International Conference on Multimedia, 2023, pp. 1190–1201. [9] X. Zhang, Z. Tang, Z. Xu, R. Li, Y. Xu, B. Chen, F. Gao, and J. Zhang, “Omniguard: Hybrid manipulation localization via augmented versatile deep image watermarking,” in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 3008–3018. [10] X. Zhang, R. Li, J. Yu, Y. Xu, W. Li, and J. Zhang, “Editguard: Versatile image watermarking for tamper localization and copyright protection,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 11 964–11 974. [11] A. Haliassos, K. Vougioukas, S. Petridis, and M. Pantic, “Lips don’t lie: A generalisable and robust approach to face forgery detection,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 5039–5049. [12] S. Agarwal, H. Farid, Y. Gu, M. He, K. Nagano, and H. Li, “Protecting world leaders against deep fakes.” in CVPR workshops, vol. 1, no. 38, 2019. [13] S. Fernandes, S. Raj, E. Ortiz, I. Vintila, M. Salter, G. Urosevic, and S. Jha, “Predicting heart rate variations of deepfake videos using neural ode,” in Proceedings of the IEEE/CVF international conference on computer vision workshops, 2019, pp. 0–0. [14] R. Wang, F. Juefei-Xu, M. Luo, Y. Liu, and L. Wang, “Faketagger: Ro- bust safeguards against deepfake dissemination via provenance tracking,” in Proceedings of the 29th ACM international conference on multimedia, 2021, pp. 3546–3555. 13 [15] T. Wang, M. Huang, H. Cheng, B. Ma, and Y. Wang, “Robust identity perceptual watermark against deepfake face swapping,” arXiv preprint arXiv:2311.01357, 2023. [16] T. Wang, M. Huang, H. Cheng, X. Zhang, and Z. Shen, “Lampmark: Proactive deepfake',\n",
       "  'the 29th ACM international conference on multimedia, 2021, pp. 3546–3555. 13 [15] T. Wang, M. Huang, H. Cheng, B. Ma, and Y. Wang, “Robust identity perceptual watermark against deepfake face swapping,” arXiv preprint arXiv:2311.01357, 2023. [16] T. Wang, M. Huang, H. Cheng, X. Zhang, and Z. Shen, “Lampmark: Proactive deepfake detection via training-free landmark perceptual wa- termarks,” in Proceedings of the 32nd ACM International Conference on Multimedia, 2024, pp. 10 515–10 524. [17] Y. Yang, C. Liang, H. He, X. Cao, and N. Z. Gong, “Faceguard: Proactive deepfake detection,” arXiv preprint arXiv:2109.05673, 2021. [18] N. Beuve, W. Hamidouche, and O. D´eforges, “Waterlo: Protect images from deepfakes using localized semi-fragile watermark,” in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 393–402. [19] P. Neekhara, S. Hussain, X. Zhang, K. Huang, J. McAuley, and F. Koushanfar, “Facesigns: Semi-fragile watermarks for media authenti- cation,” ACM Transactions on Multimedia Computing, Communications and Applications, vol. 20, no. 11, pp. 1–21, 2024. [20] A. Wachi, “Failure-scenario maker for rule-based agent using multi- agent adversarial reinforcement learning and its application to au- tonomous driving,” Twenty-Eighth International Joint Conference on Artificial Intelligence, 2019. [21] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual multi-agent policy gradients,” in Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1, 2018. [22] J. Wang, W. Xu, Y. Gu, W. Song, and T. C. Green, “Multi-agent reinforcement learning for active voltage control on power distribution networks,” Advances in neural information processing systems, vol. 34, pp. 3271–3284, 2021. [23] Z. Peng, Q. Li, K. M. Hui, C. Liu, and B. Zhou, “Learning to simu- late self-driven particles system with coordinated policy optimization,” Advances in neural information processing systems, vol. 34, pp. 10 784– 10 797, 2021. [24] M. Kouzehgar, M. Meghjani, and R. Bouffanais, “Multi-agent reinforce- ment learning for dynamic ocean monitoring by a swarm of buoys,” in Global Oceans 2020: Singapore–US Gulf Coast. IEEE, 2020, pp. 1–8. [25] J. Wang, Z. Ren, B. Han, J. Ye, and C. Zhang, “Towards understanding cooperative multi-agent q-learning with value factorization,” Advances in Neural Information Processing Systems, vol. 34, pp. 29 142–29 155, 2021. [26] S. Li, Y. Wu, X. Cui, H. Dong, F. Fang, and S. Russell, “Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient,” in Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, 2019, pp. 4213–4220. [27] T. van der Heiden, C. Salge, E. Gavves, and H. van Hoof, “Robust multi- agent reinforcement learning with social empowerment for coordination and communication,” arXiv preprint arXiv:2012.08255, 2020. [28] A. Bukharin, Y. Li, Y. Yu, Q. Zhang, Z. Chen, S. Zuo, C. Zhang, S. Zhang, and T. Zhao, “Robust multi-agent reinforcement learning via adversarial regularization: Theoretical foundation and stable algorithms,” Advances in neural information processing systems, vol. 36, pp. 68 121– 68 133, 2023. [29] L. Yuan, F. Chen, Z. Zhang, and Y. Yu, “Communication-robust multi- agent learning by adaptable auxiliary multi-agent adversary generation,” Frontiers of Computer Science, vol. 18, no. 6, p. 186331, 2024. [30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh,',\n",
       "  'systems, vol. 36, pp. 68 121– 68 133, 2023. [29] L. Yuan, F. Chen, Z. Zhang, and Y. Yu, “Communication-robust multi- agent learning by adaptable auxiliary multi-agent adversary generation,” Frontiers of Computer Science, vol. 18, no. 6, p. 186331, 2024. [30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable visual models from natural language supervision,” in International conference on machine learning. PmLR, 2021, pp. 8748–8763. [31] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014. [32] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture for generative adversarial networks,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 4401– 4410. [33] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 3730–3738. [34] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of gans for improved quality, stability, and variation,” arXiv preprint arXiv:1710.10196, 2017. [35] Y. He, N. Yu, M. Keuper, and M. Fritz, “Beyond the spectrum: Detecting deepfakes via re-synthesis,” arXiv preprint arXiv:2105.14376, 2021. [36] S.-Y. Wang, O. Wang, R. Zhang, A. Owens, and A. A. Efros, “Cnn- generated images are surprisingly easy to spot... for now,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 8695–8704. [37] N. Bonettini, E. D. Cannas, S. Mandelli, L. Bondi, P. Bestagini, and S. Tubaro, “Video face manipulation detection through ensemble of cnns,” in 2020 25th international conference on pattern recognition (ICPR). IEEE, 2021, pp. 5012–5019. [38] L. Chai, D. Bau, S.-N. Lim, and P. Isola, “What makes fake images detectable? understanding properties that generalize,” in European con- ference on computer vision. Springer, 2020, pp. 103–120. [39] C. Wang and W. Deng, “Representative forgery mining for fake face detection,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 14 923–14 932. [40] K. Shiohara and T. Yamasaki, “Detecting deepfakes with self-blended images,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 18 720–18 729. [41] T. Fernando, C. Fookes, S. Sridharan, and S. Denman, “Cross-branch orthogonality for improved generalization in face deepfake detection,” arXiv preprint arXiv:2505.04888, 2025. [42] X. Zhao, K. Zhang, Z. Su, S. Vasan, I. Grishchenko, C. Kruegel, G. Vi- gna, Y.-X. Wang, and L. Li, “Invisible image watermarks are provably removable using generative ai,” in Advances in Neural Information Processing Systems, 2024. [43] N. Lukas, A. Diaa, L. Fenaux, and F. Kerschbaum, “Leveraging op- timization for adaptive attacks on image watermarks,” The Twelfth International Conference on Learning Representations (ICLR 2024), 2024. [44] R. Chen, X. Chen, B. Ni, and Y. Ge, “Simswap: An efficient framework for high fidelity face swapping,” in MM ’20: The 28th ACM International Conference on Multimedia, 2020. [45] C. Xu, J. Zhang, Y. Han, G. Tian, X. Zeng, Y. Tai, Y. Wang, C. Wang, and Y. Liu, “Designing one unified framework',\n",
       "  'Chen, B. Ni, and Y. Ge, “Simswap: An efficient framework for high fidelity face swapping,” in MM ’20: The 28th ACM International Conference on Multimedia, 2020. [45] C. Xu, J. Zhang, Y. Han, G. Tian, X. Zeng, Y. Tai, Y. Wang, C. Wang, and Y. Liu, “Designing one unified framework for high-fidelity face reenactment and swapping,” in European conference on computer vision. Springer, 2022, pp. 54–71. [46] F. Rosberg, E. E. Aksoy, F. Alonso-Fernandez, and C. Englund, “Facedancer: Pose-and occlusion-aware high fidelity face swapping,” in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2023, pp. 3454–3463. [47] S. Bounareli, C. Tzelepis, V. Argyriou, I. Patras, and G. Tzimiropoulos, “Hyperreenact: one-shot reenactment via jointly learning to refine and retarget faces,” in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 7149–7159. Tharindu Fernando received his BSc (special degree in computer science) from the University of Peradeniya, Sri Lanka, and his PhD from Queens- land University of Technology (QUT), Australia. He is currently a Postdoctoral Research Fellow in the Signal Processing, Artificial Intelligence, and Vision Technologies (SAIVT) research program at the School of Electrical Engineering and Robotics at Queensland University of Technology (QUT). He is a recipient of the 2019 QUT University Award for Outstanding Doctoral Thesis, the QUT Early Career Researcher Award in 2022, the QUT Faculty of Engineering Early Career Achievement Award in 2024, and the 2024 National Intelligence Post-Doctoral Grant. His research interests include Artificial Intelligence, Computer Vision, Deep Learning, Bio Signal Processing, and Video Analytics. 14 Clinton Fookes (Senior Member, IEEE) received the B.Eng. in Aerospace/Avionics, the MBA degree, and the Ph.D. degree in computer vision. He is currently the Associate Dean Research, a Professor of Vision and Signal Processing, and Co-Director of the SAIVT Lab (Signal Processing, Artificial Intelli- gence and Vision Technologies) with the Faculty of Engineering at the Queensland University of Tech- nology, Brisbane, Australia. His research interests include computer vision, machine learning, signal processing, and artificial intelligence. He serves on the editorial boards for IEEE TRANSACTIONS ON IMAGE PROCESSING and Pattern Recognition. He has previously served on the Editorial Board for IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECU- RITY. He is a Fellow of the International Association of Pattern Recognition, a Fellow of the Australian Academy of Technological Sciences and Engineering, and a Fellow of the Asia-Pacific Artificial Intelligence Association. He is a Senior Member of the IEEE and a multi-award winning researcher including an Australian Institute of Policy and Science Young Tall Poppy, an Australian Museum Eureka Prize winner, Engineers Australia Engineering Excellence Award, Australian Defence Scientist of the Year, and a Senior Fulbright Scholar. Sridha Sridharan has obtained an MSc (Commu- nication Engineering) degree from the University of Manchester, UK, and a PhD degree from the University of New South Wales, Australia. He is currently with the Queensland University of Tech- nology (QUT) where he is a Professor in the School of Electrical Engineering and Robotics. He has pub- lished over 600 papers consisting of publications in journals and in refereed international conferences in the areas of Image and',\n",
       "  'South Wales, Australia. He is currently with the Queensland University of Tech- nology (QUT) where he is a Professor in the School of Electrical Engineering and Robotics. He has pub- lished over 600 papers consisting of publications in journals and in refereed international conferences in the areas of Image and Speech technologies during the period 1990-2023. During this period he has also graduated 85 PhD students in the areas of Image and Speech technologies. Prof Sridharan has also received a number of research grants from various funding bodies including the Commonwealth competitive funding schemes such as the Australian Research Council (ARC) and the National Security Science and Technology (NSST) unit. Several of his research outcomes have been commercialised.'],\n",
       " [\"1 An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones Taihelong Zeng 1, Yun Lin 1 *, Yuhe Shi 2, Yan Li 1, 3, Zhiqing Wei 1, Xuanru Ji 4 1. School of Management Science and Real Estate, Chongqing University, Chongqing, 400045, PR China 2. School of Management, Guizhou University, Guiyang, 550025, PR China 3. Chongqing Changan Minsheng APLL Logistics Co., Ltd., Chongqing, 401122, PR China 4. State Grid Chongqing Electric Power Company Material Branch, Chongqing, 401120, PR China • Corresponding author: Dr. Yun Lin, Email: linyun@cqu.edu.cn Homepage: https://faculty.cqu.edu.cn/YunLin/zh_CN/index.htm http://www.msre.cqu.edu.cn/szdw/jslb/ly2.htm 2 Abstract The emergence of truck-drone collaborative systems in last-mile logistics has positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal extension of classical routing optimization, where synchronized vehicle coordination promises substantial operational efficiency and reduced environmental impact, yet introduces NP-hard combinatorial complexity beyond the reach of conventional optimization paradigms. Deep reinforcement learning offers a theoretically grounded framework to address TSP-D's inherent challenges through self-supervised policy learning and adaptive decision-making. This study proposes a hierarchical Actor-Critic deep reinforcement learning framework for solving the TSP-D problem. The architecture consists of two primary components: a Transformer-inspired encoder and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel, optimized k-nearest neighbors sparse attention mechanism specifically for focusing on relevant spatial relationships, further enhanced by the integration of global node features. The Minimal Gated Unit decoder processes these encoded representations to efficiently generate solution sequences. The entire framework operates within an asynchronous advantage actor-critic paradigm. Experimental results show that, on benchmark TSP-D instances of various scales (N=10 to 100), the proposed model can obtain competitive or even superior solutions in shorter average computation times compared to high-performance heuristic algorithms and existing reinforcement learning methods. Moreover, compared to advanced reinforcement learning algorithm benchmarks, the proposed framework significantly reduces the total training time required while achieving superior final performance, highlighting its notable advantage in training efficiency. Keywords Traveling salesman problem Drones Deep Reinforcement learning Attention mechanism Minimal gated unit 3 1. Introduction Contemporary logistics systems, especially in last-mile delivery operations, increasingly confront challenges related to costs, operational efficiency, and environmental sustainability (Pourmohammadreza et al., 2025). In this context, Unmanned Aerial Vehicles (UAVs) or drones have emerged as promising solutions due to their operational flexibility and ability to navigate around ground-based obstacles (Li et al., 2022). Collaborative delivery models combining drones with traditional trucks have consequently emerged, among which the Traveling Salesman Problem with Drones (TSP-D) has garnered significant attention as a representative paradigm (Murray and Chu, 2015; Zhou et al., 2025). This paradigm leverages the large payload capacity and long range of trucks combined with the efficient short-haul delivery capabilities of drones, aiming to significantly reduce delivery times, lower operational costs, and improve delivery coverage and responsiveness in complex environments (such as urban or remote areas), demonstrating significant theoretical research value and practical application prospects (Lu et al., 2025). The TSP-D problem can be described as a hybrid delivery system where one or more trucks, carrying several drones, depart from a distribution center to serve a set of customer\",\n",
       "  'environments (such as urban or remote areas), demonstrating significant theoretical research value and practical application prospects (Lu et al., 2025). The TSP-D problem can be described as a hybrid delivery system where one or more trucks, carrying several drones, depart from a distribution center to serve a set of customer locations (Wang et al., 2023). During the delivery process, drones can be launched from truck stopping points to independently serve nearby customers before rendezvousing with the truck at predetermined locations (Murray and Chu, 2015). TSP-D is an extension of the classic Traveling Salesman Problem (TSP) and Vehicle Routing Problem (VRP). However, unlike traditional VRP, TSP-D introduces collaborative decision-making between trucks and drones, task allocation, path planning, and precise time synchronization requirements, which significantly increases the problem’s complexity (Duan et al., 2025). The solution space expands dramatically with the number of customers, drones, and associated constraints, and the problem has been proven to be NP-hard, posing severe challenges to the efficiency and quality of solution algorithms (Yu et al., 2023). For combinatorial optimization problems like TSP-D, traditional solution approaches are primarily categorized into two classes: exact algorithms and heuristic algorithms. Exact algorithms including branch-and-bound, dynamic programming, and integer linear programming guarantee global optimal solutions but suffer from exponential computational complexity growth in large- scale instances, making them impractical for real-world applications (Laporte, 1992). Heuristic algorithms like simulated annealing, genetic algorithms, and ant colony optimization achieve computational efficiency at the expense of solution quality, gaining widespread adoption in practical implementations. However, these methods often require manual design for specific problem instances, exhibit poor adaptability, and risk converging to local optima (Karimi-Mamaghan et al., 2022). Recently, Deep Reinforcement Learning (DRL) has emerged as a cutting-edge AI technology 4 demonstrating significant potential in solving combinatorial optimization problems through its powerful representation learning and decision optimization capabilities (Bengio et al., 2021). Compared with traditional methods, DRL approaches offer advantages including end-to-end learning, enhanced generalization capabilities, and elimination of manual heuristic design. Growing evidence demonstrates their superior performance over conventional heuristics in solving large- scale TSP problems (Liu et al., 2022; Zhao et al., 2021), establishing a new paradigm for path planning solutions. While deep reinforcement learning has made remarkable progress in solving conventional TSP and VRP problems, research specifically addressing TSP-D remains relatively scarce, with existing studies predominantly limited to single-agent perspectives. The challenges of TSP-D primarily manifest in three aspects: (1) Dual-agent coordination requiring simultaneous optimization of truck and drone routes; (2) The speed difference between trucks and drones complicates the calculation of travel time; (3) Spatiotemporal synchronization requiring precise coordination of truck-drone rendezvous at specified locations and timestamps (Agatz et al., 2018). Therefore, we propose an Actor-Critic-based deep reinforcement learning framework specifically designed for TSP-D resolution. The design of this solution involves several crucial components: (1) We propose a Transformer-inspired encoder architecture. By integrating enhanced k-nearest neighbors sparse attention mechanisms with dynamic graph masking and global node features, this encoder effectively captures the crucial spatial structural patterns inherent in TSP-D problems while significantly reducing computational complexity. (2) We replace conventional Gated Recurrent Units (GRU) /',\n",
       "  \"(1) We propose a Transformer-inspired encoder architecture. By integrating enhanced k-nearest neighbors sparse attention mechanisms with dynamic graph masking and global node features, this encoder effectively captures the crucial spatial structural patterns inherent in TSP-D problems while significantly reducing computational complexity. (2) We replace conventional Gated Recurrent Units (GRU) / Long Short-Term Memory (LSTM) structures with an efficient Minimal Gated Unit (MGU) to enhancing the encoder's efficiency. This substitution maintains the model’s representational capacity for sequential decision-making while substantially reducing the parameter count and accelerating inference speed, contributing to a leaner overall model. (3) For effectively training process, we employ an asynchronous advantage actor-critic framework. This learning process is further enhanced through prioritized experience replay and adaptive learning rate scheduling, strategically improving sample efficiency and overall training stability. (4) Building upon the adaptive training mechanisms, we introduce a reward plateau-aware adaptive learning rate strategy. This specifically targets and resolves convergence challenges often encountered in large-scale TSP-D instances. The remainder of this paper is organized as follows. Section 2 briefly reviews current research 5 on TSP-D path optimization, introducing both traditional methods and deep reinforcement learning approaches. Section 3 formally defines the TSP-D problem and formulates it through Markov Decision Processes. Section 4 presents the proposed Actor-Critic framework and training strategies. Section V details computational experiments and analytical results. Finally, Section 5 concludes the paper and outlines future research directions. 6 2 Literature Review 2.1. Traveling Salesman Problem with Drones Since (Murray and Chu, 2015) first proposed the vehicle-drone collaborative delivery problem, the TSP-D has gained significant attention in recent years as an optimization method for delivery systems. Current research identifies two primary collaborative delivery models integrating vehicles and drones. The first model employs independent delivery operations, where vehicles and drones depart from distribution centers with cargo, complete their respective delivery tasks, and subsequently return to the centers. (Ham, 2018) addressed this model by proposing a multi-vehicle, multi-drone, multi-depot pickup-delivery problem, where drones can collect goods at subsequent nodes after deliveries before returning to depots, optimizing independent vehicle-drone routes for global efficiency. (Zhang et al., 2023) established a mixed-integer programming model for electric vehicle-drone collaborative delivery systems with time windows and endurance constraints, and developed an extended adaptive large neighborhood search algorithm for efficient solution generation. (Nguyen et al., 2022) restricted drone usage by defining maximum operation time constraints, extending the problem with parallel drone scheduling. The second category features collaborative delivery operations, where drones are carried by vehicles departing from depots, deployed at specific nodes, and later rendezvous with vehicles after completing deliveries. (Wang et al., 2017) first established mixed-integer linear programming formulations for dual-drone delivery problems. (Poikonen et al., 2017) extended (Wang et al., 2017) 's work by analyzing worst- case scenarios under battery constraints, incorporating crow-flight distance metrics for drones versus truck routing, and considering transportation costs beyond temporal optimization. (Schermer et al., 2018) comparatively evaluated two novel heuristic approaches: Two-Phase Heuristic (TPH) and Single-Phase Heuristic (SPH). Their work first produced numerical results for large-scale instances, demonstrating TPH's superior performance over SPH in most scenarios. (Sacramento et al., 2019) developed\",\n",
       "  \"versus truck routing, and considering transportation costs beyond temporal optimization. (Schermer et al., 2018) comparatively evaluated two novel heuristic approaches: Two-Phase Heuristic (TPH) and Single-Phase Heuristic (SPH). Their work first produced numerical results for large-scale instances, demonstrating TPH's superior performance over SPH in most scenarios. (Sacramento et al., 2019) developed an adaptive large neighborhood search metaheuristic incorporating time- window constraints. (Kitjacharoenchai et al., 2019) addressed operational constraints related to drone launch windows and delivery time coordination. (Mahmoudinazlou and Kwon, 2024) hybridized genetic algorithms with dynamic programming to create the escape strategy-based HGA- TAC+ algorithm for TSP-D resolution, (Wang et al., 2022) formulated a time-dependent truck-drone hybrid routing problem accounting for traffic conditions, and proposed an iterated local search algorithm. A key limitation of the existing drone-assisted truck delivery solutions lies in their reliance on the simplified assumption of single-customer service per drone sortie. To address this constraint and enhance practicality, (Gu et al., 2022) developed a novel approach enabling multi- customer service per drone mission. (Nguyen et al., 2022) incorporated constraints on total truck 7 payload capacity and combined operational durations for both trucks and drones. (Montaña et al., 2021) quantified carbon emissions from last-mile drone deliveries, emphasizing the sustainability implications. (Rave et al., 2023) proposed a novel micro-depot concept. In the problem they defined, drones can be launched from trucks or from microdepots on trucks. (Sitek et al., 2022) defined this problem as the Extended Vehicle Routing Problem with Drones, which differs from the traditional Vehicle Routing Problem with Drones in that they consider what are called mobile hubs (similar to microdepots) (Rave et al., 2023) as mobile points from which drones can be launched. At the same time, a genetic algorithm was developed to minimize operational costs and select mobile hubs. (Wu et al., 2022) investigated the effects of payload and flight duration on drone energy consumption, creating a variable neighborhood descent-based heuristic algorithm. 2.2. Deep reinforcement learning solves path planning With the rapid advancement of artificial intelligence, Deep Reinforcement Learning, integrating deep learning's representational power with reinforcement learning's decision-making capabilities, has demonstrated significant potential in solving combinatorial optimization problems, particularly in path planning. Ranging from the initial exploration of (Hopfield and Tank, 1985) neural networks to the widespread application of deep learning in recent years, DRL-based path planning research has achieved remarkable progress. Early research primarily focused on employing neural networks to directly solve small-scale TSP and VRP. The Hopfield neural network represents one of the earliest attempts, utilizing energy function concepts to derive approximate solutions. The subsequent development of Pointer Networks (PN) marked a new phase in applying deep learning techniques to path planning challenges. (Sutskever et al., 2014) enhanced the Seq2Seq framework to develop the PN, which employs LSTM-based encoding and attention mechanism decoding to effectively extract node coordinate features. This work laid the foundation for applying deep reinforcement learning to path planning challenges, However, this supervised training paradigm required extensive TSP datasets with optimal tours, limiting its practicality. This methodology proved challenging to implement, as the supervised training process inherently constrained solution quality to the performance\",\n",
       "  \"node coordinate features. This work laid the foundation for applying deep reinforcement learning to path planning challenges, However, this supervised training paradigm required extensive TSP datasets with optimal tours, limiting its practicality. This methodology proved challenging to implement, as the supervised training process inherently constrained solution quality to the performance level of the training data. To address these limitations, (Bello et al., 2016) implemented an Actor-Critic DRL training algorithm to optimize PN without requiring pre- computed optimal tours. This demonstrates the feasibility of DRL in learning high-quality strategies without supervised labels, and establishes an important foundational framework for numerous subsequent DRL-based path planning studies, including our current work. (Nazari et al., 2018) observed that the RNN structure in the original PN might introduce unnecessary complexity. They simplified the architecture, directly used embedded inputs, and incorporated dynamic element inputs, proposing the NM model. For multi-objective planning problems, (Li et al., 2021) adopted an 8 improved PN similar to (Nazari et al., 2018) for modeling. Using an RNN model based on GRU (Cho et al., 2014) , this model has similar performance to the LSTM used in the original PN in (Nazari et al., 2018), but with fewer parameters. (Kong and Jiang, 2024) designed a composite PN framework integrating graph neural network representations and attention mechanisms to address Drone-Assisted truck transportation. (Shi, 2022) integrated PN with heuristic algorithms like Variable Neighborhood Search to enhance initial solution quality in Traveling Salesman Problem applications. Experimental results demonstrated improved convergence rates and optimization outcomes. The proposed method exhibited superior generalization capability and faster training speeds compared to PN networks on TSP and Capacitated Vehicle Routing Problem (CVRP) benchmarks. Despite the relative success of PN and their variants, they still show limitations in capturing complex dependencies between nodes, especially in large-scale problems, which has prompted researchers to explore more powerful architectures. Following Google's introduction of the Transformer architecture combining attention mechanisms and multilayer perceptron (Vaswani et al., 2017), attention-based approaches have proliferated in this problem domain. (Kool et al., 2018) first adapted Transformer architectures for route optimization, developing an Attention model (AM) framework that significantly improved both training efficiency and solution optimality for various path planning tasks, The AM model became an important benchmark for subsequent research, but it completely removed positional encoding in the encoder, which may result in the loss of important spatial information. Additionally, the self-attention mechanism of standard Transformers faces a bottleneck of quadratic computational complexity when processing large-scale graphs. Building on (Kool et al., 2018) 's experimental framework, (Joshi et al., 2019) combined supervised learning and reinforcement learning (RL) to train models on 100-node TSP instances, achieving improved prediction accuracy. (Peng et al., 2019) enhanced the AM framework with dynamic decoding mechanisms, achieving superior solution accuracy and generalization capabilities for CVRP problems, (Zhang et al., 2020) developed a Multi-Agent Attention Model (MAAM) encoder-decoder framework using Transformer architectures, featuring attention layers that iteratively generate multi-vehicle routes. (Xu et al., 2022) advanced AM research by incorporating dynamic network architectures through attention aggregation modules across multiple relational structures, enabling context-aware embeddings that enhance model performance and\",\n",
       "  'et al., 2020) developed a Multi-Agent Attention Model (MAAM) encoder-decoder framework using Transformer architectures, featuring attention layers that iteratively generate multi-vehicle routes. (Xu et al., 2022) advanced AM research by incorporating dynamic network architectures through attention aggregation modules across multiple relational structures, enabling context-aware embeddings that enhance model performance and generalization. These works demonstrate the powerful potential of the Transformer architecture in path planning, but also highlight the need to further enhance its representational capacity and efficiency, especially in scenarios requiring the capture of complex graph structures and long-distance dependencies. Researchers have also explored combining Transformer with other technologies or modifying 9 it. (Deudon et al., 2018) replaced the recurrent encoder in the PN framework with Transformer- based attention encoding for TSP heuristic learning. Integration with 2-opt heuristics further enhanced solution quality. (Ma et al., 2019) proposed a Dual-Aspect Collaborative Transformer (DACT) with cyclic positional encoding (CPE) to separately learn node and positional embeddings, applying it to TSP and CVRP. (Gebreyesus et al., 2023) implemented a Transformer-based encoder- decoder architecture with Edge-Enhanced Multi-Head Attention (EEMHA) layers in the encoder. In terms of decoders, those based on recurrent networks (especially LSTM) have been widely applied due to their ability to maintain state information during the sequential decision-making process (Nazari et al., 2018; Peng et al., 2019). For the TSP-D problem, the relative positions and coordination between vehicles in a heterogeneous fleet are crucial, and the hidden state memory capability of LSTM may be more advantageous than stateless attention decoders. As (Bogyrbayeva et al., 2023) innovatively integrated AM and NM, adopted an attention-based encoder and LSTM decoder in their model for TSP-D. However, LSTM has a relatively large number of parameters. This inspires us to consider whether there exists a more lightweight recurrent unit that can both capture the temporal dynamics needed for TSP-D coordination and maintain high efficiency. Through a review and analysis of existing literature, we found that: (1) Actor-Critic framework has proven to be an effective DRL paradigm for solving path planning problems. (2) Attention models show greater potential in encoding graph structure information compared to earlier PN and its variants, but standard attention mechanisms have limitations in position information encoding and computational efficiency for large-scale graphs. (3) The choice of decoder is important for sequential decision tasks. For heterogeneous agent systems requiring coordination (such as TSP-D), the state memory capability of recurrent networks may be superior to stateless attention, though efficiency issues need to be considered. (4) Despite the progress of DRL in problems such as TSP and VRP, research on heterogeneous agent systems, especially the drone-assisted TSP-D problem, remains relatively scarce, with existing methods mostly limited to single-agent scenarios. The trade-off rule between solution quality and runtime in existing algorithms for TSP-D solutions is unavoidable. To address this issue as much as possible, based on the opportunities and shortcomings identified in the above literature analysis, we designed an end-to-end deep reinforcement learning architecture. 10 3 Problem description 3.1. Problem Definition This study focuses on the TSP-D, where a truck and a drone collaboratively execute delivery services for multiple customer',\n",
       "  \"as much as possible, based on the opportunities and shortcomings identified in the above literature analysis, we designed an end-to-end deep reinforcement learning architecture. 10 3 Problem description 3.1. Problem Definition This study focuses on the TSP-D, where a truck and a drone collaboratively execute delivery services for multiple customer nodes. The problem scenario is defined within a network containing n nodes, where nodes represent customers and the remaining node serves as the depot. Each customer node possesses unit demand, while the depot requires no service. Both vehicles depart from the depot, with the objective of minimizing total service time through optimized routing coordination while satisfying all customer demands. Fig. 1. TSP-D problem diagram As shown in Fig. 1, the truck executes main route delivery tasks along the path 1→3→4→6→7→1 in this distribution scheme. This route forms the backbone of the distribution network, providing multiple launch/recovery points for the drone. The drone services customers deviating from the main route through two distinct flight missions, requiring return to the truck after each delivery. With the truck's dynamic repositioning, the drone may: (1) rendezvous at the truck's next delivery node (1→2→3). (2) meet at a subsequent multi-node service location (4→5→7). (3) remain idle during certain segments (3→4). 3.2. MDP-based Formalization We formulate the TSP-D as a Markov Decision Process (MDP). The MDP provides a mathematical framework for sequential decision-making, where the agent (coordinated truck-drone system) learns optimal policies through environmental interactions. (1) State Space The state space encapsulates complete information about truck/drone statuses and customer demands at each decision epoch. A state can be represented as a tuple: 1 n - S s 11 (1) We define the system state as follows: the environment contains nodes comprising, customers and one depot. Spatial positions are defined through Cartesian coordinates for each node 𝑖. The coordinate set is denoted as ,demand vector tracks customer statuses: indicates unmet demand for customer , denotes fulfilled demand. The truck's position and drone's location are continuously tracked. indicates drone operation status: (engaged in delivery), (available). records drone return status: (not returned), (returned). Truck and drone may have pending actions. Let represent the truck's pending action, where is the target node and the remaining time to reach . If no pending action exists, . Similarly, define for drone operations, where is the drone's target node and, the remaining flight time. If the drone has no pending action, . The state space is constrained by: In the initial state, all customer demands are unserved , ; The truck and drone initially start at the depot node ; The drone can only launch when docked with/returned to the truck ( ⟹ ); When servicing customers, the drone cannot be at the depot ( ⟹ ). (2) Action Space The action space encompasses all feasible actions available to both truck and drone at each decision epoch. Given their collaborative operation, joint actions are represented as , where denotes the truck's next target node and specifies the drone's subsequent destination. The action space is constrained by: Trucks and drones cannot visit customer nodes with satisfied\",\n",
       "  'feasible actions available to both truck and drone at each decision epoch. Given their collaborative operation, joint actions are represented as , where denotes the truck\\'s next target node and specifies the drone\\'s subsequent destination. The action space is constrained by: Trucks and drones cannot visit customer nodes with satisfied demands . When the drone is servicing a customer , it must return to the truck\\'s current node . If the drone has returned ( ), it cannot relaunch until the truck visits another customer. Vehicles with pending actions ( ≠0 or ≠0) must continue executing them until completion. (3) Transition Function The state transition function specifies the probability of transitioning to state given current state and action . In our problem formulation, state transitions are deterministic: given the current state and action, the subsequent state is uniquely determined. The state transition function can be expressed as , where represents a deterministic mapping. The state transition process comprises: First, calculate the time step required to reach the next state based on vehicle actions and distances to target nodes. Specifically, truck arrival time and drone arrival time , where is the distance between nodes ( ) { } , , , , , , , i N t d d d s l l s r Î = i t d p c a a s ÎS n 1 n - ( , ) i i x y { }i N Î ip ( ) 1 1 2 1 , , , {0,1}n n c c c - - = … Î c 1 ic = i 0 ic = tl N Î dl N Î { } 0,1 ds Î 1 ds = 0 ds = { } 0,1 dr Î 1 dr = 0 dr = ( ) , t j t = ta j N Î t R t + Î j ( ) ,0 tl = ta ( ) , d k t = da k N Î d R t + Î ( ) ,0 dl = da 1, ic i C = \" Î { } 1,2, , 1 C n = … - t d l l n = = 0 ds = d t l l = 1 ds = dl n ¹ A ( ) , t d a a = a ta N Î da N Î , { | 0} t d i a a i C c Ï Î = 1 ds = d t a l = 0 dr = ta d a : ´ ® P S A S s s¢ a ( ) , s f s a ¢ = f Δt , t t t l a t t d v = , d d d l a d t d v = ,i j d i 12 and , and denote respective speeds. Incorporating pending action residuals, = (excluding terms where or = 0). Update vehicle positions using and selected actions: if , truck arrives: ; if , drone arrives: . Else positions remain unchanged. Update',\n",
       "  \"a d t d v = ,i j d i 12 and , and denote respective speeds. Incorporating pending action residuals, = (excluding terms where or = 0). Update vehicle positions using and selected actions: if , truck arrives: ; if , drone arrives: . Else positions remain unchanged. Update customer states: if or , set or . Update drone status: if , set service state . If drone returns to truck , set and . If transitions from , set , Finally, update pending actions: for truck, for drone. (4) Reward Function The reward function defines the immediate reward for executing action in state . To minimize total service duration, we formulate the reward as negative temporal progression: (2) Where represents the time step required to transition from to through action . This design incentivizes the agent to prioritize time-efficient action sequences. j tv d v Δt ( ) min , , , t t d d t d t t t t t t - - tt d t Δt Δ t t t t t = - ' t t l a = Δ d d t t t = - ' d d l a = ' tl C Î ' dl C Î ' ' 0 tlc = ' ' 0 dlc = ' dl C Î ' 1 ds = ( ) ' ' d t l l = ' 0 ds = ' 0 dr = ds 0 1 ® 1 dr = ( ) ( ) ' ,max 0, Δ t t a a t t = - t ( ) ( ) ' ,max 0, Δ d d a a t t = - d : R ´ ® R S A a s ( ) , Δ r s t = - a Δt s s¢ a 13 4 Solution Model Architecture To address TSP-D, we propose an end-to-end deep reinforcement learning architecture. Building upon the demonstrated efficacy of Actor-Critic frameworks in solving complex routing problems (Bello et al., 2016; Kool et al., 2018; Nazari et al., 2018), we adopt this paradigm as the foundation of our approach. At the core of our contribution, we develop an Adaptive Expansion Graph Attention for graph representation learning, Our encoder substantially enhances attention mechanisms, employing relative and sparse positional encoding instead of removing them as in AM (Kool et al., 2018) and hybrid model (HM) that uses multi-head attention for encoding (Bogyrbayeva et al., 2023), To overcome computational bottlenecks in traditional Transformers for large-scale graphs and capture long-range dependencies, we adapt Google's Exphormer sparse attention framework (Shirzad et al., 2023), introducing an innovative adaptive graph expansion mechanism. This enables dynamic construction of sparse interaction graphs. Additionally, our encoder incorporates global node mechanisms to enhance contextual awareness. LSTM-based decoders have proven effective for routing problems (Nazari et al., 2018; Peng et al., 2019), For drone-assisted planning where relative vehicle positions in heterogeneous fleets are crucial, LSTM's hidden state memory outperforms stateless attention decoders (Bogyrbayeva et al., 2023). Diverging from HM's LSTM, we investigate MGU decoders as\",\n",
       "  'awareness. LSTM-based decoders have proven effective for routing problems (Nazari et al., 2018; Peng et al., 2019), For drone-assisted planning where relative vehicle positions in heterogeneous fleets are crucial, LSTM\\'s hidden state memory outperforms stateless attention decoders (Bogyrbayeva et al., 2023). Diverging from HM\\'s LSTM, we investigate MGU decoders as lightweight alternatives, our findings show that MGU achieves comparable generalization to LSTM through simplified gating, sufficiently capturing temporal dynamics essential for TSP-D coordination. As shown in Fig. 2, our model architecture diagram is clearly presented. Fig. 2. The encoder-decoder framework in our model 14 4.1. Encoder Given a graph composed of node set where each node possesses input features . To embed this graph structure, we initialize node embeddings through linear transformation: (3) Where and are learnable parameters, with denoting hidden layer dimension. These initial node embeddings are processed through stacked Expander Graph Attention (EGA) encoder layers. Each EGA encoder layer dynamically expands the graph structure , where denotes edge connections. To enhance graph connectivity and incorporate global context, we employ: (1) k-Nearest Neighbor Connection: Selects the top 𝑘= ⌈log! |𝑁|⌉ neighbors based on latent space distance: (4) (2) Hierarchical Connection: Divides nodes into layers and establishes connections within each layer. (5) Where stride . (3) Depot connections: guarantee direct links between all nodes and a predefined depot node to enable rapid information aggregation and distribution. Recognizing that expanded graphs may compromise long-range dependency modeling compared to fully-connected graphs, we augment the architecture with a special global node , extending the adjacency matrix to , this composite matrix integrates original edges, expanded connections, and global node linkages. The relevant schematic diagram is shown in Fig. 3. To precisely regulate node interactions in attention mechanisms while maintaining structural alignment with , we implement attention mask , each mask element corresponds respectively with entries. During attention score calculation. is additively combined with raw attention scores. This configuration assigns attention scores to unconnected node pairs, which become zero after Softmax normalization. N n N Î in D nx R Î ( ) 0 nh ( ) 0 n input n input h W x b n N = + \" Î in h D D input W R ´ Î h D input b R Î h D ( ) { } 0 : nh n N Î L ( ) , N = G E E ( ) ( ) 1 1 2 l l ij i j d h h - - = - ! ! 8 L N = { } layer 1 ( , ) | ,| | 2 L i j l E v v i s j s i j = = = - £ ê ú ê ú ë û ë û ! s N L = global n ( ) ( ) 1 1 {0,1} N N A + ´ + Î ! A! ( ) ( ) 1 1 {0, } N N M ¥ + ´ + Î - ,i j M A! M ¥ - 15 Fig. 3. Schematic Diagram of expander graph Masking',\n",
       "  'n ( ) ( ) 1 1 {0,1} N N A + ´ + Î ! A! ( ) ( ) 1 1 {0, } N N M ¥ + ´ + Î - ,i j M A! M ¥ - 15 Fig. 3. Schematic Diagram of expander graph Masking For node representations input to this sub-layer, we first compute query vectors , key vectors , and value vectors : (6) (7) (8) are learnable weight matrices with dimensions , and respectively. To compute attention scores between nodes and , we employ masked dot-product attention combined with relative positional encoding: (9) where represents the relative positional encoding vector between nodes and . In our implementation, relative positional encodings are retrieved from a sparse positional embedding matrix , where denotes the number of attention heads and the dimensionality of sparse positional embeddings. Attention weights are obtained by Softmax normalization of attention scores: (10) The updated representation for node is computed as the weighted sum of all nodes\\' ( )1 l nh - k D nq R Î k D nk R Î v D nv R Î ( )1 l n Q n q W h - = ( )1 l n K n k W h - = ( )1 , l n V n v W h n N - = \" Î , , Q K V W W W ( ) ( ) , h k h k D D D D ´ ´ ( ) h v D D ´ ,i j s i j , T T i j i j k i ij s q k D q r = + ijr i j sparse k H D D R R ´ ´ Î H sparse D ,i j a ( ) ( ) , , , exp exp i j i j i j j a s s ¢ ¢ = å n h¢ n 16 value vectors: (11) To enhance the model\\'s expressive capability, we employ a multi-head attention mechanism utilizing independent attention heads. The messages computed from each head are concatenated and linearly transformed through an output projection matrix to produce the final EGA sublayer output: (12) To stabilize the training procedure, we apply Instance Normalization after the EGA sublayer and incorporate residual connections: (13) The normalized output is subsequently fed into the Feed-Forward Network (FFN) sublayer. The FFN sublayer consists of two linear layers with a ReLU activation function. (14) Mirroring the attention sublayer, we apply Instance Normalization and residual connections after the FFN sublayer: (15) At the inception of each EGA layer, we compute mean-pooled features from original node inputs (excluding the global node): (16) This averaged feature is then added to the global node\\'s representation to enable layer- wise global information updating: (17) After processing through EGA layers, we obtain final node representations . To integrate global information across EGA layers, we compute multi-scale mean-pooled features : (18) Finally, we add to the terminal layer\\'s node features to accomplish ultimate information fusion: (19) , n n j j j h',\n",
       "  \"information updating: (17) After processing through EGA layers, we obtain final node representations . To integrate global information across EGA layers, we compute multi-scale mean-pooled features : (18) Finally, we add to the terminal layer's node features to accomplish ultimate information fusion: (19) , n n j j j h a v ¢ = å H ( ) v h H D D O W R × ´ Î ( ) ( ) ( ) ( ) ( ) ( ) 1 1 1 1 1 EGA , , Concat , , l l H N O n n h h W h h - - + û ¢ é ù … = … ë ¢ ( ) ( ) ( ) ( ) ( ) ( ) 1 1 1 1 1 ˆ InstanceNorm EGA , , l l l l n n N h h h h - - - + = + … ( ) ˆ l nh ( ) ( ) ( ) ( ) ( ) ,2 ,1 ,1 ,2 F ˆ F ReL ˆ U l l l n ff ff n ff ff h W W h b b = × + + ( ) ( ) ( ) ( ) ( ) ( ) InstanceNorm ˆ ˆ FF l l l l n n n h h h = + ( )l p ( ) ( ) ( ) 1 1: Mean ,dim 1 l l N p h - = = ( )1 1 l N h - + ( ) ( ) ( ) 1 1 1 1 l l l N N h h p - - + + = + L ( ) { } : 1 L nh n N Î + multi scale p - ( ) ( ) ( ) 1 Mean , , ,dim 1 L multi scale p p p - é ù = … = ë û multi scale p - ( ) L nh ( ) ( ) L L n n multi scale h h p - = + 17 To prepare encoder outputs for decoding, we process the final node features through specific transformations. Specifically, we extract original node features (excluding the global node), apply dimension unification via operation, and designate them as static decoder inputs . 4.2. Decoder These encoded node features will be used to autoregressively generate action sequences for solving combinatorial optimization problems. The decoder employs a unidirectional MGU recurrent neural network integrated with attention mechanisms to select appropriate actions at each decoding step. The decoder's objective is to progressively construct solution sequences using graph node representations from the encoder and evolving dynamic state information. To achieve this, we implement an L-layer unidirectional MGU recurrent neural network as the decoder's core component. At each decoding timestep , the network receives: static features , dynamic features , from the encoder, decoder input , and previous hidden state . The decoding process proceeds autoregressively until complete solution sequences are generated. The decoder core consists of L stacked MGU recurrent layers.\",\n",
       "  \"the decoder's core component. At each decoding timestep , the network receives: static features , dynamic features , from the encoder, decoder input , and previous hidden state . The decoding process proceeds autoregressively until complete solution sequences are generated. The decoder core consists of L stacked MGU recurrent layers. At timestep , the MGU processes decoder input and previous hidden state to update its state, generating current output and updated hidden state , The MGU update procedure can be formally expressed as: (20) Here, MGU(⋅) represents the forward propagation of the multi-layer MGU network. We employ an optimized MGU cell architecture designed to enhance computational efficiency and model performance. This MGU cell selectively updates/resets hidden states via gating mechanisms to better capture long-term dependencies in sequential data. To enable decoder focus on critical graph nodes at each step, we incorporate an attention mechanism. Specifically, a content-based attention mechanism processes: static features , dynamic features and MGU output . The mechanism computes node relevance scores per decoding step to generate selection logits. First, we transform dynamic features , static features and MGU output through linear projection layers, obtaining projected features , and . We then compute attention scores , measuring the compatibility between decoder states and node: (21) Where is a learnable parameter vector, and denote the column permute static E t h B D N R ´ ´ Î static E h B D N dynamic E R ´ ´ Î ( ) h t B D dec x R ´ Î ( )1 h t L B D h R - ´ ´ Î t ( )t dec x ( )1 t h - ( ) h t B D r R ´ Î ( ) h t L B D h R ´ ´ Î ( ) ( ) ( ) ( ) ( ) 1 , MGU , t t t t dec r h x h - = static E dynamic E ( )t r dynamic E static E ( )t r ( ) h t B D N d R ´ ´ Î ( ) h t B D N e R ´ ´ Î ( ) h t B D q R ´ Î ( )t iu ( ) ( ) ( ) ( ) ( ) tanh t t t t T i i i u v e q d = + + h D v R Î ( )t ie ( )t id i th - 18 vectors of and respectively, with broadcasted to match dimensions. The attention weights are obtained by Softmax normalization of scores, indicating relative selection probabilities: (22) These weights generate context vectors aggregating node information weighted by decoder states. To regulate the scale of logits and introduce nonlinearity, we apply activation to the attention scores and multiply by a learnable scaling factor, obtaining final node selection logits : (23) Where . The decoder utilizes the attention-generated logits to compute node selection probability distribution through the Softmax function: (24) During the training phase, we employ a sampling-based action selection strategy.\",\n",
       "  \"apply activation to the attention scores and multiply by a learnable scaling factor, obtaining final node selection logits : (23) Where . The decoder utilizes the attention-generated logits to compute node selection probability distribution through the Softmax function: (24) During the training phase, we employ a sampling-based action selection strategy. To ensure the validity of selected actions, we implement an action masking mechanism prior to generating probability distributions. The action mask dynamically filters out invalid action options (e.g., visited nodes or constraint-violating nodes) based on real-time environmental states. During model evaluation, we adopt a greedy action selection strategy. To capture state evolution during decoding, the dynamic features are updated at each decoding timestep. 4.3. Training strategy As show in Algorithm 1, This approach employs an Asynchronous Advantage Actor-Critic framework enhanced with prioritized experience sampling and adaptive learning rate scheduling, specifically tailored for combinatorial optimization in vehicle routing problems. The core objective minimizes the expected completion time of coordinated truck-drone delivery routes, mathematically formulated as: where denotes policy parameters, represents problem instances sampled from distribution. and is the action sequence generated by the policy network. To address computational challenges in large-scale routing optimization, we designed a hybrid training architecture decoupling environment interaction from parameter updates. As show in Algorithm 2 and Algorithm 3, The policy network (actor) and value network (critic) operate through parallel threads: the main thread generates trajectories via environment interaction, while auxiliary threads asynchronously update network parameters using gradients queued in a prioritized buffer. This design minimizes computational resource idling and accelerates convergence. ( )t e ( )t d ( )t q ( ) α t B N R ´ Î ( ) ( ) ( ) ( ) ( ) α exp exp t t t i i j j N u u Î = å ( )t B N l R ´ Î ( ) ( ) ( ) tanh t t l C u = × ( ) ( ) ( ) ( ) 1 2 , , , T t t t t N u u u u é ù = … ë û ( )tl ( ) ( ) ( ) Softmax t t p l = dynamic E ( ) ( ) ( ) | s p s J E E C s q p q p ~ ~ × é ù = ë û S q s p 19 The policy gradient is derived using the REINFORCE algorithm with a critic-based baseline for variance reduction. For a batch of B problem instances , the gradient estimate is expressed as: (25) where, denotes the critic network's predicted expected completion time, and represents trajectories sampled from the policy. The critic network is trained by minimizing the mean squared error (MSE): (26) A dynamic prioritization mechanism filters high-impact experiences to accelerate the learning process. Let the advantage function be defined as: (27) When the trajectory's average absolute advantage exceeds threshold , prioritized immediate gradient updates are triggered. This prioritization focuses training on episodes where policy performance significantly deviates from critic estimates, efficiently allocating computational resources to\",\n",
       "  \"filters high-impact experiences to accelerate the learning process. Let the advantage function be defined as: (27) When the trajectory's average absolute advantage exceeds threshold , prioritized immediate gradient updates are triggered. This prioritization focuses training on episodes where policy performance significantly deviates from critic estimates, efficiently allocating computational resources to challenging instances. Network optimization employs the AdaBelief optimizer (Zhuang et al., 2020), which adaptively adjusts step sizes based on gradient magnitude and directional consistency. (28) (29) (30) Where is the stochastic gradient, is the initial learning rate, and the hyperparameters are set to , , . Weight decay (0.01) and decoupled weight updates regularize the network. The learning rate follows a cosine annealing schedule, completing five full cycles throughout the entire training period: (31) Where , , is the total number of training cycles. This cyclical schedule promotes exploration by periodically resetting the learning rate, preventing premature convergence to suboptimal policies. 1 ({ } ) B i i s = ( ) ( ) ( ) ( ) ( ) 1 1 log B i i i i i i J C s b s p s B q f q q q p p = Ñ » - Ñ å ( ) ( ) i b s f ( ) ( ) | i i p s q p ~ × ( ) ( ) ( ) 2 2 1 1 || π | || B i i i i b s C s B f f = = - å L ( ) ( ) ( ) ( ) ,π π | i i i i i A s C s b s f = - ( ) τ 0.5 = ( ) 1 1 , B i i i A s B p t = > å ( ) 1 1 1 β 1 β , t t t m m g - = + - ( )( ) 2 2 1 2 β 1 β t t t t v v g m - = + - - + ， 1 θ θ η t t t t m v + = - + tg h 1 0.9 b = 2 0.999 b = 16 10- = ( ) ( ) ( ) min max min max η η 1 2 η η 1 cos π t t T = + - + max train 5 T N = min max η 0.01η = train N 20 Algorithm 1: Training Process Input: : Initial actor network parameters : Initial critic network parameters : Maximum training epochs : Priority threshold : Maximum steps per episode (decode_len) 1 Initialize and critic with 2 Start asynchronous and 3 4 5 // Environment Interaction 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 0 q 0 j K t T ( ) actor q p Vj ( ) 0 ActorUpdateThread q ( ) 0 CriticUpdateThread j 1 forepoch to K do ¬ ( ) nodes G GenerateTrainingData n ¬ ( ) (\",\n",
       "  '14 15 16 17 18 19 20 21 22 23 24 0 q 0 j K t T ( ) actor q p Vj ( ) 0 ActorUpdateThread q ( ) 0 CriticUpdateThread j 1 forepoch to K do ¬ ( ) nodes G GenerateTrainingData n ¬ ( ) ( ) , . S avail Env Reset G ¬ () h InitializeMGU ¬ 0 time current ¬ 0 1 fort to T do ¬ - ( ) , , , , tr tr tr tr S h avail a logp h Actorq ¬ dr tr Updateavail based ona ( ) , , , , dr dr dr dr S h avail a logp h Actorq ¬ ( ) min , , step tr dr time time t t remaining ¬ time step current time + = , , S avail S avail ¬ ¢ ¢ end for ( ) , – static w A R Vj ¬ ( ) if mean A then t > ( ) ( ) ( ) 1 0 mean log π logπ T t t loss tr dr t actor A a a - = é ù ¬ - × + ë û å ( ) 2 loss critic mean A ¬ loss Send actor to ActorQueue loss Send critic toCriticQueue end if 0 interval if epochmod S then = 21 25 26 27 28 29 30 31 32 33 Algorithm 2: ActorUpdateThread 1 while training not terminated do 2 3 4 5 6 7 Algorithm 3: CriticUpdateThread 1 while training not terminated do 2 3 4 5 6 7 ( ) , val R Validate q j ¬ val best if R R then < best val R R ¬ * q q ¬ * j j ¬ ( ) *, * SaveModel q j end if end if end for if ActorQueuenotemptythen ( ) . loss actor ActorQueue get ¬ ( ) , loss AdaBeliefUpdate actor q q ¬ ( ) UpdateActorLearningRate q end if end while if CriticQueuenotemptythen ( ) . loss critic CriticQueue get ¬ ( ) , loss AdaBeliefUpdate critic j j ¬ ( ) UpdateCriticLearningRate j end if end while 22 5 Computational Experiments To comprehensively evaluate the performance of this deep reinforcement learning method, this chapter compares our algorithm with traditional heuristic algorithms and high-performance reinforcement learning algorithms in the field. In all experiments, we assume the drone is twice as fast as the truck (α = 2) and has unlimited flight range. Few standard datasets exist for TSP-D. (Bogyrbayeva et al., 2023) developed a new dataset based on (Haider et al., 2019) \\'s study , which has subsequently been adopted in TSP-D research (Mahmoudinazlou and Kwon, 2024), This collection contains two TSP-D instance subsets (\"Random\" and \"Amsterdam\") with unlimited drone range, differing in instance generation methodologies. In the first instance subset, uniform distributions over [0, 1] × [0, 1] and [0, 100] × [0, 100] are employed to sample x and y coordinates of depots and customer nodes, respectively. This configuration ensures depot positioning at the lower-left corner. The',\n",
       "  'drone range, differing in instance generation methodologies. In the first instance subset, uniform distributions over [0, 1] × [0, 1] and [0, 100] × [0, 100] are employed to sample x and y coordinates of depots and customer nodes, respectively. This configuration ensures depot positioning at the lower-left corner. The generation method resembles that proposed by (Agatz et al., 2018) We present 100 instance samples for node sizes of 20, 50, and 100. The second instance subset comprises 100 samples with node sizes of 10, 20 and 50. In this subset, depots are randomly selected from the nodes. Additionally, we evaluated another uniform dataset for TSP-D maintained in the TSBlib project on GitHub. We employ the following formula to report relative gaps between solutions: (32) Here, denotes the cost of a specific solution for instance 𝑖, and represents the solution method widely recognized as high-quality for instance 𝑖 among existing comparative approaches. We then report as the average of all values under identical configurations. 5.1. Model Inference Performance For reinforcement learning algorithms, since this study builds upon AM (Kool et al., 2018) and HM (Bogyrbayeva et al., 2023), natural comparisons should be made with them. Notably, AM has demonstrated inferior performance to HM in both solution accuracy and speed across varying node scales (Bogyrbayeva et al., 2023), thus excluded from our comparison. Inference was conducted using a single RTX 4090 (24GB) GPU with 12 vCPU Intel Xeon Platinum 8352V CPU @ 2.10GHz, under PyTorch 1.10.0, Python 3.8 (Ubuntu 20.04), and CUDA 11.3. For heuristic algorithms, (Bogyrbayeva et al., 2023) extended the \"TSP-ep-all\" algorithm by (Agatz et al., 2018) to DPS by adopting the \"divide-and-conquer heuristic\" (DCH) from (Poikonen et al., 2019) . We compared the performance of \"TSP-ep-all\" and DPS. (Mahmoudinazlou and Kwon, 2024) proposed a Hybrid Genetic Algorithm with Type-Aware Chromosomes that integrates local search and dynamic programming to solve TSP-D, showing strong performance on “Amsterdam” ( ) * * Gap 100% i i i i Z Z Z = - ´ i Z * iZ i Gap Gap 23 and “Random” datasets, which we also compared against. The key innovation in their approach lies in observing that escape strategies improve solution quality while increasing runtime. This demonstrates effective mitigation of metaheuristic methods\\' inherent local optima issues. Consequently, we only compared HGA-TAC+ implementations employing escape strategies. All three heuristic algorithms were implemented in Julia and evaluated using an AMD EPYC 9654 96- core processor. For the greedy policy in reinforcement learning, instances are not evaluated in batches but sequentially processed one-by-one on the GPU. Heuristic methods follow the same strategy but utilize a single CPU thread instead of GPU. For batch sampling strategies, HM (n_samples) (where n_samples denotes batch size) demonstrates that increasing n_samples generally improves solution accuracy while reducing computational speed. Each algorithm was executed 10 times, with the best reward and computation time per instance averaged across trials. We initially evaluated using the \"Random\" dataset. To rigorously evaluate the performance of our proposed model for the TSP-D, we executed each algorithm 10 times and calculated the average of the',\n",
       "  'speed. Each algorithm was executed 10 times, with the best reward and computation time per instance averaged across trials. We initially evaluated using the \"Random\" dataset. To rigorously evaluate the performance of our proposed model for the TSP-D, we executed each algorithm 10 times and calculated the average of the cost and computation times for each instance. we conducted extensive computational experiments on benchmark instances with varying network sizes (N=11, 20, 50, and 100 nodes). Performance was assessed based on two primary metrics: the total solution cost (lower values indicate better performance) and the computational time required (in seconds, lower values indicate greater efficiency). We employed TSP-ep-all, an approach known for generating high-quality solutions, as the baseline for comparison. The percentage gap (Gap) relative to this baseline is reported to quantify the deviation in solution quality. We first examine the performance of rapid decision-making strategies, including DPS/10, HM (greedy), and our own greedy implementation (Ours(greedy)). As show in Table 1, These methods prioritize computational speed, consistently delivering solutions within fractions of a second across all instance sizes (e.g., ≤ 0.01s for greedy methods, ≤ 0.27s for DPS/10 at N=100). However, this velocity is achieved at the expense of solution quality. Both HM (greedy) and Ours (greedy) exhibit similar performance, yielding positive Gaps ranging from approximately 0.7% to 5.4% compared to the baseline. DPS/10 shows comparable Gaps (3.70% to 5.90%). While extremely fast, these methods demonstrate the limitations of purely greedy strategies in navigating the complex TSP-D solution landscape. The core of our evaluation lies in comparing Ours (sampling_k) against the HM (sampling_k) using equivalent number of samples for batch sampling (k=100, 1200, 2400, 4800). For any given number of samples and problem size, ours (sampling_k) consistently achieves lower costs than HM (sampling_k). This advantage is particularly evident in larger instances. For example, at N=50 and 24 k=4800, our method yields a cost of 391.51 (Gap: -1.42%), significantly outperforming HM\\'s 396.31 (Gap: -0.22%). Similarly, at N=100 and k=4800, our method achieves a 1.22% Gap (Cost: 542.21) compared to HM\\'s 1.63% Gap (Cost: 544.42). This suggests that our RL framework integrates the sampled information more effectively to guide the policy towards higher-quality solutions. Concurrent with superior solution quality, our algorithm exhibits enhanced computational efficiency. When comparing Ours (sampling_k) and HM (sampling_k) at identical N and k, our method consistently requires less computational time. For instance, solving the N=100 instance with k=4800 takes 8.43s using our method, whereas HM requires 9.54s, representing an approximate 11.6% reduction in runtime. Increasing the sampling width (k) predictably improves solution quality for RL method, when compared against the strong heuristic HGVAC+, our sampling method presents a compelling alternative. While HGVAC+ performs well, particularly on smaller instances (N=11, 20) where it achieves negative Gaps relatively quickly, our method (Ours (sampling_k) with sufficient k) often matches or surpasses its solution quality on larger instances while offering significant computational savings. For N=50, Ours(sampling_4800) achieves a better Gap (-1.42%) in nearly half the time (2.41s vs 4.65s) compared to HGVAC+ (Gap 0.39%). For N=100, Ours (sampling_4800) delivers a marginally better Gap (1.22% vs',\n",
       "  'with sufficient k) often matches or surpasses its solution quality on larger instances while offering significant computational savings. For N=50, Ours(sampling_4800) achieves a better Gap (-1.42%) in nearly half the time (2.41s vs 4.65s) compared to HGVAC+ (Gap 0.39%). For N=100, Ours (sampling_4800) delivers a marginally better Gap (1.22% vs 1.53%) while being substantially faster (8.43s vs 14.83s). This suggests our method scales more effectively in terms of maintaining high-quality solutions within reasonable time limits as problem complexity increases. Table 1 TSP-D results on “Random” locations dataset N=11 N=20 N=50 N=100 Method cost Gap Time(s) cost Gap Time(s) cost Gap Time(s) cost Gap Time(s) TSP-ep-all(Baseline) 230.07 0.00% 0.01 281.62 0.00% 0.11 397.17 0.00% 23.95 535.67 0.00% 2352.53 DPS/10 292.05 3.70% 0.02 420.61 5.90% 0.05 565.14 5.50% 0.27 HGVAC ＋ 227.45 -1.14% 0.52 279.88 -0.62% 1.22 398.72 0.39% 4.65 543.88 1.53% 14.83 HM(greedy) 233.21 1.36% 0.00 285.54 1.39% 0.00 408.84 2.94% 0.01 564.42 5.37% 0.01 Ours(greedy) 231.67 0.69% 0.00 285.80 1.48% 0.00 407.04 2.49% 0.00 564.36 5.36% 0.01 HM(sampling_100) 230.10 0.01% 0.11 282.93 0.46% 0.13 399.59 0.61% 0.35 550.13 2.70% 0.76 Ours(sampling_100) 229.05 -0.45% 0.10 282.10 0.17% 0.13 396.41 -0.19% 0.35 550.89 2.84% 0.74 HM(sampling_1200) 229.22 -0.37% 0.14 282.13 0.18% 0.18 397.38 0.05% 0.70 546.01 1.93% 2.57 Ours(sampling_1200) 228.55 -0.66% 0.13 280.80 -0.29% 0.17 392.94 -1.06% 0.68 544.41 1.63% 2.21 HM(sampling_2400) 229.12 -0.42% 0.18 281.84 0.08% 0.28 397.01 -0.04% 1.41 545.13 1.77% 4.90 Ours(sampling_2400) 228.36 -0.74% 0.15 280.67 -0.34% 0.23 392.07 -1.28% 1.27 543.18 1.40% 4.22 HM(sampling_4800) 228.93 -0.50% 0.29 281.67 0.02% 0.50 396.31 -0.22% 2.59 544.42 1.63% 9.54 Ours(sampling_4800) 228.30 -0.77% 0.21 280.44 -0.42% 0.42 391.51 -1.42% 2.41 542.21 1.22% 8.43 25 As show in Table 2. On the uniform dataset, our proposed reinforcement learning algorithm demonstrates superior performance compared to the HM method across TSP-D instances of varying scales (N=10, 20, 50). Utilizing the sampling_4800, our approach consistently yielded lower costs, achieving negative Gaps of -0.12%, -1.10%, and -0.58% for N=10, N=20, and N=50, respectively, relative to the computationally intensive HM (4800) baseline. This signifies a notable improvement in solution quality. Crucially, this enhancement was realized with comparable or marginally reduced computational times (e.g., 2.47s vs. 2.57s for N=50). While both algorithms improve solution quality with increased sampling, our method exhibits greater effectiveness, achieving superior solutions faster. Notably, for N=20 and N=50, our algorithm with only 1200 or 2400 samples already surpasses the cost performance of the HM (4800) baseline (indicated by negative Gaps: -0.89%/- 1.10% for N=20, -0.31%/-0.50% for N=50). Fig. 4 presents detailed routing visualizations on the 20-node uniform dataset, clearly demonstrating that our model with sampling strategy outperforms HM in nearly every instance. Table 2 TSP-D results on “uniform” locations dataset N=11 N=20 N=50 Method cost Gap Time cost Gap Time cost Gap Time HM(greedy) 228.38 0.28% 0.00 277.87 0.66% 0.00 426.93 4.26% 0.01 HM(sampling_100) 228.38 0.28% 0.09 276.95 0.33% 0.12 413.22 0.91% 0.35 HM(sampling_1200) 227.98 0.10% 0.15 276.09 0.02% 0.17 410.57 0.27% 0.69 HM(sampling_2400) 227.75 0.00% 0.18 276.09 0.02% 0.26 409.94 0.11% 1.31 HM(sampling_4800)(Baseline) 227.75 0.00% 0.29 276.05 0.00% 0.50 409.48 0.00% 2.57 Ours(greedy) 228.38 0.28% 0.00',\n",
       "  '277.87 0.66% 0.00 426.93 4.26% 0.01 HM(sampling_100) 228.38 0.28% 0.09 276.95 0.33% 0.12 413.22 0.91% 0.35 HM(sampling_1200) 227.98 0.10% 0.15 276.09 0.02% 0.17 410.57 0.27% 0.69 HM(sampling_2400) 227.75 0.00% 0.18 276.09 0.02% 0.26 409.94 0.11% 1.31 HM(sampling_4800)(Baseline) 227.75 0.00% 0.29 276.05 0.00% 0.50 409.48 0.00% 2.57 Ours(greedy) 228.38 0.28% 0.00 279.23 1.15% 0.00 425.68 3.96% 0.01 Ours(sampling_100) 227.76 0.00% 0.11 275.51 -0.19% 0.14 412.42 0.72% 0.35 Ours(sampling_1200) 227.49 -0.12% 0.14 273.60 -0.89% 0.19 408.23 -0.31% 0.74 Ours(sampling_2400) 227.49 -0.12% 0.18 273.00 -1.10% 0.24 407.43 -0.50% 1.28 Ours(sampling_4800) 227.49 -0.12% 0.29 273.00 -1.10% 0.46 407.09 -0.58% 2.47 Sampling_100 Sampling_2400 HM Ours HM Ours 0 Best reward=233.24 Best reward=241.15 Best reward=233.24 Best reward=228.73 26 1 Best reward=281.72 Best reward=273.12 Best reward=281.72 Best reward=267.86 2 Best reward=275.20 Best reward=267.71 Best reward=275.19 Best reward=267.71 3 Best reward=239.17 Best reward=238.75 Best reward=239.17 Best reward=238.75 4 Best reward=298.38 Best reward=296.13 Best reward=298.38 Best reward=293.62 5 Best reward=284.89 Best reward=284.90 Best reward=284.89 Best reward=284.89 6 Best reward=300.33 Best reward=300.12 Best reward=300.13 Best reward=300.12 27 7 Best reward=300.16 Best reward=300.03 Best reward=298.12 Best reward=299.29 8 Best reward=279.25 Best reward=278.35 Best reward=279.25 Best reward=274.22 9 Best reward=277.19 Best reward=274.80 Best reward=270.82 Best reward=274.80 Fig. 4. Test results of our model and HM on the \"uniform\" dataset position with 20 nodes We also compared the model performance on the Amsterdam dataset, As show in Table 3, since the Amsterdam dataset has 10 nodes, the performance differences of multiple models on 11 nodes are not compared. For the N=20 instances, our proposed algorithm (Ours(sampling_4800)) demonstrates highly competitive performance. It achieves a solution cost of 2.34, marginally outperforming the TSP-ep-all baseline (Gap: -0.70%) and matching the heuristic HGVAC+. Notably, its computation time (0.48s) is significantly faster than HGVAC+ (1.81s) and comparable to the HM reinforcement learning method (0.50s), while being only moderately slower than the baseline\\'s 0.12s. Although DPS methods offer extreme speed (≤0.05s), their solution quality is substantially compromised (Gap: 33.20%), highlighting a clear trade-off. Our method effectively balances near- optimal solution quality with efficient computation time at this scale. This advantage in computational efficiency becomes more pronounced for the larger N=50 instances. Our algorithm yields a cost of 3.29, representing the best performance among the RL methods (Gap: 0.76%) and remaining highly competitive with the baseline cost (3.27). Crucially, our execution time (2.47s) remains efficient and similar to HM (2.58s), offering a nearly 10-fold speed advantage over the baseline (24.53s) and a 2.5-fold advantage over HGVAC+ (6.20s). These results underscore our method\\'s strong scalability, delivering high-quality solutions with significantly reduced computational burden compared to both the heuristic baseline and other 28 competitive approaches as problem size increases. Table 3 TSP-D results on “Amsterdam” locations dataset N=20 N=50 Method cost Gap Time cost Gap Time TSP-ep-all(Baseline) 2.36 0.00% 0.12 3.27 0.00% 24.53 DPS/10 3.14 33.20% 0.05 3.80 16.49% 0.08 DPS/25 0.00 4.23 29.46% 0.07 HGVAC ＋ 2.34 -0.79% 1.81 3.33 2.10% 6.20 HM(sampling_4800) 2.38 1.00% 0.50 3.31 1.37% 2.58 Ours(sampling_4800) 2.34 -0.70% 0.48 3.29 0.76% 2.47 5.2. Model Training Performance We found that compared to the improvement in inference speed, the model\\'s improvement',\n",
       "  \"3.14 33.20% 0.05 3.80 16.49% 0.08 DPS/25 0.00 4.23 29.46% 0.07 HGVAC ＋ 2.34 -0.79% 1.81 3.33 2.10% 6.20 HM(sampling_4800) 2.38 1.00% 0.50 3.31 1.37% 2.58 Ours(sampling_4800) 2.34 -0.70% 0.48 3.29 0.76% 2.47 5.2. Model Training Performance We found that compared to the improvement in inference speed, the model's improvement in training speed and convergence speed is more significant. Specifically, our model can achieve better results within the same number of epochs, and the training speed per epoch is also significantly increased. We compared the differences in training speed and convergence speed with HM on a single 4090 GPU. To evaluate the training characteristics of the proposed model (Ours) and the baseline model HM in solving TSP-D problems of different scales, we conducted detailed comparative experiments on instances with 11, 20, 50 and 100 nodes respectively. As show in Fig. 5 and Fig. 6, both models were trained for 10,000 epochs, and key performance indicators, including the reward value (Reward, representing path cost here, lower is better) and training time, were recorded every 200 epochs. Analyzing the training curves (reward value change over epochs), both models exhibit the typical characteristics of reinforcement learning, i.e., the reward value shows an overall downward trend as training progresses, indicating continuous optimization of the model's policy. However, our model (Ours) demonstrates significant advantages in terms of convergence speed and final solution quality. Taking n=11 as an example, the Ours model obtained a much lower reward value than the HM model in the initial stage (Epoch 0) (311.7 vs 409.5), and reached a reward value close to the final level (~253) around 1200 epochs, while the HM model's reward value was still above 260 during the same period and required more epochs to stabilize. This advantage of rapid convergence is even more pronounced on larger-scale problems (n=20, 50, 100). For example, at n=100, the Ours model could rapidly reduce the reward value from about 958 to around 700 in the early training stages (around 1000-1400 epochs), while the HM model required a longer exploration period to reach a similar level. At the end of training (Epoch 10000), for all tested node scales, the final average reward value obtained by the Ours model was lower than that of the HM model (n=11: 29 242.7 vs 245.3; n=20: 304.4 vs 312.5; n=50: 477.5 vs 487.8; n=100: 677.5 vs 688.0), which demonstrates the effectiveness of our model in finding better TSP-D solutions. Furthermore, our proposed model exhibits training stability significantly superior to the HM baseline model. Across all tested node scales (n=11, 20, 50, 100), the evolution of the Ours model's reward value over training epochs shows smaller fluctuations, especially in the middle and later stages of training, its learning curve is smoother, and rarely exhibits the drastic performance oscillations or significant temporary policy degradation phenomena observed in the HM model. In terms of computational efficiency, our model (Ours) also shows significant superiority. By comparing the recorded average training time per epoch, it can be found that under all node scales, the time consumption per epoch of the Ours model\",\n",
       "  'or significant temporary policy degradation phenomena observed in the HM model. In terms of computational efficiency, our model (Ours) also shows significant superiority. By comparing the recorded average training time per epoch, it can be found that under all node scales, the time consumption per epoch of the Ours model is significantly lower than that of the HM model. For example, for n=11, the average epoch time of the Ours model is about 0.13 seconds, while the HM model is about 0.27-0.30 seconds. When the scale increases to n=100, the epoch time of the Ours model is about 0.9-1.1 seconds, while the HM model requires 1.3-1.7 seconds. The cumulative effect of this improvement in per-epoch efficiency is significant, leading to the total cumulative time of the Ours model being much less than the HM model when completing the same number of training epochs (10,000 Epochs). Specifically, for n=11, Ours total time consumption is about 1438 seconds, HM time consumption is about 2838 seconds. For n=100, Ours total time consumption is about 10241 seconds (about 2.8 hours), while HM time consumption is 16730 seconds (about 4.6 hours). This indicates that the Ours model not only trains faster but also has lower computational costs. From the perspective of scalability, although the per-epoch time consumption and total time consumption of both models inevitably increase with the increase in problem size, the Ours model always maintains a relative time efficiency advantage. Overall, our model can not only converge faster to higher-quality solutions during the training process but also has significant advantages in computational resource consumption, especially when dealing with larger-scale problems, this dual improvement in efficiency and effectiveness makes it a more potential reinforcement learning solution for solving TSP-D problems. 30 Fig. 5. The training learning curves and training time graphs of our model and the HM model at 11, 20 nodes Fig. 6. The training learning curves and training time graphs of our model and the HM model at 50, 100 nodes 31 6 Conclusions In this study, a novel end-to-end deep reinforcement learning framework was proposed to solve TSP-D. The contributions of this study are as follows: First, a Transformer-inspired encoder architecture with optimized k-nearest neighbors sparse attention was developed to efficiently extract crucial structural patterns within TSP-D instances. Second, we introduced dynamic expander graph Masking and global node features to capture the intricate spatial and collaborative dynamics of the problem. Third, an efficient MGU based decoder was implemented alongside an advanced asynchronous advantage actor-critic training strategy, incorporating prioritized experience replay and adaptive learning rate adjustments to enhance training efficiency. We evaluated the performance of the proposed model in solving the TSP-D problem using three sets of benchmark instances. Compared with state-of-the-art heuristic algorithms (TSP-ep-all, DPS, HGVAC+) and the reinforcement learning algorithm (HM) in the field. Experimental results demonstrate that our proposed method generates high-quality solutions across various TSP-D instances, exhibiting competitive solution quality compared to leading heuristic algorithms while demonstrating superior generalization capabilities on large-scale problems. Notably, when handling medium to large scale instances with 20, 50 and 100 nodes, our method achieves',\n",
       "  \"the field. Experimental results demonstrate that our proposed method generates high-quality solutions across various TSP-D instances, exhibiting competitive solution quality compared to leading heuristic algorithms while demonstrating superior generalization capabilities on large-scale problems. Notably, when handling medium to large scale instances with 20, 50 and 100 nodes, our method achieves high-quality solutions across nearly all datasets while maintaining low computational costs. Furthermore, it significantly reduces training time compared to state-of-the-art reinforcement learning algorithms during model training. The proposed method offers valuable insights for combinatorial optimization problems in multi-agent collaborative decision-making. It also introduces novel ideas and technical support for developing intelligent decision-making systems in logistics. This study had some limitations. While demonstrating strong performance up to N=100, the framework's scalability and performance on significantly larger or more densely connected instances warrant further investigation. Moreover, the current model focuses primarily on core routing optimization without incorporating more nuanced real-world operational constraints. Therefore, in future work, we will focus on enhancing the scalability and robustness of the proposed architecture, possibly through exploring more advanced graph representation techniques or hierarchical learning approaches. Additionally, validating the model's performance on diverse real- world datasets, potentially incorporating operational constraints such as varying drone battery endurance, payload capacities, or specific customer time windows, will be critical for bridging the gap towards practical deployment in logistics systems. Extending this framework to tackle other complex multi-agent or heterogeneous fleet routing problems also represents a valuable direction for future research. 32 Acknowledgments This work was supported by the National Natural Science Foundation of China (Nos: 52105507 and 52275476) and Specialized Research Fund for Chongqing Technology Innovation and Application Development (No: CSTB2022TIAD-KPX0061). Declaration of Interest statement Authors declare that there is no conflict of interest due to the publication of this paper. 33 References Agatz, N., Bouman, P., Schmidt, M., 2018. Optimization Approaches for the Traveling Salesman Problem with Drone. Transportation Science 52(4), 965-981. Bello, I., Pham, H., Le, Q.V., Norouzi, M., Bengio, S., 2016. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940. Bengio, Y., Lodi, A., Prouvost, A., 2021. Machine learning for combinatorial optimization: A methodological tour d'horizon. European Journal of Operational Research 290(2), 405-421. Bogyrbayeva, A., Yoon, T., Ko, H.B., Lim, S., Yun, H.Y.K., Kwon, C., 2023. A deep reinforcement learning approach for solving the Traveling Salesman Problem with Drone. Transp. Res. Pt. C- Emerg. Technol. 148, 19. Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y., 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. Costa, L., Contardo, C., Desaulniers, G., 2019. Exact Branch-Price-and-Cut Algorithms for Vehicle Routing. Transportation Science 53(4), 946-985. Deudon, M., Cournut, P., Lacoste, A., Adulyasak, Y., Rousseau, L.M., 2018. Learning Heuristics for the TSP by Policy Gradient, 15th International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR). Springer International Publishing Ag, Delft, NETHERLANDS, pp. 170-181. Duan, J., Luo, H., Wang, G., 2025. Approaches to the truck-drone routing problem: A systematic review. Swarm and Evolutionary Computation 92, 101825. Gebreyesus, G., Fellek, G., Farid, A., Fujimura, S., Yoshie, O., 2023. Gated-Attention\",\n",
       "  'Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR). Springer International Publishing Ag, Delft, NETHERLANDS, pp. 170-181. Duan, J., Luo, H., Wang, G., 2025. Approaches to the truck-drone routing problem: A systematic review. Swarm and Evolutionary Computation 92, 101825. Gebreyesus, G., Fellek, G., Farid, A., Fujimura, S., Yoshie, O., 2023. Gated-Attention Model with Reinforcement Learning for Solving Dynamic Job Shop Scheduling Problem. IEEJ Trans. Electr. Electron. Eng. 18(6), 932-944. Gu, R.X., Poon, M., Luo, Z.H., Liu, Y., Liu, Z., 2022. A hierarchical solution evaluation method and a hybrid algorithm for the vehicle routing problem with drones and multiple visits. Transp. Res. Pt. C-Emerg. Technol. 141, 23. Haider, Z., Charkhgard, H., Kim, S.W., Kwon, C., 2019. Optimizing the relocation operations of free- floating electric vehicle sharing systems. Available at SSRN 3480725. Ham, A.M., 2018. Integrated scheduling of <i>m</i>-truck, <i>m</i>-drone, and <i>m</i>-depot constrained by time-window, drop-pickup, and <i>m</i>-visit using constraint programming. Transp. Res. Pt. C-Emerg. Technol. 91, 1-14. Hopfield, J.J., Tank, D.W., 1985. “Neural” computation of decisions in optimization problems. Biological Cybernetics 52(3), 141-152. Joshi, C.K., Laurent, T., Bresson, X., 2019. An efficient graph convolutional network technique for the travelling salesman problem. arXiv preprint arXiv:1906.01227. Karimi-Mamaghan, M., Mohammadi, M., Meyer, P., Karimi-Mamaghan, A.M., Talbi, E., 2022. Machine learning at the service of meta-heuristics for solving combinatorial optimization problems: A state- of-the-art. European Journal of Operational Research 296(2), 393-422. Kitjacharoenchai, P., Ventresca, M., Moshref-Javadi, M., Lee, S., Tanchoco, J.M.A., Brunese, P.A., 2019. Multiple traveling salesman problem with drones: Mathematical model and heuristic approach. Computers & Industrial Engineering 129, 14-30. Kong, F.H., Jiang, B., 2024. Delivery optimization for collaborative truck-drone routing problem considering vehicle obstacle avoidance. Computers & Industrial Engineering 198, 14. Kool, W., Van Hoof, H., Welling, M., 2018. Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475. Kuo, R.J., Lu, S.H., Lai, P.Y., Mara, S.T.W., 2022. Vehicle routing problem with drones considering time windows. Expert Systems with Applications 191, 19. Laporte, G., 1992. The traveling salesman problem: An overview of exact and approximate algorithms. European Journal of Operational Research 59(2), 231-247. Li, K., Zhang, T., Wang, R., 2021. Deep Reinforcement Learning for Multiobjective Optimization. IEEE Transactions on Cybernetics 51(6), 3103-3114. Li, Y., Liu, M., Jiang, D.D., 2022. Application of Unmanned Aerial Vehicles in Logistics: A Literature Review. Sustainability 14(21), 18. Liu, M., Wang, Z., Li, J., 2022. A deep reinforcement learning algorithm for large-scale vehicle routing problems, International Conference on Electronic Information Technology (EIT 2022). SPIE, pp. 34 824-829. Lu, H., Zhang, X., Yang, S., 2019. A learning-based iterative method for solving vehicle routing problems, International conference on learning representations. Lu, J., Liu, Y.M., Jiang, C.M., Wu, W.W., 2025. Truck-drone joint delivery network for rural area: Optimization and implications. Transp. Policy 163, 273-284. Ma, Q., Ge, S., He, D., Thaker, D., Drori, I., 2019. Combinatorial optimization by graph pointer networks and hierarchical reinforcement learning. arXiv preprint arXiv:1911.04936. Mahmoudinazlou, S., Kwon, C., 2024. A hybrid genetic algorithm with type-aware chromosomes for Traveling Salesman Problems with Drone. European Journal of Operational Research 318(3), 719- 739. Montaña, L.C., Malagon-Alvarado, L., Miranda, P.A., Arboleda, M.M., Solano-Charris, E.L., Vega- Mejía,',\n",
       "  'optimization by graph pointer networks and hierarchical reinforcement learning. arXiv preprint arXiv:1911.04936. Mahmoudinazlou, S., Kwon, C., 2024. A hybrid genetic algorithm with type-aware chromosomes for Traveling Salesman Problems with Drone. European Journal of Operational Research 318(3), 719- 739. Montaña, L.C., Malagon-Alvarado, L., Miranda, P.A., Arboleda, M.M., Solano-Charris, E.L., Vega- Mejía, C.A., 2021. A novel mathematical approach for the Truck-and-Drone Location-Routing Problem, 3rd International Conference on Industry 4.0 and Smart Manufacturing (ISM). Elsevier Science Bv, Upper Austria Univ Appl Sci, Hagenberg Campus, Linz, AUSTRIA, pp. 1378-1391. Murray, C.C., Chu, A.G., 2015. The flying sidekick traveling salesman problem: Optimization of drone- assisted parcel delivery. Transp. Res. Pt. C-Emerg. Technol. 54, 86-109. Nazari, M., Oroojlooy, A., Snyder, L., Takác, M., 2018. Reinforcement learning for solving the vehicle routing problem. Advances in neural information processing systems 31. Nguyen, M.A., Dang, G.T.H., Hà, M.H., Pham, M.T., 2022. The min-cost parallel drone scheduling vehicle routing problem. European Journal of Operational Research 299(3), 910-930. Peng, B., Wang, J., Zhang, Z., 2019. A deep reinforcement learning algorithm using dynamic attention model for vehicle routing problems, International Symposium on Intelligence Computation and Applications. Springer, pp. 636-650. Poikonen, S., Golden, B., Wasil, E.A., 2019. A Branch-and-Bound Approach to the Traveling Salesman Problem with a Drone. INFORMS J. Comput. 31(2), 335-346. Poikonen, S., Wang, X.Y., Golden, B., 2017. The vehicle routing problem with drones: Extended models and connections. Networks 70(1), 34-43. Pourmohammadreza, N., Jokar, M.R.A., Van Woensel, T., 2025. Last-mile logistics with alternative delivery locations: A systematic literature review. Results in Engineering 25, 104085. Rave, A., Fontaine, P., Kuhn, H., 2023. Drone location and vehicle fleet planning with trucks and aerial drones. European Journal of Operational Research 308(1), 113-130. Sacramento, D., Pisinger, D., Ropke, S., 2019. An adaptive large neighborhood search metaheuristic for the vehicle routing problem with drones. Transp. Res. Pt. C-Emerg. Technol. 102, 289-315. Schermer, D., Moeini, M., Wendt, O., 2018. Algorithms for Solving the Vehicle Routing Problem with Drones, In: Nguyen, N.T., Hoang, D.H., Hong, T.-P., Pham, H., Trawiński, B. (Eds.), Intelligent Information and Database Systems. Springer International Publishing, Cham, pp. 352-361. Shi, C., 2022. Pointer Network Solution Pool : Combining Pointer Networks and Heuristics to Solve TSP Problems, 2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA), pp. 1236- 1242. Shirzad, H., Velingker, A., Venkatachalam, B., Sutherland, D.J., Sinop, A.K., 2023. EXPHORMER: Sparse Transformers for Graphs, 40th International Conference on Machine Learning. Jmlr-Journal Machine Learning Research, Honolulu, HI. Sitek, P., Wikarek, J., Jagodzinski, M., 2022. A Proactive Approach to Extended Vehicle Routing Problem with Drones (EVRPD). Applied Sciences-Basel 12(16), 21. Sutskever, I., Vinyals, O., Le, Q., 2014. Sequence to Sequence Learning with Neural Networks, 28th Conference on Neural Information Processing Systems (NIPS). Neural Information Processing Systems (Nips), Montreal, CANADA. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention Is All You Need, 31st Annual Conference on Neural Information Processing Systems (NIPS). Neural Information Processing Systems (Nips), Long Beach, CA. Wang, X.Y., Poikonen, S., Golden, B., 2017. The',\n",
       "  '(Nips), Montreal, CANADA. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., 2017. Attention Is All You Need, 31st Annual Conference on Neural Information Processing Systems (NIPS). Neural Information Processing Systems (Nips), Long Beach, CA. Wang, X.Y., Poikonen, S., Golden, B., 2017. The vehicle routing problem with drones: several worst- case results. Optimization Letters 11(4), 679-697. Wang, Y., Wang, Z., Hu, X.P., Xue, G.Q., Guan, X.Y., 2022. Truck-drone hybrid routing problem with time-dependent road travel time. Transp. Res. Pt. C-Emerg. Technol. 144, 27. Wang, Y., Yang, X.X., Chen, Z.B., 2023. An Efficient Hybrid Graph Network Model for Traveling Salesman Problem with Drone. Neural Process. Lett. 55(8), 10353-10370. Wu, G.H., Mao, N., Luo, Q.Z., Xu, B.J., Shi, J.M., Suganthan, P.N., 2022. Collaborative Truck-Drone 35 Routing for Contactless Parcel Delivery During the Epidemic. Ieee Transactions on Intelligent Transportation Systems 23(12), 25077-25091. Xu, Y.Q., Fang, M., Chen, L., Xu, G.Y., Du, Y.L., Zhang, C.Q., 2022. Reinforcement Learning With Multiple Relational Attention for Solving Vehicle Routing Problems. Ieee Transactions on Cybernetics 52(10), 11107-11120. Yu, V.F., Lin, S.W., Jodiawan, P., Lai, Y.C., 2023. Solving the Flying Sidekick Traveling Salesman Problem by a Simulated Annealing Heuristic. Mathematics 11(20), 21. Zhang, K., He, F., Zhang, Z.C., Lin, X., Li, M., 2020. Multi-vehicle routing problems with soft time windows: A multi-agent reinforcement learning approach. Transp. Res. Pt. C-Emerg. Technol. 121, 14. Zhang, S., Liu, S.L., Zhang, W.Y., 2023. Vehicle routing problems with time windows under the collaborative delivery mode of electric vehicle-drone. Chinese Journal of Management Science. Zhao, J.X., Mao, M.J., Zhao, X., Zou, J.H., 2021. A Hybrid of Deep Reinforcement Learning and Local Search for the Vehicle Routing Problems. Ieee Transactions on Intelligent Transportation Systems 22(11), 7208-7218. Zhou, J., Yu, Q., Xue, Z.M., Yang, L.B., 2025. Research on the Route Planning Problem of Drone and Truck Collaborative Delivery in Restricted Areas. IEEE Access 13, 33062-33073. Zhuang, J.T., Tang, T., Ding, Y.F., Tatikonda, S., Dvornek, N., Papademetris, X., Duncan, J.S., 2020. AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients, 34th Conference on Neural Information Processing Systems (NeurIPS). Neural Information Processing Systems (Nips), Electr Network.'],\n",
       " ['The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss Rui Wu wurui22@mail.ustc.edu.cn School of Management, University of Science and Technology of China 96 Jinzhai Road, Hefei, 230026, Anhui, China Lizheng Wang lzwang@ustc.edu.cn School of Management, University of Science and Technology of China 96 Jinzhai Road, Hefei, 230026, Anhui, China Yongjun Li ∗ lionli@ustc.edu.cn School of Management, University of Science and Technology of China 96 Jinzhai Road, Hefei, 230026, Anhui, China Abstract Judea Pearl’s vision of Structural Causal Models (SCMs) as engines for counterfactual reasoning hinges on faithful abduction: the precise inference of latent exogenous noise. For decades, operationalizing this step for complex, non-linear mechanisms has remained a significant computational challenge. The advent of diffusion models, powerful universal function approximators, offers a promising solution. However, we argue that their standard design, optimized for perceptual generation over logical inference, introduces a fundamen- tal flaw for this classical problem: an inherent information loss we term the Structural Reconstruction Error (SRE). To address this challenge, we formalize the principle of Causal Information Conservation (CIC) as the necessary condition for faithful abduc- tion. We then introduce BELM-MDCM, the first diffusion-based framework engineered to be causally sound by eliminating SRE by construction through an analytically in- vertible mechanism. To operationalize this framework, a Targeted Modeling strategy provides structural regularization, while a Hybrid Training Objective instills a strong causal inductive bias. Rigorous experiments demonstrate that our Zero-SRE framework not only achieves state-of-the-art accuracy but, more importantly, enables the high-fidelity, individual-level counterfactuals required for deep causal inquiries. Our work provides a foundational blueprint that reconciles the power of modern generative models with the rigor of classical causal theory, establishing a new and more rigorous standard for this emerging field. Keywords: Causal Inference, Diffusion Models, Causal Information Conservation, Struc- tural Causal Models, Counterfactual Generation, BELM, Structural Reconstruction Error 1 Introduction The fundamental challenge of causal inference, as articulated by Rubin (1974), is our in- ability to simultaneously observe an individual’s potential outcomes. Generating authentic counterfactuals is thus the field’s grand challenge. Structural Causal Models (SCMs), in- troduced by Pearl (2009), provide the formal language for this pursuit. An SCM posits that an outcome Vi is generated by a function of its parents Pai and a unique exogenous ∗. Corresponding author. ©2025 Rui Wu, Lizheng Wang, and Yongjun Li. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. arXiv:2511.05236v1 [cs.LG] 7 Nov 2025 Wu, Wang, and Li noise variable Ui. This noise, Ui, represents the primordial causal information—the collection of unobserved factors unique to an individual. This concept aligns directly with the long-standing focus in econometrics on unobserved individual heterogeneity, a central challenge in structural modeling for decades (Heckman, 2001). Pearl’s framework for causal reasoning, the Abduction-Action-Prediction cycle, hinges on the fidelity of the first step: abduction. To answer any ”what if” question, one must first perfectly infer this primordial information Ui from an observed outcome vi. For decades, while this theoretical blueprint was clear, its practical realization for complex, non-linear mechanisms remained a major computational hurdle, often addressed in econometrics through strong parametric assumptions or linear approximations (Angrist and Pischke, 2008). The advent',\n",
       "  'must first perfectly infer this primordial information Ui from an observed outcome vi. For decades, while this theoretical blueprint was clear, its practical realization for complex, non-linear mechanisms remained a major computational hurdle, often addressed in econometrics through strong parametric assumptions or linear approximations (Angrist and Pischke, 2008). The advent of deep generative models, particularly diffusion models (Ho et al., 2020), offers a powerful new hope for bridging this gap. As near-universal function approxima- tors, they possess the expressive power to learn the complex, non-linear functions that have long challenged classical methods (Chao et al., 2023; Sanchez and Tsaftaris, 2022). However, this promise is shadowed by a critical, yet overlooked, ”impedance mismatch.” These models were engineered for perceptual tasks like image synthesis, where visual plau- sibility is paramount, not for the logical rigor demanded by causal abduction. We argue that their standard design, which relies on approximate inversion schemes like DDIM (Song et al., 2021a), is fundamentally at odds with the strict requirements of this classical causal problem. In this work, we diagnose and resolve this conflict. We begin by giving the classic re- quirement for faithful abduction a modern name: Causal Information Conservation (CIC)1. Our core contribution is the identification that standard diffusion models system- atically violate this principle due to an inherent algorithmic flaw. We formalize this flaw as the Structural Reconstruction Error (SRE)—a quantifiable information loss that imposes a hard theoretical ceiling on the fidelity of any counterfactual generated by such methods. The SRE is not an estimation error to be solved with more data, but a structural defect in the tool itself. To solve the long-standing challenge of operationalizing faithful abduction, we introduce BELM-MDCM. It is not merely a new model, but the first diffusion-based framework re- engineered from first principles to be causally sound. Architected around an analytically invertible sampler (Liu et al., 2024), it is the first Zero-SRE causal framework by construction. This design choice reconciles the expressive power of modern diffusion models with the logical rigor of Pearl’s causal theory, ensuring the abduction step is lossless. Our primary contributions are therefore: (i) Diagnosing a Fundamental Barrier in a Classic Problem. We are the first to identify that standard diffusion models, when applied to the classic problem of SCM abduction, suffer from a structural flaw we term the Structural Reconstruction Error (SRE), which violates the foundational principle of Causal Information Conservation. 1. In this work, ’Causal Information Conservation’ is defined operationally as the lossless, deterministic recovery of the exogenous noise variable U. Its novelty lies in its application as a design principle and diagnostic tool for the diffusion model paradigm in causality, rather than as a formal information- theoretic quantity. Connecting this operational principle to formal measures, such as mutual information, is a compelling avenue for future research. 2 The Causal Round Trip (ii) Proposing the First Causally-Sound Diffusion Framework. We introduce BELM-MDCM, the first framework to eliminate SRE by design. By leveraging an analytically invertible mechanism, it ensures that the power of diffusion models can be applied to causality without compromising the integrity of the',\n",
       "  'future research. 2 The Causal Round Trip (ii) Proposing the First Causally-Sound Diffusion Framework. We introduce BELM-MDCM, the first framework to eliminate SRE by design. By leveraging an analytically invertible mechanism, it ensures that the power of diffusion models can be applied to causality without compromising the integrity of the abduction process. (iii) Developing a Principled Methodology to Operationalize the Framework. To make our Zero-SRE framework practical and robust, we introduce two synergistic innovations: a Targeted Modeling strategy to manage complexity and a Hybrid Training Objective to provide a strong causal inductive bias, both supported by our theoretical analysis. Through a comprehensive experimental evaluation, we demonstrate that BELM-MDCM not only sets a new state-of-the-art in estimation accuracy but, more critically, unlocks the generation of authentic individual-level counterfactuals for deep causal inquiries. By providing a foundational blueprint that resolves a core tension between modern machine learning and classical causal theory, our work establishes a new, more rigorous standard for this research direction. 1.1 The Inversion Challenge in Diffusion-Based Causality Diffusion models (Ho et al., 2020) are powerful generative models that learn to reverse a fixed, gradual noising process. They train a neural network, ϵθ(xt, t), to predict the noise component of a corrupted sample xt by optimizing a simple mean-squared error objective: Lsimple(θ) = Et,x0,ϵ \\x14 ϵ −ϵθ \\x00√¯αtx0 + √ 1 −¯αtϵ, t \\x01 2\\x15 (1) where ¯αt defines the noise schedule and ϵ ∼N(0, I). This trained network is then used to iteratively denoise a variable from pure noise back to a clean sample. A standard determin- istic method for this generative process is the Denoising Diffusion Implicit Model (DDIM) (Song et al., 2021a): xt−1 = √¯αt−1 \\x12xt −√1 −¯αtϵθ(xt, t) √¯αt \\x13 + p 1 −¯αt−1 · ϵθ(xt, t) (2) However, causal abduction requires the inverse operation: encoding an observed data point x0 into its latent noise code xT . Standard frameworks (Chao et al., 2023) use the DDIM inversion, which only approximates this path: xt+1 = √¯αt+1 \\x12xt −√1 −¯αtϵθ(xt, t) √¯αt \\x13 + p 1 −¯αt+1 · ϵθ(xt, t) (3) This inversion is approximate because it relies on the noise prediction ϵθ(xt, t) remain- ing constant across the step, which introduces discretization errors that accumulate (Liu et al., 2022). This structural flaw, which we term the Structural Reconstruction Er- ror (SRE), systematically corrupts the inferred exogenous noise Ui. The initial error in the abduction step then propagates through the entire Abduction-Action-Prediction cycle, compromising the fidelity of the final counterfactual. 3 Wu, Wang, and Li 1.2 Our Solution: A Zero-SRE Causal Framework To eliminate SRE by construction, we build our framework upon an analytically invertible sampler: the Bidirectional Explicit Linear Multi-step (BELM) sampler (Liu et al., 2024). BELM overcomes the ”memoryless” limitation of single-step samplers like DDIM by using a history of noise predictions, a principle grounded in classical theory for solving ODEs (Hairer and Wanner, 2006). Specifically, we employ a second-order BELM. During decoding, it computes a more stable effective noise, ϵeff, using predictions from the current and previous timesteps: ϵeff = 3 2ϵθ(xt, t) −1 2ϵθ(xt+1, t',\n",
       "  'a history of noise predictions, a principle grounded in classical theory for solving ODEs (Hairer and Wanner, 2006). Specifically, we employ a second-order BELM. During decoding, it computes a more stable effective noise, ϵeff, using predictions from the current and previous timesteps: ϵeff = 3 2ϵθ(xt, t) −1 2ϵθ(xt+1, t + 1) (4) This improved estimate is then used in a DDIM-like update. The key innovation is that the corresponding encoding process is constructed to be the exact algebraic inverse of this decoding process, guaranteeing that the round-trip is lossless, i.e., H(T(x0)) = x0. While the original work on BELM focused on general generative tasks, we are the first to iden- tify, leverage, and theoretically justify its analytical invertibility as the key to satisfying the principle of Causal Information Conservation for rigorous counter- factual generation. Our choice of a second-order BELM represents a deliberate trade-off, providing substantial accuracy gains over single-step methods while maintaining practical efficiency (Liu et al., 2024), making it ideal for our causal framework. 1.3 Methodological Gaps in Applying Invertible SCMs However, achieving high-fidelity causal inference requires more than a simple substitution of one sampler for another. The principle of analytical invertibility, while theoretically sound, exposes new challenges in practical SCM implementation that our framework is designed to address. The Challenge of Model Specification: Targeted Modeling. A key decision in SCM construction is assigning a causal mechanism to each node. Naively applying a complex, computationally expensive BELM-based diffusion model to every node in the causal graph is suboptimal. This motivates our Targeted Modeling strategy, where model complexity is treated as a resource to be allocated judiciously across the graph. The Challenge of Downstream Tasks: Hybrid Training. The second challenge arises from a fundamental mismatch in objectives. A diffusion model is trained on a gen- erative objective, Ldiffusion(θ), while a downstream predictive task is optimized using a dis- criminative loss, Ltask(ϕ). These two objectives are not aligned. This ”objective mismatch” motivates our Hybrid Training strategy, which seeks to unify these two goals. 2 Theoretical Analysis: An Operator-Theoretic Framework To formalize our thesis that Causal Information Conservation is paramount and its violation via Structural Reconstruction Error is a fundamental barrier, we develop a rigorous operator-theoretic framework. This perspective is essential for analyzing the fidelity of the causal mapping process itself, moving beyond simple prediction errors. We 4 The Causal Round Trip present the first formal analysis that decomposes the counterfactual error in diffusion-based causal models to explicitly isolate the SRE, proving how our Zero-SRE design eliminates this critical structural limitation. Our analysis first establishes the conditions for perfect counterfactual generation (§2.1- §2.3) and proves that standard methods produce a non-zero SRE, which our sampler elimi- nates by construction (Proposition 5-6; §2.4). The centerpiece is a novel error decomposition theorem that isolates the SRE, motivating our Zero-SRE design (§2.5-§2.7). We conclude with learnability guarantees and a discussion of implications for advanced causal tasks like transportability (§2.8-§2.10). 2.1 Problem Formulation and Causal Operators Let (Ω, F, P) be a probability space. We consider endogenous variables V as elements of the Hilbert space of',\n",
       "  'the SRE, motivating our Zero-SRE design (§2.5-§2.7). We conclude with learnability guarantees and a discussion of implications for advanced causal tasks like transportability (§2.8-§2.10). 2.1 Problem Formulation and Causal Operators Let (Ω, F, P) be a probability space. We consider endogenous variables V as elements of the Hilbert space of square-integrable random variables, X := L2(Ω, Rd). Unless otherwise specified, all vector norms ∥· ∥in the subsequent analysis refer to the standard Euclidean (L2) norm. Definition 1 (Functional SCM Operator) A Structural Causal Model is defined by a set of unknown, true functional operators {Fi}d i=1, where each Fi : X pai × Ui →Xi is a map such that Vi := Fi(Pai, Ui), with Pai being the set of parent random variables and Ui an exogenous noise variable. We establish the convention that the corresponding lowercase bold letter, pai, denotes a specific vector of observed values for these parents. Our goal is to learn a model parameterized by θ that approximates this SCM. Our model consists of a pair of conditional operators for each variable Vi: 1. A decoder (generative) operator Hθ : U × X p →X, which aims to approximate F. 2. An encoder (inference) operator Tθ : X × X p →U, which aims to perform abduction by inferring the latent noise. These operators are realized by solving the probability flow ODE (Appendix A). The decoder Hθ solves the ODE from t = T to t = 0, while the encoder Tθ solves it from t = 0 to t = T. Our BELM sampler is a high-fidelity numerical solver designed such that these forward and backward operations are exact algebraic inverses. 2.2 Identifiability and Exact Counterfactual Generation We adapt principles from identifiable generative modeling (Chao et al., 2023) to formalize the conditions for exact counterfactuals. This requires assuming the SCM is invertible with respect to its noise term, a condition discussed in Section 2.11. Theorem 2 (Identifiability via Statistical Independence) Given an SCM operator X := F(Pa, U) where U ⊥⊥Pa and F is invertible w.r.t. U. If a learned encoder Tθ (with sufficient capacity) yields a latent representation Z = Tθ(X, Pa) that is statistically independent of the parents Pa, then Z is an isomorphic representation of the exogenous noise U. 5 Wu, Wang, and Li 2.3 Geometric Inductive Bias for Identifiability The score-matching objective’s geometric inductive biases strengthen our identifiability ar- gument. We leverage the principle of implicit regularization, where optimizers favor ”simpler” functions (Hochreiter and Schmidhuber, 1997; Neyshabur et al., 2018).2 This suggests the model learns the most parsimonious geometric transformation required to ex- plain the data. Considering the local geometry of the data density p(x) provides powerful intuition. In a local region R, if the data is isotropic (spherically symmetric), the simplest score function is a radial vector field, yielding a conformal map. If the structure is simply anisotropic (e.g., ellipsoidal), the model is biased towards learning a local affine map. This refines the notion of a purely conformal bias and leads to the following proposition. Proposition 3 (Implicit Bias towards Simple Geometric',\n",
       "  'function is a radial vector field, yielding a conformal map. If the structure is simply anisotropic (e.g., ellipsoidal), the model is biased towards learning a local affine map. This refines the notion of a purely conformal bias and leads to the following proposition. Proposition 3 (Implicit Bias towards Simple Geometric Maps) Assume (A1) the true data density p(x) is smooth (C2) and (A2) the optimization process has a simplicity bias (e.g., favoring low-complexity solutions, see Appendix H). (i) If there exists a local region R where p(x) is isotropic, the optimal learned score function is a radial vector field, and the flow map it generates is a conformal map on R. (ii) If we relax the condition to a local region R where p(x) has an ellipsoidal structure, the optimal learned score function is normal to the ellipsoidal iso-contours, and the flow map it generates is a local affine transformation on R. The formal argument is detailed in Appendix H. This proposition is significant: it suggests that the model defaults to learning the most parsimonious, well-behaved, and locally in- vertible map that can explain the data’s geometry. This bias is crucial for the abduction step, as it prevents the pathological distortions that would corrupt the inferred causal noise U. Theorem 4 (Operator Isomorphism Guarantees Exact Counterfactuals) Let the con- ditions of Theorem 2 hold. If the learned operator pair(Tθ, Hθ) constitutes a conditional isomorphism (i.e., Hθ(Tθ(·, pa), pa) = I, the identity operator), then the model’s prediction under an intervention do(Pa := α) is exact. Proof A full proof, covering cases for different dimensions of the exogenous noise variable, is provided in Appendix B. 2.4 Analysis of Inversion Fidelity We now formally analyze the inversion error. We prove that standard approximate schemes produce a non-zero SRE (Proposition 5), whereas our chosen sampler eliminates it by construction (Proposition 6). 2. We adopt the principle of simplicity bias, a cornerstone of modern deep learning theory that, while empirically supported, remains an active and not yet universally proven area of research. Our conclusions are conditioned on its validity, as discussed further in Section 2.11. 6 The Causal Round Trip Proposition 5 (Structural Error of Approximate Inversion) Let TDDIM be the op- erator for one step of DDIM inversion from xt to xt+1, and HDDIM be the generative step operator from xt+1 to xt. The single-step reconstruction error is non-zero and of second order in the time step ∆t: (HDDIM ◦TDDIM)(xt) −xt = O((∆t)2) This error accumulates over the full trajectory, leading to a non-zero Structural Reconstruc- tion Error. Proof See Appendix C for a rigorous proof. Proposition 6 (Analytical Invertibility of the Sampler) Let TBELM and HBELM be the operators corresponding to the full-trajectory BELM sampler for inference and genera- tion, respectively. For a fixed noise prediction network ϵθ, the operators are exact algebraic inverses: HBELM ◦TBELM = I Proof The proof follows from the algebraic construction of the BELM update rules, as detailed in Appendix C. 2.5 Error Decomposition for Counterfactual Estimation This brings us to our central theoretical result: an error decomposition theorem',\n",
       "  'noise prediction network ϵθ, the operators are exact algebraic inverses: HBELM ◦TBELM = I Proof The proof follows from the algebraic construction of the BELM update rules, as detailed in Appendix C. 2.5 Error Decomposition for Counterfactual Estimation This brings us to our central theoretical result: an error decomposition theorem that rig- orously partitions the total counterfactual error. This decomposition isolates the SRE and mathematically demonstrates why its elimination is critical. Definition 7 (Counterfactual Error Components) We formally define the two pri- mary sources of error in counterfactual estimation for the invertible case: 1. The Structural Reconstruction Error (ESR) measures the information loss from the model’s abduction-action cycle on a given sample X: ESR(X) := ∥(Hθ ◦Tθ −I)X∥2 2. The Latent Space Invariance Error (ELSI) measures the failure of the learned latent space to remain invariant under interventions on parent variables: ELSI := ∥Tθ(X, Pa) −Tθ(Xtrue α , α)∥2 Theorem 8 (Counterfactual Error Bound) Let a model be defined by (Tθ, Hθ) and the true SCM by F. Assume the decoder Hθ is LH-Lipschitz. The expected squared error of the model’s counterfactual prediction ˆXα is bounded by the expectation of the two error components: E h ∥ˆXα −Xtrue α ∥2i ≤2E \\x02 ESR(Xtrue α ) \\x03 + 2L2 HE [ELSI] 7 Wu, Wang, and Li Proof The proof is in Appendix D. Remark 9 (Elimination of Structural Error) By Proposition 6, the Structural Re- construction Error for BELM-MDCM is identically zero. This is the central theoretical advantage of our framework. It disentangles the error sources, allowing us to isolate the entire modeling challenge to learning a high-quality score function (ϵθ) without the con- founding factor of an imperfect inversion algorithm. Any remaining error is now purely a function of statistical estimation, not a structural bias of the model itself. Proposition 10 (Bound on Latent Space Invariance Error) We assume the learned score network, ϵθ, is Lipschitz continuous, ensuring the existence and uniqueness of the probability flow ODE solution via the Picard-Lindel¨of theorem. Under standard integrability conditions (Fubini’s theorem), the Latent Space Invariance Error is bounded by the expected score-matching loss: E [ELSI] ≤C′ · E \\x02 ∥ϵθ −ϵ∗∥2\\x03 for some constant C′, where ϵ∗is the true score function. Proof The proof is in Appendix D. This proposition formally establishes that by eliminating structural error, the causal fidelity of BELM-MDCM is directly and provably controlled by its ability to accurately learn the data’s score function. 2.6 Decomposing Error: A Motivation for Empirical Validation The error decomposition in Theorem 8 provides a clear strategy for empirical validation by isolating two distinct error sources: the Structural Reconstruction Error (ESR) and the Latent Space Invariance Error. While developing a single score combining these is future work, these components directly motivate our empirical investigations. Our ablation study (Section 5.4.2) is designed to measure the impact of a non-zero ESR, while our stress-test (Section 5.4.1) probes robustness when latent space invariance is challenged by a non-invertible SCM. 2.7 Theoretical Roles of Targeted Modeling and Hybrid Training With algorithmic error eliminated by our Zero-SRE design, the challenge becomes minimiz-',\n",
       "  '(Section 5.4.2) is designed to measure the impact of a non-zero ESR, while our stress-test (Section 5.4.1) probes robustness when latent space invariance is challenged by a non-invertible SCM. 2.7 Theoretical Roles of Targeted Modeling and Hybrid Training With algorithmic error eliminated by our Zero-SRE design, the challenge becomes minimiz- ing the modeling error (ELSI). Our two methodological innovations, Targeted Modeling and Hybrid Training, are principled strategies for this purpose. Targeted Modeling as Formal Complexity Control. Our Targeted Modeling strat- egy acts as a form of structural regularization. The finite sample bound in Theorem 15 is governed by the Rademacher complexity Rn(FΘ) of the entire SCM’s hypothesis space. By assigning low-complexity models to a subset of nodes, we directly constrain the overall complexity. 8 The Causal Round Trip Remark 11 (Effect on Generalization Bound) Our Targeted Modeling strategy is for- mally justified as a complexity control mechanism. The Rademacher complexity of a com- posite SCM is bounded by the sum of the complexities of its individual mechanisms (Mohri et al., 2018). By strategically substituting a high-complexity diffusion model Fdiff with a lower-complexity alternative Fsimple for non-critical nodes, Targeted Modeling directly min- imizes this upper bound. This leads to a tighter generalization bound and improves the statistical efficiency of the overall SCM. Hybrid Training as a Weighted Score-Matching Objective. The Hybrid Training Objective, Ltotal = Ldiffusion+λ·Ltask, imparts a crucial inductive bias for learning a causally salient score function. The task-specific loss acts as a conductor’s baton, forcing the model to prioritize learning an accurate score function in regions of the data manifold most critical to the causal question. We formalize this by proposing that the auxiliary loss implicitly implements a weighted score-matching objective. Proposition 12 (Hybrid Objective as a Weighted Score-Matching Regularizer) The auxiliary task loss Ltask provides a lower bound for the model’s error, weighted by a function reflecting the causal salience of the data manifold. Minimizing the hybrid objec- tive Ltotal is thereby equivalent to solving a weighted score-matching problem that prioritizes accuracy in causally salient regions, leading to a smaller effective Latent Space Invariance Error. (A rigorous proof is provided in Appendix E.) This proposition formally grounds our hybrid training strategy, revealing that the task- specific loss intelligently forces the diffusion model to prioritize accuracy in regions of the data manifold most critical to the causal question. This reinforces the CIC principle by avoiding information loss where it matters most, effectively implementing the simplicity bias principle from Section 2.3. We can deepen this insight by analyzing its information-theoretic implications. Proposition 13 (Disentanglement via Hybrid Objective) Information-theoretically, the hybrid objective provides a strong inductive bias towards learning a disentangled la- tent representation. It encourages a ”division of labor” where the task-specific compo- nent explains variance from the parents Pa, while the diffusion component’s latent code Z = Tθ(V, Pa) models the residual information. This implicitly pushes Z towards being independent of Pa, a crucial step towards satisfying the identifiability conditions. Proof A detailed information-theoretic argument is provided in Appendix E. 2.8 BELM-MDCM as a Unifying Framework The principle of Causal Information',\n",
       "  'diffusion component’s latent code Z = Tθ(V, Pa) models the residual information. This implicitly pushes Z towards being independent of Pa, a crucial step towards satisfying the identifiability conditions. Proof A detailed information-theoretic argument is provided in Appendix E. 2.8 BELM-MDCM as a Unifying Framework The principle of Causal Information Conservation also unifies our framework with classical models. Simpler models like Additive Noise Models (ANMs) can be seen as special cases where this principle is met trivially, positioning our work as a generalization of established causal principles. For instance, in a classic ANM (Hoyer et al., 2009), Vi = fi(Pai) + Ui, the noise is recovered by a direct, lossless inversion: Ui = Vi −fi(Pai). Our framework gen- eralizes this principle to arbitrarily complex, non-additive mechanisms, offering a flexible, 9 Wu, Wang, and Li non-parametric extension to classical structural equation models (Wooldridge, 2010). The importance of noise distributions, particularly non-Gaussianity, for identifiability in linear models is also a well-established principle (Shimizu et al., 2006). 2.9 Learnability and Statistical Guarantees We now provide finite-sample learnability guarantees for our SCM framework. Proposition 14 (Asymptotic Consistency) Under standard regularity conditions, as the number of data samples n →∞and model capacity N →∞, the learned operators (ˆTn, ˆHn) are consistent estimators of the ideal operators (T∗, H∗): ˆTn p−→T∗and ˆHn p−→H∗. Theorem 15 (Finite Sample Bound for Causal Diffusion SCMs) Let an SCM con- sist of d endogenous nodes, with a causal graph having a maximum in-degree of dmax in . As- sume each causal mechanism is implemented by a score network ϵθ that is an L-layer MLP with ReLU activations, and the spectral norm of each weight matrix is bounded by B. Let the input space be appropriately normalized. Let the loss function be bounded by M. Then, for the parameters ˆθn learned from n samples, the excess risk is bounded with probability at least 1 −δ: R(ˆθn) −R(θ∗) ≤C · d · L · BL · p dmax in + dembed + 1 √n + M r log(1/δ) 2n where C is a constant independent of the network architecture and sample size, and dembed is the dimension of the time embedding. Proof The proof, which combines the sub-additivity of Rademacher complexity over the SCM with standard bounds for deep neural networks (Bartlett et al., 2017; Neyshabur et al., 2018), is detailed in Appendix G. Remark 16 (Interpretation of the Bound) This refined bound quantitatively links the generalization error to: (i) Causal Complexity (d · p dmax in ): The error scales with the number of causal mech- anisms (d) and the graph’s complexity (dmax in ), formalizing the intuition that more complex causal systems are harder to learn. (ii) Network Complexity (L · BL): The error scales with the depth and spectral norm of the score networks. This provides direct theoretical grounding for our Targeted Modeling strategy, as using simpler models tightens this generalization bound. 2.10 Implications for Causal Transportability Causal Information Conservation also provides a foundation for transportability—applying knowledge from a source domain S to a target domain T (Pearl and Bareinboim, 2014).',\n",
       "  'the score networks. This provides direct theoretical grounding for our Targeted Modeling strategy, as using simpler models tightens this generalization bound. 2.10 Implications for Causal Transportability Causal Information Conservation also provides a foundation for transportability—applying knowledge from a source domain S to a target domain T (Pearl and Bareinboim, 2014). Transportability requires separating invariant causal knowledge from domain-specific mech- anisms. By losslessly recovering the exogenous noise U (the invariant ”causal essence”), our framework achieves this separation by design; the decoders Hθ represent the domain-specific mechanisms. This insight is formalized in the following theorem. 10 The Causal Round Trip Theorem 17 (Condition for Lossless Causal Transport) Let a source domain S and a target domain T be described by SCMs MS and MT , respectively. Assume the following conditions hold: (i) Shared Structure: Both domains share the same causal graph G and the same ex- ogenous noise distributions {pi(Ui)}. The domains differ only in a subset of causal mechanisms Kchanged. (ii) Noise Independence: The exogenous noise variables {Ui}d i=1 are mutually indepen- dent. (iii) Information Conservation: A model (Tθ, Hθ) trained on data from S satisfies the Causal Information Conservation principle, achieving zero Structural Reconstruction Error. Then, causal knowledge can be losslessly transported from S to T by re-learning only the operators {Tθk, Hθk} corresponding to the changed mechanisms k ∈Kchanged, while directly reusing all operators for invariant mechanisms. Proof The proof is provided in Appendix F. 2.11 Discussion of Assumptions Our framework rests on several key assumptions, which we now critically examine. Our geometric inductive bias argument (Proposition 3) rests on the principle of sim- plicity bias. While this principle is a cornerstone of modern deep learning theory with substantial empirical backing, it remains an active area of research and is not a universally proven theorem. Our conclusions are therefore conditioned on the validity of this powerful but conjectural assumption. The cornerstone of our identifiability theory (Theorem 2) is the SCM’s invertibility with respect to its noise term U. This is a strong assumption; when violated (e.g., by a many-to-one function), the abduction task becomes ill-posed. To address this foundational challenge, we provide an exhaustive theoretical treatment in Appendix C. There, we formalize the irreducible ”representational error” and derive a tighter, more general error bound (Theorem 21). More importantly, we propose a concrete mitigation strategy: a novel prior-matching regularizer (Definition 23), theoretically shown to reduce the error by encouraging the learned encoder to approximate the ideal Maximum a Posteriori (MAP) solution (Proposition 24). This highlights a primary contribution: even in the challenging non-invertible case, BELM-MDCM’s zero-SRE design eliminates the algorithmic error, thereby isolating the more fundamental representational challenge. Our stress-test in Section 5.4.1 empirically confirms this advantage, while validating our regularizer provides a clear direction for future work. Our identifiability proof is dimension-dependent, leveraging Liouville’s theorem for d ≥3 and requiring stronger assumptions like asymptotic linearity for the special case of d = 2. Other assumptions, such as Lipschitz continuity of the score network, are mild regularity conditions standard in deep generative model analysis and can be encouraged through architectural choices like',\n",
       "  'dimension-dependent, leveraging Liouville’s theorem for d ≥3 and requiring stronger assumptions like asymptotic linearity for the special case of d = 2. Other assumptions, such as Lipschitz continuity of the score network, are mild regularity conditions standard in deep generative model analysis and can be encouraged through architectural choices like spectral normalization. 11 Wu, Wang, and Li 3 Architectural Design and Training The BELM-MDCM architecture embodies our core principles through a non-monolithic, theoretically-motivated design. Its central philosophy is Targeted Modeling: judiciously allocating the expressive power of our Zero-SRE CausalDiffusionModel to nodes of causal interest (e.g., Treatment T, Outcome Y), while using simpler, efficient mechanisms for confounders, as illustrated in Figure 1. This strategy provides practical complexity control, tightening the generalization bound as established in Theorem 15. W X T Y Empirical Distribution Additive Noise Model CausalDiffusionModel (BELM-MDCM) CausalDiffusionModel (BELM-MDCM) Targeted Modeling Principle: The expressive power of the CausalDif- fusionModel is judiciously allocated to key causal nodes (Treatment T, Outcome Y) for high-fidelity counterfactual genera- tion. Simpler, eﬀicient mechanisms (e.g., ANM, Empirical Distribution) are used for con- founder nodes (W, X) to ensure stability and eﬀiciency. Figure 1: Illustration of the Targeted Modeling Principle. The expressive CausalDiffusionModel is judiciously allocated to key causal nodes (Treatment T, Out- come Y) for high-fidelity counterfactual generation. Simpler, efficient mechanisms (e.g., ANM, Empirical Distribution) are used for confounder nodes (W, X) to ensure stability and efficiency. The internal architecture of the CausalDiffusionModel itself, depicted in Figure 2, is engineered to learn the complex, non-linear mapping vi := fi(pai, ui) with high fidelity. 12 The Causal Round Trip ǜ푛ǙǑ ǜ푐ǅǘ1 M ǜ푐ǅǘ2 50 257 -3.0 woman ǜ푐ǅǘ3 StandardScal er OneHotEncod er ǜ푛ǙǑ⨁ǜ푐ǅǘ Connection Timestep embedding Select BELM- MDCM module Noisy Target Variable Inverse Transformati on Pre-Processing Embedding Train Post-Processing Causal Identification Results Figure 2: The detailed internal architecture of the CausalDiffusionModel. This diagram illustrates the end-to-end workflow of the causal mechanism designed for key nodes like Treatment T and Outcome Y, detailing the pre-processing, embedding, training, and post- processing stages. 3.1 Mechanism for Exogenous Nodes Exogenous nodes (without parents in the causal graph G) are modeled non-parametrically via the Empirical Distribution of the observed data. This approach avoids distributional assumptions and provides a robust foundation for the Structural Causal Model (SCM). 3.2 Mechanism for Endogenous Nodes: The CausalDiffusionModel For endogenous nodes Vi, particularly those central to the causal query (treatment, outcome, key mediators), we employ our bespoke CausalDiffusionModel to learn the functional mapping vi := fi(pai, ui). 3.2.1 Conditioning via Parent Node Transformation The denoising process is conditioned on the parent nodes pai, which are transformed into a fixed-dimensional conditioning vector c ∈Rdc. A ColumnTransformer handles heteroge- neous data types: continuous parents are standardized (StandardScaler) to unify scales, while categorical parents are one-hot encoded (OneHotEncoder) to prevent artificial or- dinality. The resulting vectors are concatenated into c, which remains constant for a given sample’s diffusion trajectory. 3.2.2 The Denoising Process The core of the CausalDiffusionModel is a denoising network ϵθ(vt, t, c), implemented as a Residual MLP (He et al., 2016). It takes as input the noisy variable',\n",
       "  'or- dinality. The resulting vectors are concatenated into c, which remains constant for a given sample’s diffusion trajectory. 3.2.2 The Denoising Process The core of the CausalDiffusionModel is a denoising network ϵθ(vt, t, c), implemented as a Residual MLP (He et al., 2016). It takes as input the noisy variable vt, a sinusoidal Time Embedding of timestep t, and the conditioning vector c. Before the diffusion process, the target variable Vi is also preprocessed (standardized for continuous values or label-encoded 13 Wu, Wang, and Li for categorical ones). The denoising process is driven by the BELM sampler, ensuring a mathematically exact and stable inversion path as established in Section 2. 3.2.3 Hybrid Training Objective We introduce a Hybrid Training Objective to reconcile generative fidelity with predictive accuracy. As established in our theoretical analysis (Proposition 12), this is more than a standard multi-task learning scheme; it acts as a powerful inductive bias, creating a weighted score-matching objective that prioritizes accuracy in causally salient regions of the data manifold. The total loss is a linearly weighted combination: Ltotal = Ldiffusion + λ · Ltask (5) where Ldiffusion is the noise prediction error (Equation 1). The auxiliary loss Ltask is a Mean Squared Error for continuous nodes (Lregression) and a Cross-Entropy loss for discrete nodes (Lclassification). 3.2.4 Decoding and Counterfactual Generation For generation, the BELM sampler produces an output in the normalized space. This is then mapped back to the original data domain using the inverse transformations of the pre- fitted preprocessors (StandardScaler for continuous, LabelEncoder for categorical). For categorical outputs, the continuous value is rounded and clipped to the valid class range before the inverse mapping, ensuring that generated (counterfactual) data is interpretable and resides in the correct space. 4 New Evaluation Metrics for Generative Causal Models The principle of Causal Information Conservation demands new evaluation dimensions that traditional metrics like ATE and PEHE cannot capture. An accurate ATE score, for in- stance, could arise from a model with high SRE where individual errors fortuitously cancel out at the population level. To move beyond mere outcome accuracy and directly assess a model’s adherence to our foundational principle, we propose a new, theoretically-grounded evaluation framework. 4.1 Causal Information Conservation Score (CIC-Score) The Causal Information Conservation Score (CIC-Score) is a direct empirical di- agnostic for the Structural Reconstruction Error. It quantifies a framework’s adherence to the CIC principle by disentangling algorithmic information loss (from an imperfect inver- sion process) from modeling error (from the statistical challenge of learning the true causal mechanism). We define the score, bounded in [0, 1], using an exponential formulation: CIC-Score = exp (−(δU + δSRE)) The error components are designed to isolate distinct failure modes: • δU, the Relative Noise Recovery Error, quantifies the modeling error. It measures how well the trained network approximates the true score function, reflected in the fidelity 14 The Causal Round Trip of the recovered noise ˆU versus the ground-truth Utrue: δU = E[∥ˆUscaled −Utrue, scaled∥2] E[∥Utrue, scaled∥2] This term captures all inaccuracies from finite data and imperfect optimization. • δSRE, the Normalized Structural Error, exclusively',\n",
       "  'trained network approximates the true score function, reflected in the fidelity 14 The Causal Round Trip of the recovered noise ˆU versus the ground-truth Utrue: δU = E[∥ˆUscaled −Utrue, scaled∥2] E[∥Utrue, scaled∥2] This term captures all inaccuracies from finite data and imperfect optimization. • δSRE, the Normalized Structural Error, exclusively quantifies the algorithmic error inherent to the inversion process itself. Its definition is model-dependent to allow for fair comparisons: – For frameworks with analytical invertibility (e.g., our BELM-MDCM, ANMs), the algorithm introduces no information loss, so we set δSRE ≡0 by construction. Any observed reconstruction error is a symptom of modeling error, already captured by δU. – For frameworks relying on approximate inversion (e.g., DDIM), δSRE is empirically measured to quantify this inherent algorithmic flaw: δSRE = E[∥(Hθ ◦Tθ −I)X∥2] E[∥X∥2] This principled decomposition allows the CIC-Score to fairly assess different frameworks by isolating structural design advantages from the universal challenge of model training. 4.2 Causal Mechanism Fidelity Score (CMF-Score) A generative causal model’s core promise is to learn true causal mechanisms, not just outcomes. Na¨ıve metrics like pairwise correlations fail to capture the non-linear, multi- variable, and directional nature of causality. We therefore propose the Causal Mechanism Fidelity (CMF) score, a hierarchical framework with two levels of increasing rigor. 4.2.1 Level 1 (Pragmatic): The Conditional Mutual Information Score (CMI-Score) The Conditional Mutual Information (CMI), I(Vi; Vj|Paj \\\\ {Vi}), is a non-parametric, non- linear measure of the direct influence a parent Vi has on its child Vj after accounting for all other parents. The CMI-Score evaluates whether this influence is preserved. For a single mechanism Vj, it is the average consistency across all parent-child edges: CMI-Score(Vj) = 1 |Paj| X Vi∈Paj \\uf8eb \\uf8ed1 − Iobs(Vi; Vj|·) −Icf(Vi; V ′ j |·) Iobs(Vi; Vj|·) + ϵ \\uf8f6 \\uf8f8 where Iobs and Icf are the CMI values from observational and counterfactual data, respec- tively. The final CMI-Score is the average over all SCM mechanisms. 4.2.2 Level 2 (Gold Standard): The Kernelized Mechanism Discrepancy (KMD) Score To rigorously compare entire conditional distributions, we use the Maximum Mean Dis- crepancy (MMD) (Gretton et al., 2012), a kernel-based statistical test for distributional 15 Wu, Wang, and Li equality. The KMD-Score applies this test to the conditional distributions p(Vj|Paj) that define each causal mechanism, measuring the discrepancy between the learned and observed conditionals. The final score is mapped to a similarity measure in [0, 1]: KMD-Score = exp(−γ · Epaj∼p(Paj)[MMD(p(Vj|paj), pθ(Vj|paj))]) where γ is a scaling parameter. A score of 1 indicates that the learned conditional mecha- nism is statistically indistinguishable from the observed one. Complementary and Validated Evaluation Metrics. Our proposed metrics comple- ment, rather than replace, traditional ones like ATE and PEHE. They evaluate distinct facets of performance: while ATE/PEHE measure outcome accuracy, the CMF-Score as- sesses mechanism fidelity. This distinction is critical, as a model can achieve a high ATE via fortuitous error cancellation despite failing to learn the true data-generating process. To ensure our metrics are practically reliable, we conducted a controlled micro-simulation study, detailed in Appendix J. The results provide strong empirical evidence',\n",
       "  'sesses mechanism fidelity. This distinction is critical, as a model can achieve a high ATE via fortuitous error cancellation despite failing to learn the true data-generating process. To ensure our metrics are practically reliable, we conducted a controlled micro-simulation study, detailed in Appendix J. The results provide strong empirical evidence for their validity and complementary roles: the CIC-Score acts as a high-sensitivity SRE detector; the CMI-Score robustly tracks the fidelity of causal associations; and the KMD-Score serves as a final arbiter of distributional similarity. This validation confirms that our evaluation framework offers a more complete, nuanced, and reliable assessment of generative causal models. 5 Experiments Our empirical evaluation is designed as a comprehensive test of our central thesis: that eliminating SRE is a necessary condition for generating authentic counterfactuals and un- locks analytical capabilities beyond the reach of conventional methods. We structured the study as a four-act narrative to rigorously test our claims. Act I establishes our model’s state-of-the-art predictive fidelity on standard benchmarks. Act II provides a deep diag- nostic analysis, using our proposed metrics as empirical evidence for the destructive effect of SRE. Act III showcases the unique capabilities unlocked by an information-conserving framework. Finally, Act IV validates the framework’s robustness through a series of stress tests and a full ablation study. Evaluation Protocol. For a rigorous evaluation, we employ two complementary proto- cols. This distinction is crucial, as it separates the assessment of our methodology’s peak performance from the diagnostic analysis of its components. (a) Ensemble Evaluation for SOTA Performance: To benchmark against state-of- the-art methods (specifically, ITE estimation in Section 5.1.3), we adopt the standard Deep Ensemble methodology. We train N=5 independent models and report the final metric (e.g., PEHE) on the ensembled prediction. (b) Individual Model Evaluation for Diagnostic Analysis: In all other experiments where the goal is a fair, apples-to-apples architectural comparison or stability assess- ment, we report the mean and standard deviation of metrics from individual model instances across N=5 runs. This isolates the effect of design choices from the gains of ensembling. 16 The Causal Round Trip We estimate the Average Treatment Effect (ATE) throughout our experiments using a standard counterfactual imputation procedure, the pseudo-code for which is detailed in Algorithm 1 in Appendix K. Baseline Estimators. The Directed Acyclic Graphs (DAGs) for our experiments are shown in Figure 3. We benchmark BELM-MDCM against a suite of baselines from the DoWhy library (Sharma and Kiciman, 2022), spanning classical statistical methods to state- of-the-art machine learning estimators to ensure a comprehensive comparison. W1 (Covariate) C1 (Categorical Confounder) T (Treatment) Y (Outcome) W2 (Covariate) (a) PSM Failure Scenario X1 (Confounder) T (Treatment) Y (Outcome) X2 (Confounder) M (Mediator) Z (Instrumental Variable) (b) Ablation Study Scenario Confounders (age, educ, re74, etc.) Treatment (treat) Outcome (re78) (c) Lalonde Confounding Structure Figure 3: Directed Acyclic Graphs (DAGs) for key experiments. (a) A structure designed to challenge propensity score methods. (b) A mediation structure used for the ablation study. (c) The standard confounding structure assumed for both Lalonde-based experiments. 5.1 Act I: Establishing State-of-the-Art Predictive Fidelity We first establish that',\n",
       "  'Confounding Structure Figure 3: Directed Acyclic Graphs (DAGs) for key experiments. (a) A structure designed to challenge propensity score methods. (b) A mediation structure used for the ablation study. (c) The standard confounding structure assumed for both Lalonde-based experiments. 5.1 Act I: Establishing State-of-the-Art Predictive Fidelity We first establish that our principled design achieves superior predictive fidelity on standard causal inference benchmarks. 5.1.1 Robustness in Non-Linear Confounding Scenarios We tested our model in a challenging synthetic scenario (Figure 3a) designed with highly non-linear confounding to cause propensity-based methods to fail. Table 1 shows the results. While Causal Forest is exceptionally accurate on this specific DGP, our BELM-MDCM framework secures its position as the second most accurate method, delivering a highly stable and competitive ATE estimate. Crucially, it significantly outperforms the entire suite of propensity-based methods and powerful estimators like DML in accuracy. The high standard deviation of DML highlights its unreliability in this context, validating our model as a robust estimator where traditional approaches are compromised. 17 Wu, Wang, and Li Table 1: ATE Estimation on the PSM Failure Scenario (True ATE = 5000). We report the mean ATE and standard deviation across multiple runs. Method Mean ATE ± Std Dev Absolute Error BELM-MDCM 5266.87 ± 197.14 266.87 Causal Forest 4895.77 ± 69.26 104.23 Propensity Score Stratification 5309.38 ± 185.36 309.38 Linear Regression 5348.82 ± 23.23 348.82 Propensity Score Matching 5353.93 ± 191.36 353.93 Inverse Propensity Weighting 5385.68 ± 52.03 385.68 Double Machine Learning 4285.63 ± 550.97 714.37 5.1.2 Accuracy and Robustness on Real-World Observational Data We next evaluated our framework on the canonical Lalonde dataset (Lalonde, 1986), a challenging real-world benchmark with a known RCT ground truth. Table 2 demonstrates the comprehensive superiority of our BELM-MDCM framework. It achieved a mean ATE estimate of 1567.36 ± 201.62, the lowest error among all methods that correctly identified the treatment effect’s positive direction. More critically, the results highlight a stark contrast in reliability. Classical methods failed entirely, while the powerful Causal Forest baseline suffered from extreme instability (Std Dev of 785.59). In contrast, BELM- MDCM exhibited remarkable robustness, with a standard deviation approximately four times lower. This outstanding performance on a canonical benchmark validates that our framework delivers accurate estimates with the consistency essential for trustworthy causal inference. Table 2: ATE Estimation Stability on the Lalonde Dataset (RCT Benchmark ATE ≈1794). Results for all models are reported as Mean ± Standard Deviation across 5 independent runs. Method ATE (Mean ± Std) Abs. Error (Mean) BELM-MDCM 1567.36 ± 201.62 226.64 Causal Forest 1085.30 ± 785.59 708.70 Linear Regression 46.33 ± 76.80 1747.67 Propensity Score Matching -3.96 ± 118.37 1797.96 Propensity Score Stratification -35.54 ± 81.44 1829.54 Propensity Score Weighting -122.55 ± 50.51 1916.55 Double Machine Learning nan ± nan nan 5.1.3 High-Fidelity ITE Estimation and Stability Analysis Objective. We evaluate performance at the individual level using a semi-synthetic version of the Lalonde dataset. This experiment leverages real-world covariates and assumes the causal structure depicted in Figure 3c. To rigorously assess both accuracy and reliability, we 18 The Causal Round Trip follow our Individual Model',\n",
       "  'and Stability Analysis Objective. We evaluate performance at the individual level using a semi-synthetic version of the Lalonde dataset. This experiment leverages real-world covariates and assumes the causal structure depicted in Figure 3c. To rigorously assess both accuracy and reliability, we 18 The Causal Round Trip follow our Individual Model Evaluation protocol, reporting the mean and standard deviation of performance across 5 independent runs for each method. Results. The PEHE results, presented in Table 3, confirm the exceptional fidelity and robustness of our framework. BELM-MDCM achieves the lowest average PEHE score of 537.84 and demonstrates remarkable stability with the lowest standard deviation of just 60.11. This performance is closely followed by Causal Forest. However, the results also highlight the instability of other meta-learners; X-Learner, in particular, exhibits extremely high variance, with a standard deviation more than three times larger than its competitors, rendering its single-run estimates unreliable. This highlights the dual advantage of our framework: superior accuracy combined with consistent, trustworthy performance. Figure 4 provides visual confirmation, showing the tight clustering of our model’s ensembled ITE estimates around the ground truth. 0 1000 2000 3000 4000 True ITE 0 1000 2000 3000 4000 Estimated ITE (Ensemble) Accuracy of Individual Treatment Effect (ITE) Estimation Individual Samples (BELM-MDCM Ensemble) Perfect Match (y=x) Figure 4: Accuracy of Individual Treatment Effect (ITE) Estimation on the semi-synthetic Lalonde dataset. The plot shows the ensembled estimated ITE from our model versus the true ITE. The tight clustering of our model’s estimates (blue dots) around the perfect-match line (red dash) visually demonstrates its low PEHE score. 19 Wu, Wang, and Li Table 3: ITE Estimation Accuracy (PEHE) on the Semi-Synthetic Lalonde Dataset. Results are reported as Mean ± Standard Deviation across 5 independent runs. Lower is better. Method PEHE Score (Mean ± Std) BELM-MDCM 537.84 ± 60.11 Causal Forest 563.90 ± 73.66 S-Learner 816.26 ± 79.17 X-Learner 1546.38 ± 679.09 Table 4: Causal Mechanism Fidelity (CMI-Score) on the Semi-Synthetic Lalonde Dataset. Results are reported as Mean ± Standard Deviation across 5 runs. Higher is better. Method CMI-Score (Mean ± Std) S-Learner 0.9905 ± 0.0062 BELM-MDCM 0.9824 ± 0.0092 Causal Forest 0.9786 ± 0.0099 X-Learner 0.9782 ± 0.0145 T-Learner 0.9555 ± 0.0113 5.2 Act II: Uncovering the Accuracy-Invertibility Trade-off We now conduct the pivotal experiment of our study: a deep diagnostic analysis using our novel CIC-Score to reveal the trade-off between predictive accuracy and mechanism invertibility. This provides the core empirical evidence for our thesis by comparing three paradigms: our BELM-MDCM (Learned Invertibility), a DDIM variant (Flawed Invertibil- ity), and a classic RF-ANM (Assumed Invertibility). The results in Table 5 decisively validate our framework’s principles. Our BELM- MDCM is the clear leader, achieving the lowest PEHE score (1071.95) with high stabil- ity. Critically, its CIC-Score of 0.3679 is orders of magnitude higher than the alternatives, proving its unique ability to learn an invertible mapping that conserves causal informa- tion. In stark contrast, the DDIM-MDCM model exemplifies the failure predicted by our theory: its near-zero CIC-Score confirms a near-total collapse of causal information due to SRE, leading to unreliable predictions (high',\n",
       "  'magnitude higher than the alternatives, proving its unique ability to learn an invertible mapping that conserves causal informa- tion. In stark contrast, the DDIM-MDCM model exemplifies the failure predicted by our theory: its near-zero CIC-Score confirms a near-total collapse of causal information due to SRE, leading to unreliable predictions (high PEHE and variance). The classical RF-ANM, while structurally invertible, lacks the capacity to learn the true mechanism, resulting in a zero CIC-Score and poor accuracy. This ”Golden Table” experiment underscores that both structural integrity and powerful modeling capacity are essential for high-fidelity causal inference. The Likelihood-Fidelity Dilemma: Why Natively Invertible Models Can Fail. To rigorously test the limits of models that are natively invertible, we conducted a com- prehensive stability analysis on a Conditional Normalizing Flow (NF) baseline, a model class that satisfies the Causal Information Conservation principle by construction (SRE ≡0). Across five independent runs with different random seeds, the NF model consistently demonstrated successful statistical learning, with its training loss stably converging to a high log-likelihood in each instance (a representative example is shown in Figure 5). 20 The Causal Round Trip Table 5: The ”Ultimate Golden Table”: A comparative analysis of model classes on pre- dictive accuracy (PEHE) and structural integrity (CIC-Score). This table includes the NF-SCM baseline, which empirically validates the likelihood-fidelity dilemma. Results are reported as Mean ± Standard Deviation across 5 runs. Lower PEHE is better; higher CIC- Score is better. Model PEHE (Mean ± Std) CIC-Score (Mean ± Std) RF-ANM 1533.18 ± 134.24 0.0000 ± 0.0000 DDIM-MDCM 2085.98 ± 788.12 0.0065 ± 0.0130 NF-SCM 442229.96 ± 66963.73 0.1572 ± 0.0232 BELM-MDCM 1071.95 ± 152.11 0.3679 ± 0.0000 0 25 50 75 100 125 150 175 200 Epoch 2 0 2 4 6 Negative Log-Likelihood Loss Training Loss Curve Figure 5: The training loss curve for the Conditional Normalizing Flow (NF) baseline. The smooth, stable convergence to a low negative log-likelihood value indicates a successful statistical training run. However, this did not correspond to learning the true causal mech- anism, as evidenced by its extremely high PEHE score. However, this statistical success was starkly contrasted by a systematic and catas- trophic failure in the causal task. The model yielded an average PEHE score of 442,229.96 ± 66,963.73, confirming that its generated counterfactuals were fundamentally incor- rect. This consistent result provides decisive evidence for a critical challenge we term the likelihood-fidelity dilemma: a model can perfectly learn to replicate a data distribution while remaining completely ignorant of the underlying causal mechanism. The root of this dilemma is the fundamental mismatch between the optimization ob- jective and the causal goal. The maximum likelihood objective incentivizes the NF to find any invertible mapping that transforms the data to a simple base distribution. While an infinite number of such mappings may be statistically equivalent, only one corresponds to the true, unique causal data-generating process. Without a guiding signal, the NF is mathematically predisposed to learn a causally-incorrect ”statistical shortcut.” This find- ing powerfully underscores the contribution of our Hybrid Training Objective. It acts 21 Wu, Wang, and Li as the',\n",
       "  'may be statistically equivalent, only one corresponds to the true, unique causal data-generating process. Without a guiding signal, the NF is mathematically predisposed to learn a causally-incorrect ”statistical shortcut.” This find- ing powerfully underscores the contribution of our Hybrid Training Objective. It acts 21 Wu, Wang, and Li as the crucial causal inductive bias that resolves this dilemma, compelling the model to learn the unique, causally salient structure and enabling valid causal inference where pure likelihood-based methods, even those with zero SRE, are destined to fail. 5.3 Act III: Unlocking Deeper Causal Inquiry with a High-Fidelity Model An information-conserving model serves as a trustworthy ”world model” for deep causal inquiry. We showcase three applications uniquely enabled by our framework’s high-fidelity counterfactuals. Heterogeneity Analysis: Conditional ATE (CATE). A reliable ITE model can act as a “causal microscope.” We use it to explore treatment effect heterogeneity by estimat- ing the Conditional Average Treatment Effect (CATE) for subpopulations. By averaging the results from our five independently trained models, we obtain stable and robust esti- mates. Our model’s mean CATE estimates track the true CATE trends with high fidelity across different education levels, a capability crucial for policy-making. For instance, for individuals with an education level of 3.0, the estimated CATE was $2562.79 (true CATE: $2280.79). For levels 8.0, 12.0, and 16.0, the estimates were $2092.91 (true: $2118.61), $2253.76 (true: $2384.44), and $2434.89 (true: $2490.21), respectively, demonstrating a close correspondence to the ground truth. Victim (Index 201) Actual Outcome Victim (Index 201) with Top Responder\\'s \"Luck\" 0 5000 10000 15000 20000 25000 30000 Outcome (Y) $10,993.98 $33,328.30 Causal Attribution: The Impact of Exogenous \"Luck\" Top Responder (Index 281) Actual Outcome ($4,296.47) Figure 6: Causal Attribution Analysis. This chart shows the counterfactual outcome for the ’Victim’ if they had possessed the individual-specific exogenous factors of the ’Top Responder’. Causal Attribution: Isolating the Effect of Exogenous Factors. We conducted a causal attribution experiment via a counterfactual intervention of the form do(Uvictim := 22 The Causal Round Trip uresponder). Figure 6 shows that our framework can losslessly recover these factors, reveal- ing that unobserved exogenous ’luck’3 had a massive and stable causal effect, averaging a +22,334.32 change in the ’Victim’s’ outcome. This capacity for reliable attribution is a unique advantage of our information-conserving framework. 0 20000 40000 60000 80000 100000 Outcome (Y) 0 1 2 3 4 5 6 7 Density 1e 5 Audited Group: black=1 (N=371) Average Fairness Gap: $-1,678.16 Counterfactual Fairness Audit for Attribute: \"black\" Actual Outcome Distribution (black=1) Mean Actual: $8,829.19 Counterfactual Outcome Distribution (if black=0) Mean Counterfactual: $7,151.03 (a) Fairness audit for attribute ’black’. 20000 10000 0 10000 20000 30000 40000 50000 60000 Outcome (Y) 0 1 2 3 4 5 Density 1e 5 Audited Group: hisp=1 (N=39) Average Fairness Gap: $1,862.12 Counterfactual Fairness Audit for Attribute: \"hisp\" Actual Outcome Distribution (hisp=1) Mean Actual: $5,145.18 Counterfactual Outcome Distribution (if hisp=0) Mean Counterfactual: $7,007.30 (b) Fairness audit for attribute ’hisp’. Figure 7: Counterfactual fairness audits reveal significant outcome disparities based on sensitive attributes. The plots show the distribution of actual outcomes for each group versus the',\n",
       "  'for Attribute: \"hisp\" Actual Outcome Distribution (hisp=1) Mean Actual: $5,145.18 Counterfactual Outcome Distribution (if hisp=0) Mean Counterfactual: $7,007.30 (b) Fairness audit for attribute ’hisp’. Figure 7: Counterfactual fairness audits reveal significant outcome disparities based on sensitive attributes. The plots show the distribution of actual outcomes for each group versus the distribution of their counterfactual outcomes had their sensitive attribute been different. Counterfactual Fairness Audit. Finally, we applied our framework to a counterfactual fairness audit. Only a model that faithfully represents the data generating process can reliably answer questions about fairness. Figure 7 reveals significant and stable disparities: our model estimates an average fairness gap of -1,678.16 for the ’black’ attribute and +1,862.12 for the ’hisp’ attribute, demonstrating its capacity as a powerful tool for ethical audits. 5.4 Act IV: Final Validation: Stress Tests and Ablation Study We conclude by subjecting the framework to two final tests: a stress test on a non-invertible SCM and a comprehensive ablation study. 5.4.1 Stress Test on a Non-Invertible SCM We tested our framework’s robustness when the theoretical assumption of an invertible SCM is violated, using a DGP where Y ∝U2 Y . The results in Table 6 and Figure 8 decisively validate our hypothesis. On the definitive metric of individual-level fidelity (PEHE), our zero-SRE BELM framework achieves an error of 0.77, a 44% reduction compared to the SRE-prone DDIM model. This empirically proves that even when the true SCM is non- invertible, eliminating algorithmic SRE provides a substantial advantage. This result is fully consistent with our theoretical analysis in Appendix C (Theorem 21), where the total 3. In this context, the term ’luck’ serves as an intuitive shorthand for the exogenous noise variable U. Within the Structural Causal Model (SCM) framework, U represents all unobserved, individual-specific factors (e.g., intrinsic ability, random chance, measurement errors) that, together with the observed parent variables (Pa), determine the final outcome for an individual. 23 Wu, Wang, and Li error is decomposed into algorithmic, modeling, and representational errors. By eliminating the algorithmic error (ESR ≡0), our framework’s performance approaches the theoretical limit set by the other two components. Interestingly, the DDIM sampler achieves a slightly higher KMD-Score. We hypoth- esize that its inherent inversion noise acts as a form of implicit regularization, making the marginal generated distribution appear closer to the truth, even while individual-level counterfactuals are less accurate. This highlights the important distinction between distri- butional fidelity and individual-level causal accuracy. DDIM BELM 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Score 0.973 0.740 Group-Level Accuracy (ATE Error) Lower is Better DDIM BELM 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 1.376 0.766 Individual-Level Fidelity (PEHE) Lower is Better DDIM BELM 0.0 0.2 0.4 0.6 0.8 1.0 Score 0.980 0.994 Mechanism Fidelity (CMI-Score) Higher is Better DDIM BELM 0.0 0.2 0.4 0.6 0.8 1.0 0.907 0.830 Distributional Fidelity (KMD-Score) Higher is Better Robustness to Many-to-One SCM: BELM vs. DDIM Figure 8: Robustness comparison on the many-to-one SCM. Our zero-SRE framework (BELM) demonstrates significantly superior performance on PEHE. Table 6: Performance on the non-invertible SCM (Y ∝U2). Results are averaged over 5',\n",
       "  '0.6 0.8 1.0 0.907 0.830 Distributional Fidelity (KMD-Score) Higher is Better Robustness to Many-to-One SCM: BELM vs. DDIM Figure 8: Robustness comparison on the many-to-one SCM. Our zero-SRE framework (BELM) demonstrates significantly superior performance on PEHE. Table 6: Performance on the non-invertible SCM (Y ∝U2). Results are averaged over 5 runs (± std). Lower PEHE is better. Sampler PEHE(Mean ± Std) KMD-Score(Mean ± Std) BELM (Zero SRE) 0.77 ± 0.16 0.830 ± 0.009 DDIM (with SRE) 1.38 ± 0.19 0.907 ± 0.023 5.4.2 Ablation Study: Deconstructing the Framework’s Success We conducted a comprehensive ablation study on a challenging synthetic dataset (Figure 3b) to validate the contribution of each core component. The findings, presented in Table 7 and Figure 9, provide unequivocal evidence for our integrated design. The full BELM-MDCM 24 The Causal Round Trip model establishes the gold standard for both accuracy and stability. The study reveals three critical insights: • Decisive Role of the Hybrid Objective: Removing it causes a catastrophic decline in performance (400%+ error increase), demonstrating it is a core driver of the causal inductive bias, not merely a fine-tuning mechanism. • Critical Importance of Targeted Modeling: Removing it leads to a complete collapse in model stability (Std Dev explodes from 4.57 to 138.01), validating our theoretical analysis on complexity control. A judicious allocation of model complexity is paramount for reproducibility. • Robust Advantage of Exact Invertibility: Replacing BELM with DDIM leads to a clear degradation in both accuracy and stability, confirming that SRE from approximate inversion systematically erodes the quality of the final causal estimate. 125 150 175 200 225 250 275 Mean Estimated ATE (Error bars show ±1 Std Dev) w/o Hybrid Objective w/o Targeted Modeling w/o Exact Invertibility (DDIM) BELM-MDCM (Full Model) ATE Estimation Accuracy True ATE = 202.29 100 50 0 50 100 150 200 Mean Absolute Error (Lower is Better) w/o Hybrid Objective w/o Targeted Modeling w/o Exact Invertibility (DDIM) BELM-MDCM (Full Model) Impact of Ablation on Absolute Error Ablation Study Results (5 Runs) on Challenge Dataset Figure 9: Visualization of the ablation study results. The top panel shows the mean es- timated ATE relative to the true value (red dashed line), with error bars indicating ±1 standard deviation. The bottom panel highlights the mean absolute error for each configu- ration. The full BELM-MDCM model is demonstrably the most accurate and stable. Conclusion. The ablation study confirms that our framework’s three core design prin- ciples: analytical invertibility, a hybrid training objective, and targeted model- ing—work in synergy. The removal of any single component creates a significant vulnera- bility, validating the integrity and effectiveness of our integrated architectural design. 25 Wu, Wang, and Li Table 7: Ablation study results on a challenging synthetic dataset (True ATE = 202.29), validating the necessity of each framework component. The mean and standard deviation are computed over 5 runs. Model Configuration Mean Est. ATE (± Std) Abs. Error BELM-MDCM (Full Model) 190.16 ± 4.57 12.14 w/o Exact Invertibility (DDIM) 219.98 ± 7.48 17.68 w/o Hybrid Objective 137.77 ± 23.99 64.53 w/o Targeted Modeling 253.20 ± 138.01 50.90 6 Causal',\n",
       "  'The mean and standard deviation are computed over 5 runs. Model Configuration Mean Est. ATE (± Std) Abs. Error BELM-MDCM (Full Model) 190.16 ± 4.57 12.14 w/o Exact Invertibility (DDIM) 219.98 ± 7.48 17.68 w/o Hybrid Objective 137.77 ± 23.99 64.53 w/o Targeted Modeling 253.20 ± 138.01 50.90 6 Causal Information Conservation as a Unifying Principle The principle of Causal Information Conservation extends beyond a foundation for our model; it offers a unifying lens—a new taxonomy—for analyzing the suitability of any generative model for individual-level causal inference. Applying this principle and its oper- ational metric, SRE, allows us to situate our work and clarify its unique advantages within the broader landscape. 6.1 Normalizing Flows: Natively Information-Conserving Normalizing Flows (NFs) (Dinh et al., 2014, 2017; Kingma and Dhariwal, 2018) are designed around an invertible mapping. By construction, their Structural Reconstruction Error is identically zero, making NFs a native implementation of the Causal Information Con- servation principle. However, their strengths come with limitations: the requirement of a tractable Jacobian imposes heavy architectural constraints, which can limit expressive power and introduce strong topological assumptions on the data manifold. 6.2 VAEs and GANs: Architecturally High-SRE Variational Autoencoders (VAEs) (Kingma and Welling, 2014) and Generative Adversar- ial Networks (GANs) (Goodfellow et al., 2014) are fundamentally ill-suited for this task. Their Structural Reconstruction Error is large and non-zero due to a fundamental ar- chitectural mismatch, as their separate encoder and decoder networks lack any structural guarantee of being inverses. Furthermore, their sources of information loss are inher- ent to their design; optimization objectives like the ELBO’s KL-divergence term actively encourage lossy compression, theoretically impeding the recovery of precise causal informa- tion. 6.3 A Comparative Perspective on Diffusion Models Viewed through our principle, diffusion models occupy a unique and compelling space in this taxonomy. Standard diffusion-based approaches, using samplers like DDIM, aspire to conserve information, but their reliance on approximate inversion results in a non-zero SRE; they are ”aspirational” but ultimately lossy. In contrast, BELM-MDCM (this work) achieves zero SRE by integrating an analytically invertible sampler, matching the theoretical purity of Normalizing Flows but without their rigid architectural constraints. 26 The Causal Round Trip Furthermore, unlike NFs trained with a generic likelihood objective, our framework’s Hybrid Training objective provides a causally-oriented inductive bias. BELM-MDCM thus uniquely combines the rigorous invertibility of NFs with the modeling flexibility and task-specific power of the diffusion paradigm, making it ideally suited for principled, high-fidelity causal inference. 7 Conclusion and Future Work This paper introduced Causal Information Conservation as a guiding principle for the emerging field of diffusion-based causal inference. Our primary contribution is not the concept of invertibility itself, but framing it as a foundational design requirement and identifying the Structural Reconstruction Error (SRE) as the precise, quantifiable cost of its violation. Our proposed framework, BELM-MDCM, serves as a constructive proof of this prin- ciple. By being architected around analytical invertibility, it is the first to achieve zero SRE by design, shifting the focus from mitigating numerical errors to upholding a fundamental causal principle. This work provides a foundational blueprint and a more',\n",
       "  'proposed framework, BELM-MDCM, serves as a constructive proof of this prin- ciple. By being architected around analytical invertibility, it is the first to achieve zero SRE by design, shifting the focus from mitigating numerical errors to upholding a fundamental causal principle. This work provides a foundational blueprint and a more rigorous standard for applying the power of diffusion models to the profound challenges of causal inference, reconciling their flexibility with the logical rigor demanded by classical theory. 7.1 Limitations and Future Work Our work highlights several avenues for future research: • Handling Non-Invertible SCMs: Our framework excels when the true SCM is invertible. While our stress test shows robustness when this is violated, developing models inherently resilient to such misspecifications is a key challenge. Empirically validating the proposed prior-matching regularizer (Appendix C) is a concrete next step. • Robustness to Graph Misspecification: Like most SCM-based methods (Peters et al., 2017), our framework assumes a correctly specified causal graph. Analyzing how structural errors (e.g., omitted confounders) propagate through our error decom- position framework is a significant future research direction. • Formalizing CIC within Information Theory: Our work defines CIC as an operational principle. A compelling direction is to formalize it within a rigorous information-theoretic framework, for instance by proving that zero SRE maximizes the mutual information I(U; ˆU) between the true and recovered noise, connecting our work to rate-distortion theory. • Scalability and Generalizability: While powerful, the BELM sampler is com- putationally intensive. Improving its efficiency for high-dimensional settings is an important practical challenge. Furthermore, extending the principles of CIC and zero-SRE modeling to other data modalities, such as time-series or images, represents an exciting frontier. 27 Wu, Wang, and Li Acknowledgments and Disclosure of Funding We thank the anonymous reviewers for their insightful feedback which significantly improved the clarity and rigor of this paper. Appendix A. Core Diffusion Model Equations This appendix provides the essential equations for the diffusion models referenced in this work. Diffusion models learn a data distribution by training a neural network ϵθ to reverse a fixed, gradual noising process. The model is trained by optimizing a simplified score-matching objective (Ho et al., 2020): Lsimple(θ) = Et,x0,ϵ \\x14 ϵ −ϵθ \\x00√¯αtx0 + √ 1 −¯αtϵ, t \\x01 2\\x15 (A.1) where ¯αt is a predefined noise schedule. For deterministic generation and inversion, we use the Denoising Diffusion Implicit Model (DDIM) update step (Song et al., 2021a): xt−1 = √¯αt−1 \\x12xt −√1 −¯αtϵθ(xt, t) √¯αt \\x13 + p 1 −¯αt−1ϵθ(xt, t) (A.2) This process can be viewed as a discretization of a continuous-time probability flow Ordinary Differential Equation (ODE) (Song et al., 2021b): dx = \" 1 2 d log α(s) ds x(s) −1 2 d log(1 −α(s)) ds p 1 −α(s) p α(s) ϵθ(x(s), s) # ds (A.3) Within our theoretical framework, the encoder operator Tθ corresponds to solving this ODE forward in time (from s = 0 to s = 1), while the decoder operator Hθ corresponds to solving it backward in time (from s = 1 to s = 0). Appendix B. Detailed Proofs for Identifiability (Theorems 2',\n",
       "  'theoretical framework, the encoder operator Tθ corresponds to solving this ODE forward in time (from s = 0 to s = 1), while the decoder operator Hθ corresponds to solving it backward in time (from s = 1 to s = 0). Appendix B. Detailed Proofs for Identifiability (Theorems 2 & 4) This appendix provides detailed, dimension-specific proofs for the identifiability of the ex- ogenous noise U and the subsequent correctness of counterfactual generation. The core challenge lies in showing that the statistical independence of the latent code Z from the parents Pa is a sufficient condition to establish an isomorphic relationship between Z and U. The mathematical tools required differ based on the dimensionality of U. B.1 The High-Dimensional Case (d ≥3) For cases where the exogenous noise U has a dimensionality d ≥3, the proof leverages Liouville’s theorem on conformal mappings. Proof [Proof of Theorems 2 and 4 for d ≥3] We adapt the proof from identifiable generative modeling (Chao et al., 2023) to our conditional operator setting. Let Qpa(U) := Tθ(F(pa, U), pa) be the composite function mapping the noise U to the latent code Z for a given context pa. By assumption, Qpa is invertible and differentiable. 28 The Causal Round Trip The core assumption, Z ⊥⊥Pa, implies that the conditional density pZ(z|pa) must equal a marginal density pZ(z) that is independent of pa. Using the change of variables formula, we relate the density of Z to that of U: pZ(z|pa) = pU(Q−1 pa(z)) det JQpa(Q−1 pa(z)) (B.1) where JQpa is the Jacobian of Qpa. Since the left-hand side is independent of pa and pU(·) is a fixed distribution, this imposes a strong constraint on the Jacobian determinant. Under regularity conditions, this implies that the Jacobian JQpa(u) must be a scaled orthogonal matrix, making Qpa a conformal map. By Liouville’s theorem, for dimensions d ≥3, any conformal map must be a M¨obius transformation (a composition of translations, scalings, orthogonal transformations, and inversions). For the map to be well-behaved, it must exclude the inversion component, which would introduce singularities. This is consistent with the regularity of functions representable by neural networks, simplifying the map to an affine form: Qpa(u) = Apau + dpa (B.2) where Apa is a scaled orthogonal matrix. The argument then uses the independence of the distribution’s moments and support to show that Apa and dpa must be constant w.r.t. pa. This leads to the isomorphic relationship Tθ(F(Pa, U), Pa) = AU + d = g(U). The proof of counterfactual correctness follows directly, as detailed in Chao et al. (2023). The conditional isomorphism Hθ(Tθ(·, pa), pa) = I combined with the identi- fiability result Tθ(F(pa, u), pa) = g(u) implies that Hθ(g(u), pa) = F(pa, u). The de- coder thus perfectly mimics the true causal mechanism, making an intervention exact: ˆXα = Hθ(g(u), α) = F(α, u) = Xtrue α . B.2 The One-Dimensional Case (d = 1) For the one-dimensional case where Liouville’s theorem does not apply, this section provides a dedicated proof leveraging properties of 1D functions and a uniform noise assumption from (Chao',\n",
       "  'making an intervention exact: ˆXα = Hθ(g(u), α) = F(α, u) = Xtrue α . B.2 The One-Dimensional Case (d = 1) For the one-dimensional case where Liouville’s theorem does not apply, this section provides a dedicated proof leveraging properties of 1D functions and a uniform noise assumption from (Chao et al., 2023) that does not sacrifice generality. We first establish a helper lemma characterizing a specific class of 1D functions. Lemma 18 For U, Z ⊂R, consider a family of invertible functions qpa : U →Z for pa ∈Xpa ⊂Rd. The derivative expression dqpa du (q−1 pa(z)) is a function of z only, i.e., c(z), if and only if qpa(u) can be expressed as qpa(u) = q(u + r(pa)) for some function r and invertible function q. Proof The proof is provided in (Chao et al., 2023). The reverse direction follows from direct differentiation, while the forward direction uses the inverse function theorem to show that the inverses spa(z) = q−1 pa(z) must have the same derivative 1/c(z), implying they can 29 Wu, Wang, and Li only differ by an additive constant, which yields the desired form after inversion. This lemma enables the proof of the theorem for the 1D case. Theorem 19 (Identifiability for d = 1) Assume for X ∈X ⊂R and exogenous noise U ∼Unif[0, 1], the SCM is X := f(Pa, U). Assume an encoder-decoder model with encoding function g and decoding function h. Assume the following conditions: 1. The encoding is independent of the parents, g(X, Pa) ⊥⊥Pa. 2. The structural equation f is differentiable and strictly increasing w.r.t. U. 3. The encoding g is invertible and differentiable w.r.t. X. Then, g(f(Pa, U), Pa) = ˜q(U) for an invertible function ˜q. Proof Let qpa(U) := g(f(pa, U), pa). The conditions on f and g ensure qpa is strictly monotonic and thus invertible. By the independence assumption, the conditional distri- bution of Z = qpa(U) does not depend on pa. We assume U ∼Unif[0, 1] without loss of generality.4 The change of density formula gives: pZ(z) = pU(q−1 pa(z)) |dqpa du (q−1 pa(z))| (B.3) Since pU(u) = 1 on its support and qpa is increasing, the denominator must be independent of pa. This implies dqpa du (q−1 pa(z)) = c(z) for some function c. This meets the condition of Lemma 18, allowing us to express qpa(u) = q(u + r(pa)) for some invertible q. Since Z ⊥⊥Pa, its support must also be independent of pa. The support is q([0, 1] + r(pa)), which is constant only if the interval [r(pa), 1 + r(pa)] is constant. This requires r(pa) to be a constant, r. Thus, qpa(u) = q(u + r). Defining ˜q(u) = q(u + r), we find that the mapping is solely a function of U, which completes the proof. B.3 The Two-Dimensional Case (d = 2) The two-dimensional case is a well-known geometric exception, as the group of conformal maps is infinite-dimensional. Consequently, the proof strategy used for higher dimensions via Liouville’s theorem does not directly apply. Here, we show that identifiability still holds under additional,',\n",
       "  'completes the proof. B.3 The Two-Dimensional Case (d = 2) The two-dimensional case is a well-known geometric exception, as the group of conformal maps is infinite-dimensional. Consequently, the proof strategy used for higher dimensions via Liouville’s theorem does not directly apply. Here, we show that identifiability still holds under additional, plausible regularity assumptions aligned with our modeling framework. Assumption 1 (Asymptotic Linearity) The composite mapping Qpa(u) : C →C is an entire function (analytic on the whole complex plane) with at most linear growth. That is, there exist constants A, B such that |Qpa(u)| ≤A|u| + B for all u ∈C. 4. For any SCM with a continuous noise E and a strictly increasing CDF FE, X = f(Pa, E) can be re-parameterized to an equivalent SCM X = ˜f(Pa, U), where U = FE(E) ∼Unif[0, 1] and ˜f(·, ·) = f(·, F −1 E (·)). The modeling task is then to learn the potentially more complex function ˜f. 30 The Causal Round Trip This assumption reflects a fundamental inductive bias of neural network architectures. Standard activation functions (e.g., ReLU, Tanh) produce functions that cannot exhibit super-polynomial growth or essential singularities at infinity, aligning our analysis with the function classes our model can represent. Assumption 2 (Non-Rotationally Symmetric Base Noise) The base distribution of the exogenous noise U is not rotationally symmetric.5 Proof The proof proceeds in three steps. First, as established previously, the statistical independence condition Z ⊥⊥Pa implies that the learned mapping Qpa(u) must be a conformal map, and thus an analytic function on C. Second, by Assumption 1, Qpa(u) is an entire function with at most linear growth. The Generalized Liouville’s Theorem states that an entire function whose growth is bounded by a polynomial of degree k must itself be a polynomial of degree at most k. In our case, this implies Qpa(u) must be a polynomial of degree at most one, giving it the affine form: Qpa(u) = a(pa)u + b(pa) where a, b are complex coefficients that can depend on pa. Third, we use the full statisti- cal independence condition to show that the coefficients a and b must be constant. For the distribution of Z = a(pa)U + b(pa) to be independent of pa, all of its properties must be constant. Mean: The mean E[Z|pa] = a(pa)E[U] + b(pa) must be constant. Assuming E[U] = 0 without loss of generality implies b(pa) must be a constant, b. Covariance: By Assumption 2, the distribution of U is not rotationally symmetric, so its covariance matrix is not proportional to the identity. Any rotation induced by the phase of a(pa) would alter the covariance structure of Z. For the distribution of Z to be invariant, the rotation angle (phase) of a(pa) must be constant. Scale: The change of variables formula implies that the magnitude |a(pa)| must also be constant. Since both the magnitude and phase of a(pa) must be constant, a(pa) must be a constant complex number, a. Therefore, Qpa(u) = au + b, which is an isomorphic mapping of u. This completes the proof. Appendix C. Extended Analysis',\n",
       "  'formula implies that the magnitude |a(pa)| must also be constant. Since both the magnitude and phase of a(pa) must be constant, a(pa) must be a constant complex number, a. Therefore, Qpa(u) = au + b, which is an isomorphic mapping of u. This completes the proof. Appendix C. Extended Analysis of Inversion Fidelity and Non-Invertible SCMs This appendix provides a unified analysis of inversion errors. We first provide rigorous proofs for inversion fidelity (Propositions 5 and 6), then extend our error decomposition framework to the more challenging non-invertible SCM setting. C.1 Proofs for Inversion Fidelity (Propositions 5 & 6) Proof [Proof of Proposition 5] The proof proceeds by deriving the explicit one-step recon- struction error and then analyzing its order of magnitude. 5. This assumption is made without loss of generality. For any arbitrary continuous noise E = (E1, E2), one can define a new noise U = (FE1(E1), E2), where FE1 is the CDF of the first component. The resulting distribution of U is uniform along its first axis, thus breaking any rotational symmetry. The structural function F then absorbs this transformation. 31 Wu, Wang, and Li 1. Derivation of the One-Step Reconstruction Error. Let the single-step DDIM inversion operator be Tt, mapping an observation xt to x′ t+1 using the noise prediction ϵt = ϵθ(xt, t). The corresponding generative operator is Ht, which reconstructs x′ t from x′ t+1 using a new prediction at the new state, ϵ′ t+1 = ϵθ(x′ t+1, t + 1). By substituting the formula for x′ t+1 into the update for x′ t, the single-step reconstruction error x′ t −xt is found to be: x′ t −xt = \\x12√ 1 −¯αt − √¯αt √1 −¯αt+1 √¯αt+1 \\x13 (ϵ′ t+1 −ϵt) (C.1) This error is non-zero if and only if the noise prediction changes after one inversion step, i.e., ϵ′ t+1 ̸= ϵt. 2. Analysis of the Error’s Order of Magnitude. In the continuous-time limit with time step ∆s = 1/T, a Taylor expansion shows that both the coefficient term and the difference in noise predictions (ϵ′ t+1 −ϵt) are of order O(∆s). The one-step reconstruction error is therefore the product of these two terms: Errorstep = O(∆s) × O(∆s) = O((∆s)2) = O(1/T 2) (C.2) This local error accumulates over the T steps of the trajectory, resulting in a total accumu- lated error of order O(1/T). For any finite T, this global error is non-zero, constituting the Structural Reconstruction Error (SRE) for DDIM. Proof [Proof of Proposition 6] The proof is constructive, following from the exact algebraic invertibility of the BELM sampler (Liu et al., 2024). For the second-order BELM used in this work, the one-step decoder is an affine transformation of the form: xt−1 = Atxt + Btϵt + Ctϵt+1 (C.3) where At, Bt, Ct are schedule-dependent coefficients, ϵt = ϵθ(xt, t), and ϵt+1 = ϵθ(xt+1, t + 1). The full-trajectory decoder, HBELM, is a composition of these one-step affine maps. The BELM encoder, TBELM, is constructed using a symmetric update rule designed to be the exact algebraic inverse of the decoder. As rigorously',\n",
       "  'Ct are schedule-dependent coefficients, ϵt = ϵθ(xt, t), and ϵt+1 = ϵθ(xt+1, t + 1). The full-trajectory decoder, HBELM, is a composition of these one-step affine maps. The BELM encoder, TBELM, is constructed using a symmetric update rule designed to be the exact algebraic inverse of the decoder. As rigorously shown by Liu et al. (2024), this construction ensures that the composite operator TBELM is the exact inverse of HBELM, assuming the same sequence of noise function evaluations is used for both processes. Therefore, by its algebraic construction, the BELM sampler guarantees a lossless round trip: HBELM ◦TBELM = I (C.4) The Structural Reconstruction Error is thus identically zero by construction. C.2 Exhaustive Analysis for the Non-Invertible SCM Setting This section rigorously extends our framework to the non-invertible case, providing a theo- retical underpinning for the stress test results in §5.4.1. 32 The Causal Round Trip C.2.1 Assumptions and Definitions We formalize the problem with the following. Assumption 3 (Well-Posed Abduction) For any observed (v, pa), the inverse image set U(v,pa) = {u′ ∈U | F(pa, u′) = v} is non-empty, and the Maximum a Posteriori (MAP) solution over this set, given the prior p(U), is unique. This solution defines the ideal amortized inverse operator, T∗(v, pa) = arg maxu′∈U(v,pa) p(u′). Definition 20 (Tripartite Error Sources) In the non-invertible case, we refine the er- ror sources into three distinct components: (i) Algorithmic Error (SRE): The error from an imperfect inversion algorithm, ESR := ∥(Hθ ◦Tθ −I)X∥2. For our framework, ESR ≡0. (ii) Modeling Error: The error from imperfectly learning the ideal amortized inverse, EModeling := ∥Tθ(V, Pa) −T∗(V, Pa)∥2. (iii) Representational Error: The fundamental, irreducible error from the SCM’s non- invertibility, ERep := ∥T∗(V, Pa) −Utrue∥2. C.2.2 A Tighter Error Decomposition We now present a tighter error bound for the non-invertible case. Theorem 21 (Tighter Counterfactual Error Bound) Let the conditions of Theorem 8 hold (Hθ is LH-Lipschitz). The expected squared error of the counterfactual prediction is bounded by: E h ∥ˆXα −Xtrue α ∥2i ≤2E[ESR] + 4L2 HE[EModeling] + 4L2 HE[ERep] (C.5) Proof We decompose the total error ∥ˆXα −Xtrue α ∥using the triangle inequality and the ideal amortized inverse T∗as an intermediate step. The result follows from applying the inequality (a + b + c)2 ≤2(a2 + b2 + c2), bounding terms with the LH-Lipschitz property of Hθ, and taking expectations. C.2.3 Information-Theoretic Interpretation of Representational Error The Representational Error (ERep) is deeply connected to information conservation. The non-invertibility of the SCM F means that observing (V, Pa) is not sufficient to uniquely determine U. Information-theoretically, this implies the conditional entropy H(U|V, Pa) is greater than zero. Remark 22 (Representational Error as Information Loss) The ideal amortized in- verse T∗yields the mode of the posterior p(U|V, Pa). The expected representational error, E[ERep], can be seen as the expected squared error of this MAP estimator, which is related to the variance and shape of the posterior. Thus, ERep is a direct consequence of the infor- mation about U that is fundamentally lost in the forward causal process, a loss captured by',\n",
       "  'error, E[ERep], can be seen as the expected squared error of this MAP estimator, which is related to the variance and shape of the posterior. Thus, ERep is a direct consequence of the infor- mation about U that is fundamentally lost in the forward causal process, a loss captured by H(U|V, Pa) > 0. 33 Wu, Wang, and Li C.2.4 Theoretical Guarantee for the Mitigation Strategy We now provide a theoretical justification for the ‘Prior-Matching Regularizer‘ proposed in §7.1. Definition 23 (Prior-Matching Regularizer) The regularizer is defined as R(Tθ) = E(v,pa) \\x02 ∥sp(Tθ(v, pa))∥2\\x03 , where sp(u) = ∇u log p(u) is the score function of the prior distribution p(U). Proposition 24 (Regularizer Induces Convergence to MAP Solution) Minimizing the regularizer R(Tθ) provides an inductive bias that encourages the encoder output, ˆu = Tθ(v, pa), to lie on a mode of the prior distribution p(U). Proof [Proof Sketch] The objective is a form of score matching on the prior. The score function sp(u) is zero if and only if u is a stationary point of the log-prior. Minimizing the expected squared norm of the score at the encoder’s output penalizes the production of latent codes ˆu in low-probability regions of the prior. This incentivizes the encoder to map observations to the most probable latent code, thereby encouraging Tθ to approximate the ideal MAP estimator T∗. Appendix D. Main Proofs for Theoretical Framework This appendix provides the proofs for the main theoretical results presented in the text. Proof [Proof of Theorem 8] This bound is a direct specialization of the more general bound for non-invertible SCMs derived in Theorem 21 (Appendix C). For an invertible SCM, abduction is perfect, meaning the ideal amortized inverse T∗is the true inverse of the SCM function F. Consequently, the recovered noise is the true noise, T∗(V, Pa) = Utrue, which implies that the Representational Error is identically zero: E[ERep] = 0. In this context, the Latent Space Invariance Error, ELSI, becomes equivalent to the Modeling Error, EModeling. Applying the inequality (a + b)2 ≤2a2 + 2b2, the general bound from Theorem 21 reduces to the two terms presented in Theorem 8. Proof [Proof of Proposition 10] This proof relies on the identifiability of the true SCM (Theorem 2) and the Lipschitz continuity of the score network ϵθ, which guarantees unique ODE solutions via the Picard-Lindel¨of theorem. We also assume standard integrability conditions (Fubini’s theorem). The encoder Tθ maps an initial condition x(0) to the terminal state x(T) of the prob- ability flow ODE (Eq. A.3). Let xθ(t; x0) and x∗(t; x0) denote the ODE solutions with the learned score ϵθ and the true score ϵ∗, respectively. The Latent Space Invariance Error is E[ELSI] = E[∥xθ(T; X) −xθ(T; Xtrue α )∥2]. By the triangle inequality: ∥xθ(T; X) −xθ(T; Xtrue α )∥≤∥xθ(T; X) −x∗(T; X)∥ + ∥x∗(T; X) −x∗(T; Xtrue α )∥ + ∥x∗(T; Xtrue α ) −xθ(T; Xtrue α )∥ 34 The Causal Round Trip The middle term is zero under ideal identifiability, as the true encoder T∗maps both an observation and its true counterfactual to the same',\n",
       "  'Xtrue α )∥≤∥xθ(T; X) −x∗(T; X)∥ + ∥x∗(T; X) −x∗(T; Xtrue α )∥ + ∥x∗(T; Xtrue α ) −xθ(T; Xtrue α )∥ 34 The Causal Round Trip The middle term is zero under ideal identifiability, as the true encoder T∗maps both an observation and its true counterfactual to the same underlying noise. We thus only need to bound terms of the form ∥xθ(T; x0) −x∗(T; x0)∥. Let z(t) = xθ(t; x0) −x∗(t; x0). Since the ODE vector field is Lipschitz, applying Gr¨onwall’s inequality to the differential of z(t) yields: ∥z(T)∥≤ Z T 0 eLf(T−t)Cf∥ϵθ(x∗(t), t) −ϵ∗(x∗(t), t)∥dt where Lf and Cf are constants from the ODE coefficients. Squaring, taking expectations, and applying Jensen’s inequality leads to: E[ELSI] ≤C′ · Ex,t[∥ϵθ −ϵ∗∥2] where the final expectation is the score-matching loss. Appendix E. Proofs for Theoretical Roles of Hybrid Training E.1 Proof of Proposition 12 (Weighted Score-Matching) This section provides a rigorous proof that the auxiliary task loss Ltask provides a lower bound on a weighted score-matching objective. E.1.1 Preliminaries and Setup Probability Flow ODE. The generative process is the solution to the reverse-time prob- ability flow ODE from t = T to t = 0: dxt = f(t, xt, ϵ(t, xt))dt, xT ∼N(0, I) (F.1) where the vector field f is determined by the diffusion scheduler. Core Objects. • ϵθ, ϵ∗: Learned and true score functions. • xθ t, x∗ t : ODE trajectories driven by ϵθ and ϵ∗. • xθ 0, x∗ 0: Generated and true (counterfactual) data points at t = 0. • g : Rd →Rk: Downstream prediction function. • Ypred = g(xθ 0), Ytrue = g(x∗ 0). • Ltask = ExT [∥Ypred −Ytrue∥2]. Assumptions. We assume standard regularity conditions for the proof: (A1) The vector field f(t, x, ϵ) is Lipschitz continuous in x and ϵ. (A2) The downstream task function g(x) is Lipschitz continuous and differentiable. (A3) The learned score ϵθ and true score ϵ∗are well-behaved. 35 Wu, Wang, and Li E.1.2 Formal Proposition Statement Proposition 25 (Hybrid Objective as a Weighted Score-Matching Regularizer) Under the regularity assumptions (A1-A3), the auxiliary task loss Ltask provides a lower bound on a weighted score-matching objective: Ltask ≥C · ExT ,t \\x02 w(x∗ t ) · ∥ϵθ(x∗ t ) −ϵ∗(x∗ t )∥2\\x03 where C > 0 and the weight function w(x∗ t ) measures the sensitivity of the final prediction Y to score perturbations along the ideal data generation trajectory. Proof The proof proceeds in three main steps. Step 1: Bounding Sample Error by Score Error (Error Propagation). Let z(t) = xθ t −x∗ t be the error between the two ODE trajectories. By linearizing the vector field f around the true trajectory, we can analyze the impact of the score perturbation ∆ϵt = ϵθ(x∗ t ) −ϵ∗(x∗ t ). From Duhamel’s Principle, the final sample error ∆x0 = z(0) can be expressed as an integral over the perturbation: ∆x0 = Z 0 T K(s)∆ϵsds (F.2) where the kernel K(s) captures the influence of a score perturbation at time s on the final sample at time 0. Step 2: Linearizing the',\n",
       "  'Duhamel’s Principle, the final sample error ∆x0 = z(0) can be expressed as an integral over the perturbation: ∆x0 = Z 0 T K(s)∆ϵsds (F.2) where the kernel K(s) captures the influence of a score perturbation at time s on the final sample at time 0. Step 2: Linearizing the Task Error. The error in the downstream prediction is ∆Y = g(xθ 0) −g(x∗ 0). A first-order Taylor expansion gives: ∆Y ≈∇xg(x∗ 0) · ∆x0 (F.3) The task loss is the expected squared norm, Ltask = E[∥∆Y ∥2]. Step 3: Deriving the Weighted Relationship. Substituting the integral form of ∆x0 into the task error approximation and applying the Cauchy-Schwarz inequality for integrals yields a lower bound for the task loss: Ltask ≥C · ExT \\x14Z T 0 ∥W(x∗ 0, s)∥2 F · ∥∆ϵs∥2ds \\x15 (F.4) where ∥· ∥F is the Frobenius norm and the influence operator W(x∗ 0, s) captures the end-to-end sensitivity from a score perturbation at time s to the final prediction. Rewriting the expectation gives the final form: Ltask ≥C · Et∼U[0,T]Ex∗ t \\x02 w(x∗ t ) · ∥ϵθ(x∗ t ) −ϵ∗(x∗ t )∥2\\x03 (F.5) where the weight function is w(x∗ t ) := T · ExT |x∗ t [∥W(x∗ 0, t)∥2 F ]. Interpretation and Conclusion. The weight function w(x∗ t ) is large when the gradient norm ∥∇xg(x∗ 0)∥is large—i.e., in causally salient regions where the outcome is highly sensi- tive to the features. The inequality therefore shows that minimizing Ltask provides a lower bound on a score-matching error that is weighted to prioritize accuracy in these causally salient regions. 36 The Causal Round Trip E.2 Argument for Proposition 13 (Latent Space Disentanglement) This section provides a qualitative, information-theoretic argument for Proposition 13, showing how the hybrid objective encourages a ”division of labor” that promotes disentan- glement. The SCM posits that an observation V is determined by (Pa, U), and the encoder maps (V, Pa) to a latent code Z = Tθ(V, Pa), where a perfect encoder would yield Z = U. The process works as follows: the diffusion loss (Ldiffusion) maximizes the log-likelihood log pθ(V |Pa), forcing the pair (Pa, Z) to contain all information to reconstruct V , thus max- imizing the mutual information I(V ; (Pa, Z)). Simultaneously, the task loss (Ltask) learns a prediction from the parents, capturing all predictive information that Pa has about V and thus maximizing I(V ; Pa). The dual objective must satisfy both constraints. From the chain rule of mutual information, we know I(V ; (Pa, Z)) = I(V ; Pa) + I(V ; Z|Pa). Since Ldiffusion maximizes the left-hand side and Ltask captures the first term on the right, the op- timization incentivizes the latent code Z to model the remaining information, I(V ; Z|Pa). This leads to a connection to disentanglement: the ideal exogenous noise U is, by definition, independent of Pa. By forcing Z to model the residual information, the opti- mization process actively encourages the learned representation Z to be independent of Pa. In summary, the hybrid objective creates a division of labor: the',\n",
       "  'to a connection to disentanglement: the ideal exogenous noise U is, by definition, independent of Pa. By forcing Z to model the residual information, the opti- mization process actively encourages the learned representation Z to be independent of Pa. In summary, the hybrid objective creates a division of labor: the task-specific head explains the variance from Pa, while the diffusion process’s latent code Z models the residual. This residual is, by construction, the information in V orthogonal to Pa, forcing Z to be an empirical approximation of the true, disentangled exogenous noise U, thereby serving the identifiability condition of Theorem 2. Appendix F. Proof of Theorem on Causal Transportability This appendix provides the proof for the theorem on lossless causal transportability. Proof [Proof of Theorem 17] This proof operates under an idealized setting, assuming a perfectly trained model (i.e., zero statistical error), to isolate the structural properties of transportability. We assume the learned decoder Hθi is identical to the true mechanism Fi, and its encoder Tθi is its perfect inverse. Let the source and target SCMs be MS and MT , with mechanisms {Fi} and {F′ i} respectively. The set of changed mechanisms is indexed by Kchanged. The proof relies on the modularity of the SCM, which is guaranteed by the mutually independent exogenous noises (Condition ii of the theorem). This independence ensures that a change in one mechanism Fk to F′ k does not affect the conditional distributions of other nodes Vj (j ̸= k), given their parents. We analyze the transportability of operators for each mechanism: 1. For invariant mechanisms (j /∈Kchanged): By definition, the true mechanism is unchanged, F′ j = Fj. Since the model operators (Tθj, Hθj) perfectly learned Fj in the source domain, they remain valid for the target domain T and can be directly reused. 2. For changed mechanisms (k ∈Kchanged): The original operators (Tθk, Hθk) are now invalid as they model Fk, not the new mechanism F′ k. However, a new operator pair (T′ θk, H′ θk) can be learned from target domain data. This training only requires 37 Wu, Wang, and Li samples (v′ k, pa′ k) from domain T and the shared noise distribution pk(Uk) (Condition i). Because the noises are independent, this re-learning process for mechanism k is modular and does not affect the other invariant mechanisms. The procedure for adapting the model is therefore modular: freeze all invariant operators {(Tθj, Hθj)}j /∈Kchanged and re-train only those for the changed mechanisms {k ∈Kchanged} on target domain data. The resulting adapted model is valid for the target domain T and can perform abduction on a target individual by applying the correct (reused or re-trained) encoders, thereby losslessly recovering the full vector of exogenous noises. This fulfills the condition for lossless transport. Appendix G. Proof of the Specific Finite Sample Bound (Theorem 15) This derivation combines standard generalization bounds with known complexity bounds for deep neural networks. We first state a key lemma regarding the complexity of neural networks. Lemma 26 (Rademacher Complexity of Neural Networks) Let Fϵ be the function class of an',\n",
       "  'Proof of the Specific Finite Sample Bound (Theorem 15) This derivation combines standard generalization bounds with known complexity bounds for deep neural networks. We first state a key lemma regarding the complexity of neural networks. Lemma 26 (Rademacher Complexity of Neural Networks) Let Fϵ be the function class of an L-layer MLP with ReLU activations, input dimension p, and weight matrices {Wj}L j=1. Assume the input data X is contained within a ball of radius RX. If the spectral norm of each weight matrix is bounded, ∥Wj∥2 ≤Bj, the Rademacher complexity of the function class is bounded by: Rn(Fϵ) ≤Cnet RXL√p √n \\uf8eb \\uf8ed L Y j=1 Bj \\uf8f6 \\uf8f8 For simplicity and under normalization, we often consider RX = 1. This result is a sim- plified form derived from (Bartlett et al., 2017; Neyshabur et al., 2018), where Cnet is a universal constant. Proof The proof proceeds by combining the standard excess risk bound with the two lemmas above. First, from standard learning theory, the excess risk is bounded by the Rademacher complexity of the total loss function class: R(ˆθn) −R(θ∗) ≤4Rn(FLSCM) + M r log(1/δ) 2n Next, by the sub-additivity property of Rademacher complexity, we decompose the SCM’s complexity into the sum of its individual components: Rn(FLSCM) ≤ d X i=1 Rn(FLi) The next step is to relate the loss complexity to the network complexity. The score-matching loss for mechanism i is Li = ∥ϵ −ϵθi(·)∥2. Since this loss is Lipschitz with respect to the 38 The Causal Round Trip output of ϵθi, its Rademacher complexity is upper-bounded by a constant multiple of the complexity of the score network’s function class Fϵi via Talagrand’s contraction lemma, i.e., Rn(FLi) ≤C1 · Rn(Fϵi). We then apply Lemma 26 to bound the complexity of each score network. The input dimension pi for mechanism i is pi = dim(xt) + dim(pai) + dim(time embedding). Assuming univariate variables, this is pi = 1 + |Pai| + dembed. Let dmax in = maxi |Pai|, so the maximum input dimension is pmax = 1+dmax in +dembed. Assuming all networks have depth L and a uniform spectral norm bound B, we have: Rn(Fϵi) ≤Cnet L√pmax √n BL = Cnet L p 1 + dmax in + dembed √n BL Finally, we assemble the complete bound by substituting all components back into the initial inequality: R(ˆθn) −R(θ∗) ≤4 d X i=1 Rn(FLi) + M r log(1/δ) 2n ≤4 d X i=1 C1 · Rn(Fϵi) + M r log(1/δ) 2n = (4C1Cnet) d · L · BL · p 1 + dmax in + dembed √n + M r log(1/δ) 2n By defining C = 4C1Cnet as a generic constant independent of the network architecture and sample size, we arrive at the final form stated in the theorem. Appendix H. Formal Analysis of the Geometric Inductive Bias This section provides the formal argument for Proposition 3, demonstrating how the score- matching objective, under a simplicity bias, compels the learned generative map to adopt the local data geometry, yielding a parsimonious and well-behaved transformation. H.1 Preliminaries and Definitions Let M ⊂Rd',\n",
       "  'Formal Analysis of the Geometric Inductive Bias This section provides the formal argument for Proposition 3, demonstrating how the score- matching objective, under a simplicity bias, compels the learned generative map to adopt the local data geometry, yielding a parsimonious and well-behaved transformation. H.1 Preliminaries and Definitions Let M ⊂Rd be the data manifold with a smooth probability density p(x). The true score function is the vector field s∗(x) := ∇x log p(x). We learn a parameterized score network sθ(x) by minimizing the score-matching objective LSM(θ) = Ex∼p(x)[∥sθ(x) −s∗(x)∥2]. The generative process is described by the probability flow ODE, whose vector field f(x, t) is a function of the score. The map Hθ : Rd →M is the flow map of this ODE integrated from t = 1 to t = 0. A map is conformal if its Jacobian is a scaled orthogonal matrix, and affine if it is a linear transformation plus a translation. H.2 Assumptions Assumption 4 (Smoothness of the Data Density) The true data density p(x) is at least twice continuously differentiable (C2) on M. 39 Wu, Wang, and Li Assumption 5 (Adoption of the Simplicity Bias Principle) Our analysis relies on the principle of implicit regularization: the conjecture that optimizers like SGD favor solu- tions with a simplicity bias. We formalize this as a preference for score functions sθ with lower complexity (e.g., lower Dirichlet Energy or smaller spectral norms) (Hochreiter and Schmidhuber, 1997; Neyshabur et al., 2018). H.3 Proof of Proposition 3 The proof proceeds in four steps: Step 1: The Irrotational Property of the True Score Field. By definition, the true score s∗(x) is the gradient of the scalar potential log p(x). By a fundamental theorem of vector calculus, the curl of any gradient field is zero. Thus, s∗is an irrotational (or conservative) vector field. Step 2: The Variational Perspective of Score Matching. The simplicity bias (As- sumption 5) reinforces the irrotational nature of the learned field sθ. This is understood via the Helmholtz decomposition, which splits any vector field into irrotational (curl-free) and solenoidal (divergence-free) components. The simplicity bias conjectures that the optimiza- tion preferentially minimizes the energy of the solenoidal component, driving the learned field sθ towards a purely irrotational solution (∇×sθ ≈0) to match the conservative nature of the true score s∗. Step 3: The Geometry of the Score Field under Local Structural Assump- tions. We analyze the score field’s structure under two local geometric cases for the density p(x) in a region R. Case (i): Local Isotropy. If p(x) is locally spherically symmet- ric around a center c, its value depends only on the radius r = ∥x −c∥. The gradient s∗= ∇log p(x) must point along the radial direction, making the true score a radial vector field. The learned field converges to this simple structure. Case (ii): Local Ellipsoidal Structure. If the iso-contours of p(x) are concentric ellipsoids, the gradient s∗must be orthogonal to these ellipsoidal surfaces at every point. Such a field is still irrotational but no longer radial. Step 4: The Geometry of the Flow Map. The ODE vector',\n",
       "  'to this simple structure. Case (ii): Local Ellipsoidal Structure. If the iso-contours of p(x) are concentric ellipsoids, the gradient s∗must be orthogonal to these ellipsoidal surfaces at every point. Such a field is still irrotational but no longer radial. Step 4: The Geometry of the Flow Map. The ODE vector field f(x, t) is a linear combination of the score field sθ∗(x) and the position vector x. The geometry of the resulting flow map Hθ∗depends on this vector field’s geometry. Result for Case (i): In the isotropic case, f(x, t) is also a radial vector field. A flow generated by a radial vector field is, by its rotational symmetry, necessarily a conformal map, as it preserves angles. Result for Case (ii): In the ellipsoidal case, the flow must transform the isotropic latent space into the anisotropic data space. The simplest such transformation is a local affine transformation, involving direction-dependent scaling and rotation. While this geometric argument is standard, a fully rigorous proof would require directly computing the Jacobian of the integrated flow map Hθ∗to verify it satisfies the required mathematical conditions. The model’s inductive bias thus compels it to learn the most parsimonious geometric transformation necessary to explain the local data geometry, promoting the well- behaved, locally invertible mappings that are crucial for abduction. 40 The Causal Round Trip Appendix I. Experimental Details This appendix provides comprehensive details for all experiments presented in Section 5, ensuring full transparency and reproducibility. I.1 General Setup Software Environment. All experiments were conducted in a unified software environ- ment to ensure full reproducibility. Key library versions used were: dowhy (0.12), econml (0.16.0), numpy (1.26.4), pandas (1.3.5), scikit-learn (1.6.1), torch (1.10.0), lightgbm (4.6.0), and networkx (3.2.1). All stochastic processes were controlled with a fixed global random seed (42), except for the ensemble runs, which used a set of distinct seeds for training. Baseline Estimator Configurations. All baseline estimators were implemented using the dowhy library. For machine learning-based methods like Causal Forest and DML, we utilized their robust implementations from the econml library. To ensure a fair and repro- ducible comparison against established benchmarks, we used their default hyperparameter settings, which are widely recognized and have been optimized by the library authors to provide strong performance across a broad range of tasks. Our proposed model, in turn, un- derwent a systematic grid search; the final parameters and a detailed analysis are provided in Appendix I.2. Details for PSM Failure Scenario (Act I). The Data Generation Process (DGP) for this experiment (N = 5000, true ATE τ = 5000) follows the graph in Figure 3a and is defined by: W1, W2 ∼N(0, 1) C1 ∼Categorical(softmax(z)), where \\uf8f1 \\uf8f4 \\uf8f2 \\uf8f4 \\uf8f3 zA = W1 −W2 zB = cos(πW1) + sin(πW2) zC = W 2 1 −W 2 2 logit(P(T = 1)) = 2 sin(πW1) + 1.5W 2 2 + 2W1W2 −1.5 · I[C1 = A] + 2.5 · I[C1 = B] + UT Y = 5000 · T + 60 · (15W1 −25W2 + 10W1W2) + 60 · (−40 · I[C1 = A] + 50 · I[C1 = C])',\n",
       "  '= 1)) = 2 sin(πW1) + 1.5W 2 2 + 2W1W2 −1.5 · I[C1 = A] + 2.5 · I[C1 = B] + UT Y = 5000 · T + 60 · (15W1 −25W2 + 10W1W2) + 60 · (−40 · I[C1 = A] + 50 · I[C1 = C]) + UY with exogenous noises UT ∼Logistic(0, 1) and UY ∼N(0, 60002). Details for Lalonde Benchmark (Act I). To evaluate performance on real-world data, we used the canonical Lalonde dataset, analyzing the effect of the NSW job training pro- gram (treat) on 1978 real earnings (re78). The assumed causal structure is the standard confounding model (Figure 3c), a DAG structure that reflects the broad consensus in the causal inference community for this benchmark. In line with our Targeted Modeling prin- ciple, we applied the expressive CausalDiffusionModel only to the key treat and re78 nodes, modeling all confounders non-parametrically via their empirical distributions. Details for Semi-Synthetic Analysis (Act II). This dataset uses the real-world co- variates from the Lalonde dataset as a foundation. The outcome Y and the ground-truth 41 Wu, Wang, and Li Individual Treatment Effect (ITEtrue) are then synthetically generated according to the following structural equations: Ybase = 2 · Xre74 + 1.5 · Xre75 + 100 · Xeduc −50 · Xage + 2000 · Xblack −1000 · Xhisp + Ubase ITEtrue = 1500 + 350 log(1 + Xeduc) −3(Xage −40)2 + 1200 · (1 −Xnodegr) · (1 −Xblack) −1000 tanh \\x12Xre74 −µre74 1000 \\x13 Y = Ybase + ITEtrue · Xtreat where Ubase ∼N(0, 5002) and µre74 is the mean of the re74 covariate. Details for the Stress Test on Non-Invertible SCMs (Act IV). This experiment’s primary objective is to evaluate the framework’s robustness when the core theoretical as- sumption of SCM invertibility is explicitly violated. The DGP (N = 2000) is defined as follows: W ∼U(−2, 2) P(T = 1|W) = σ(W + 0.5W 2) (where σ(·) is the sigmoid function) UY ∼N(0, 1.52), Y = 5T + 2W + U2 Y The true ATE is exactly τ = 5.0. The SCM was structured with W as an Empirical- Distribution, and T and Y as CausalDiffusionModel. Details for the Ablation Study (Act IV). The ablation study was conducted on a challenging synthetic mediation dataset (N = 4000) designed to highlight the benefits of generative models. The causal graph is shown in Figure 3b, with the following data generation process: X1 ∼N(0, 1), X2 ∼U(−2, 2), Z ∼Bernoulli(0.5) T ∼Bernoulli (σ (2.0 sin(πX1)X2 −1.5Z + UT )) M = 5 tanh(X2) + I[T = 1](15 cos(2πX2) + 5X1) + I[T = 0](−10|X1|) + UM Y = 25M + 10 sinc(2X1) + UY where σ(·) is the sigmoid function, UT ∼Logistic(0, 0.3), UM ∼N(0, 1.52), and UY is drawn from a Gaussian Mixture Model conditioned on Z. The true ATE for this DGP is approximately 202.29. BELM-MDCM (Full Model): The complete proposed framework. The nodes T, M, and Y are all modeled by a CausalDiffusionModel with sampler type=’belm’ and a hybrid objective weight of λ = 5.0. w/o Analytical Invertibility: Identical',\n",
       "  'Gaussian Mixture Model conditioned on Z. The true ATE for this DGP is approximately 202.29. BELM-MDCM (Full Model): The complete proposed framework. The nodes T, M, and Y are all modeled by a CausalDiffusionModel with sampler type=’belm’ and a hybrid objective weight of λ = 5.0. w/o Analytical Invertibility: Identical to the full model, but the sampler type for all diffusion models was set to ’ddim’. 42 The Causal Round Trip w/o Hybrid Objective: Identical to the full model, but the hybrid objective weight λ for all diffusion models was set to 0.0. w/o Targeted Modeling: The key mediator node M was modeled with a simpler gcm. AdditiveNoiseModel (backed by an LGBMRegressor), while T and Y remained as Causal- DiffusionModels with λ = 5.0 and sampler type=’belm’. I.2 Model Hyperparameter Justification The hyperparameters for our BELM-MDCM model, reported in Table 9, were identified through a systematic grid search for each experiment. The variation in these parameters reflects principled adaptations to different data characteristics, guided by the trade-off be- tween generative fidelity and discriminative accuracy, as well as the signal-to-noise ratio (SNR) of the underlying causal relationships. Below, we provide a holistic analysis for each scenario. I.2.1 Hyperparameter Search Details To ensure full reproducibility, we detail the hyperparameter search space used to arrive at the configurations in Table 9. The final values for each experiment were selected based on the best performance on a held-out validation set, typically comprising 20-30% of the training data. The primary selection criterion was the lowest PEHE score for experiments with a ground-truth ITE (e.g., Semi-Synthetic), or the best balance of low absolute ATE error and low estimation variance for observational data scenarios. For the Lalonde experiment in particular, we prioritized a configuration that demonstrated superior stability, as detailed in the analysis below. Table 8: Hyperparameter Search Space and Selection Criteria. Hyperparameter Search Space (Grid Search) Selection Criterion Hybrid Weight λ {0.0, 0.1, 0.3, 1.0, 2.0, 5.0, 10.0} Best balance of low error and low variance on the validation set. Guidance Weight w {0.0, 0.1, 0.2, 0.3, 1.0, 5.0, 10.0} Diffusion Timesteps T {50, 100, 200, 500} Learning Rate {5e-5, 1e-4, 1.1e-4, 1.2e-4, 2e-4} Scenario 1: The ”Perfectionist Learner” (PSM Failure Experiment) Data Pro- file: This synthetic dataset features complex, non-linear functions (e.g., sin, cos) but is characterized by a pure, high-SNR signal. The main challenge is to perfectly learn this intricate generative process. Hyperparameter Strategy: The strategy prioritizes gener- ative modeling. A low hybrid weight (λ = 0.1) directs the model to focus on the diffusion loss. Since the conditional signal is strong, classifier-free guidance is disabled (w = 0.0). A large model capacity and extended training are employed to capture the ground-truth functions. Scenario 2: The ”Pragmatic Signal Extractor” (Lalonde Experiment) Data Profile: This real-world data is characterized by a weak, noisy signal and a small sam- 43 Wu, Wang, and Li Table 9: Consolidated BELM-MDCM Hyperparameters for All Key Experiments. Column headers correspond to the following experiments: PSM (PSM Failure), Lalonde, Semi- Synth (Semi-Synthetic), Ablation (Ablation Study), and Stress Test (Act IV). Hyperparameter PSM Lalonde',\n",
       "  'is characterized by a weak, noisy signal and a small sam- 43 Wu, Wang, and Li Table 9: Consolidated BELM-MDCM Hyperparameters for All Key Experiments. Column headers correspond to the following experiments: PSM (PSM Failure), Lalonde, Semi- Synth (Semi-Synthetic), Ablation (Ablation Study), and Stress Test (Act IV). Hyperparameter PSM Lalonde Semi-Synth Ablation Stress Test Number of Epochs 1500 1000 1200 700 500 Batch Size 128 64 64 128 128 Hidden Dimension 512 512 768 768 256 Learning Rate 1 × 10−4 1 × 10−4 1.1 × 10−4 1 × 10−4 1 × 10−4 Diffusion Timesteps (T) 200 200 50 200 200 Hybrid Weight λ 0.1 2.0 2.0 5.0 0.5 Guidance Weight (w) 0.0 1.0 0.1 0.2 0.0 ple size. The primary challenge is to robustly extract the causal signal. Hyperparameter Strategy: The priority shifts from pure accuracy to a balance of accuracy and stability. A high hybrid weight (λ = 2.0) provides a strong inductive bias towards the predictive task. Crucially, our systematic grid search revealed that high guidance weights (w > 1.0) dra- matically increased estimation variance, leading to unreliable results. We therefore selected a moderate guidance weight of w = 1.0. This configuration provides a stabilizing effect, guiding the model towards the causal signal without amplifying noise, thereby achieving the optimal balance between accuracy and the robustness required for reliable inference on real-world data. Scenario 3: The ”Precision Artist” (Semi-Synthetic Experiment) Data Profile: This setup presents a hybrid-SNR environment, using noisy real-world covariates but a pure, synthetic outcome function. The task demands high precision. Hyperparameter Strategy: The diffusion timesteps are reduced (T = 50), plausibly because a shorter generative path better preserves the fine-grained details of the synthetic ITE function. The hybrid weight is high (λ = 2.0) to focus on the estimation task, while guidance is light (w = 0.1) as the ITE signal is cleaner than in the purely observational Lalonde case. Scenario 4: The ”Component Analyst” (Ablation Study) Data Profile: This is a complex, synthetic mediation structure with a clean, high-SNR signal, specifically designed to isolate the performance impact of individual framework components. Hyper- parameter Strategy: The strategy is to create a strong, stable baseline. A high hybrid weight (λ = 5.0) heavily orients the model towards the predictive task, making any per- formance degradation from ablations more pronounced. A large model capacity (Hidden Dim=768) ensures the model can learn the complex functions, while light guidance (w = 0.2) provides a minor stabilizing effect. Scenario 5: The ”Robustness Tester” (Stress Test) Data Profile: A simple DGP featuring a non-invertible causal mechanism (Y ∝U2), designed to test the framework’s behavior when its core assumption is violated. Hyperparameter Strategy: The goal is stable learning of a simple function. A moderate hybrid weight (λ = 0.5) balances generative and predictive learning, while a smaller model (Hidden Dim=256) is sufficient for 44 The Causal Round Trip the simpler DGP and helps prevent overfitting. Guidance is turned off (w = 0.0) as the signal is clean. Table 10: Summary of Adaptive Hyperparameter Strategies. Aspect PSM Failure Lalonde Semi- Synthetic Ablation Study Stress',\n",
       "  'learning, while a smaller model (Hidden Dim=256) is sufficient for 44 The Causal Round Trip the simpler DGP and helps prevent overfitting. Guidance is turned off (w = 0.0) as the signal is clean. Table 10: Summary of Adaptive Hyperparameter Strategies. Aspect PSM Failure Lalonde Semi- Synthetic Ablation Study Stress Test Data Signal Pure & Complex Noisy & Weak Hybrid & Complex Pure & Mediated Pure & Non- Invertible Guidance (w) 0.0 (Off) 1.0 (Balanced Guidance) 0.1 (Slight Nudge) 0.2 (Light) 0.0 (Off) Timesteps (T) 200 (Standard) 200 (Standard) 50 (Short Path) 200 (Standard) 200 (Standard) Hybrid (λ) 0.1 (Generative) 2.0 (Predictive) 2.0 (Predictive) 5.0 (Strongly Predictive) 0.5 (Balanced) Core Strategy Perfect Generation Robust Prediction Precision Estimation Component Isolation Stable Learning Holistic Comparison In conclusion, the variability in hyperparameters demonstrates the flexibility of our framework. It showcases the model’s ability to deploy different tools—such as prioritizing the generative loss or amplifying weak signals with guidance—to optimally adapt to the specific challenges posed by diverse causal inference problems. I.3 CMF Metric Implementation Details To ensure the rigor and reproducibility of our evaluation, this section provides the imple- mentation details for the CMF scores. CMI-Score Estimation Method. For Conditional Mutual Information (CMI) estima- tion, we adopt the widely-used k-Nearest Neighbors (k-NN) based estimator from Kraskov et al. (Kraskov et al., 2004). This method is chosen for its strong theoretical properties and practical robustness, particularly for continuous and high-dimensional data, as it avoids explicit density estimation. Its key advantages include: • Non-parametric: It makes no assumptions about the underlying data distributions. • Data-adaptive: The estimation is based on local distances, adapting to the data manifold’s geometry. • Robustness: It is more robust than methods that rely on fixed binning, which can be sensitive to bin size. Following common practice, we set the number of neighbors to k = 5 for all our experiments to ensure a stable and reliable estimation. KMD-Score Kernel Parameter Selection. The performance of the MMD test is sen- sitive to kernel parameter choices. For the KMD-Score, we employed a standard Radial Basis Function (RBF) kernel, k(x, y) = exp(−∥x −y∥2/(2σ2)). The bandwidth parameter 45 Wu, Wang, and Li σ is critical. Following best practices, we set the bandwidth using the median heuristic, a robust and common data-driven approach. For the analysis on the Lalonde dataset’s re78 mechanism, this heuristic yielded a bandwidth of σ = 0.1, a value confirmed as effective in preliminary experiments. All KMD-Scores are reported using this configuration. Appendix J. Empirical Validation of Proposed Evaluation Metrics To empirically validate the reliability, sensitivity, and complementary nature of our proposed evaluation metrics (CIC-Score, CMI-Score, and KMD-Score), we conducted a controlled micro-simulation study assessing how each metric responds to a spectrum of increasingly severe model errors. Experimental Setup. We designed a simple ground-truth Structural Causal Model (SCM) and created five simulated models (A through E) representing a clear ”degrada- tion gradient” of model quality: • Model A (Oracle): Represents a theoretically perfect model with zero SRE and a perfectly learned causal mechanism. This serves as our gold standard, with expected scores',\n",
       "  'simple ground-truth Structural Causal Model (SCM) and created five simulated models (A through E) representing a clear ”degrada- tion gradient” of model quality: • Model A (Oracle): Represents a theoretically perfect model with zero SRE and a perfectly learned causal mechanism. This serves as our gold standard, with expected scores of 1.0. • Model B (Lossy Inverter): Simulates a model with a non-zero SRE. It uses the correct causal mechanism but introduces a systematic error during the reconstruction cycle, mimicking the core flaw of DDIM-based approaches. • Model C (Wrong Mechanism): Represents a model that fails to learn the correct functional form of the causal mechanism (e.g., learning a linear instead of a sinusoidal relationship). • Model D (Maximal Error): Simulates a more severe failure where both the causal mechanism and the inferred noise distribution are fundamentally incorrect. • Model E (Total Mismatch): Represents the worst-case scenario where the model ignores the causal graph and inputs, generating outputs from an unrelated distribu- tion. Analysis of Results. The results (Figure 10) demonstrate the distinct and complemen- tary roles of our proposed metrics: 1. The CIC-Score acts as a high-sensitivity ”SRE detector.” It exhibits a dra- matic drop from a perfect 1.0 (Model A) to approximately 0.23 (Model B) the moment SRE is introduced, while showing less sensitivity to the specific form of mechanism error. This confirms its primary role as a diagnostic for adherence to the Causal Information Conservation principle. 2. The CMI-Score serves as a robust ”mechanism association tracker.” It degrades gracefully and monotonically as the learned causal mechanism deviates from the ground truth (from 0.99 to 0.76). This demonstrates its utility in quantifying the fidelity of learned parent-child conditional dependencies. 3. The KMD-Score functions as the ”final arbiter” of distributional fidelity. Possessing the widest dynamic range, it is sensitive to all forms of error and provides a holistic judgment of the similarity between the generated and true counterfactual distributions. As the most rigorous metric, it correctly assigns the lowest score to the completely mismatched Model E. 46 The Causal Round Trip A: Oracle B: Lossy Inverter (SRE Error) C: Wrong Mechanism D: Maximal Error E: Total Mismatch 0.0 0.2 0.4 0.6 0.8 1.0 Score Value 1.000 0.154 0.158 0.157 0.136 CIC-Score A: Oracle B: Lossy Inverter (SRE Error) C: Wrong Mechanism D: Maximal Error E: Total Mismatch 0.999 0.994 0.949 0.836 0.758 CMI-Score A: Oracle B: Lossy Inverter (SRE Error) C: Wrong Mechanism D: Maximal Error E: Total Mismatch 1.000 0.971 0.822 0.790 0.570 KMD-Score Empirical Validation of Proposed Metrics via Micro-simulation Figure 10: Results of the micro-simulation study for metric validation. The three plots show the response of the CIC-Score, CMI-Score, and KMD-Score to five models (A-E) of progressively decreasing quality. The scores demonstrate a clear monotonic degradation, confirming their ability to reliably track model fidelity. Note the CIC-Score’s sharp drop from Model A to B, highlighting its specific sensitivity to the Structural Reconstruction Error (SRE). This simulation thus validates our proposed metrics as a reliable and nuanced evaluation framework. They work in synergy to diagnose specific model failings (CIC-Score),',\n",
       "  'ability to reliably track model fidelity. Note the CIC-Score’s sharp drop from Model A to B, highlighting its specific sensitivity to the Structural Reconstruction Error (SRE). This simulation thus validates our proposed metrics as a reliable and nuanced evaluation framework. They work in synergy to diagnose specific model failings (CIC-Score), assess relational accuracy (CMI-Score), and provide an overall quality judgment (KMD-Score), offering a more insightful assessment than traditional metrics alone. Discussion on the Non-Zero Lower Bound of Scores. Notably, even for the worst- performing model (Model E), the CMI and KMD scores do not fall to zero. This behavior is not a limitation but a desirable feature that reflects their ability to capture ”residual statistical structure” in the evaluation setting. • For the KMD-Score: The MMD compares the joint distributions Pmodel(Y, W, T) and Poracle(Y, W, T). Crucially, as all models operate on the same observed parent data, the marginal parent distribution P(W, T) is identical between the two. The difference lies only in the conditional P(Y |W, T). Because the joint distributions share a substantial common subspace, their MMD will be finite, preventing the KMD-Score from reaching zero. Furthermore, using a ‘StandardScaler‘ maps both distributions to a similar feature space, which is necessary for a fair, scale-invariant comparison. • For the CMI-Score: This metric quantifies the I(Y ; parent|other parents). Even an incorrect mechanism (like in Model C or D) still generates an output Y as a deterministic function of its parents, resulting in a non-zero CMI. For Model E, where Y is independent of its parents, the theoretically zero CMI is not observed due to the inherent finite-sample variance of the non-parametric k-NN estimator. This behavior is advantageous, ensuring the metrics provide a meaningful, continuous gra- dient of failure rather than a simplistic binary judgment. This enhances their diagnostic power, allowing for fine-grained distinctions between types and degrees of model imperfec- tion. 47 Wu, Wang, and Li Appendix K. Algorithm for ATE Estimation This appendix provides the detailed pseudo-code for the counterfactual imputation proce- dure used to estimate the Average Treatment Effect (ATE) in our experiments. Algorithm 1: ATE Estimation with Invertible SCMs via Counterfactual Imputa- tion Input: Observational data D = {v(j)}N j=1 where v(j) ∈Rd; Causal graph G. Output: Estimated Average Treatment Effect ( ˆ ATE). Define Invertible SCM Mθ: Mθ = {fi(·; θi)}d i=1 based on G, where vi = fi(pai, ui); // 1. Train the invertible SCM on observational data ˆθ ←arg minθ PN j=1 Pd i=1 Li \\x10 fi(pa(j) i ; θi), v(j) i \\x11 ; // 2. Generate counterfactual outcomes for each individual for j ←1 to N do tj ←v(j) T ; // Observed treatment u(j) ←M−1 ˆθ (v(j)) ; // Abduction via invertible BELM encoder yj(1 −tj) ←Predict \\x00Mˆθ, u(j), do(T := 1 −tj) \\x01 ; // Action & Prediction // Store factual and counterfactual outcomes Yj(tj) ←v(j) Y ; Yj(1 −tj) ←yj(1 −tj); end // 3. Compute the ATE from factual and counterfactual outcomes ˆ ATE ←1 N PN j=1 (Yj(1) −Yj(0)); return ˆ ATE; References Joshua D. Angrist and J¨orn-Steffen',\n",
       "  '1 −tj) \\x01 ; // Action & Prediction // Store factual and counterfactual outcomes Yj(tj) ←v(j) Y ; Yj(1 −tj) ←yj(1 −tj); end // 3. Compute the ATE from factual and counterfactual outcomes ˆ ATE ←1 N PN j=1 (Yj(1) −Yj(0)); return ˆ ATE; References Joshua D. Angrist and J¨orn-Steffen Pischke. Mostly Harmless Econometrics: An Empiri- cist’s Companion. Princeton University Press, 2008. Peter L Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, volume 30, 2017. Patrick Chao, Patrick Bl¨obaum, and Shiva Prasad Kasiviswanathan. Interventional and counterfactual inference with diffusion models. arXiv preprint arXiv:2302.00860, 2023. Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent compo- nents estimation. arXiv preprint arXiv:1410.8516, 2014. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In International Conference on Learning Representations (ICLR), 2017. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27, 2014. 48 The Causal Round Trip Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexan- der Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(25): 723–773, 2012. Ernst Hairer and Gerhard Wanner. Solving Ordinary Differential Equations II: Stiff and Differential-Algebraic Problems, volume 14 of Springer Series in Computational Mathe- matics. Springer Science & Business Media, 2nd edition, 2006. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016. James J Heckman. Micro data, heterogeneity, and the evaluation of public policy: Nobel lecture. Journal of Political Economy, 109(4):673–748, 2001. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840–6851, 2020. Sepp Hochreiter and J¨urgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997. Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jonas Peters, and Bernhard Sch¨olkopf. Nonlinear causal discovery with additive noise models. In Advances in Neural Information Processing Systems, volume 21, 2009. Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, volume 31, 2018. Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations (ICLR), 2014. Alexander Kraskov, Harald St¨ogbauer, and Peter Grassberger. Estimating mutual informa- tion. Physical Review E, 69(6):066138, 2004. Robert J. Lalonde. Evaluating the econometric evaluations of training programs with ex- perimental data. The American Economic Review, 76(4):604–620, 1986. Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022. Zimeng Liu, Ziqiong Li, Jian Liu, and Wei Su. BELM: Bidirectional explicit-form linear multi-step samplers for diffusion models. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Mehdi Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learn- ing. The MIT Press, second edition, 2018. Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. A PAC- Bayesian approach to spectrally-normalized margin bounds for neural',\n",
       "  'for diffusion models. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Mehdi Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learn- ing. The MIT Press, second edition, 2018. Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. A PAC- Bayesian approach to spectrally-normalized margin bounds for neural networks. In In- ternational Conference on Learning Representations (ICLR), 2018. 49 Wu, Wang, and Li Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, Cambridge, second edition, 2009. Judea Pearl and Elias Bareinboim. Transportability of causal and statistical relations: A formal approach. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1149–1157, 2014. Jonas Peters, Dominik Janzing, and Bernhard Sch¨olkopf. Elements of Causal Inference: Foundations and Learning Algorithms. Adaptive Computation and Machine Learning series. The MIT Press, 2017. Donald B. Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of Educational Psychology, 66(5):688–701, 1974. Pedro Sanchez and Sotirios A. Tsaftaris. DCMs: Diffusion causal models for counterfactual estimation. arXiv preprint arXiv:2202.10166, 2022. Amit Sharma and Emre Kiciman. DoWhy: An end-to-end library for causal inference. In Proceedings of the First Conference on Causal Learning and Reasoning, volume 177 of Proceedings of Machine Learning Research, pages 734–747. PMLR, 2022. Shohei Shimizu, Patrik O. Hoyer, Aapo Hyv¨arinen, and Antti Kerminen. A linear, non- gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7: 2003–2030, 2006. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations (ICLR), 2021b. Jeffrey M Wooldridge. Econometric Analysis of Cross Section and Panel Data. MIT press, 2010. 50'],\n",
       " ['AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly Alexander Htet Kyaw Massachusetts Institute of Technology Cambridge, MA, United States alexkyaws@mit.edu Haotian Ma Cornell University Cornell, NY, United States hm443@mit.edu Sasa Zivkovic Cornell University Ithaca, NY, United States sz382@cornell.edu Jenny Sabin Cornell University Ithaca, NY, United States jes557@cornell.edu Abstract We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures. CCS Concepts • Human-centered computing →Interaction techniques; Aug- mented reality; • Computing methodologies →Computer vision problems; Machine learning approaches. Keywords Augmented Reality, Object Recognition, Assembly Instruction, As- sembly Tracking, Feedback-Based Fabrication ACM Reference Format: Alexander Htet Kyaw, Haotian Ma, Sasa Zivkovic, and Jenny Sabin. 2025. AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly. In Proceedings of ACM Symposium on Computational Fabrication (SCF ’25). ACM, New York, NY, USA, 3 pages. https://doi.org/XXXXXXX.XXXXXXX 1 Introduction By using Augmented Reality (AR), we can overlay digital informa- tion directly onto the physical world, providing 3D instructions throughout the assembly process. For example, prior work has explored combining AR assembly tasks with gesture recognition [Kyaw et al. 2024], computer vision [Jahn et al. 2022] and physics simulation to enhance interaction [Kyaw et al. 2023a]. This work is licensed under a Creative Commons Attribution 4.0 International License. SCF ’25, Cambridge, MA, USA © 2025 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-XXXX-X/2025/06 https://doi.org/XXXXXXX.XXXXXXX Figure 1: Object Recognition for AR Assisted Assembly Previous research has utilized AI algorithms such as object recog- nition to recognize assembly states, automatically advance AR in- structions [Stanescu et al. 2023], align virtual model registration [Canadinc and Yan 2024], and verify whether a part has reached its target placement location [Kästner et al. 2021]. Building upon this body of work, our system incorporates ob- ject recognition to detect individual parts, associate them with corresponding digital instructions, and dynamically highlight both their current and target spatial regions. This research advances the field of AR-assisted assembly by introducing a framework that delivers contextual, stepwise digital guidance aligned with physical components within the assembly space [Ma and Kyaw 2023]. 2 System Overview Leveraging deep learning for object recognition, our system is capable of identifying each component within the assembly area and associating it with the relevant assembly instruction. The system highlights the necessary component for every assembly step by showing a bounding box around the component’s current location and its intended placement location. It then automatically advances to the subsequent step once it detects that the current step has been completed. For the purpose of this research, the Microsoft',\n",
       "  'system highlights the necessary component for every assembly step by showing a bounding box around the component’s current location and its intended placement location. It then automatically advances to the subsequent step once it detects that the current step has been completed. For the purpose of this research, the Microsoft Hololens 2 AR headset and Lego components were chosen as the case study to demonstrate the system’s capabilities. arXiv:2511.05394v1 [cs.CV] 7 Nov 2025 SCF ’25, November 20–21, 2025, Cambridge, MA, USA Kyaw, Ma, Zivkovic, and Sabin Figure 2: System overview diagram illustrating the various software components and data -flow of the system 2.1 Object Recognition in Augmented Reality The object recognition algorithm is trained on synthetic data repre- senting eight distinct primitive yellow LEGO components using the YOLOv5 model. These synthetic image datasets are generated using a rendering engine that simulates various orientations and lighting conditions to mimic real-world scenarios and improve detection ro- bustness. The HoloLens 2 camera captures the physical workspace and streams video data to a server, where it is split into individual frames for object detection using the YOLOv5 model. The model identifies 2D bounding boxes around LEGO components in each frame. These bounding boxes are then projected into the AR envi- ronment using a homography-based 2D-to-3D planar projection, computed from the camera’s pose and field of view. See figure 2 for software implementation details. Figure 3: Synthetic data generation for training object recog- nition algorithm Figure 4: Bounding Box Corresponding to Assembly Step 2.2 Step by Step 3D Assembly Instruction The interface displays step-by-step assembly instructions, indicat- ing where the user should pick up each component and where to place it in relation to the rest of the assembly. For each instruction step, a corresponding 3D bounding box is generated to visually link the digital instruction with the physical component, highlighting both its current position and its intended placement. 3D bounding boxes are shown only for components relevant to the current as- sembly step, and each box is annotated with the component type, allowing users to identify both the location and the nature of the part. Since LEGO assemblies are built layer by layer, only the geom- etry of the current layer is visualized to reduce cognitive load. In our demonstration, the assembly parts are positioned on the right side of the workspace, while the assembly area is on the left. 3 Results We demonstrated the assembly of two distinct LEGO sculptures: an ellipsoidal egg and a twisted wall. Using our system, we success- fully assembled both sculptures without referencing any 2D paper drawings or 3D digital models. Further user studies are planned to quantitatively evaluate the system’s effectiveness. While the cur- rent study is limited to tabletop assemblies, future research could explore more complex assembly tasks or improve the accuracy of the AR projections [Kyaw et al. 2023b]. Additionally, future work could explore using this system to assemble designs generated by 3D generative AI. [??]. By bridging assembly instructions with the real-time localization of components in physical space, this research demonstrates the potential of object recognition',\n",
       "  'or improve the accuracy of the AR projections [Kyaw et al. 2023b]. Additionally, future work could explore using this system to assemble designs generated by 3D generative AI. [??]. By bridging assembly instructions with the real-time localization of components in physical space, this research demonstrates the potential of object recognition for AI-assisted AR assembly. Figure 5: Artifacts made using AI-Assisted AR Assembly AI Assisted AR Assembly: Object Recognition and Computer Vision for Augmented Reality Assisted Assembly SCF ’25, November 20–21, 2025, Cambridge, MA, USA References Seda Tuzun Canadinc and Wei Yan. 2024. Multi-3D-Models Registration-Based Augmented Reality Instructions for Assembly. In 2024 IEEE Conference on Vir- tual Reality and 3D User Interfaces Abstracts and Workshops (VRW). 997–998. doi:10.1109/VRW62533.2024.00295 Gwyllim Jahn, Cameron Newnham, and Nick Berg. 2022. Depth Camera Feedback for Guided Fabrication in Augmented Reality. doi:10.52842/conf.acadia.2024.1.684 Alexander Htet Kyaw, Jack Otto, and Leslie Lok. 2023a. Active Bending in Physics-Based Mixed Reality: The design and fabrication of a reconfigurable modular bamboo system. Graz, Austria, 169–178. doi:10.52842/conf.ecaade.2023.1.169 Alexander Htet Kyaw, Lawson Spencer, Sasa Zivkovic, and Leslie Lok. 2024. Gesture Recognition for Feedback Based Mixed Reality and Robotic Fabrication: A Case Study of the UnLog Tower. In Phygital Intelligence, Chao Yan, Hua Chai, Tongyue Sun, and Philip F. Yuan (Eds.). Springer Nature, Singapore, 331–345. doi:10.1007/978- 981-99-8405-3_28 Alexander Htet Kyaw, Arvin HaoCheng Xu, Gwyllim Jahn, Nick van den Berg, Cameron Newnham, and Sasa Zivkovic. 2023b. Augmented Reality for high precision fabri- cation of Glued Laminated Timber beams. Automation in Construction 152 (Aug. 2023), 104912. doi:10.1016/j.autcon.2023.104912 Linh Kästner, Leon Eversberg, Marina Mursa, and Jens Lambrecht. 2021. Integrative Ob- ject and Pose to Task Detection for an Augmented-Reality-based Human Assistance System using Neural Networks. In 2020 IEEE Eighth International Conference on Com- munications and Electronics (ICCE). 332–337. doi:10.1109/ICCE48956.2021.9352121 Haotian Ma and Alexander Htet Kyaw. 2023. AI ASSEMBLY: OBJECT RECOGNITION, COMPUTER VISION, AND DIGITAL TWIN FOR MIXED REALITY ASSEMBLY. Cornell University (May 2023). doi:10.7298/ha3k-4e73 Ana Stanescu, Peter Mohr, Mateusz Kozinski, Shohei Mori, Dieter Schmalstieg, and Denis Kalkofen. 2023. State-Aware Configuration Detection for Augmented Re- ality Step-by-Step Tutorials. In 2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). 157–166. doi:10.1109/ISMAR59233.2023.00030 ISSN: 2473-0726. Received 27 Sep 2025'],\n",
       " ['SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning Xiaodong Wang1, Jing Huang1, Kevin J Liang1 1Meta AI Recent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised and unsupervised learning. However, the application of clustering is often heuristic, and the optimal methodology remains unclear. In this work, we establish connections between these unsupervised clustering methods and classical mixture models from statistics. Through this framework, we demon- strate significant enhancements to these clustering methods, leading to the development of a novel model named SiamMM. Our method attains state-of-the-art performance across various self-supervised learning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground truth labels, uncovering potential instances of mislabeling. Date: October 24, 2024 Correspondence: First Author at xiaodongwang@meta.com 1 Introduction Recent self-supervised and unsupervised learning methods have demonstrated the ability to learn represen- tations that are surprisingly competitive with fully supervised learning Caron et al. (2020); Chen and He (2021); Misra and van der Maaten (2020); Chen et al. (2020b); Zbontar et al. (2021). Rather than relying on human annotations, which can be costly or biased, recent approaches have used Siamese network architectures with implicit labels: augmentations of the same image instance are considered positive samples, while any other image is considered to be negative. This instance-based approach has proven successful, but ignores the semantic similarities present in the image data. For example, ImageNet Deng et al. (2009), a commonly used dataset for unsupervised learning, is actually a supervised dataset purposefully collected from a hierarchical set of labels Miller (1995), and even non-curated image data contain examples with similar characteristics Pope et al. (2021). Disregarding this structure can lead to similar images repelling each other in the embedding space, hampering the self-supervised learning process. To address this, several recent works have incorporated clustering strategies Caron et al. (2018, 2020); Li et al. (2021a); Huang et al. (2022b). Although learned clusters have been shown to lead to strong performance in various downstream tasks, several design choices of these past clustering methods, often based somewhat heuristically on K-means Lloyd (1982), remain unclear. To enhance our understanding of clustering methods, we leverage a principled statistical foundation and conceptualize clustering-based self-supervised learning as a mixture model (MM). Mixture models characterize data as a combination of component distributions, with each component representing a distinct sub-population within the overall distribution. Given the semantic structure inherent in the image data, we posit that MMs serve as a natural fit for unsupervised representation learning. Depending on whether embeddings are normalized, we cast clustering in representation learning as either a Gaussian mixture model (GMM) or von Mises-Fisher mixture model (vMFMM). Examining previous methodologies through this lens allows us to discern the underlying techniques and perceive improvements within more generalized mixture models. For example, K-means, often employed in cluster-based self-supervised learning through contrastive loss with a large number of clusters, can be viewed as an empirical execution within this broader framework. By relaxing some of these constraints or drawing analogies to common Expectation Maximization (EM) procedures for learning mixture models, we gain insights into more effective ways',\n",
       "  'in cluster-based self-supervised learning through contrastive loss with a large number of clusters, can be viewed as an empirical execution within this broader framework. By relaxing some of these constraints or drawing analogies to common Expectation Maximization (EM) procedures for learning mixture models, we gain insights into more effective ways of utilizing clustering in self-supervised learning. The mixture model formulation also suggests alternative procedures, such as consistent clustering, cluster variance, and soft assignment, as opposed to the prevalent methods that involve re-initialized clustering, equal spherical clusters, 1 arXiv:2511.05462v1 [cs.LG] 7 Nov 2025 Figure 1 SiamMM model architecture. Assuming all the embeddings are L2 normalized, we cast clustering in representation learning as a von Mises-Fisher mixture model (vMFMM). The optimization objective tends to minimize the distance between an embedding and its clustering centroid (or nearest centroids) without negative samples. and hard assignment. Another fundamental question in the clustering or mixture model is determining the number of clusters, which is often picked by heuristic intuition, cross-validation, or information criterion Sugar and James (2003). In the context of deep representation learning, previous unsupervised learning methods tend to overcluster the data population to achieve high model performance Li et al. (2021a), which results in quite a lot of clusters where some of them only capture a single point inside. We argue that this phenomenon contrasts with the original motivation of clustering semantically similar subpopulations. In order to disentangle the uncertainty in determining the number of cluster components, we seek a non-parametric solution to merge clusters throughout the training process. Although a simpler model is obtained, the performance achieves state-of-the-art performance on a series of self-supervised learning benchmarks. We suggest changes to clustering in representation learning to reflect the insights of a mixture model perspective. Because of this inspiration, we name our method SiamMM. We conducted a number of self-supervised learning experiments on ImageNet Deng et al. (2009) with ResNet architectures He et al. (2016), demonstrating that our perspective of the mixture model indeed leads to empirical improvements. Our main contributions are summarized as follows: 1. We introduce a new representation learning approach, SiamMM, which interprets clustering as a statistical mixture model and applies its insights in a self-supervised manner. Our MLE-based loss function is more adaptive and improves the accuracy of Siamese networks without the need to sample negative samples. 2. We propose a novel scheme that dynamically reduces the number of clusters during pretraining. This reduction not only cuts down on training time but also facilitates interpretation. The learned clusters exhibit a strong resemblance to the unseen ground truth labels for ImageNet. 3. SiamMM achieves advancements over previous methods across various Self-Supervised Learning (SSL) benchmarks. We further explore the impact of negative samples in clustering, consistent centroid updates, cluster concentration estimation, and soft cluster assignment through an extensive ablation study. We hope our valuable insights can propel the field of unsupervised learning forward. 2 Related work Self-supervised contrastive learning. Contrastive learning Hadsell et al. (2006) approaches to self-supervised learning in computer vision aim to learn an embedding space where views or augmented crops of an',\n",
       "  'through an extensive ablation study. We hope our valuable insights can propel the field of unsupervised learning forward. 2 Related work Self-supervised contrastive learning. Contrastive learning Hadsell et al. (2006) approaches to self-supervised learning in computer vision aim to learn an embedding space where views or augmented crops of an image have more similar embeddings than those of different images. Many contrastive methods rely on mechanisms of sampling a large number of negative samples Chen et al. (2020a); He et al. (2020); Tian et al. (2020); Wu et al. (2018) to prevent the model from learning trivial solutions with unlabeled data. For example, SimCLR Chen et al. (2020a) requires a large batch size to have enough negative samples; MoCo He et al. (2020) 2 maintains negative samples in a memory bank Wu et al. (2018) and consistently updates the embeddings via a separate momentum encoder. Negative samples were initially thought to prevent representation collapse: without something to counteract the gradients pulling embeddings together, it was thought that self-supervised methods would lead to the trivial solution of all inputs mapping to a constant embedding. However, a recent series of methods Grill et al. (2020); Chen and He (2021); Zbontar et al. (2021); Bardes et al. (2022) have shown it possible to directly learn invariant features from distorted versions of an image without any negative samples. BYOL Grill et al. (2020) and SimSiam Chen and He (2021) introduce asymmetry in the network and learn updates using one distorted version and a stop gradient operation on the other version. Barlow Twins Zbontar et al. (2021) and VICReg Bardes et al. (2022) add regularization in the cross-correlation matrix of embeddings of positive samples to reduce over-fitting of redundant parameters. Deep clustering. Clustering-based methods for self-supervised learning have seen success in self-supervised learning YM. et al. (2020); Caron et al. (2019, 2018, 2020); Li et al. (2021a). Though negative samples are not directly used, pseudo-labels or cluster centroids play a role in making a contrastive loss. DeepCluster Caron et al. (2018) and SeLa YM. et al. (2020) cluster representation embeddings from previous iterations and train a classifier based on the cluster index as the pseudo-label. SwAV Caron et al. (2020), ProPosHuang et al. (2022b), and PCL Li et al. (2021a) incorporate negative cluster centroids in a contrastive loss, though SwAV and ProPos pulls the centroid embeddings of two distorted versions of an image closer, while PCL uses one distorted version to predict its cluster centroid computed from another version. Another difference among these clustering-based methods is that DeepCluster and PCL adopt K-means in cluster analysis, while SeLa and SwAV perform online clustering via Sinkhorn-Knopp Cuturi (2013). Mixture Models. Mixture models are common statistical models that have been in use for more than a century, with Gaussian mixture models (GMMs) a popular choice McLachlan et al. (2000); Day (1969). These models are particularly adept at modeling data with sub-populations, which tend to form clusters within the overall distribution. As an unsupervised learning approach, GMM and its variant can lie well in Deep Neural Network for density estimation van',\n",
       "  'models (GMMs) a popular choice McLachlan et al. (2000); Day (1969). These models are particularly adept at modeling data with sub-populations, which tend to form clusters within the overall distribution. As an unsupervised learning approach, GMM and its variant can lie well in Deep Neural Network for density estimation van den Oord and Schrauwen (2014a), clustering Yang et al. (2019), sampling Gepperth and Pfülb (2021), and anomaly detection Zong et al. (2018), and has been successfully applied in multiple fields of computer vision, such as image processing Zoran and Weiss (2011); van den Oord and Schrauwen (2014b), segmentation Gupta and Sortrakul (1998), generation Gepperth (2022), and face verification Hasnat et al. (2017). The interpretation of clustering in self-supervised representation learning as a mixture model are relatively limited in the literature. 3 Background 3.1 Siamese Networks Representation learning methods seek to learn a mapping from an image x ∈X to a d-dimensional embedding v ∈V, such that the embedding space V captures useful information and structure applicable to downstream tasks. This mapping is often parameterized as a deep neural network E(·), with v = E(x). Recent works often adopt a Siamese framework, seeking to maximize the embedding similarity between two augmentations x1 and x2 of a single image x. An encoder followed by a small multilayer perceptron (MLP) predictor g generates embedding v1 = g(E(x1)) on one branch, while on the other branch, momentum encoder Em(·) generates a second embedding vm 2 = Em(x2). The loss is defined as L(v1, vm 2 ) = D(v1, stopgrad(vm 2 )) (1) where D is a distance metric defined in the embedding space and stopgrad(·) is a stop-gradient operation applied to the second view’s embedding and D. 3.2 Clustering As posed in (1), each instance is considered to be an independent entity, with the loss only enforcing similarity between augmented views for a particular example. For methods with contrastive losses, embeddings of all 3 other samples are additionally pushed away. Since this may not be ideal if there exists many subpopulations within the data distribution, clustering methods allow grouping samples by semantic similarity. Given N samples xi ∈X, 1 ≤i ≤N and their corresponding embedding vectors vi = E(xi) ∈V, a clustering method will produce a cluster assignment A(xi) ∈{1, 2, ..., K}, and Ck := {vi ∈V|A(xi) = k, 1 ≤i ≤N}, 1 ≤k ≤K indicates the embedding point cloud of the k-th cluster. 3.3 Mixture Models The Gaussian Mixture Model (GMM) is a statistical method commonly used in modeling probabilistic clustering and is widely applied in data mining and machine learning Murphy (2013). Given the cluster Ck, assume that the image embeddings v ∈Ck follow a Gaussian distribution centered at µk with variance Σk. The mixture distribution of K Gaussian clusters is given by the summation: f(v) = K X k=1 πkfk(v|µk, Σk) (2) where fk is the density function of a d-dimensional Gaussian distribution of the k-th cluster Ck; πk is the prior probability of Ck. Due to the common practice of L2 normalization, the embedding v lies on a hypersphere and thus',\n",
       "  'the summation: f(v) = K X k=1 πkfk(v|µk, Σk) (2) where fk is the density function of a d-dimensional Gaussian distribution of the k-th cluster Ck; πk is the prior probability of Ck. Due to the common practice of L2 normalization, the embedding v lies on a hypersphere and thus can more appropriately be modeled with a von Mises-Fisher mixture model (vMFMM). Suppose that v is a d−dimensional unit random variable, i.e. v ∈Rd, ||v|| = 1. The likelihood function that v is generated from fk(.) is fk(v|µk, κk) = cd(κk) exp (κkµT k v) (3) where µ is a mean direction with ||µ|| = 1, κ is the concentration, and normalization term cd(κ) is given by cd(κ) = κd/2−1 (2π)d/2Id/2−1(κ) (4) where Id(.) is the modified Bessel function of the first kind and order d. Expectation Maximization (EM) algorithm. The parameters of mixture models are commonly estimated with the EM algorithm, by maximizing the log-likelihood function. In the E-step, the probability that v is assigned to the cluster index k is calculated, denoted as p(k|v). Let the unnormalized mean vector of cluster k be defined as rk = PN i=1 vip(k|vi) PN i=1 p(k|vi) (5) Then, µ and κ can be approximated by Maximum Likelihood Estimation (MLE) Banerjee et al. (2005): ˆµk = rk ||rk|| (6) ˆκk = ||rk||d −||rk||3 1 −||rk||2 (7) for k = 1, 2, ..., K. 4 SiamMM for Clustering Representations We propose viewing clustering in unsupervised representation learning through the lens of mixture models. Under such a paradigm, each cluster can be viewed as a mixture component, representing a subpopulation of the data distribution. Though inspired by mixture models, we do not directly model the learned embeddings as a standard mixture model, as there are several key differences of the deep representation setting. Firstly, the 4 goal of representation learning is to learn the encoder E, whose parameter updates must occur in conjunction with the EM learning of the mixture model parameters. Learning the mapping of X 7→V also means that the deep feature distribution modeled by the MM is constantly evolving, in contrast to the fixed data features commonly modeled by mixture models in statistics. Instead, we primarily use mixture models to motivate changes in the common clustering methodology. To adapt mixture models to deep representation learning, we adopt a two-tier EM algorithm to model cluster distributions and learn representation embeddings simultaneously. In the E step, the assignment function is updated based on the current centroids, and so is the mean vector of (5). This is followed by an M step to estimate the mixture model parameters µ and κ via MLE (6, 7), given the current parameters of E. These E and M steps are iterated, updating the cluster model. We then perform an outer loop M step, updating E to maximize the likelihood of the data. We provide a deeper dive into changes to individual components below. 4.1 MLE: Non-negative Soft-assignment Loss Previous methods have posed a variety of loss objectives for clustering, including pseudo cluster label prediction Caron et al. (2018), swapped',\n",
       "  'outer loop M step, updating E to maximize the likelihood of the data. We provide a deeper dive into changes to individual components below. 4.1 MLE: Non-negative Soft-assignment Loss Previous methods have posed a variety of loss objectives for clustering, including pseudo cluster label prediction Caron et al. (2018), swapped assignment prediction Caron et al. (2020), or directly replacing one of the views Li et al. (2021a) or both views Huang et al. (2022b) with cluster centroids in the contrastive loss. Several of these methods utilize negative centroids as an additional repelling force between clusters. Additionally, many previous clustering approaches assume a hard-assignment strategy, where the assignment probability πk is implicitly treated as a binary 0-1 variable given by πk(v) = ( 1 if v ∈Ck 0 otherwise (8) In the case that v ∈Ck, the density function of mixture models (2) reduces to a single distribution fk. While hard assignment simplifies clustering, it is also an assumption that limits the model expressivity. Alternatively, we can generalize and allow the mixture model to have soft assignment. Within the context of representation learning, this allows a single sample to be driven towards multiple prototypes, with varying probability. In practice, we limit soft assignment to a set of H nearest centroids around v. Suppose that the set of nearest centroids is M with size |M| = H, so πk(v) = 0 for k /∈M. In mixture models, the M step of the EM algorithm typically estimates the model parameters with maximum likelihood estimation (MLE). The negative log-likelihood can be expressed as the following weighted summation: LH(v|ˆκA(x), ˆµA(x)) = −log X k∈M πk exp(ˆκkˆµT k v) (9) where ˆκ and ˆµ are now fixed estimators and carry no gradient operation; πk is the weight scale for the k-th assignment. We define πk by considering the relative similarity score between v and its centroids by πk(v) = αk exp(µT k v/τ) PH l αl exp(µT l v/τ) (10) where αk is a measure of the size of cluster; τ > 0 is a tuning parameter; for a large temperature τ, the weight tends to be evenly distributed over H; for a small τ, the weight lies more on the closest centroids. Note that in the special case where H = 1, this soft assignment reduces to the hard assignment of (8). Notably, MLE does not rely on any negative sampling; instead, one only needs to maximize the similarity between embeddings and their nearest clusters. By removing any notion of negatives from our objective functions, we can improve computational efficiency by avoiding computing additional negatives terms in the loss. Note that this corroborates recent works finding that negative sampling is unnecessary for non-clustering self-supervised methods Grill et al. (2020); Chen and He (2021); Zbontar et al. (2021); Bardes et al. (2022). 5 4.2 Cluster Merging One of the prevalent challenges in many mixture models or clustering algorithms is the determination of the number of clusters, often unknown a priori and requiring estimation through methods such as cross-validation or analytic criteria. In the realm of self-supervised learning, this',\n",
       "  'Bardes et al. (2022). 5 4.2 Cluster Merging One of the prevalent challenges in many mixture models or clustering algorithms is the determination of the number of clusters, often unknown a priori and requiring estimation through methods such as cross-validation or analytic criteria. In the realm of self-supervised learning, this challenge is compounded by the presence of embedding shift: as the encoder is trained, the distribution and clustering behavior of embeddings change over time. Although employing a large number of clusters (K) tends to yield strong empirical performance, it can compromise the interpretability of clustering results. For instance, advocating for 100k clusters, as suggested in Li et al. (2021a), is a hundred times larger than the number of classes in ImageNet. Furthermore, a fixed high value of K increases computational costs, underscoring the importance of minimizing K when feasible. Instead of manually specifying the number of clusters, we propose a merging strategy that dynamically adapt the parameter K to the evolving embedding space. In the initial stages, when the representations lack substantial meaning, it is advantageous to commence with a larger number of clusters. Subsequently, at each iteration, we refine the clustering by merging the closest clusters by a predefined threshold ζ. Suppose the pair-wise distance between cluster index i and j is Zij := ||µi −µj||2, then the two cluster are merged if Zij −E{Zij} SE{Zij} < ζ (11) where E{.} and SE{.} are the expectation and standard deviation of the pair-wise distance, respectively. This approach, characterized by the initial use of a large number of clusters followed by a gradual reduction to a smaller set of key centroids, resonates with the principles of the Lottery Ticket Hypothesis Frankle and Carbin (2019): by initially overseeding clusters, there are increased chances of identifying the “correct” centroids, thereby enhancing clustering performance. Such an approach mitigates the computational costs associated with a fixed large number of clusters, ultimately converging to a number on par with the number of classes in ImageNet without sacrificing accuracy. 4.3 Consistent Centroids Update There is a well-known tendency of K-means to fall into poor local minima, and the final result is often sensitive to centroid initialization Pena et al. (1999). In practice, it is common to run many re-initializations and pick the best result Jain et al. (1999); Li et al. (2021a). The practice of re-initializing clustering after each epoch results in several negative side effects in self-supervised learning. Firstly, these re-initializations mean that the cluster centroids are constantly changing, beyond just the embeddings being updated by backpropagation; the very clustering structure, which is meant to represent learned semantic structure in the data, is inconsistent, leading to a learning signal full of thrash. In contrast, we observe that the mixture model formulation suggests that the mixture distributions should be learned consistently. Rather than re-initializing from scratch multiple times each epoch, centroids should be initialized exactly once, at the start of training; subsequent clustering per epoch should be initialized with the previous epoch’s centroids. By doing so, we provide the algorithm with more consistent cluster targets, and removing the need',\n",
       "  'learned consistently. Rather than re-initializing from scratch multiple times each epoch, centroids should be initialized exactly once, at the start of training; subsequent clustering per epoch should be initialized with the previous epoch’s centroids. By doing so, we provide the algorithm with more consistent cluster targets, and removing the need for multiple restarts significantly speeds up training. The difference between one round vs. five random initializations Li et al. (2021a) per epoch is an approximately 25% increase in training time, even with an efficient implementation like Faiss Johnson et al. (2019). 4.4 Non-uniform Cluster Size K-means is well known for producing clusters of roughly equal size, as every point in V is assigned to the nearest centroid, without any notion of the learned per-cluster size. This may not be a good assumption in certain applications. For example, while the commonly used ImageNet-1K dataset Deng et al. (2009) has a relatively balanced number of samples per class, the classes themselves are not necessarily evenly distributed semantically (e.g., over 10% of the categories are breeds of dogs), nor are the clusters one might recover when learning unsupervised structure. In the context of the von Mises-Fisher mixture model, each component of the mixture is characterized not only by a mean µk (i.e., the centroid) but also by a concentration parameter per cluster κk. The ability to learn per-cluster variance allows for a more expressive representation of the embedding distribution. 6 4.5 Final Loss Function Clustering algorithms like K-means may fail to converge at the beginning of the training process, when the representations have yet to learn much meaning. Empirically, we found that adding an instance-wise loss can boost clustering efficiency and improve performance. This finding is consistent to the additive loss suggested by Li et al. (2021a); Khorasgani et al. (2022); Huang et al. (2022b); Li et al. (2021b). We define the symmetrized instance-wise loss from (1) as Linst = L(v1, vm 2 ) + L(v2, vm 1 ) (12) This instance-wise loss is combined with the cluster-wise loss, specifically the soft-assignment loss LH in (9). The final loss for SiamMM takes an additive form Lfinal = LH + Linst (13) Importantly, neither of these terms relies on negative samples. 5 Experiments 5.1 Implementation details We pre-train SiamMM on the ImageNet-1k dataset Deng et al. (2009), with a standard ResNet-50 He et al. (2016) as encoder E. The projection head is a 3-layer MLP, and the prediction head is a 2-layer MLP. Following Chen et al. (2021), all MLP layers have Batch Normalization Ioffe and Szegedy (2015), with 4096 dimensional hidden layers with ReLU activations, and the output layers are 256 dimensional without ReLU. Our training procedure proceeds as follows: First, we update the cluster assignments using the centroids computed from the previous epoch, and estimate the mixture model parameters µ and κ using equations (6) and (7), respectively. Next, we backpropagate through the encoder E using the loss function in (13). Finally, we apply the cluster merging strategy described in Section 4.2 and update the cluster centroids accordingly. We use Faiss Johnson et al.',\n",
       "  'estimate the mixture model parameters µ and κ using equations (6) and (7), respectively. Next, we backpropagate through the encoder E using the loss function in (13). Finally, we apply the cluster merging strategy described in Section 4.2 and update the cluster centroids accordingly. We use Faiss Johnson et al. (2019) for clustering and the nearest centroid search, with a default initial number of K = 100k for cluster merging. We find that setting the merging threshold ζ = −1.2 in (11), which corresponds to the 10-th percentile of the normalized pairwise distance between cluster centroids, yields a promising merging curve. This threshold works well for large cluster numbers, though it may vary with data distribution and initial cluster count. We recommend that researchers adjust this parameter through methods like grid search to suit their specific datasets and clustering needs. We set α = 1 and τ = 0.02 in (10) according to soft assignment when H > 1; by default, H = 5 unless otherwise stated. In the estimation of concentration parameter κ, we find it beneficial to apply principal component analysis (PCA) to reduce dimensional correlations in the features Jing et al. (2022). More details on this process can be found in Appendix C.1. We adopt the same configuration of data augmentations as MoCo v3 Chen et al. (2021). We use the LARS optimizer Ginsburg et al. (2018) by default with a weight decay of 1e-6, a momentum of 0.9, batch size 4096, and a base learning rate lrbase = 0.6 and 0.5 for 100-epoch and 200-epoch, respectively. The learning rate is adjusted for different batch size lr = lrbase × batch_size/256 and follows a cosine decay schedule. 5.2 Image classification benchmarks We evaluated the pre-trained ResNet-50 backbone with SiamMM for 100 epochs and 200 epochs on ImageNet. ImageNet Linear Evaluation. We compare SiamMM’s learned representations with recent state-of-art methods by training a linear classifier on top of a frozen ResNet-50 backbone. The validation set top-1 accuracies are reported in Table 1. We use an SGD optimizer with cosine learning rate with lrbase = 0.1, momentum of 0.9, weight decay of 0.0, and batch size of 1024, for 90 epochs. The 200-epoch pretraining results of BYOL Grill et al. (2020) and SwAV Caron et al. (2020) are from the implementation in Chen and He (2021), which reproduces the results with two 224×224 crops of each image for a fair comparison. We observe that SiamMM outperforms the previous methods when considering a similar number of pre-training epochs. In particular, 7 Table 1 Linear classification on ImageNet. We report top-1 accuracy (in %) with a frozen pretrained ResNet-50. †: result reproduced by us. Method Epochs Top-1 Epochs Top-1 SimCLRChen et al. (2020a) 100 64.6 200 66.8 MoCo v2Chen et al. (2020b) 100 - 200 67.5 PCLLi et al. (2021a) 100 - 200 67.6 SwAVCaron et al. (2020) 100 66.5 200 69.1 SimSiamChen and He (2021) 100 68.1 200 70.0 BYOLGrill et al. (2020) 100 66.5 200 70.6 All4OneEstepa et al. (2023) 100 66.6 200 - NNCLRDwibedi et al. (2021) 100 69.2 200',\n",
       "  '- 200 67.5 PCLLi et al. (2021a) 100 - 200 67.6 SwAVCaron et al. (2020) 100 66.5 200 69.1 SimSiamChen and He (2021) 100 68.1 200 70.0 BYOLGrill et al. (2020) 100 66.5 200 70.6 All4OneEstepa et al. (2023) 100 66.6 200 - NNCLRDwibedi et al. (2021) 100 69.2 200 70.7 MoCo v3Chen et al. (2021) 100 68.9 200 71.0 † ProPosHuang et al. (2022b) 100 - 200 72.2 SNCLRGe et al. (2023) 100 69.6 200 72.4 LEWELHuang et al. (2022a) 100 71.9 200 72.8 SiamMM (ours) 100 71.9 200 73.2 Table 2 Other downstream evaluation tasks. (left) semi-supervised evaluation for ResNet-50 backbone fine-tuned on 1% and 10% ImageNet data; the top-5 accuracy are reported (in %). (right) transfer learning results for other downstream tasks, Places205 Zhou et al. (2014) scene classification with top-1 accuracy, and VOC07 Everingham et al. (2009) multi-label image classification with mAP. Method Epochs ImageNet Places205 VOC07 1% 10% SimCLRChen et al. (2020a) 200 56.5 82.7 - - MoCo v2Chen et al. (2020b) 200 66.3 84.4 - - PCLLi et al. (2021a) 200 73.9 85.0 50.3 85.4 NNCLRDwibedi et al. (2021) 400 79.2 88.6 - - SNCLRGe et al. (2023) 400 80.1 89.1 - - PIRLMisra and van der Maaten (2020) 800 57.2 83.8 49.8 81.1 SimCLRChen et al. (2020a) 800 75.5 87.8 53.3 86.4 BYOLGrill et al. (2020) 1000 78.4 89.0 54.0 86.6 SwAVCaron et al. (2020) 1000 78.5 89.9 - - Barlow TwinsZbontar et al. (2021) 1000 79.2 89.3 54.1 86.2 VICRegBardes et al. (2022) 1000 79.4 89.5 54.3 86.6 SiamMM (ours) 200 79.4 89.2 53.2 87.3 SiamMM outperforming other clustering methods such as SwAV Caron et al. (2020), PCL Li et al. (2021a), and ProPos Huang et al. (2022b) demonstrate the value of our perspective of the mixture model. Semi-supervised Learning. We also evaluate the performance of the pretrained backbone by fine-tuning it with a linear classifier using 1% and 10% of the labels. We use the split of Chen et al. (2020a) to select a subset of ImageNet training data with labels and report the top-5 accuracy on the ImageNet validation set in Table 2. Our method is competitive with the state-of-the-art even compared with the previous methods pre-trained on a larger number of epochs. Transfer Learning. Following the setup of Misra and van der Maaten (2020), we train a linear classifier on top of the frozen SiamMM backbone on other downstream tasks: Places205 Zhou et al. (2014) scene classification and VOC07 Everingham et al. (2009) multi-label image classification. For Places205 dataset, we train a fully-connected layer followed by softmax and report the top-1 accuracy (in %); for VOC07, we train a linear SVM and report mAP in Table 2. Our SiamMM achieves results on par with the state-of-the-art on both downstream tasks. Clustering Evaluation. We evaluate the clustering quality by computing the Adjusted Mutual Information (AMI) Vinh et al. (2009) between clustering index obtained by different methods and the ground truth label 8 Table 3 Evaluation of Quality of Clustering Results. Following Li et al. (2021a), Adjusted Mutual Information (AMI) are reported under',\n",
       "  'Evaluation. We evaluate the clustering quality by computing the Adjusted Mutual Information (AMI) Vinh et al. (2009) between clustering index obtained by different methods and the ground truth label 8 Table 3 Evaluation of Quality of Clustering Results. Following Li et al. (2021a), Adjusted Mutual Information (AMI) are reported under 200 epochs pretraining on ImageNet with 25k clusters. Following Tian et al. (2021), every clusters mapped to the most frequent true class label to compute top-1 accuracy (%). Method AMI (K = 25k) Acc (K = 1k) DeepClusterCaron et al. (2018) 0.28 - MoCoHe et al. (2020) 0.29 - SwAVCaron et al. (2020) 0.40 - PCLLi et al. (2021a) 0.41 - ProPosHuang et al. (2022b) 0.53 - SimCLRChen et al. (2020a) - 33.3 BYOLGrill et al. (2020) - 51.0 MoCLRTian et al. (2021) - 51.6 SiamMM (ours) 0.60 56.3 Table 4 Instance-wise and cluster-wise losses ablation (row 1-2); effect of instance-wise contrastive loss of Moco v3 Chen et al. (2021) (row 3-5). Method inst.-wise neg. inst. clus.-wise Top-1 SimSiam X 70.0 SiamMM(w/o i) X 71.8 Moco v3 X X 71.0 SiamMM(w/ n) X X X 73.1 SiamMM X X 73.2 on ImageNet. In the second experiment, we map to the most frequent true class label to every cluster and evaluate the top-1 accuracy by following Tian et al. (2021). Table 3 demonstrates the higher quality of the clusters generated by SiamMM that significantly exceeds the previous unsupervised learning methods. 6 Ablation Experiments 6.1 Instance-wise v.s Cluster-wise Losses To evaluate the importance of the clustering objective, we ablate the instance loss and only implement the loss (9). The linear evaluation top-1 accuracy drops to 71. 8% under 200-epoch pre-training, though it is still better than that of SimSiam (see row #1 & #2 in Table 4). 6.2 Instance-wise Negative Samples The proposed method can incorporate instance-wise self-supervised learning objectives. We replace the Linst in (12) with contrastive loss of MoCo v3 Chen et al. (2021) and denote the new additive loss as “SiamMM w/ negative inst” (see row#4 in Table 4). It shows that the MoCo v3 can get boosted by more than 2% after adding our cluster-wise loss (9). Besides, as “SiamMM w/ negative inst” achieves similar result to SiamMM, it shows that negative instance is not needed. 6.3 Cluster Merging We evaluated the performance and training time of the model with and without the cluster-merging strategy. Table 5 presents the top-1 accuracy of linear evaluation for various numbers of clusters. Initiating with a substantial number like 100k and progressively merging down to a smaller cluster count markedly enhances model performance compared to fixed-number clustering algorithms when the final number of clusters is similar. Furthermore, in comparison to employing a fixed large number of clusters, the training time is approximately halved without a significant loss in accuracy. In comparison, SimSiam requires only 65% of 9 Table 5 Fix or merge the number of clusters during training. Top-1 accuracy (%) of linear evaluation on ImageNet, and training time compared with the baseline (row #1) are reported. final # of clusters TOP-1 Train Time',\n",
       "  'a significant loss in accuracy. In comparison, SimSiam requires only 65% of 9 Table 5 Fix or merge the number of clusters during training. Top-1 accuracy (%) of linear evaluation on ImageNet, and training time compared with the baseline (row #1) are reported. final # of clusters TOP-1 Train Time 1k fixed 71.6 1x merged 73.0 1.15x 5k fixed 72.4 1.1x merged 73.2 1.20x 100k fixed 73.2 2.33x Table 6 Initial number of clusters. Initiating with different number of clusters (25k, 50k, 100k) and merging down to a similar number of cluster count. Top-1 accuracy (%) of linear evaluation on ImageNet. init # of clusters hard assign soft assign train time 25k 72.6 72.8 x 50k 72.9 73.1 1.1x 100k 73.0 73.2 1.3x the training time of SiamMM (starting from 100k clusters), but it results in a 3.2% drop in performance compared to SiamMM. 6.4 Initial Number of Clusters for Merging We conduct a comparative analysis of the performance of the model based on three different numbers of initial clusters, considering both hard and soft cluster assignments. Regardless of the initial number of clusters, the final number of clusters converges to a to almost the same number of classes included in ImageNet, as depicted in Figure 2. We extended this analysis to two subsets of ImageNet—ImageNet100Tian et al. (2019) and TinyImageNetLe and Yang (2015), which contain 100 and 200 classes, respectively—and found similar convergence patterns, underscoring the robustness of our approach across different datasets. Table 6 illustrates that initiating with a larger number of clusters and subsequently merging down increases the top-1 accuracy. This finding supports our hypothesis that overseeding clusters initially provides more opportunities to discover the “correct” centroids, resulting in enhanced clustering. However, increasing the cluster count from 50k to 100k yielded 0.1% returns in accuracy, while largely increasing time complexity. Based on these findings, we recommend using 50k clusters as a balanced choice, offering substantial computational efficiency while maintaining strong performance. Additionally, we observed that the soft-assignment strategy improved top-1 accuracy by 0.2% compared to the hard-assignment approach. 6.5 Per-cluster Concentration Parameter We ablate the effect of having per cluster concentration parameter by replacing κk with a constant number. Without taking into account the concentration scale per cluster, model performance drops by 0.3% under 200-epoch pre-training (see Table 7). 6.6 Consistent Centroid Updating As discussed in Section 4.3, we consistently update cluster centroids, propagating the learned centroids from the previous iteration to the next. In the ablation study, we adhere to the default setting Li et al. (2021a); Huang et al. (2022b); Caron et al. (2018), randomly initializing cluster centroids when applying clustering per iteration. Appendix Figure 7 illustrates the training time for varying numbers of cluster re-initializations and with different fixed numbers of clusters. The training time for 200k clusters doubles when the number of re- initializations increases from 1 to 5. Furthermore, we observe a slight drop in model performance without 10 Figure 2 Starting from different initial numbers, the number of clusters converge almost to the true number of cluster in the datasets (left: ImageNet1kDeng et',\n",
       "  '200k clusters doubles when the number of re- initializations increases from 1 to 5. Furthermore, we observe a slight drop in model performance without 10 Figure 2 Starting from different initial numbers, the number of clusters converge almost to the true number of cluster in the datasets (left: ImageNet1kDeng et al. (2009); right: ImageNet100Tian et al. (2019)). Table 7 Concentration Parameter and Consistent Update. Top-1 accuracy (in %) of linear evaluation on ImageNet under 200 epochs: w/ and w/o the per-cluster concentration parameter κ (row #1 & #3); w/ and w/o the consistent update (row #2 & #3). Method Consist Update Concent. κ TOP-1 SiamMM X 72.9 SiamMM X 73.1 SiamMM X X 73.2 applying the consistency strategy, as shown in Table 7. This highlights a major advantage of consistent update to inherit centroids from the previous epoch as initialization and avoiding re-initialization. 6.7 Negative Sample is Unnecessary for SiamMM Many previous unsupervised learning methods borrow from contrastive methods and use large numbers of negative cluster centroids to form a contrastive or cross-entropy loss Li et al. (2021a); Huang et al. (2022b); Caron et al. (2018, 2020). As discussed in Section 4.1, these negative centroids are not needed from a mixture model point of view. We thus reframe the cluster-wise loss (9) (with H = 1 for simplicity) by contrasting v from its negative centroids as suggested by PCL Li et al. (2021a), Lnce1 = −log exp(ˆκA+ ˆµT A+v) exp(ˆκA+ ˆµT A+v)+ X A− exp(ˆκA−ˆµT A−v) (14) where A+ and A−indicates the positive centroid and negative centroids, respectively. Alternatively, another sampling strategy is to sample negative image embedding v−that are out of the cluster CA. So, the variant loss contrasts v+ from the negative embeddings v−by Lnce2 = −log exp(ˆκA ˆµT Av+) exp(ˆκA ˆµT Av+)+ X v− exp(ˆκAˆµT Av−) (15) where v+ and v−are positive and negative image embedding, respectively. The evaluation results reported on Appendix Table 8 show that the two variations perform similarly to SiamMM. We conclude that simply integrating more negative samples does not help, possibly due to noise or redundant information. 6.8 Cluster visualization We showcase visualizations of samples from the approximately 1000 clusters learned by SiamMM, along with the corresponding ground truth labels for each image. Each cluster is characterized by the most common 11 Figure 3 Visualization of Merged 1000 Clusters. Left: Images grouped under the same cluster label, where each row corresponds to a true class label in ImageNet. Right: Images grouped under class label “eggnog”, where each row corresponds to a predicted clustering label. ground truth label within it in Figure 3. In particular, despite the absence of truth labels during training, SiamMM learns clusters that exhibit a coherent alignment with ground truth labels. Furthermore, SiamMM reveals subtle texture or background differences that are consistently present across various image classes. These observations underscore the potential of SiamMM to improve data quality by identifying mislabeled images Northcutt et al. (2021) and highlighting more challenging negative samples. 7 Conclusion We have presented a mixture model perspective on clustering in self- and unsupervised learning methods. This framework',\n",
       "  'consistently present across various image classes. These observations underscore the potential of SiamMM to improve data quality by identifying mislabeled images Northcutt et al. (2021) and highlighting more challenging negative samples. 7 Conclusion We have presented a mixture model perspective on clustering in self- and unsupervised learning methods. This framework allows us to develop a generalized framework and propose refined techniques for clustering in representation learning. Empirical evaluations confirm our individual insights and show that the combination of our parts result in learned representations performing better on a number of downstream tasks. For future work, we would like to see how the mixture model perspective generalizes on classification datasets with class imbalance, detection, and video. 12 References Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit hypersphere using von mises-fisher distributions. Journal of Machine Learning Research, 6(46):1345–1382, 2005. http://jmlr.org/papers/ v6/banerjee05a.html. Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. https://openreview.net/forum?id=xm6YD62D1Ub. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Computer Vision – ECCV 2018, pages 139–156, Cham, 2018. Springer International Publishing. Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-training of image features on non-curated data. In Proceedings of the International Conference on Computer Vision (ICCV), 2019. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural Information Processing Systems, volume 33, pages 9912–9924. Curran Associates, Inc., 2020. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 1597–1607. PMLR, 13–18 Jul 2020a. Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15745–15753, 2021. doi: 10.1109/CVPR46437.2021.01549. Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. CoRR, abs/2003.04297, 2020b. https://arxiv.org/abs/2003.04297. Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021. Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. Neil E Day. Estimating the components of a mixture of normal distributions. Biometrika, 56(3):463–474, 1969. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2009. Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: Nearest-neighbor contrastive learning of visual representations, 2021. https://arxiv.org/abs/2104.14548. Imanol G. Estepa, Ignacio Sarasúa, Bhalaji Nagarajan, and Petia Radeva. All4one: Symbiotic neighbour contrastive learning via self-attention and redundancy reduction, 2023. https://arxiv.org/abs/2303.09417. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge.',\n",
       "  'Nearest-neighbor contrastive learning of visual representations, 2021. https://arxiv.org/abs/2104.14548. Imanol G. Estepa, Ignacio Sarasúa, Bhalaji Nagarajan, and Petia Radeva. All4one: Symbiotic neighbour contrastive learning via self-attention and redundancy reduction, 2023. https://arxiv.org/abs/2303.09417. Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303–308, 2009. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. International Conference on Learning Representations, 2019. Chongjian Ge, Jiangliu Wang, Zhan Tong, Shoufa Chen, Yibing Song, and Ping Luo. Soft neighbors are positive supporters in contrastive visual representation learning. arXiv preprint arXiv:2303.17142, 2023. Alexander Rainer Tassilo Gepperth. A new perspective on probabilistic image modeling. 2022 International Joint Conference on Neural Networks (IJCNN), pages 1–10, 2022. Alexander Rainer Tassilo Gepperth and Benedikt Pfülb. Image modeling with deep convolutional gaussian mixture models. 2021 International Joint Conference on Neural Networks (IJCNN), pages 1–9, 2021. Boris Ginsburg, Igor Gitman, and Yang You. Large batch training of convolutional networks with layer-wise adaptive rate scaling, 2018. https://openreview.net/forum?id=rJ4uaX2aW. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, 13 and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning. In Advances in Neural Information Processing Systems, volume 33, pages 21271–21284. Curran Associates, Inc., 2020. Lalit Gupta and Thotsapon Sortrakul. A gaussian-mixture-based image segmentation algorithm. Pattern Recognition, 31 (3):315–325, 1998. ISSN 0031-3203. doi: https://doi.org/10.1016/S0031-3203(97)00045-9. https://www.sciencedirect. com/science/article/pii/S0031320397000459. R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), volume 2, pages 1735–1742, 2006. doi: 10.1109/CVPR.2006.100. Abul Hasnat, Julien Bohné, Jonathan Milgram, Stéphane Gentric, and Liming Chen. von mises-fisher mixture model- based deep learning: Application to face verification. CoRR, abs/1706.04264, 2017. http://arxiv.org/abs/1706.04264. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2016. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9726–9735, 2020. doi: 10.1109/CVPR42600.2020.00975. Lang Huang, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, and Toshihiko Yamasaki. Learning where to learn in cross-view self-supervised learning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14431–14440, 2022a. doi: 10.1109/CVPR52688.2022.01405. Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. Learning representation for clustering via prototype scattering and positive sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022b. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448–456. pmlr, 2015. Anil K Jain, M Narasimha Murty, and Patrick J Flynn. Data clustering: a review. ACM computing surveys (CSUR), 31(3):264–323, 1999. Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. In International Conference on Learning Representations, 2022. https://openreview.net/ forum?id=YevsQ05DEN7. Jeff Johnson, Matthijs',\n",
       "  'pmlr, 2015. Anil K Jain, M Narasimha Murty, and Patrick J Flynn. Data clustering: a review. ACM computing surveys (CSUR), 31(3):264–323, 1999. Li Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in contrastive self-supervised learning. In International Conference on Learning Representations, 2022. https://openreview.net/ forum?id=YevsQ05DEN7. Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535–547, 2019. Salar Hosseini Khorasgani, Yuxuan Chen, and Florian Shkurti. Slic: Self-supervised learning with iterative clustering for human action videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16091–16101, June 2022. Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. Junnan Li, Pan Zhou, Caiming Xiong, and Steven C.H. Hoi. Prototypical contrastive learning of unsupervised representations. In ICLR, 2021a. Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. Proceedings of the AAAI Conference on Artificial Intelligence, 35(10):8547–8555, May 2021b. doi: 10.1609/aaai.v35i10.17037. https://ojs.aaai.org/index.php/AAAI/article/view/17037. Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129–137, 1982. Geoffrey J McLachlan, Sharon X Lee, and Suren I Rathnayake. Finite mixture models. Wiley, 2000. George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, 1995. Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6706–6716, 2020. doi: 10.1109/CVPR42600.2020.00674. Kevin P. Murphy. Machine learning : a probabilistic perspective. MIT Press, Cam- bridge, Mass. [u.a.], 2013. ISBN 9780262018029 0262018020. https://www.amazon.com/ Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref=sr_1_2?ie=UTF8&qid= 1336857747&sr=8-2. 14 Curtis G Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine learning benchmarks. arXiv preprint arXiv:2103.14749, 2021. José M Pena, Jose Antonio Lozano, and Pedro Larranaga. An empirical comparison of four initialization methods for the k-means algorithm. Pattern recognition letters, 20(10):1027–1040, 1999. Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of images and its impact on learning. International Conference on Learning Representations, 2021. Catherine A. Sugar and Gareth M. James. Finding the number of clusters in a data set: An information theoretic approach. Journal of the American Statistical Association, 98:750–763, 2003. Y. Tian, O. J. Henaff, and A. Van Den Oord. Divide and contrast: Self-supervised learning from uncurated data. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 10043–10054, Los Alamitos, CA, USA, oct 2021. IEEE Computer Society. doi: 10.1109/ICCV48922.2021.00991. https://doi.ieeecomputersociety. org/10.1109/ICCV48922.2021.00991. Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019. Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer Vision – ECCV 2020, pages 776–794, Cham, 2020. Springer International Publishing. Aaron van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep gaussian mixture models. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014a. https://proceedings.neurips.cc/paper/ 2014/file/8c3039bd5842dca3d944faab91447818-Paper.pdf. Aäron van den Oord and Benjamin Schrauwen. The student-t mixture as a natural image patch prior with application to image compression. Journal of Machine Learning Research, 15(60):2061–2086, 2014b.',\n",
       "  'C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014a. https://proceedings.neurips.cc/paper/ 2014/file/8c3039bd5842dca3d944faab91447818-Paper.pdf. Aäron van den Oord and Benjamin Schrauwen. The student-t mixture as a natural image patch prior with application to image compression. Journal of Machine Learning Research, 15(60):2061–2086, 2014b. http://jmlr.org/papers/ v15/vandenoord14a.html. Nguyen Xuan Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings comparison: is a correction for chance necessary? In Proceedings of the 26th annual international conference on machine learning, pages 1073–1080, 2009. Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3733–3742, 2018. Linxiao Yang, Ngai-Man Cheung, Jiaying Li, and Jun Fang. Deep clustering by gaussian mixture variational autoencoders with graph embedding. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 6439–6448, 2019. doi: 10.1109/ICCV.2019.00654. Asano YM., Rupprecht C., and Vedaldi A. Self-labelling via simultaneous clustering and representation learning. In International Conference on Learning Representations, 2020. Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12310–12320. PMLR, 2021. Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. Advances in neural information processing systems, 27, 2014. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International Conference on Learning Representations, 2018. https://openreview.net/forum?id=BJJLHbb0-. Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration. In 2011 International Conference on Computer Vision, pages 479–486, 2011. doi: 10.1109/ICCV.2011.6126278. 15 Appendix A Algorithm Pseudocode Algorithm 1 SiamMM Pseudocode # E: backbone + projection MLP # Em: momentum encoder of E # g: prediction MLP Input: training data X, number of cluster K Initial: cluster centroids {rk}K k=1 while not Max Epochs do E-step: Update assignment A(.) by clustering embeddings on the centroid {rk}K k=1 from the last iteration M-step: Update µk and κk for k = 1, 2, ..., K for x in X do x1, x2 = aug(x), aug(x) # image augmentation v1, v2 = g(E(x1)), g(E(x2)) vm 1 , vm 2 = Em(x1), Em(x2) L(v1, v2, vm 1 , vm 2 |µA(x), κA(x)) # loss in (13) update(E,g) # optimization update update(Em) # momentum update end for Merge: Merge centroids {rk}K k=1 based on (11) Update number of clusters K end while B Relation to other methods We compare the proposed method with the previous state-of-the-art unsupervised learning method such as PCL Li et al. (2021a), SwAV Caron et al. (2020), and ProPos Huang et al. (2022b). PCL. Both PCL Li et al. (2021a) and SiamMM attempt to improve the within-cluster compactness in an EM manner. However, PCL is based on an InfoNCE instance-wise contrastive loss requiring negative samples. As a result, PCL can',\n",
       "  'al. (2021a), SwAV Caron et al. (2020), and ProPos Huang et al. (2022b). PCL. Both PCL Li et al. (2021a) and SiamMM attempt to improve the within-cluster compactness in an EM manner. However, PCL is based on an InfoNCE instance-wise contrastive loss requiring negative samples. As a result, PCL can suffer from the class collision issue Huang et al. (2022b) from contrasting false negative instances. SiamMM does not require negative centroids (Section 4.1). SwAV. Though SwAV Caron et al. (2020) and SiamMM can be implemented by soft-assignment clustering, they represent different conceptual approaches to representation learning. SwAV poses clustering as a prediction problem, using the embedding of one augmented view to fit the assignment distribution of the other view, while SiamMM directly models the clustering distribution by maximizing the likelihood of a mixture model. ProPos. ProPos Huang et al. (2022b) and SiamMM have similarities in using non-contrastive losses as the instance-wise objective. The primary difference is that ProPos implements clustering on two augmented views to compute paired cluster centroids, with an infoNCE contrastive loss requiring a large number of negative centroids. However, SiamMM has a more straightforward interpretation of the mixture model and is more sample efficient in not requiring negative centroids. C Additional Implementation Details C.1 Details on parameter estimation The loss function of SiamMM attempts to pull image embeddings closer to the nearest cluster centroids on a sphere. The mean estimator ˆµ is a normalized cluster centroid. The concentration parameter κ scales the 16 gradient for each cluster, respectively. It tries to make the loosely distributed cluster more compact compared with the densely distributed clusters. Based on (7), the estimator includes the length of centroids vector ||r|| and the dimensionality d. However, as the dimension of embedding might be correlated, it is non-trivial to estimate the dimensionality d. One simple way to address this problem is to use PCA to reduce the dimension of r and re-measure it length in a low-dimensional space. Based on our experiments, with only 150 principle components the rP CA can reserve more than 80% norm out of the original 256-dim r, so we rewrite (7) by ˆκk P CA = ||rP CA k ||dP CA −||rP CA k ||3 1 −||rP CA k ||2 (16) where dP CA is the number of principle components selected to form rP CA k . Note that the dimension reduction has only been done in the estimation of κ. We still keep the original dimensionality of image representations. The discussion of dimension reduction in representation learning is out of the scope of this paper. D Variants of SiamMM loss Figure 4 Illustration of the differenct negative sampling strategies; the dot and cross indicate the embedding of a data point and a cluster centroid, respectively; the blue arrow illustrates a positive pair, while the red arrow illustrates a negative pair. (Left) sampling negative cluster centroids (14) either restored in a memory bank Li et al. (2021a) or from a batch size Caron et al. (2020); (Right) sampling negative data points out of a target cluster, introduced in (15) .',\n",
       "  'a positive pair, while the red arrow illustrates a negative pair. (Left) sampling negative cluster centroids (14) either restored in a memory bank Li et al. (2021a) or from a batch size Caron et al. (2020); (Right) sampling negative data points out of a target cluster, introduced in (15) . Table 8 Linear evaluation on ImageNet for the variant losses of SiamMM; top-1 accuracy is reported (in %); Compared with SiamMM, SiamMM+Lnce1 (14) introduces negative centroids whose number is chosen as |A−| = 16000 as suggested in Li et al. (2021a); SiamMM+Lnce2 (15) introduces additional negative instances. Method Neg. inst. Neg. cent. Top-1 SiamMM Lnce1 X 72.3 SiamMM Lnce2 X 72.2 SiamMM 72.2 17 E Additional Figures Figure 5 Training time per 100 epochs for different numbers of cluster re-initializations and with different number of clusters. Figure 6 The number of clusters converge to almost the same number of classes included in TinyImageNet. Figure 7 Read from right to left, the top-1 accracy increases linearly as the number of clusters converges on par with the true number of classes during the training process. 18 Figure 8 Visualization of Merged 1000 Clusters. Left: Images grouped under the same cluster label, where each row corresponds to a true class label in ImageNet. Right: Images grouped under class label “whiskey jar”, where each row corresponds to a predicted clustering label. Figure 9 Visualization of Merged 1000 Clusters. Left: Images grouped under the same cluster label, where each row corresponds to a true class label in ImageNet. Right: Images grouped under class label “spiny lobster”, where each row corresponds to a predicted clustering label. Identifiable faces are blurred in the images. Figure 10 Visualization of Merged 1000 Clusters. Left: Images grouped under the same cluster label, where each row corresponds to a true class label in ImageNet. Right: Images grouped under class label “mosque”, where each row corresponds to a predicted clustering label. 19']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform chunking for each document\n",
    "from utils.text_chunking import chunk_text\n",
    "\n",
    "chunk_list = []\n",
    "for doc in documents:\n",
    "    chunk_list.append(chunk_text(doc))\n",
    "\n",
    "chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06553820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iankt/ai-engineer/submissions/Homework4-Submission/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-10 14:15:17,490 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:17,519 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:17,842 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:18,569 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:18,608 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:18,625 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:18,629 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:18,653 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:18,657 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05471 HTTP/1.1\" 200 11430828\n",
      "2025-11-10 14:15:18,695 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05302 HTTP/1.1\" 200 974827\n",
      "2025-11-10 14:15:18,697 [INFO] Saved: data/downloads/pdf/2511.05236.pdf\n",
      "2025-11-10 14:15:18,763 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:18,813 [INFO] Saved: data/downloads/pdf/2511.04984.pdf\n",
      "2025-11-10 14:15:18,822 [INFO] Saved: data/downloads/pdf/2511.05169.pdf\n",
      "2025-11-10 14:15:18,858 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04892 HTTP/1.1\" 200 1301329\n",
      "2025-11-10 14:15:18,892 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:18,943 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04981 HTTP/1.1\" 200 2072817\n",
      "2025-11-10 14:15:18,946 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04971 HTTP/1.1\" 200 516651\n",
      "2025-11-10 14:15:18,948 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05250 HTTP/1.1\" 200 862383\n",
      "2025-11-10 14:15:19,062 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04886 HTTP/1.1\" 200 16937816\n",
      "2025-11-10 14:15:19,082 [INFO] Saved: data/downloads/pdf/2511.05452.pdf\n",
      "2025-11-10 14:15:19,088 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05360 HTTP/1.1\" 200 26211386\n",
      "2025-11-10 14:15:19,185 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05421 HTTP/1.1\" 200 7143611\n",
      "2025-11-10 14:15:19,319 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:19,439 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:19,525 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:19,536 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05106 HTTP/1.1\" 200 5749538\n",
      "2025-11-10 14:15:19,723 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05462 HTTP/1.1\" 200 7992921\n",
      "2025-11-10 14:15:19,728 [INFO] Saved: data/downloads/pdf/2511.04971.pdf\n",
      "2025-11-10 14:15:19,732 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05059 HTTP/1.1\" 200 32771623\n",
      "2025-11-10 14:15:19,769 [INFO] Saved: data/downloads/pdf/2511.04988.pdf\n",
      "2025-11-10 14:15:19,841 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05483 HTTP/1.1\" 200 498800\n",
      "2025-11-10 14:15:19,954 [INFO] Saved: data/downloads/pdf/2511.05302.pdf\n",
      "2025-11-10 14:15:20,061 [INFO] Saved: data/downloads/pdf/2511.05250.pdf\n",
      "2025-11-10 14:15:20,501 [INFO] Saved: data/downloads/pdf/2511.05483.pdf\n",
      "2025-11-10 14:15:20,787 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:21,000 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:21,099 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:21,301 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:21,333 [INFO] Saved: data/downloads/pdf/2511.04892.pdf\n",
      "2025-11-10 14:15:21,509 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05073 HTTP/1.1\" 200 12722981\n",
      "2025-11-10 14:15:21,514 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04949 HTTP/1.1\" 200 6565105\n",
      "2025-11-10 14:15:21,563 [DEBUG] Starting new HTTPS connection (1): arxiv.org:443\n",
      "2025-11-10 14:15:21,708 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05394 HTTP/1.1\" 200 11204424\n",
      "2025-11-10 14:15:21,895 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.05131 HTTP/1.1\" 200 612171\n",
      "2025-11-10 14:15:22,101 [DEBUG] https://arxiv.org:443 \"GET /pdf/2511.04998 HTTP/1.1\" 200 3356213\n",
      "2025-11-10 14:15:22,378 [INFO] Saved: data/downloads/pdf/2511.04981.pdf\n",
      "2025-11-10 14:15:23,251 [INFO] Saved: data/downloads/pdf/2511.05131.pdf\n",
      "2025-11-10 14:15:25,525 [INFO] Saved: data/downloads/pdf/2511.04998.pdf\n",
      "2025-11-10 14:15:25,726 [INFO] Saved: data/downloads/pdf/2511.05265.pdf\n",
      "2025-11-10 14:15:25,778 [INFO] Use pytorch device_name: cpu\n",
      "2025-11-10 14:15:25,782 [INFO] Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-11-10 14:15:25,786 [DEBUG] Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-11-10 14:15:26,524 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:26,626 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:26,881 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:27,043 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:27,352 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:27,512 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:27,588 [INFO] Saved: data/downloads/pdf/2511.05462.pdf\n",
      "2025-11-10 14:15:27,911 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:28,008 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:28,306 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:28,382 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:28,622 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:28,644 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:28,771 [INFO] Saved: data/downloads/pdf/2511.05421.pdf\n",
      "2025-11-10 14:15:29,121 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "2025-11-10 14:15:29,343 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:29,448 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:29,773 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:29,832 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:30,127 [DEBUG] https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "2025-11-10 14:15:30,443 [DEBUG] https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1\" 307 0\n",
      "2025-11-10 14:15:30,542 [INFO] Saved: data/downloads/pdf/2511.04970.pdf\n",
      "2025-11-10 14:15:30,569 [DEBUG] https://huggingface.co:443 \"HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1\" 200 0\n",
      "2025-11-10 14:15:30,820 [DEBUG] https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6876\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]2025-11-10 14:15:30,895 [INFO] Saved: data/downloads/pdf/2511.05471.pdf\n",
      "2025-11-10 14:15:31,030 [INFO] Saved: data/downloads/pdf/2511.04949.pdf\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n",
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]2025-11-10 14:15:33,253 [INFO] Saved: data/downloads/pdf/2511.05106.pdf\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "2025-11-10 14:15:34,051 [INFO] Saved: data/downloads/pdf/2511.05073.pdf\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.16it/s]\n",
      "2025-11-10 14:15:34,693 [INFO] Saved: data/downloads/pdf/2511.05394.pdf\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.82314031e-02, -7.78400972e-02,  3.95499989e-02,  9.56113413e-02,\n",
       "        7.85755366e-02,  7.73619413e-02,  1.42369037e-02, -4.97719608e-02,\n",
       "       -5.04984055e-03, -8.39825124e-02,  4.56140935e-03, -1.11647122e-01,\n",
       "        2.69271596e-03,  3.19037810e-02, -1.88204981e-02, -5.35240881e-02,\n",
       "        1.32682726e-01,  2.03895438e-02, -8.01058710e-02, -1.15907878e-01,\n",
       "       -1.63084045e-02,  1.06365206e-02,  5.24183773e-02,  2.92758420e-02,\n",
       "        6.28594831e-02,  4.78915200e-02,  1.48241762e-02, -3.10902987e-02,\n",
       "        2.04181466e-02, -6.58422783e-02,  7.71445781e-02,  7.59808272e-02,\n",
       "       -8.34938511e-03,  1.55127887e-02, -3.11178267e-02,  2.52445806e-02,\n",
       "        1.27997482e-02,  7.53192306e-02, -2.44741980e-02,  3.19549032e-02,\n",
       "        3.77611071e-02, -2.36717891e-02,  5.65756951e-03,  1.63864240e-03,\n",
       "        4.37469631e-02,  3.16651650e-02, -3.06592230e-02, -5.23050409e-03,\n",
       "       -1.88517664e-02, -5.87517675e-03, -5.35309091e-02,  3.99306752e-02,\n",
       "       -5.51401339e-02,  5.54434545e-02, -4.60914373e-02,  6.24966808e-02,\n",
       "        1.07655399e-01,  4.20956723e-02,  2.32243780e-02, -3.66858728e-02,\n",
       "        5.02584092e-02, -9.19349641e-02, -2.33313739e-02, -2.82262694e-02,\n",
       "        2.38207970e-02,  1.70415249e-02,  2.01682132e-02,  4.73315194e-02,\n",
       "        1.80241410e-02,  3.91610041e-02,  6.05429299e-02,  7.27402940e-02,\n",
       "       -1.17372861e-02,  7.61279762e-02,  3.13328244e-02,  5.18895723e-02,\n",
       "        3.43605727e-02, -3.08906119e-02,  8.24938044e-02, -1.17585540e-03,\n",
       "       -1.70837939e-02,  6.70933723e-02,  1.33471601e-02, -6.45620748e-03,\n",
       "        1.02247812e-01,  3.01938727e-02, -3.58292833e-03,  1.08382886e-03,\n",
       "        3.83981094e-02, -2.23284289e-02, -5.31711988e-02,  2.30266107e-03,\n",
       "       -5.24213016e-02, -2.86406633e-02,  2.87518948e-02, -5.08336201e-02,\n",
       "       -5.81090897e-03, -3.74313481e-02,  5.29444963e-02,  7.47153908e-02,\n",
       "        1.12331510e-02,  6.48059249e-02, -4.72276621e-02, -2.14693975e-02,\n",
       "        3.74881248e-03,  3.63932154e-03, -1.53834722e-03,  3.25495773e-03,\n",
       "        1.05302908e-01, -1.28860883e-02,  1.12765317e-03,  2.38449536e-02,\n",
       "       -2.79423352e-02, -7.56792277e-02, -2.67086457e-02,  1.80955902e-02,\n",
       "        1.10814743e-01,  2.22368073e-02,  4.16814955e-03,  2.09035855e-02,\n",
       "       -1.38204582e-02, -6.61875680e-02,  5.08554578e-02,  4.32430953e-02,\n",
       "       -1.29011730e-02, -5.08736521e-02, -1.19852066e-01,  4.89312050e-33,\n",
       "        2.70660259e-02, -2.79577579e-02,  8.60197693e-02, -6.21343106e-02,\n",
       "       -5.20651368e-03, -4.27666195e-02, -1.05449453e-01,  2.32817531e-02,\n",
       "       -8.74158740e-03,  3.14821210e-03,  1.40918687e-03,  7.08938688e-02,\n",
       "       -2.48038583e-02,  4.15595211e-02,  2.68797483e-02, -4.84440289e-03,\n",
       "       -4.67946865e-02,  4.72009182e-02, -8.29627290e-02, -1.39360696e-01,\n",
       "        3.97997573e-02,  4.89433408e-02,  5.65076806e-03, -4.40761819e-02,\n",
       "       -3.62068973e-02,  2.95723360e-02,  1.16370633e-01, -8.99759680e-03,\n",
       "        5.18856421e-02,  1.42906252e-02, -6.52439147e-02, -1.43727823e-03,\n",
       "        7.01668113e-03,  8.98223072e-02,  2.73075346e-02, -7.79970875e-03,\n",
       "       -6.18275441e-03, -3.77715752e-02,  9.95572936e-03, -2.85699349e-02,\n",
       "       -3.15302238e-02,  2.77625099e-02,  7.25178272e-02, -1.17826194e-01,\n",
       "       -8.22357833e-02, -8.11335817e-02, -1.09325082e-03, -3.97827849e-02,\n",
       "        1.46606145e-02, -7.00740591e-02, -2.26626154e-02, -5.93718737e-02,\n",
       "       -1.43170003e-02,  5.55734849e-03, -4.86901477e-02,  4.12888899e-02,\n",
       "        7.16479961e-03,  8.72995183e-02,  4.19195332e-02,  3.68676186e-02,\n",
       "        4.84780379e-04, -7.18628522e-03, -3.22751999e-02,  7.25215077e-02,\n",
       "       -1.49051417e-02,  4.63150479e-02,  1.84263494e-02,  1.09856546e-01,\n",
       "        6.76636323e-02, -2.31878944e-02, -4.09371853e-02,  9.62656587e-02,\n",
       "       -6.01872429e-02, -5.37402295e-02,  9.77679491e-02, -5.00701973e-03,\n",
       "        5.12457341e-02, -5.27011901e-02, -7.48426020e-02,  9.07373987e-03,\n",
       "        3.71214235e-04,  8.96956678e-03, -1.00507140e-01, -3.56106795e-02,\n",
       "       -7.52385184e-02,  1.36925308e-02,  9.39025078e-03, -6.87300041e-02,\n",
       "       -2.79616681e-03,  4.27671634e-02, -7.20364042e-03, -4.78358474e-03,\n",
       "       -2.99748667e-02,  1.39136519e-02, -2.96245534e-02, -4.42801779e-33,\n",
       "       -1.70394909e-02,  5.72371073e-02, -8.12517852e-02,  2.35748384e-02,\n",
       "       -6.30515162e-03, -4.99184355e-02,  1.13745099e-02,  1.27707794e-01,\n",
       "       -2.52554193e-02, -5.03453501e-02, -7.04288557e-02, -6.55490309e-02,\n",
       "        4.67438698e-02,  1.31229619e-02, -9.76040028e-03,  4.40087076e-03,\n",
       "       -9.33340192e-03,  6.61522746e-02, -4.47194092e-03,  7.87839741e-02,\n",
       "       -3.41568179e-02,  3.00955158e-02, -7.37720355e-02,  2.37856526e-02,\n",
       "       -5.55209368e-02,  2.67272461e-02, -3.42932642e-02,  5.15560322e-02,\n",
       "       -9.22579505e-03,  4.03817110e-02,  3.81304659e-02,  8.29134043e-03,\n",
       "       -1.14391021e-01, -4.33527790e-02, -6.05077036e-02,  2.12732349e-02,\n",
       "       -5.61747700e-03, -5.00143319e-02, -2.84346789e-02, -1.31824901e-02,\n",
       "       -2.28999685e-02,  3.57985720e-02, -1.10055223e-01, -7.55920773e-03,\n",
       "       -1.99589599e-02, -1.13035776e-01, -4.71419916e-02,  1.48632387e-02,\n",
       "        2.34309630e-03, -1.03861839e-02, -7.59613588e-02, -1.18231885e-02,\n",
       "       -1.01898894e-01,  5.47788478e-02, -5.44765219e-02, -3.52859832e-02,\n",
       "       -2.66264309e-03,  8.20240472e-03,  3.09726130e-02,  5.20838834e-02,\n",
       "       -3.65849324e-02, -2.12889425e-02, -2.93230228e-02, -4.00217623e-02,\n",
       "       -8.35831463e-03, -1.14657670e-01,  1.64609682e-02,  5.60849793e-02,\n",
       "        3.19608785e-02,  2.06960272e-02, -1.04762539e-02,  2.99024638e-02,\n",
       "        4.06087674e-02,  8.73167813e-03, -3.79811935e-02, -3.14407870e-02,\n",
       "        1.17513202e-02,  2.36883610e-02, -3.71594504e-02, -2.76661441e-02,\n",
       "        6.24726824e-02, -1.01682223e-01,  3.31016537e-03,  6.39516562e-02,\n",
       "        1.04366876e-01,  7.60650262e-02,  3.44779752e-02,  7.47539029e-02,\n",
       "        1.34580955e-02,  4.54246700e-02,  1.41246347e-02, -9.81323654e-04,\n",
       "       -1.42527539e-02,  1.05674185e-01, -6.83491603e-02, -4.80230220e-08,\n",
       "       -3.14575359e-02, -6.16325289e-02,  1.28549412e-02,  1.03881126e-02,\n",
       "       -3.40166986e-02,  7.16778496e-03, -1.95446722e-02,  1.11237094e-01,\n",
       "       -4.35766950e-02,  8.68878909e-04,  6.59776106e-02, -5.18269725e-02,\n",
       "       -1.17576018e-01, -3.47264260e-02, -2.88198870e-02,  4.85409871e-02,\n",
       "        5.99395595e-02,  1.04703717e-01,  9.71332844e-03,  2.11788490e-02,\n",
       "        1.54573622e-03, -3.80452648e-02,  3.09166815e-02, -6.36561289e-02,\n",
       "        3.87816355e-02, -6.61056787e-02,  1.97866708e-02,  7.31725544e-02,\n",
       "       -2.07465105e-02, -3.11748572e-02, -5.01309149e-02, -1.80618502e-02,\n",
       "        6.42400887e-03, -2.82625686e-02,  6.77433377e-03,  1.72456592e-01,\n",
       "       -1.52632101e-02,  3.82097550e-02, -1.30469967e-02, -4.55671884e-02,\n",
       "       -7.29624322e-03,  1.08302988e-01, -7.12521225e-02,  3.09432186e-02,\n",
       "        4.58888113e-02,  2.96136457e-02,  3.74849327e-03, -4.90517952e-02,\n",
       "        2.88732033e-02, -6.88308105e-02, -2.60940660e-03, -6.08759075e-02,\n",
       "       -9.02173668e-03,  6.75139651e-02,  5.72029576e-02,  4.51483577e-02,\n",
       "       -8.13698769e-02, -1.03069451e-02,  5.42213768e-02,  6.35260418e-02,\n",
       "        4.31378484e-02, -1.05580324e-02, -7.26438612e-02,  9.55535565e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.embedding_generation import get_embedding\n",
    "embeddings = get_embedding(documents)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77691fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
