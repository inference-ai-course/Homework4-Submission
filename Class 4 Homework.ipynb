{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Retrieval-Augmented Generation (RAG) with arXiv Papers\n",
    "This week marks a major shift in your AI agent's capabilities: youâ€™ll build the foundation for a Retrieval-Augmented Generation (RAG) system tailored to scientific research. Rather than relying on an LLMâ€™s memory alone, RAG architectures allow your agent to search a structured knowledge base and generate grounded, document-aware answers.\n",
    "\n",
    "Your task is to create a RAG pipeline using recent arXiv cs.CL papers, converting them into searchable chunks, embedding them, and indexing them with FAISS. Youâ€™ll then implement a simple query interface that takes a user question, retrieves the top relevant chunks, and displays them for further processing.\n",
    "\n",
    "This week marks the beginning of building your agentâ€™s private research knowledge baseâ€”a semantic index that youâ€™ll evolve into a full-featured hybrid database in Week 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "* Understand the components of a Retriever-Reader QA pipeline.\n",
    "* Explore document chunking strategies (e.g., sections vs. sliding windows) and their impact on retrieval performance.\n",
    "* Index scientific text using vector embeddings and FAISS.\n",
    "* Build and query a semantic index via a FastAPI endpoint that returns relevant passages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Design\n",
    "\n",
    "The project will guide you through building a RAG pipeline on arXiv cs.CL papers:\n",
    "\n",
    "1. **Data Collection:** Obtain 50 arXiv cs.CL PDFs (you can scrape via the arXiv API or use a provided sample set).\n",
    "2. **Text Extraction:** Extract raw text from each PDF (for example, using PyMuPDF's `get_text()` on each page). Clean and concatenate the page text into full-document strings.\n",
    "3. **Text Chunking:** Split each paper into chunks (â‰¤ 512 tokens each). You might split at section boundaries or use a sliding-window approach (e.g., 500-token windows with overlap). Chunking into smaller, meaningful segments (around 250â€“512 tokens) often yields better retrieval precision.\n",
    "4. **Embedding Generation:** Compute dense vector embeddings for each chunk. For instance, using the `sentence-transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(list_of_chunks)  # embeds each text chunk into a 384-d vecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   (Alternatively, you can use a Hugging Face Transformer model and apply pooling manually to get chunk embeddings.)\n",
    "5. **Indexing with FAISS:** Build a FAISS index of the chunk embeddings. For example, use a simple index like `IndexFlatL2` with the same dimensionality as your embeddings. Add all chunk vectors to the index (e.g., `index.add(np.array(embeddings))`).\n",
    "6. **Notebook Demo:** Create a notebook where a user query is embedded and passed to the index (`index.search(query_embedding, k)`) to retrieve the top-3 matching chunks. Display the original chunk text for these results.\n",
    "7. **FastAPI Service:** Build a simple FastAPI app. Define an endpoint (e.g. `@app.get(\"/search\")`) that accepts a query parameter `q`. In the handler, embed `q`, perform the FAISS search, and return the top passages as JSON. (For example, a FastAPI endpoint can accept a question and return relevant documents.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Starter Code Snippets\n",
    "\n",
    "Below are skeleton code templates. Fill in the details (indicated by comments or ellipses).\n",
    "\n",
    "**Data Extraction (PDF â†’ Text):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from typing import List\n",
    "import urllib, urllib.request\n",
    "import feedparser\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import ssl, certifi, urllib.request\n",
    "\n",
    "context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "def ssl_read_url(url: str) -> str:\n",
    "    return urllib.request.urlopen(url, context=context).read()\n",
    "\n",
    "def get_pdf_urls() -> List[str]:\n",
    "    url = f\"https://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=50\"\n",
    "    data = ssl_read_url(url)\n",
    "    res = feedparser.parse(data)\n",
    "    return [\n",
    "            link.href\n",
    "            for entry in res.entries\n",
    "            for link in entry.links\n",
    "            if \"pdf\" in link.href\n",
    "          ]\n",
    "\n",
    "    \n",
    "def extract_text_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Open a PDF and extract all text as a single string.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    pdf_bytes = BytesIO(response.content)\n",
    "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        page_text = page.get_text()  # get raw text from page\n",
    "        # (Optional) clean page_text here (remove headers/footers)\n",
    "        pages.append(page_text)\n",
    "    full_text = \"\\n\".join(pages)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "urls = get_pdf_urls()\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "full_texts = [extract_text_from_url(url) for url in urls]\n",
    "print(len(full_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking Logic (Sliding Window):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_tokens: int = 512, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    step = max_tokens - overlap\n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk = tokens[i:i + max_tokens]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    return chunks\n",
    "\n",
    "chunks = [\n",
    "        chunk  \n",
    "        for text in full_texts \n",
    "        for chunk in chunk_text(text)\n",
    "]\n",
    "\n",
    "with open(\"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wiley-Interscience Publication, p61. [11] E. Benedetto et al., Proceedings of PAC07, p. 4033 (2007).', 'Proceedings of PAC07, p. 4033 (2007).', 'parallel plates separated by 0.2 mm is depicted. The electric ï¬eld, driving the electron, 0 0.05 0.1', 'ctron and Positron Accelerators (MBI97), KEK, Tsukuba, Japan, 15-18 July 1997, KEK Proceedings 97-17', 'build up sim- ulations at CERN, in Proc.of ECLOUD12, La Biodola, Isola dâ€™Elba, Italy, 5-9 June 2012.']\n"
     ]
    }
   ],
   "source": [
    "print([chunk[100:] for chunk in chunks[-5:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Embedding Generation (Sentence-Transformers):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy import ndarray\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embeddings(chunks: List[str]) -> ndarray:\n",
    "    return model.encode(chunks) \n",
    "\n",
    "embeddings = get_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**FAISS Indexing and Search:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Assume embeddings is a 2D numpy array of shape (num_chunks, dim)\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)  # using a simple L2 index\n",
    "index.add(np.array(embeddings))  # add all chunk vectors\n",
    "\n",
    "# Example: search for a query embedding\n",
    "queries = [\"How do electron holes in materials work\",\n",
    "           \"Explain how spin-background affects low-lying excitations\",\n",
    "           \"Explain the properties of metallic adatoms\",\n",
    "           \"Explain hole-spin qubits\",\n",
    "           \"Explain why it's hard to scale up quantum computers\"\n",
    "          ]\n",
    "query_embeddings = model.encode(queries)  # get embedding for the query (shape: [1, dim])\n",
    "k = 3\n",
    "distances, indicesList = index.search(query_embeddings, k)\n",
    "indicesList = indicesList.tolist()\n",
    "retrieval_report = [\n",
    "    {\n",
    "        \"request\": queries[i],\n",
    "        \"response\": [chunks[index] for index in indices]\n",
    "    }\n",
    "    for i, indices in enumerate(indicesList)\n",
    "]\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(index, f)\n",
    "\n",
    "with open(\"retrieval_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(retrieval_report, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**FastAPI Route Skeleton:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fastapi import FastAPI\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(q: str):\n",
    "    \"\"\"\n",
    "    Receive a query 'q', embed it, retrieve top-3 passages, and return them.\n",
    "    \"\"\"\n",
    "    # TODO: Embed the query 'q' using your embedding model\n",
    "    query_vector = ...  # e.g., model.encode([q])[0]\n",
    "    # Perform FAISS search\n",
    "    k = 3\n",
    "    distances, indices = faiss_index.search(np.array([query_vector]), k)\n",
    "    # Retrieve the corresponding chunks (assuming 'chunks' list and 'indices' shape [1, k])\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append(chunks[idx])\n",
    "    return {\"query\": q, \"results\": results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Code Notebook / Script:** Complete code for the RAG pipeline (PDF extraction, chunking, embedding, indexing, retrieval).\n",
    "* **Data & Index:** The FAISS index file and the set of 50 processed paper chunks (e.g., as JSON or pickled objects).\n",
    "* **Retrieval Report:** A brief report showing at least 5 example queries and the top-3 retrieved passages for each, to demonstrate system performance.\n",
    "* **FastAPI Service:** The FastAPI app code (e.g. `main.py`) and instructions on how to run it. The `/search` endpoint should be demonstrable (e.g. returning top-3 passages in JSON for sample queries).\n",
    "\n",
    "## Student Exploration Tips\n",
    "\n",
    "* Experiment with different chunk sizes and overlaps. Smaller chunks (âˆ¼250 tokens) often give more precise retrieval, while larger chunks include more context.\n",
    "* Try different embedding models (e.g. using `'all-mpnet-base-v2'` or `'paraphrase-MiniLM-L6-v2'`) to see how retrieval results change.\n",
    "* Implement a simple reranking step: for example, after retrieving candidates with FAISS, re-score them with a cross-encoder model for finer ranking.\n",
    "* Use metadata: consider filtering or weighting chunks by paper metadata (e.g. year, authors, keywords) to improve relevance if needed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
