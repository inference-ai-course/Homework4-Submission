{
  "metadata": {
    "total_queries": 5,
    "results_per_query": 3,
    "index_stats": {
      "total_chunks": 998,
      "dimension": 384,
      "total_papers": 50,
      "avg_chunk_tokens": 500.1743486973948
    }
  },
  "queries_and_results": [
    {
      "query": "What are attention mechanisms in transformer models?",
      "results": [
        {
          "rank": 1,
          "distance": 0.8743600845336914,
          "paper_id": "2511.12832",
          "chunk_id": "2511.12832_chunk_19",
          "text": "to the ‚Äôrealism‚Äô diagnostic task. Figure 5: Layer-wise attention head contributions to the ‚Äôcounter offer‚Äô diagnostic task. Causal Influence of Attention Heads on Responses Countering an Offer Attribution Map: Layer-wise Head Contributions Attention Head Index (0-31) Transformer Layer (0-31) Transformer Layer (0-31) Attribution Score (Logit Œî on Patch) Attribution Score (Logit Œî on Patch) Causal Influence of Attention Heads on Offer Acceptance Responses Attribution Map: Layer-wise Head Contributions Figure 6: Attribution heatmap showing the causal contribution of each attention head (layer √ó head index) to model predictions on the Offer Acceptance diagnostic task. Positive scores (blue) indi - cate heads that increase the likelihood of a supportive completion when patched in; negative scores (red) indicate disruptive or neu - tralizing effects. This analysis guides the selection of target layers for steering interventions. to ensure equal token lengths before activation differences were computed. The pairs have been listed in table?? Scaling Coefficient: The scaling coefficient for steering vector addition/subtrac- tion was set to 2.0 and 2.5. This value was determined empirically by sweeping values from 0.5 to 4.0 in incre - ments of 0.5 and selecting the value that produced the most pronounced desired effects on a small validation set with - out significantly degrading fluency, aligning with common practices in activation engineering (Turner et al. 2024; Pan- ickssery et al. 2024). Steering Vector Construction and Application: Target Layer Identification To determine where steering should be applied within the model, attribution patching ex- periments were conducted. These experiments revealed the layers most responsible for the desired traits. Specifically, Layer 2 was found to be the key intervention point for emo- tional support, while Layer 3 was identified for emotional disclosure. Activation Extraction from Contrastive Sets For each emotional dimension (e.g., emotional support), two sets of texts‚Äîpositive and negative‚Äîwere used. Each set included a seed pair along with GPT -4 generated examples, all nor - malized to the same token length. The model was run over each text, and hidden state activa- tions were extracted at the identified target layer (e.g., Layer 2 for support). These activations were collected across all to- ken positions in the text. The process was identical for both the positive and negative sets. To reduce variance across samples, the extracted activa - tions were averaged within each set. This yielded a single mean positive activation tensor and a mean negative activa- tion tensor for each emotional dimension and layer of inter- est. Steering Vector Calculation The steering vector ( Vsteer) was computed as the difference between the mean positive and negative activation tensors: Vsteer = Mean Positive Activation Tensor ‚àí Mean Negative Activation Tensor (6) This vector captures the directional distinction in the ac - tivation space between positively and negatively expressed emotional traits. To influence model output, the computed Vsteer was ap- plied during inference. Specifically, for any given input prompt, the hidden activations at the target layer were mod- ified by adding a scaled version of the steering vector to the final 15 token positions. These positions were identified as most influential via attribution patching. Experimental Design"
        },
        {
          "rank": 2,
          "distance": 0.9693042039871216,
          "paper_id": "2511.12874",
          "chunk_id": "2511.12874_chunk_17",
          "text": "processing pre-trained on a large corpus of text. GPT-2 Generative Pre-trained Transformer 2. An autoregressive language model that uses unidirec- tional attention (each token can only attend to previous tokens). It contains 124 million parameters in its base version and was pre-trained on a larger corpus than BERT, but its unidirectional nature may limit contextual understanding for classification tasks. DeBERTa Decoding-enhanced BERT with Disentangled Attention. A transformer model that imple- ments a novel attention mechanism which separately computes attention weights for content and position information. This architecture aims to provide more nuanced contextual understanding by disentangling the content and position information in the self-attention mechanism. bert-base-uncased A specific pre-trained variant of BERT that uses a vocabulary of uncased (lower- case) text. It contains 12 transformer layers, 12 attention heads, and 110 million parameters. Generalized HopeA broad, non-specific form of hope that is not tied to a particular outcome, time- frame, or realistic expectation. Often expressed as general optimism about the future. Realistic Hope Hope that is grounded in reality, with reasonable expectations of what could potentially happen based on evidence, experience, or logical reasoning. Unrealistic Hope Hope characterized by expectations that have a very low probability of being realized, often disregarding evidence or practical limitations. Sarcasm In the context of hope classification, expressions that superficially appear hopeful but actually convey the opposite meaning through irony, often with the intent to mock or criticize. Fine-tuning The process of taking a pre-trained model (like BERT) and further training it on a specific task or domain with a smaller dataset to adapt its knowledge to that particular application. Attention Masks Binary tensors used in transformer models to indicate which tokens should be attended to and which should be ignored (such as padding tokens). Transfer Learning A machine learning technique where knowledge gained while solving one problem is applied to a different but related problem, often allowing models to perform well with less task-specific data. Tokenization The process of breaking text into smaller units called tokens, which could be words, subwords, or characters, that serve as the input to NLP models. Transformer Architecture A deep learning architecture that uses self-attention mechanisms to pro- cess sequential data, allowing the model to weigh the importance of different words in relation to each other regardless of their position in the sequence. TFBertForSequenceClassification A TensorFlow implementation of BERT specifically designed for sequence classification tasks, with an additional classification layer on top of the BERT model. SparseCategoricalCrossentropy A loss function used in multi-class classification problems when the target values are represented as integers rather than one-hot encoded vectors. Legacy Adam Optimizer A version of the Adam optimization algorithm in TensorFlow that maintains compatibility with older implementations. Adam (Adaptive Moment Estimation) combines the benefits of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. Learning Rate A hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. The value 2e-5 (0.00002) is commonly used for fine-tuning BERT models. ModelCheckpoint Callbacks Functions in TensorFlow that save the model‚Äôs state at"
        },
        {
          "rank": 3,
          "distance": 1.044034719467163,
          "paper_id": "2511.12832",
          "chunk_id": "2511.12832_chunk_20",
          "text": "computed Vsteer was ap- plied during inference. Specifically, for any given input prompt, the hidden activations at the target layer were mod- ified by adding a scaled version of the steering vector to the final 15 token positions. These positions were identified as most influential via attribution patching. Experimental Design And Evaluation Design for Emotional Support and Disclosure To evaluate the efficacy and consistency of activation steer- ing for nuanced emotional expression, we designed two dis- Attention Head Index (0-31) Transformer Layer (0-31) Attribution Score (Logit Œî on Patch) tinct experimental paradigms: a single-turn experiment , measuring the immediate affective response to a conversa - tional context, and a multi-turn experiment, assessing tem- poral consistency and contextual adaptation of steered emo- tional expression over a sustained interaction. Shared Methodology Across both experimental setups, the core methodology remained consistent. All simulations utilized Mistral-7B-Instruct as the conversational partner (Person A) and a steered Llama-3.1-8B as the target agent (Person B). Steering vectors for Emotional Support and Emotional Disclosure were derived using the averaged acti- vation difference method (Appendix ). The intervention lay- ers‚ÄîLayer 2 for support and Layer 3 for disclosure ‚Äîwere selected based on attribution patching analysis (Appendix ). In all steered conditions, the corresponding vector was added to the hidden state activations of the final 15 tokens at the target layer. To isolate the effects of steering and en - sure reproducibility, a deterministic greedy decoding strat - egy with repetition penalty was employed for all of Person B‚Äôs generations. Single-Turn Experiment Design The single -turn exper - iment assessed the model‚Äôs ability to express the targeted emotion (support or disclosure) in immediate response to a given conversational prompt. For each dialogue, the con - versational history up to Person B‚Äôs response was used as the prompt. Two responses were generated for each prompt: one unsteered (baseline) and one steered (with activation in- tervention). Evaluation focused on linguistic and emotional features of the single generated utterance (Appendix ). Multi-Turn Experiment Design The multi -turn experi - ment tested the temporal consistency and contextual appro- priateness of steered emotional expression. Due to its com - putational cost, this experiment was conducted on the final 10% of eligible dialogues, totaling 1102 examples. Each dialogue followed a structured interactive loop: 1. Start with the initial human context. 2. Person A (Mistral) generates a response. 3. Person B (Llama) responds (first steering point). 4. Person A (Mistral) responds. 5. Person B (Llama) generates a second response (second steering point). 6. Person A (Mistral) provides a final response. To evaluate consistency and adaptation, four variants were run for each dialogue: ‚Ä¢ Unsteered ‚Üí Unsteered (UU): No steering applied. ‚Ä¢ Unsteered ‚Üí Steered (US): Steering introduced mid- conversation. ‚Ä¢ Steered ‚Üí Unsteered (SU): Steering removed mid- conversation. ‚Ä¢ Steered ‚Üí Steered (SS): Steering maintained across turns. Evaluation was conducted both at the turn level and across aggregated responses from Person B. Figure 7: Layer output contributions from attribution patching for an emotional support diagnostic task. The x-axis shows token po- sition; the y-axis shows model components. Color intensity"
        }
      ]
    },
    {
      "query": "How does fine-tuning work for large language models?",
      "results": [
        {
          "rank": 1,
          "distance": 0.6372591257095337,
          "paper_id": "2511.12991",
          "chunk_id": "2511.12991_chunk_0",
          "text": "Fine-Tuned LLMs Know They Don‚Äôt Know: A Parameter-Efficient Approach to Recovering Honesty Zeyu Shi1*, Ziming Wang1*, Tianyu Chen1‚Ä†, Shiqi Gao1, Haoyi Zhou2,3‚Ä† , Qingyun Sun1, Jianxin Li1,3 1SKLCCSE, School of Computer Science and Engineering, Beihang University 2School of Software, Beihang University 3Zhongguancun Laboratory, Beijing {szy 629, wangzm412, tianyuc, gaoshiqi, haoyi, lijx}@buaa.edu.cn Abstract The honesty of Large Language Models (LLMs) is increas- ingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by super- vised fine-tuning (SFT), a common technique for model spe- cialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models‚Äô ability to recognize their knowl- edge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we pro- pose Honesty-Critical Neurons Restoration (HCNR) to surgi- cally repair this suppressed capacity. HCNR identifies and re- stores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively re- covers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustwor- thy LLM deployment. Introduction As Large Language Models (LLMs) are increasingly in- tegrated into high-stakes domains (Zhang et al. 2024; Sarabadani 2019), their reliability is not merely a feature but a necessity (Askell et al. 2021). A cornerstone of this reliability is honesty, which has two components (Li et al. 2024). The first is self-knowledge: the ability to recognize their knowledge boundaries and distinguish what they know from what they do not. The second is faithful self-expression based on this awareness. This trait is crucial because LLMs that confidently fabricate facts or recommend false cures can cause serious harm, undermining user trust and safety. The honesty of LLMs is typically instilled during the alignment stage through techniques such as Reinforcement Learning from Human Feedback (RLHF) (Glaese et al. 2022), which enable models to refuse inappropriate ques- tions or those beyond their knowledge boundaries (Bai et al. 2022; Manish 2023). However, this acquired honesty is not * Equal contribution. ‚Ä† Corresponding author. Copyright ¬© 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Mechanism of honesty degradation in domain- specific fine-tuning. The dishonest behavior of a fine-tuned LLM arises from impaired self-expression, rather than a loss of self-knowledge, which remains intact. This understanding motivates our methods for honesty recovery. immutable. Recent researches find that the supervised fine- tuning (SFT) could greatly hurt the honesty of LLMs, such as in legal QA (Dahl et al. 2024), medical diagnosis (Kim et al. 2025) and educational content generation (Nguyen et al. 2025). To recover the honesty of LLM after SFT, previ- ous methods (Zhang et al. 2023; Li et al. 2024; Cheng et al. 2024) interfere heavily with global parameters using exten- sive datasets, under the assumption that the model‚Äôs knowl- edge boundaries have been deeply corrupted and its"
        },
        {
          "rank": 2,
          "distance": 0.7593761682510376,
          "paper_id": "2511.13368",
          "chunk_id": "2511.13368_chunk_12",
          "text": "Fei Yuan. 2025. Benchmax: A comprehensive multilingual evaluation suite for large language models.Preprint, arXiv:2502.07346. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282‚Äì6293, Online. Association for Computational Linguistics. Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Nguyen. 2023a. Okapi: Instruction-tuned large language mod- els in multiple languages with reinforcement learning from human feedback. InProceedings of the 2023 Conference on Empirical Methods in Natural Lan- guage Processing: System Demonstrations, pages 318‚Äì327, Singapore. Association for Computational Linguistics. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023b. Okapi: Instruction- tuned large language models in multiple languages with reinforcement learning from human feedback. Preprint, arXiv:2307.16039. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. InProceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214‚Äì3252, Dublin, Ireland. Association for Computational Linguistics. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Ling- ming Zhang. 2023. Is your code generated by chat- GPT really correct? rigorous evaluation of large lan- guage models for code generation. InThirty-seventh Conference on Neural Information Processing Sys- tems. Dan Malkin, Tomasz Limisiewicz, and Gabriel Stanovsky. 2022. A balanced data approach for eval- uating cross-lingual transfer: Mapping the linguistic blood bank. InProceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, pages 4903‚Äì4915, Seattle, United States. Association for Computational Linguistics. Sourab Mangrulkar, Sylvain Gugger, Lysandre De- but, Younes Belkada, Sayak Paul, and Benjamin Bossan. 2022. PEFT: State-of-the-art parameter- efficient fine-tuning methods. https://github. com/huggingface/peft. David Mueller, Mark Dredze, and Nicholas Andrews. 2024. Multi-task transfer matters during instruction- tuning. InFindings of the Association for Computa- tional Linguistics: ACL 2024, pages 14880‚Äì14891, Bangkok, Thailand. Association for Computational Linguistics. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai- ley Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Al- banie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generaliza- tion through multitask finetuning. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15991‚Äì16111, Toronto, Canada. Association for Computational Linguistics. Vera Neplenbroek, Arianna Bisazza, and Raquel Fer- n√°ndez. 2024. MBBQ: A dataset for cross-lingual comparison of stereotypes in generative LLMs. In First Conference on Language Modeling. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. InFindings of the Association for Computational Linguistics: ACL 2022, pages 2086‚Äì2105, Dublin, Ireland. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,"
        },
        {
          "rank": 3,
          "distance": 0.7787996530532837,
          "paper_id": "2511.13180",
          "chunk_id": "2511.13180_chunk_11",
          "text": "languages according to whether their mutual TE is symmetric or asymmetric and to examine how many such clusters exist. The advanced performance of MarianMT ( ~75 million weights) compared with the NLLB-200 (Facebook) (~615 million weights) and T5-Base (Google) (‚àº 215 million weights) translators (Tables 8 and 3) indicates that larger models do not necessarily produce better translations . However, the NLLB-200 (Facebook) model is a multilingual translator, designed for a broader task than MarianMT. This observation raises the question s about the optimal balance between translation quality and the size and structure of deep translation al architectures. Currently, the examined translators operate in a regime where the number of model weights far exceeds the number of tokens, suggesting that smaller architectures may achieve comparable performance. Moreover, factors such as optimization of the training d ataset, sentence types, token frequency distributions, and tokenizer quality are expected to influence translation performance, warranting further research. Finally, the entropy per pivot token fluctuates, with a small fraction of tokens exhibiting high entropy while most remain low (Fig. 4). Developing a pretraining procedure to suppress the entropy of high-entropy pivot tokens represents an important direction for future research. Acknowledgements We thank Yarden Tzach for helpful comments and discussions. References [1] R. Mattessich, The oldest writings, and inventory tags of Egypt, Accounting Historians Journal, 29 (2002) 195-208. [2] J.A. Black, G. Z√≥lyomi, The study of diachronic and synchronic variation in Sumerian, Acta Sumerologica, 22 (1999). [3] J.-J. Glassner, The invention of cuneiform: Writing in Sumer, JhU Press, 2003. [4] J.H. Tigay, The evolution of the Gilgamesh epic, Bolchazy-Carducci Publishers, 2002. [5] G. Stemberger, Megillat Ta'anit: Versions, Interpretation, History: With a Critical Edition, in, JSTOR, 2006. [6] D.J. Shepherd, J. Joosten, M. Van der Meer, Septuagint, Targum and beyond: comparing Aramaic and Greek versions from Jewish Antiquity, Brill, 2019. [7] J. Dines, The Septuagint, (2004). [8] S.M. Wagner, TRANSLATION, MIDRASH, AND COMMENTARY THROUGH THE EYES OF ONKELOS, Jewish Bible Quarterly Dor le Dor, 38 (2010). [9] C. M√ºller-Kessler, The earliest evidence for Targum Onqelos from Babylonia and the question of its dialect and origin, Journal for the Aramaic bible, 3 (2001) 181-198. [10] R.G. Soto, Esperanto and its rivals: The struggle for an international language, University of Pennsylvania Press, 2015. [11] T. Mikolov, Efficient estimation of word representations in vector space, arXiv preprint arXiv:1301.3781, 3781 (2013). [12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, ≈Å. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information processing systems, 30 (2017). [13] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, A survey on evaluation of large language models, ACM transactions on intelligent systems and technology, 15 (2024) 1-45. [14] T. Brants, A. Popat, P. Xu, F.J. Och, J. Dean, Large language models in machine translation, in: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007, pp. 858-867. [15] S. Wang, Z. Tu, Z. Tan, W. Wang, M. Sun, Y. Liu,"
        }
      ]
    },
    {
      "query": "What is the role of tokenization in NLP?",
      "results": [
        {
          "rank": 1,
          "distance": 1.1189351081848145,
          "paper_id": "2511.13180",
          "chunk_id": "2511.13180_chunk_2",
          "text": "of sentences, each containing one of the selected token s, results in the probabilities to replace this token by other ones, while preserving the translation. These probabilities constitute the entropy of the selected token, and their average across all selected pivot tokens provides an estimate of the TE of the translator, which is enhanced along the decoder blocks . This entropic measure enables the quantitative rank ing of several publicly available translators and also reveals whether the mutual TE between two languages is symmetric. Finally, extending the proposed method to include the replacement of two tokens in each pivot sentence demonstrates a multiplicative phenomenon, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. 2. Results 2.1. Formulation of translation entropy exemplified using MarianMT translator The TE estimation method is first exemplified using the MarianMT (Helsinki- NLP/opus-mt) deep learning architecture, consisting of six encoder blocks followed by six decoder blocks[20]. The model was trained on the Opus100 dataset, which contains one million training sentences and 2,000 validation sentences[21, 22] , each provided in both source and target ( translated) versions[23]. Results are presented for translations from English (source) to French (target), where each language comprises approximately 30,000 tokens (Fig. 1a). Subsequent results are presented for the reverse translation (from French to English). In the first step of the algorithm , a token ùëá1 is selected. Next, a pivot English source sentence containing up to 128 tokens, ùëá1, ‚Ä¶ , ùëá128, is chosen such that ùëáùëó = ùëá1 at position ùëó (Fig. 1a , top ). The model then generates the corresponding French translation of this pivot sentence (Fig. 1a, top). In the second step, ùëá1 at position ùëó is sequentially replaced with all other possible source tokens , producing ~30,000 generated French sentences (Fig. 1a). Generated sentences that are identical to the pivot -generated translation are identified, as presented in Fig. 1a and schematically illustrated in Fig. 1b. This form of translation d egeneracy, where several source sentences differ from the pivot only by the selected token yet produce identical translations, is exemplified in Fig. 2. This conditional measure differs from cases where the selected pivot token can be replaced within a specific sentence while still yielding the same translation. Fig. 1. Schematic of translation degeneracy. (a) A pivot English source sentence, ùëÜùëíùëõùë°ùëíùëõùëêùëíùëÜ, consisting of up to 128 tokens, ùëá1, ‚Ä¶ , ùëá128, including ùëáùëó = ùëá1 at position ùëó, and its generated French sentence, ùëÜùëíùëõùë°ùëíùëõùëêùëíùê∫ (color-coded) (top). ùëá1 is sequentially replaced by ~30,000 other poss ible source tokens, resulting in corresponding French-generated sentences (color-coded). Identical generated sentences to the pivot -generated sentence are indicated. (b) Schematic representation of panel a. The dashed line represents a pivot-source sentence (red circle) connected to its generated sentence (blue circle). Other red circles represent sentences differing from the pivot by token ùëá1 at position ùëó and yielding identical generated sentences (blue circles). Note that ùê∑ùëéùë°ùëéùë†ùëíùë°ùëÜ contains all possible source sentences. Fig. 2. Examples of translation degeneracy. A selected token ‚Äúdaughter‚Äù (red) in an English source pivot sentence, its generated and"
        },
        {
          "rank": 2,
          "distance": 1.1224372386932373,
          "paper_id": "2511.13182",
          "chunk_id": "2511.13182_chunk_1",
          "text": "when applied with various prompt templates, effectively restore diacritics in Romanian texts? ‚Äì RQ2. How do the performances of these models compare against a neutral baseline, particularly in terms of Restoration Accuracy and Restoration Error Rates? ‚Äì RQ3. How do the results of LLMs vary in the correction of diacritics, and what factors, such as model architecture or language complexity, contribute to these differences? Through the lens of these questions, our study endeavors to shed light on the nuanced dynamics of diacritics restoration, thereby contributing to the ongoing discourse on enhancing NLP methodologies for linguistically diverse contexts. 2 Background and related work Diacritics, essential in conveying meanings and nuances, play a critical role in various languages, as highlighted in [9], where they are integral to the correct representation of phonetic and grammatical aspects. 2.1 Diacritics in Natural Language Processing The restoration of diacritics in text processing, a multifaceted problem as described by [7], involves understanding context, syntax, and semantics at multiple levels. For languages such as Romanian, which employs a range of diacritical marks, the absence of these marks can lead to substantial ambiguities. This restoration process is both a technical and linguistic challenge, requiring deep understanding of the language‚Äôs nuances. In the exploration of automatic diacritics restoration for Romanian, the study by Nut, u et al. [10] presents a comparative analysis of six neural network architectures that integrate both recurrent and convolutional layers, without relying on additional linguistic or semantic input. Their approach, focusing solely on character sequences, demonstrates the effectiveness of deep learning strategies in enhancing the accuracy of diacritics restoration. The best performing model in their experiments, a CNN-based architecture (seq2seq_CNN), achieved notable accuracy levels of 97% at the word level and 89% at the diacritic level. This work underscores the potential of employing advanced neural LLMs for Romanian Diacritic Restoration 3 network models to address the complexities associated with the restoration of diacritics in Romanian text, providing a significant contribution to the field and setting a foundation for future research endeavors that might explore the integration of Large Language Models (LLMs) for this purpose. Existing datasets and characteristicsFor the purpose of diacritic restoration in Roma- nian, one notable dataset is a subset of the CoRoLa text corpus, as utilized in Nutu et al.‚Äôs [10] study. This subset comprises 51,043 sentences, encompassing over 1 million tokens and 63,194 unique words, predominantly from belletristic texts. This dataset is of particular relevance due to its comprehensive coverage of contemporary Romanian language usage, providing a reliable source of correctly diacritized text, which is crucial for training effective models. The corpus is not specifically designed for Automatic Diacritics Restoration (ADR) tasks but is manually annotated at the word level with various linguistic information, making it a valuable resource for this domain. The preparation of this dataset for diacritic restoration involved several pre-processing steps, including conversion to lowercase, removal of digits and punctuation, and stripping of diacritics. Furthermore, the text was parsed into trigrams to create input-target sequence pairs, adding a unique challenge in terms of context representation and sequence prediction for the training"
        },
        {
          "rank": 3,
          "distance": 1.1358973979949951,
          "paper_id": "2511.12573",
          "chunk_id": "2511.12573_chunk_23",
          "text": "generate responses that preserve the core semantic content while varying the length bin. Specifically, we modify verbosity, style, or structure without introducing new substantive information. The following techniques enable controlled expansion or compression while maintaining meaning: ‚Ä¢ Filler Sentences: Inserting or removing sentences that serve to maintain flow or tone, without adding new information.e.g., ‚ÄúHere are the following reasons...‚Äù, ‚ÄúIt is important to note that...‚Äù ‚Ä¢ Pleonasm (Redundant Expression): Adding or deleting phrases that are semantically redundant within a sentence.e.g.,‚ÄúWe arecurrentlyin thepresentmoment.‚Äù ‚Ä¢ Redundant Sentences: Repeating or removing information already stated to artificially modify length.e.g.,‚ÄúShe was happy to see her dog. Meeting her dog was a pleasant moment.‚Äù ‚Ä¢ Paraphrasing / Summarization: Restructuring sentences to compress or expand content while preserving meaning. This includes active/passive transformations and sentence splitting or merging.e.g.,Merging two short sentences into one long sentence. ‚Ä¢ Format Changes: Altering the presentation style or tone, such as converting prose into bullet points or shifting from academic to conversational language.e.g.,academic to conversational tone ‚Ä¢ Combination: Using LLM-guided intuition to combine multiple augmentation strategies.e.g.,Filler + Paraphrasing, Redun- dancy + Format Change. Fixing length while changing content.To construct Tc‚Ä≤,‚Ñì‚Ä≤ , we generate semantically distinct responses that match the original length while modifying core content. The following techniques are used to vary meaning while keeping verbosity constant: ‚Ä¢Removing Necessary Details: Omitting specific or descriptive elements while preserving the overall message. Before:‚ÄúThe new smartphone model, which was released last month after a series of delays and extensive testing, features an improved camera and a faster processor.‚Äù After:‚ÄúThe latest smartphone, launched last month following thorough evaluation, includes an upgraded camera and a more powerful processor‚Äù ‚Ä¢Elaboration: Adding examples or rephrasing to emphasize key points, while keeping the length constant. Before:‚ÄúStudying improves cognitive skills.‚Äù After:‚ÄúReading books enhances memory and focus.‚Äù ‚Ä¢Information Substitution: Replacing facts or details with alternative but same-type information. Before:‚ÄúIn 2020, global temperatures reached record highs due to climate change.‚Äù After:‚ÄúIn 2020, air pollution levels surged in major cities due to industrial activities.‚Äù ‚Ä¢Converting Expression: Rewriting figurative expressions into factual statements or vice versa. Before:(Figurative): ‚ÄúThe negotiation was a game of chess, with each side making calculated moves.‚Äù After:(Factual): ‚ÄúThe negotiation involved careful strategy and analysis, with each decision impacting the outcome.‚Äù ‚Ä¢ Combination: Generating diverse variations by blending multiple strategies.e.g.,Elaboration + Information Substitution, or Removing Details + Converting Expression. Category Token Range Very Short (1, 41) Short (41, 98) Medium (98, 204) Long (204, 377) Very Long (377, 5220) Table 5: Length binning criteria for responses. G Length Binning Criteria As illustrated in Table 5, we discretize response lengths into five bins to define \"fixed length\" and \"different length\" conditions, which are essential for counterfactual data augmentation and bias diagnosis. This binning allows us to simulate length-based interventions while maintaining consistency in how we interpretlength equivalenceduring training and evaluation. The bins were selected based on quantiles from the empirical distribution of response lengths in the training corpus. Responses falling within the same bin are treated as approximately equal in length, while response pairs that fall into different bins are considered to have length divergence."
        }
      ]
    },
    {
      "query": "Explain zero-shot and few-shot learning approaches",
      "results": [
        {
          "rank": 1,
          "distance": 1.0977141857147217,
          "paper_id": "2511.12630",
          "chunk_id": "2511.12630_chunk_18",
          "text": "Knots, a comprehensive dataset of 12,347 expert- annotated NOTAMs from 194 Flight Information Re- gions, whose non-extractive, inferential annotation scheme provides a robust foundation for models with genuine semantic understanding. Second, we propose a novel multi-agent collaborative framework, MDA-HDF, which systematically discovers and refines new, opera- tionally critical information fields, achieving a 92% F1- score in automated field discovery that significantly out- performs traditional and single-LLM methods. Third, we establish a set of optimized prompting strategies for the parsing task, demonstrating that 5-shot In-Context Learning (ICL) is the most effective approach, yield- ing F1-scores up to 96%. Our analysis provides crit- ical guidelines for practical implementation, confirm- ing that domain-specific optimizations and determin- istic temperature settings are essential for achieving safety-critical reliability. Our work focuses on prompt engineering rather than fine-tuning. This approach establishes a strong baseline for the NOTAM parsing task by evaluating the \"out-of- the-box\" capabilities of LLMs, demonstrating their ro- bust zero-shot and few-shot learning abilities within the aviation domain. This highlights the complementary na- ture of our contributions: the Knots dataset provides a 15 solid foundation for future supervised fine-tuning, while our validated prompting strategies can further enhance both pre-trained and fine-tuned models. Limitations and Future Work:While this work pro- vides a foundational approach to the NOTAM parsing problem, several limitations suggest avenues for future investigation. A performance gap remains in fully re- solving the task, particularly for NOTAMs involving highly complex linguistic structures or deep inferen- tial reasoning. Additionally, the generalizability of our multi-agent framework may be constrained, as its archi- tecture and expert-crafted prompts were specifically tai- lored to the semantic nuances of NOTAMs. Future work can build upon this foundation in several promising directions. A primary avenue is to leverage the Knots dataset for supervised fine-tuning of Large Language Models (LLMs), which is expected to fur- ther enhance model performance and domain adapta- tion. Furthermore, expanding our methodology to other safety-critical domains could validate the generalizabil- ity of our enhanced dataset construction and prompt op- timization principles. Appendix A. Theoretical Proofs and Example Prompts This appendix provides formal mathematical proofs of the theoretical results presented in Section 5, fol- lowed by illustrative examples of the prompts used in our multi-agent framework. Appendix A.1. Proof of Lemma 1 (Conceptual Space Expansion in MDA) Proof.DefineI 0 ={t}as the initial information (raw NOTAM text). Each agent‚Äôs outputZk is conditioned on all preceding contextI k‚àí1 =I 0 ‚à™Z 1 ‚à™¬∑¬∑¬∑‚à™Z k‚àí1. BecauseI k ‚äáI k‚àí1, the reachable latent concept space satisfies: Œò(Ik)‚äáŒò(I k‚àí1) Agents with complementary knowledge properties sample new regions: Zk ‚àºP(Œ∏|œï k,I k‚àí1), Œ∏‚ààŒò(I k‚àí1) Aggregating outputs through the consensus mecha- nism yields: ZMDA =ConsensusAggregator(Z 1 ‚à™Z 2 ‚à™Z 3,œÑ) where the aggregator merges semantically similar fields. Since the aggregation preserves conceptual content while potentially reducing redundancy: Œò(ZMDA)‚äáŒò(Z 1)‚à™Œò(Z 2)‚à™Œò(Z 3) Using monotonicity of¬µ: ¬µ(Œò(ZMDA))‚â•max k‚àà{1,2,3} ¬µ(Œò(Zk)) This completes the proof. Appendix A.2. Proof of Theorem 1 (Robustness of the HDF) Proof.The proof consists of two parts: (a) Adversarial Critique Reduces Erroneous Pro- posal Acceptance By the Misconception Refutation principle [41], the introduction of a refuting critiquez"
        },
        {
          "rank": 2,
          "distance": 1.1126949787139893,
          "paper_id": "2511.13152",
          "chunk_id": "2511.13152_chunk_3",
          "text": "for short-answer scoring tasks (Chamieh et al., 2024). LLMs have also been applied to spo- ken grammar evaluation by generating test varia- tions robust to ASR noise (Kopparapu et al., 2024). 3 Proposed Method The proposed method estimates grammar compe- tency without labeled training data by adopting a zero-shot learning paradigm. Large language model (LLM) predictions serve as pseudo-labels to train a transformer-based model. Pseudo-labels are generated using an LLM prompted with a grammar competency rubric‚Äîa strategy shown to enhance zero-shot essay scoring and feedback (Evanini et al., 2013; Wang et al., 2023). To handle pseudo- label noise, we employ a robust framework in- spired by prior work on learning from noisy, trait- specific supervision (Zhang et al., 2021; Bengio et al., 2009). This approach generalizes to both written and spoken tasks, eliminating costly human annotations while outperforming strong LLM-only baselines in grammar scoring accuracy. 3.1 Pseudo-Label Generation with LLM The first step in our method is to generate pseudo- labels for the unlabeled dataset using a Large Lan- guage Model (LLM), fLLM(.). Given an unla- beled dataset Dunlabeled ={x i}N i=1, where xi repre- sents a sample (written or spoken response), we prompt the LLM with a grammar competency rubric-based prompt P to produce predictions. Mathematically, the pseudo-labels ypseudo i are de- fined as: ypseudo i =f LLM(xi, P) Here, P is carefully designed to align with the grammar competency scoring rubric, ensuring that the LLM predictions are meaningful approxima- tions of grammar scores. These predictions, while inherently noisy, serve as the foundation for train- ing the transformer model. 3.2 Training Methodology Our proposed training strategy focuses on deriving reliable grammatical proficiency estimates from imperfect, noisy data. We adopt a robust training framework for regression using deep neural net- works, designed to mitigate the effects of noisy or low-quality data through dynamic sample weight- ing (Zhang et al., 2021; Han et al., 2018b; Song et al., 2022). Our approach iteratively re-weights training examples per epoch based on their ob- served losses, promoting learning from \"clean\" samples while down-weighting potentially noisy outliers (Jiang et al., 2018; Kumar et al., 2010; Wu et al., 2020). Using the generated pseudo- labels, we construct a training dataset Dtrain = {(xi, ypseudo i )}N i=1. The pseudo-labels ypseudo i are treated as noisy labels, as they may not perfectly align with true grammar competency scores. This introduces a critical challenge in the training pro- cess, which our framework addresses by leveraging robust loss functions and regularization techniques to mitigate the impact of label noise (Zhang et al., 2021; Song et al., 2022). We begin by leveraging a pre-trained transformer encoder, such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), without architectural modification, and add a projection layer to map its contextual embeddings to scalar proficiency scores for the regression task. Specifically, we instanti- ate a regression model fŒ∏(¬∑), parameterized by Œ∏, wherein the transformer-based architecture serves as the feature extractor, and the projection layer outputs the estimated grammar competency score ÀÜyi for each inputx i: ÀÜyi =f Œ∏(xi)(1) Recognizing that not all"
        },
        {
          "rank": 3,
          "distance": 1.11751389503479,
          "paper_id": "2511.13118",
          "chunk_id": "2511.13118_chunk_10",
          "text": "schemas as executable classes, AEC en- ables systematic disambiguation, schema enforcement, and error correction. Experiments on five benchmarks and six LLM backbones show that AEC consistently outperforms strong zero-shot baselines, especially on complex schemas. Our results demonstrate that combining multi-agent rea- soning with schema-as-code verification provides a robust paradigm for zero-shot structured prediction with LLMs. References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report.arXiv preprint arXiv:2303.08774. Cai, Z.; Kung, P.; Suvarna, A.; Ma, M. D.; Bansal, H.; Chang, B.; Brantingham, P. J.; Wang, W.; and Peng, N. 2024. Improving Event Definition Following For Zero-Shot Event Detection. InProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2842‚Äì2863. Chen, R.; Qin, C.; Jiang, W.; and Choi, D. 2024. Is a large language model a good annotator for event extraction? InProceedings of the AAAI conference on artificial intelli- gence, volume 38, 17772‚Äì17780. Deng, S.; Zhang, N.; Kang, J.; Zhang, Y .; Zhang, W.; and Chen, H. 2020. Meta-learning with dynamic-memory-based prototypical network for few-shot event detection. InPro- ceedings of the 13th international conference on web search and data mining, 151‚Äì159. Doddington, G. R.; Mitchell, A.; Przybocki, M.; Ramshaw, L.; Strassel, S.; and Weischedel, R. 2004. The Automatic Content Extraction (ACE) Program‚ÄìTasks, Data, and Eval- uation. InProceedings of the Fourth International Confer- ence on Language Resources and Evaluation. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models.arXiv preprint arxiv:2407.21783. Gao, J.; Zhao, H.; Yu, C.; and Xu, R. 2023. Exploring the feasibility of chatgpt for event extraction.arXiv preprint arXiv:2303.03836. Guo, Q.; Dong, Y .; Tian, L.; Kang, Z.; Zhang, Y .; and Wang, S. 2025a. BANER: Boundary-aware LLMs for few-shot named entity recognition. InProceedings of the 31st Inter- national Conference on Computational Linguistics, 10375‚Äì 10389. Guo, Q.; Zhang, J.; Wang, S.; Tian, L.; Kang, Z.; Yan, B.; and Xiao, W. 2025b. Bridging Generative and Discrim- inative Learning: Few-Shot Relation Extraction via Two- Stage Knowledge-Guided Pre-training. InProceedings of the Thirty-Fourth International Joint Conference on Artifi- cial Intelligence, 8068‚Äì8076. Guo, Y .; Li, Z.; Jin, X.; Liu, Y .; Zeng, Y .; Liu, W.; Li, X.; Yang, P.; Bai, L.; Guo, J.; et al. 2024. Retrieval-Augmented Code Generation for Universal Information Extraction. In CCF International Conference on Natural Language Pro- cessing and Chinese Computing, 30‚Äì42. Hou, W.; Jia, N.; Liu, X.; Zhao, W.; and Wang, Z. 2024. A multiagent-based document-level relation extraction system with entity pair awareness and sentence significance.IEEE Systems Journal, 18(4): 1905‚Äì1916. Huang, K.-H.; Hsu, I.-H.; Parekh, T.; Xie, Z.; Zhang, Z.; Natarajan, P.; Chang, K.-W.; Peng, N.; and Ji, H. 2024. TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction. InProceedings of the 62nd Annual Meeting of the Association for Computational Lin- guistics, 12804‚Äì12825. Hui, B.; Yang, J.; Cui, Z.; Yang, J.; Liu, D.; Zhang, L.; Liu, T.; Zhang, J.; Yu, B.; Lu, K.; et al. 2024. Qwen2. 5-coder technical report.arXiv preprint"
        }
      ]
    },
    {
      "query": "What are the challenges in machine translation?",
      "results": [
        {
          "rank": 1,
          "distance": 0.8410212993621826,
          "paper_id": "2511.13180",
          "chunk_id": "2511.13180_chunk_11",
          "text": "languages according to whether their mutual TE is symmetric or asymmetric and to examine how many such clusters exist. The advanced performance of MarianMT ( ~75 million weights) compared with the NLLB-200 (Facebook) (~615 million weights) and T5-Base (Google) (‚àº 215 million weights) translators (Tables 8 and 3) indicates that larger models do not necessarily produce better translations . However, the NLLB-200 (Facebook) model is a multilingual translator, designed for a broader task than MarianMT. This observation raises the question s about the optimal balance between translation quality and the size and structure of deep translation al architectures. Currently, the examined translators operate in a regime where the number of model weights far exceeds the number of tokens, suggesting that smaller architectures may achieve comparable performance. Moreover, factors such as optimization of the training d ataset, sentence types, token frequency distributions, and tokenizer quality are expected to influence translation performance, warranting further research. Finally, the entropy per pivot token fluctuates, with a small fraction of tokens exhibiting high entropy while most remain low (Fig. 4). Developing a pretraining procedure to suppress the entropy of high-entropy pivot tokens represents an important direction for future research. Acknowledgements We thank Yarden Tzach for helpful comments and discussions. References [1] R. Mattessich, The oldest writings, and inventory tags of Egypt, Accounting Historians Journal, 29 (2002) 195-208. [2] J.A. Black, G. Z√≥lyomi, The study of diachronic and synchronic variation in Sumerian, Acta Sumerologica, 22 (1999). [3] J.-J. Glassner, The invention of cuneiform: Writing in Sumer, JhU Press, 2003. [4] J.H. Tigay, The evolution of the Gilgamesh epic, Bolchazy-Carducci Publishers, 2002. [5] G. Stemberger, Megillat Ta'anit: Versions, Interpretation, History: With a Critical Edition, in, JSTOR, 2006. [6] D.J. Shepherd, J. Joosten, M. Van der Meer, Septuagint, Targum and beyond: comparing Aramaic and Greek versions from Jewish Antiquity, Brill, 2019. [7] J. Dines, The Septuagint, (2004). [8] S.M. Wagner, TRANSLATION, MIDRASH, AND COMMENTARY THROUGH THE EYES OF ONKELOS, Jewish Bible Quarterly Dor le Dor, 38 (2010). [9] C. M√ºller-Kessler, The earliest evidence for Targum Onqelos from Babylonia and the question of its dialect and origin, Journal for the Aramaic bible, 3 (2001) 181-198. [10] R.G. Soto, Esperanto and its rivals: The struggle for an international language, University of Pennsylvania Press, 2015. [11] T. Mikolov, Efficient estimation of word representations in vector space, arXiv preprint arXiv:1301.3781, 3781 (2013). [12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, ≈Å. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information processing systems, 30 (2017). [13] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, A survey on evaluation of large language models, ACM transactions on intelligent systems and technology, 15 (2024) 1-45. [14] T. Brants, A. Popat, P. Xu, F.J. Och, J. Dean, Large language models in machine translation, in: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007, pp. 858-867. [15] S. Wang, Z. Tu, Z. Tan, W. Wang, M. Sun, Y. Liu,"
        },
        {
          "rank": 2,
          "distance": 0.8655884265899658,
          "paper_id": "2511.13467",
          "chunk_id": "2511.13467_chunk_19",
          "text": "between future automated document-level scoring systems and expert human judgment, ‚Ä¢Improved fairness and consistency across evaluation samples of different lengths, ‚Ä¢ More intelligent quality evaluation systems producing evaluations closer to human perception, ‚Ä¢Stronger foundations for evaluating both human and AI-generated translation. This model equips both translation professionals and tool developers with a framework that respects the cognitive and perceptual realities of language evaluation, enhancing decision-making at every level of the translation quality pipeline. The proposed non-linear model captures how translation quality is actually perceived by humans- especially when evaluation sample sizes vary. It is simple to implement, easy to calibrate, and theoretically grounded. By adopting non-linear scoring, evaluation systems can better align numeric scores with human judgment, increasing the reliability and fairness of translation quality evaluations, as well as increasing the reliability and fairness of quality evaluations of translations created by humans or AI-generated texts. 8 Limitations and Future Work The following limitations and research directions qualify the benefits summarized in ¬ß7.8 and the conclusion. While the findings and proposed non-linear quality evaluation model presented in this paper are supported by both empirical observation and theoretical grounding, there are important limitations that must be acknowledged. Addressing these will be essential for refining the model and validating its broader applicability. 8.1 Limited Survey Group Size We conducted structured interviews with quality managers at three very large enterprise clients, and the results are corroborated by their internal practices. Despite the scale and maturity of these organizations‚Äô multilingual translation programs, the interview sample is still small. Additional evidence was drawn from our own practice, where we exclusively use a non-linear calibrated scoring model due to its universality and accuracy. However, the tolerance data we used was gathered through expert elicitation and professional experience, rather than under blind experimental conditions‚Äîintroducing subjectivity and potential sources of bias, including response bias. 22 Further research should involve a broader range of organizations‚Äîincluding LSPs, in-house localization teams, and public-sector translation departments‚Äîacross different domains and lan- guage pairs. Ideally, future studies should also include controlled experiments where evaluators independently assess translation quality using both holistic judgment and formal scoring models, enabling direct comparison between model predictions and actual pass/fail outcomes. 8.2 Content-Type Specific Effects Different content types (e.g., marketing, legal, technical, UI strings) may have different quality expectations and error tolerance. Tolerance growth is assumed to follow the same logarithmic form E(x) = aln (1 + bx); the twocoefficients‚Äîa (overall scale) andb (curvature)‚Äîmay differ by client or use case. We found that a single logarithmic formula performs well across a wide range of content types, suggesting that the logarithmic rule is quite universal‚Äîeven with fixed parameters. However, this observation requires further validation through broader field data. A valuable extension of this work would be to investigate whether different genres (content types) exhibit distinct curve shapes‚Äîfor example, steeper or flatter logarithmic profiles‚Äîor whether certain domains deviate significantly from the general trend. Such an analysis would help determine whether the proposed model serves as a general framework, as our experience suggests, or represents just one of several viable scoring paradigms. We hypothesize a"
        },
        {
          "rank": 3,
          "distance": 0.8660494089126587,
          "paper_id": "2511.13467",
          "chunk_id": "2511.13467_chunk_18",
          "text": "types are directly comparable. For example, replicating the English‚ÄìGerman marketing case from Table 3 in [LGM+24], a 5,000-word translation with 23 minor errors passed the linear model, but was rejected by human reviewers. The logarithmic model set a threshold of 16 errors‚Äîcorrectly matching the expert decision. This normalization also improves the validity of quality measurement results analytics across a portfolio of projects, enabling better tracking of supplier performance, trend analysis, and root-cause investigations. 7.6 Applications Beyond Human Translation While the immediate application of this model is human translation quality evaluation, the underlying principles are equally relevant to assessing AI-generated content. Large language model (LLM) outputs‚Äîsuch as summaries, answers, or document translations‚Äîare increasingly being evaluated using scoring methods similar to those applied for human translation quality evaluation. The model based on non-linear error tolerance provides a fairer and more realistic baseline for evaluating content. It avoids the common pitfalls of unduly penalizing shorter segments with one mistake or overlooking the accumulated minor issues in long, highly fluent AI-generated texts. As organizations adopt more automated content generation, the ability to evaluate quality in a human-aligned, perceptually realistic manner becomes essential. The proposed scoring model provides a foundation for doing so. 7.7 Document-Level Evaluation and Future Automation Translation quality never exists in a vacuum: every segment lives within the cohesive fabric of a document. A translation sample is thereforenot a roll of toilet paper to be inspected centimeter by centimeter; its quality must be judged in the context of the larger text. The larger the sample relative to the document, the more faithfully that judgment reflects holistic properties such as cohesion, coherence, register, and terminological consistency. Small extracts can still provide a rapid signal, but offer only alimitedview of document quality. They cannot capture discourse-level phenomena, and‚Äîbecause error density interacts non-linearly with length‚Äîlinear metrics break down outside their narrow reference window. TheMQM-based Non-Linear Calibratedmodel solves this by remaining simple and linear in the meso-range yet extending smoothly into the macro-range. As long as the evaluation sample is suffi- ciently large relative to the document, as recommended by the Multi-Range Framework[LGM+24]‚Äîit yields scores that are statistically reliable and consistent with the judgment of experts at the docu- ment level. 21 Of course, analytic TQE becomes increasingly expensive at large evaluation sample sizes. No current AI system can yet perform a complete analytic evaluation, but future models may acquire the ability to conductformal TQE(full analytic MQM evaluation) overlong contexts(document-scale samples), reliably capturinglocal errors(segment-level) andholistic properties(cohesion, coherence, terminology/style consistency, discourse). The logarithmic model proposed here is expressly designed to serve as the computational backbone for such future automation, ensuring that automated scoring remains perceptually valid across the full spectrum of document lengths. 7.8 Summary In practical terms, the transition to a non-linear scoring model means: ‚Ä¢ Greater alignment between future automated document-level scoring systems and expert human judgment, ‚Ä¢Improved fairness and consistency across evaluation samples of different lengths, ‚Ä¢ More intelligent quality evaluation systems producing evaluations closer to human perception, ‚Ä¢Stronger foundations for evaluating both human and AI-generated translation. This model equips both translation professionals and tool developers"
        }
      ]
    }
  ]
}