The following are the 5 questions asked and the top-3 retrieved passages for each.


Query 1: How to do end to end speech translation?

Answer: 

End-to-End Speech Translation with Knowledge Distillation Yuchen Liu1,2, Hao Xiong4, Zhongjun He4, Jiajun Zhang1,2, Hua Wu4, Haifeng Wang4 and Chengqing Zong1,2,3 1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China 2University of Chinese Academy of Sciences, China 3CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China 4Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China {yuchen.liu, jjzhang, cqzong}@nlpr.ia.ac.cn, {xionghao05, hezhongjun, wu hua, wanghaifeng}@baidu.com Abstract End-to-end speech translation (ST), which directly translates from source language speech into target language text, has at- tracted intensive attentions in recent years. Compared to con- ventional pipepine systems, end-to-end ST models have ad- vantages of lower latency, smaller model size and less error propagation. However, the combination of speech recognition and text translation in one model is more difﬁcult than each of these two tasks. In this paper, we propose a knowledge distilla- tion approach to improve ST model by transferring the knowl- edge from text translation model. Speciﬁcally, we ﬁrst train a text translation model, regarded as a teacher model, and then ST model is trained to learn output probabilities from teacher model through knowledge distillation. Experiments on English- French Augmented LibriSpeech and English-Chinese TED cor- pus show that end-to-end ST is possible to implement on both similar and dissimilar language pairs. In addition, with the in- struction of teacher model, end-to-end ST model can gain sig- niﬁcant improvements by over 3.5 BLEU points. Index Terms: Speech recognition, Speech translation, Knowl- edge distillation, Transformer 1. Introduction Conventional speech translation system is a pipeline of two main components: an automatic speech recognition (ASR) model which provides transcripts of source language utterances, and a text machine translation (MT) model which translates the transcripts to target language [1, 2, 3, 4, 5]. This pipeline system usually suffers from time delay, parameter redundancy and error accumulation. In contrast, end-to-end ST, based on an encoder- decoder architecture with attention mechanism, is more com- pact and efﬁcient. It can directly generate translations from raw audio and jointly optimize parameters on the ﬁnal goal. There- fore, this model has become a new trend in speech translation research studies [6, 7, 8, 9, 10, 11]. However, despite appealing advantages of end-to-end ST model, its performance is generally inferior. One of the im- portant reasons is due to extremely scarce data which includes speech in source language paired with text in target language. Previous studies resort pretraining or multi-task learning ap- proaches to improve the translation quality. They either pretrain ASR task on high-resource data [11], or use multi-task learn- ing to train ST model with ASR or MT model simultaneously [9, 10]. Nevertheless, they only gain limited improvements and do not take full advantage of text data. We notice that the per- formance between end-to-end ST and MT model exists a huge gap, thus how to utilize MT model to help instruct end-to-end ST model is of great signiﬁcance. It is a challenge to train an end-to-end ST model directly from speech signal without text guidance while achieving com- parable performance as text translation model. Given that text translation

END-TO-END SPEAKER DIARIZATION AS POST-PROCESSING Shota Horiguchi1 Paola Garc´ıa2 Yusuke Fujita1 Shinji Watanabe2 Kenji Nagamatsu1 1 Hitachi, Ltd. Research & Development Group, Japan 2 Center for Language and Speech Processing, Johns Hopkins University, USA ABSTRACT This paper investigates the utilization of an end-to-end diarization model as post-processing of conventional clustering-based diariza- tion. Clustering-based diarization methods partition frames into clusters of the number of speakers; thus, they typically cannot handle overlapping speech because each frame is assigned to one speaker. On the other hand, some end-to-end diarization methods can handle overlapping speech by treating the problem as multi-label classiﬁcation. Although some methods can treat a ﬂexible number of speakers, they do not perform well when the number of speakers is large. To compensate for each other’s weakness, we propose to use a two-speaker end-to-end diarization method as post-processing of the results obtained by a clustering-based method. We iteratively select two speakers from the results and update the results of the two speakers to improve the overlapped region. Experimental results show that the proposed algorithm consistently improved the perfor- mance of the state-of-the-art methods across CALLHOME, AMI, and DIHARD II datasets. Index Terms— Speaker diarization, EEND 1. INTRODUCTION Speaker diarization, which is sometimes referred to as “who spoke when”, has important roles in many speech-related applications. It is sometimes used to enrich transcriptions by adding speaker attributes [1], and at the other times, it is used to improve the performance of speech separation and recognition [2, 3]. Speaker diarization methods can be classiﬁed roughly into two: clustering-based methods and end-to-end methods. Typical clustering-based methods i) ﬁrst classify frames into speech and non-speech, ii) then extract an embedding which describes speaker characteristics from each speech frame, and iii) ﬁnally apply clus- tering to the extracted embeddings. Most methods employ hard clustering such as agglomerative hierarchical clustering (AHC) and k-means clustering; as a result, each frame belongs either to one of the speaker clusters or to the non-speech cluster. The assumption that underlies these clustering-based methods is that each frame contains at most one speaker, i.e., they treat speaker diarization as a set partitioning problem. Thus, they fundamentally cannot deal with overlapping speech. Despite the assumption, they are still strong baselines over end-to-end methods on datasets of a large number of speakers, e.g., DIHARD II dataset [4]. This is because they handle multiple speaker problems based on unsupervised clustering without using any speech mixtures as training data. Thus, the methods do not suffer from overtraining due to the lack of the overlap speech especially for a large number of speakers. On the other hand, some end-to-end methods called EEND treat speaker diarization as a multi-label classiﬁcation problem. They pre- dict whether each speaker is active or not at each frame; thus, they can deal with speaker overlap. Evaluation of the early models ﬁxed the number of speakers to two [5, 6, 7]. Some extensions are pro- posed recently to handle unknown number of speaker cases, e.g., encoder-decoder-based attractor calculation [8] and one-by-one pre- diction using speaker-conditioned model [9]. However, these meth- ods still perform poorly when

exists a huge gap, thus how to utilize MT model to help instruct end-to-end ST model is of great signiﬁcance. It is a challenge to train an end-to-end ST model directly from speech signal without text guidance while achieving com- parable performance as text translation model. Given that text translation models are superior to ST model, we consider ST model can be improved by leveraging knowledge distillation. In knowledge distillation, there is usually a big teacher model with a small student model. It has been shown that the output probabilities of teacher model are smooth, which are easier for student model to learn from than ground-truth text [12]. Thus, a student model can be taught by imitating the behaviour of teacher model, such as output probabilities [12, 13], hidden rep- resentation [14, 15], or generated sequence [16], and alleviate the performance gap between itself and the teacher model. In this paper, we present a method based on knowledge dis- tillation for end-to-end ST model to learn knowledge from text translation model. We ﬁrst train a text translation model on par- allel text data (regarded as teacher) and then an end-to-end ST model (regarded as student) is trained by learning from ground- truth translations and the outputs of teacher model simultane- ously. Experiments conducted on 100h English-French Aug- mented LibriSpeech corpus and 542h English-Chinese TED corpus show that it is possible to train a compact end-to-end speech translation model on both similar and dissimilar lan- guage pairs. With the instruction of teacher model, end-to-end ST model can gain signiﬁcant improvements, approaching to the traditional pipeline system. 2. Related Work End-to-end model has already become a dominant paradigm in machine translation task, which adopts an encoder-decoder ar- chitecture and generates target words from left to right at each step [1, 3, 5]. This model has also achieved promising results in ASR ﬁelds [2, 4, 17]. Recent works purpose a further attempt to combine these two tasks together by building an end-to-end speech-to-text translation without the use of source language translation during learning or decoding. Anastasopoulos et al. [6] use k-means clustering to cluster repeated audio patterns and automatically align spoken words with their translations. Duong et al. [7] focus on the alignment between speech and translated phrase but not to directly pre- dict the ﬁnal translations. B´erard et al. [8] give the ﬁrst proof of the potential for end-to-end speech-to-text translation with- out using source language. They further conduct experimetns on a larger English-to-French dataset and pre-train encoder and arXiv:1904.08075v1 [cs.CL] 17 Apr 2019 decoder which improves performance [10]. Weiss et al. [9] also use multi-task learning and show that end-to-end model can outperform a cascade of independently trained pipeline system on Fisher Callhome Spanish-English speech translation task. Bansal et al. [11] ﬁnd pretraining encoder on higher-resource language ASR training data can achieve gains in low-resource speech translation system. However, these work mainly focus on pretraining acoustic encoder and do not take full advantage of text data. Knowledge distillation is ﬁrst adopted to apply for model compression, the main idea of which is to


Query 2: What is Semantic Matching of Documents?

Answer: 

arXiv:1904.12550v1 [cs.CL] 29 Apr 2019 Semantic Matching of Documents from Heterogeneous Collections: A Simple and Transparent Method for Practical Applications Mark-Christoph M¨uller Heidelberg Institute for Theoretical Studies gGmbH Heidelberg, Germany mark-christoph.mueller@h-its.org Abstract We present a very simple, unsupervised method for the pairwise matching of documents from het- erogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classiﬁcation task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-speciﬁc modiﬁcations, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efﬁcient, because it is based on the aggregation of (pre-computable) word-level similarities. 1 Introduction We present a simple and efﬁcient unsupervised method for pairwise matching of documents from het- erogeneous collections. Following Gong et al. (2018), we consider two document collections heteroge- neous if their documents differ systematically with respect to vocabulary and / or level of abstraction. With these deﬁning differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous. Examples include collections in which expert an- swers are mapped to non-expert questions (e.g. InsuranceQA by Feng et al. (2015)), but also so-called community QA collections (Blooma and Kurian (2011)), where the lexical mismatch between Q and A documents is often less pronounced than the length difference. Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric. However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair simi- larity on the basis of selected pairwise word similarities. This has the following advantages, which make our method a viable candidate for practical, real-world applications: efﬁciency, because pairwise word similarities can be efﬁciently (pre-)computed and cached, and transparency, because the selected words from each document are available as evidence for what the similarity computation was based on. We demonstrate our method with the Concept-Project matching task (Gong et al. (2018)), which is de- scribed in the next section. 2 Task, Data Set, and Original Approach The Concept-Project matching task is a binary classiﬁcation task where each instance is a pair of het- erogeneous documents: one concept, which is a short science curriculum item from NGSS1, and one project, which is a much longer science project description for school children from ScienceBuddies2. 1https://www.nextgenscience.org 2https://www.sciencebuddies.org CONCEPT LABEL: ecosystems: - ls2.a: interdependent relationships in ecosystems CONCEPT DESCRIPTION: Ecosystems have carrying capacities , which are limits to the numbers of organisms and populations they can support . These limits result from such factors as the availability of living and nonliving resources and from such challenges such as predation , competition , and disease . Organisms would have the capacity to produce populations of great size were it not for the fact that environments and resources are ﬁnite . This fundamental tension affects the abundance ( number of individuals ) of species in any

and J. Jiang (2017). A compare-aggregate model for matching text sequences. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.

Conference on Computational Seman- tics (IWCS 2017) – Short Papers, pages 1–6, Mont- pellier, France. James F. Allen, Mary Swift, and Will de Beaumont. 2008. Deep Semantic Analysis of Text. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceed- ings, volume 1 of Research in Computational Se- mantics, pages 343–354. College Publications. Isotani S. Andrade F.R.H., Mizoguchi R. 2016. The bright and dark sides of gamiﬁcation. Intelligent Tutoring Systems. ITS 2016. Lecture Notes in Com- puter Science, 9684. Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In 36th An- nual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics. Proceedings of the Con- ference, pages 86–90, Universit´e de Montr´eal, Mon- treal, Quebec, Canada. David Bamman and Noah A. Smith. 2014. Unsuper- vised discovery of biographical structure from text. Transactions of the Association for Computational Linguistics, 2:363–376. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Grifﬁtt, Ulf Her- mjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186, Soﬁa, Bulgaria. Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012a. A platform for collaborative se- mantic annotation. In Proceedings of the Demon- strations at the 13th Conference of the European Chapter of the Association for Computational Lin- guistics (EACL), pages 92–96, Avignon, France. Valerio Basile, Johan Bos, Kilian Evang, and Noortje Joost Venhuizen. 2012b. Developing a large semantically annotated corpus. In Proceed- ings of the Eighth International Conference on Lan- guage Resources and Evaluation (LREC’12), Istan- bul, Turkey. European Language Resources Associ- ation (ELRA). Emily M. Bender. 2013. Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax. Synthesis Lectures on Human Language Technolo- gies. Morgan & Claypool Publishers. Emily M. Bender, Scott Drellishak, Antske Fokkens, Laurie Poulson, and Saﬁyyah Saleem. 2010. Gram- mar customization. Research on Language & Com- putation, 8(1):23–72. Emily M. Bender, Dan Flickinger, Stephan Oepen, Woodley Packard, and Ann Copestake. 2015. Layers of interpretation: On grammar and compositionality. In Proceedings of the 11th International Conference on Computational Semantics, pages 239–249, London, UK. Association for Computational Linguistics. Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–604. Chris Biemann, Kalina Bontcheva, Richard Eckart de Castilho, Iryna Gurevych, and Seid Muhie Yimam. 2017. Collaborative web-based tools for multi-layer text annotation. In Nancy Ide and James Pustejovsky, editors, The Handbook of Linguistic Annotation, Text, Speech, and Technology book series, pages 229–256. Springer Netherlands. P. Blackburn and J. Bos. 2005. Representation and Inference for Natural Language. A First Course in Computational Semantics. CSLI. Francis Bond and Ryan Foster. 2013. Linking and ex- tending an open multilingual Wordnet. In Proceed- ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1352–1362, Soﬁa, Bulgaria. Associa- tion for Computational Linguistics. Geert Booij. 1986. Form and meaning in morphology;


Query 3: What are the top works on SQuAD leaderboards?

Answer: 

with evenly spaced scores from 0 to nwords −1. 5.2 Results Figure 3 displays the results on the SQuAD 2.0 dataset in visual form. Complete Results in tabu- lar form can be found in Appdendix C. We com- pare the interpretation approaches with different aggregation methods (sum and max, as stated in Section 3.1.3). First, we observe that model access can greatly improve the explanation accuracy, reﬂected by the consistently higher scores of Saliency and Smooth- grad. Further, the performance of Black-Box mod- els LIME and SHAP increases signiﬁcantly with the number of samples used. Evaluation with the default number of samples was not possible due to computational limitations: Evaluation with a neigh- borhood of 100 samples took around 10h for LIME on only SQuAD 2.0 for 1 classiﬁer5, using the de- fault value of 5000 samples seems infeasible with reasonable computational resources, as we expect it to take close to 200 days only for LIME on a single GPU instance. We further note, surprisingly, that SmoothGrad does not seem to bring much improve- ment over standard Saliency using gradients. We attribute this to the fact that noise reduction is al- ready occurring by averaging the scores over token dimensions and over the sentences. Finally, note that both LIME and SHAP use LASSO regression as a surrogate, which results in the attribution of a score of 0 to unimportant sentences, thus reducing the noise, which increases the SNR. 5Experiments were carried out on an AWS SageMaker instance with 1/2 Nvidia Tesla K80 6 Related Work and Discussion Most work in evaluating rationales has been done by comparing rationales to human judge- ments (Doshi-Velez and Kim, 2017; Strout et al., 2019) or by letting humans evaluate the expla- nations (Ribeiro et al., 2016; Lundberg and Lee, 2017). While this approach measures the plausi- bility of interpretations, it does not guarantee that explanations actually reﬂect model behaviour. Ja- covi and Goldberg (2020) performs an in-depth analysis on the state of current NLP interpretability evaluation approaches and desired future proper- ties, notably the need to evaluate faithfulness, as plausible but unfaithful explanations are arguably the worst-case scenario. However, it would seem that all evaluation methodology based on human- annotated ground truth labels is biased towards plausibility by construction. DeYoung et al. (2019) address this problem by using the additional met- rics comprehensiveness and sufﬁciency, which mea- sure change in model prediction when removing the rationale and only keeping the rationale. Our approach also aims to address the bias towards plau- sibility, but instead of addressing the problem with metrics, we build a classiﬁcation task for which interpretability ground truth labels arise directly without human annotation. To our knowledge, it is the ﬁrst approach to propose unbiased ground truths for rationales. The take-away from our benchmark results is two-fold. First, for practitioners, with the current state of the art we see that model access should be used if possible, as it is able to signiﬁcantly increase the ﬁdelity of explanations. However this access is not always possible, in particular with complex text processing pipelines. Thus, for

Make 3 peasants. 47 Idle peasant to mine. 85 Build 2 more archers. 46 Make a workshop. 83 Send idle peasant back to mine. 46 Create a workshop. 81 Make more peasants. 46 Mine with all peasants. 80 Make 2 more peasants. 46 Build 3 more peasants. 79 Build blacksmith. 46 Create another peasant. 79 Collect minerals. 45 Send all idle peasants to mine. 77 Kill. 45 Build 3 archers. 77 Build an archer. 45 Kill peasant. 77 Keep mining. 45 Make another dragon. 76 Keep attacking. 43 Kill him. 72 Attack dragons with archers. 43 Build guard tower. 70 Create a stable. 42 Attack town hall. 70 Make 3 more peasants. 42 Start mining. 69 Attack the peasant. 41 Table 10: The top 100 instructions sorted by their usage frequency. 18 IDLE ~ action type global summary softmax over action types CONTINUE ~ action type global summary softmax over action types ... GATHER ~ ~ softmax over action output action output dot-product attention unit/instruction features action type input features (resource units) resource_unit_id global summary softmax over action types ... ATTACK ~ ~ softmax over action output action output dot-product attention unit/instruction features action type input features (enemy units) enemy_unit_id global summary softmax over action types TRAIN_UNIT ~ dot-product attention unit/instruction features action type input features (unit types) ~ softmax over action output action output unit type ... global summary softmax over action types ... BUILD_BUILDING ~ ~ softmax over action output action output dot-product attention unit/instruction features action type input features (map cells) input features (unit types) x,y ~ softmax over action output action output unit type ... global summary softmax over action types ... MOVE ~ ~ softmax over action output action output dot-product attention unit/instruction features action type input features (map cells) x,y global summary softmax over action types Figure 7: Separate classiﬁers for each of the available action types. 19

also get comfortable with the game ﬂow. We let the novice player play several games until we verify that they pass the required quality bar. We assess the performance of the player by running a set of pattern-matching scripts that verify if the performed control actions correspond to the issued instructions (for example, if an instruction says "build a barrack", we make sure that the player executes the corresponding low-level action). If the human player doesn’t pass our qualiﬁcation requirements within 5 games, we prevent them from participating in our data collection going forward and ﬁlter their games from the dataset. Player proﬁle We track performance of each player, breaking it down by a particular role (e.g. instructor or executor). We gather various statistics about each player and build a comprehensive player proﬁle. For example, for the instructor role we gather data such as overall win rate, the number of instructions issued per game, diversity of issued instructions; for the executor role we monitor 13 Strategy Name Description SIMPLE This strategy ﬁrst sends all 3 initially available PEASANTs to mine to the closest resource, then it chooses one army unit type from SPEARMAN, SWORDMAN, CAVALRY, ARCHER, or DRAGON, then it constructs a corre- sponding building, and ﬁnally trains 3 units of the selected type and sends them to attack. The strategy then continuously maintains the army size of 3, in case an army unit dies. MEDIUM Same as SIMPLE strategy, only the size of the army is randomly selected between 3 and 7. STRONG This strategy is adaptive, and it reacts to the opponent’s army. In particular, this strategy constantly scouts the map using one PEASANT and to lean the opponent’s behaviour. Once it sees the opponent’s army it immediately trains a counter army based on the attack graph (see Fig. 4). Then it clones the MEDIUM strategy. SECOND BASE This strategy aims to build a second TOWN HALL near the second closest resource and then it uses the double income to build a large army of a particular unit type. The other behaviours is the same as in the MEDIUM strategy. TOWER RUSH A non-standard strategy, that ﬁrst scouts the map in order to ﬁnd the opponent using a spare PEASANT. Once it ﬁnds it, it starts building GUARD TOWERs close to the opponent’s TOWN HALL so they can attack the opponent’s units. PEASANT RUSH This strategy sends ﬁrst 3 PEASANTs to mine, then it keeps producing more PEASANTs and sending them to attack the opponent. The hope of this strategy is to beat the opponent by surprise. Table 7: The rule-based strategies we use as an opponent to the human players during data collection. how well they perform on the issued instruction (using a pattern matching algorithm), the number of warnings they receive from the instructor, and many more. We then use this proﬁle to decide whether to upgrade a particular player to playing against stronger opponents (see Appendix C.2) in case they are performing well, or prevent them from participating in our data collection at all otherwise. Feedback We


Query 4: Can you summarize what BERT-BiDAF hybrid architecture is?

Answer: 

character level representations of question and context are Rcq ∈RLq×dchar_emb and Rcc ∈RLc×dchar_emb respectively. We apply the "Attention Flow Layer" of BiDAF[8] to obtain TChar ∈R8dchar_emb×Lc as context representation conditioned on question at character level. We simply keep the context part from the ﬁnal encoder layer of BERTBASE to obtain TW ord ∈ Rdbert×Lc as context representation conditioned on question at word level. We concatenate the two representations together to produce the joint contextual representation Gctx = [TW ord, TChar] ∈ R(8dchar_emb+dbert)×Lc. Figure 1: The Joint BERT-BiDAF Question Answering Model 3 3.1.3 Modeling Layer We keep the "BiDAF Modeling Layer"[8] for two reasons. (i). Although BERT word TW ord can be treat it as positional aware representation but the character feature TChar can not. We expect TW ord propagates the positional information to TChar in the mixture process in this layer. (ii). We want the ﬁne granularity information captured at character level propagates to TW ord. Actually we expect the two level representations aware the mutual existence and learn how to cooperate to produce the augmented feature. The output of the Modeling Layer is F ∈R(2dlstm+8dchar_emb+dbert)×Lc 3.1.4 Joint Probability Predictor Layer There are 3 components in our joint probability predictor layer which are responsible to compute the P(A), P(X1|A) (start position) and P(X2|A, X1) (end position) respectively. We propose this structure based on the observation that common practices highly rely on the special "sentry" token and actually make a solid assumption that P(X1 = 0, X1 = 0|A = 0). We argue that assumption like this introduces a tough constraint and the effort the model spends on push the (X1, x2) to (0, 0) will deﬁnitely impact the model’s internal mechanism to generate X1, X2 condition on A = 1. Our method gives up the special "sentry" token, we choose the natural domain of X1, X2 to be {0, 1, · · · , Lc −1}2, and relax the P(X1 = 0, X2 = 0|A = 0) constraint to P(X1 = i, X2 = j|A = 0), ∀i > j, that is, the predicted start position should be larger than the predicted end position condition on no-answer. The fact P(X1 = i, X2 = j|A = 1), ∀i ≤j becomes a natural extension of our concept. Thus we claim that our method does not introduce any inherent contradiction and any bias assumption. The connection of the 3 predictors are consistent with the chain rule, the P(X1|A) predictor relies on the P(A) predictor, while the P(X2|A, X1) predictor relies on the other two both. Thus the structure is expected to simulate the joint probability P(A, X1, X2) = P(A)P(X1|A)P(X2|A, X1). All the 3 predictors take the output F ∈R(2dlstm+8dchar_emb+dbert)×Lc of the "Modeling Layer" as input. For simplicity, we deﬁne f = 2dlstm + 8dchar_emb + dbert. I. P(A) Predictor In order to compute the binary classiﬁcation logits, we use a self attention mechanism by setting a learn-able parameter wA ∈Rf, the attention vector attA ∈RLc is computed by Eq.[1] attA[i] = exp(wT AF[:, i]) PLc−1 k=0 exp(wT AF[:, k]) , i ∈{0, 1,

loss function which value indeed reﬂect the model’s performance while consistent with the model’s design philosophy. We shall discuss our loss function exploration in details in section.3.2. What should be emphasized is that we don’t want to simply reproduce BERT’s success in question answer problem on SQuAD 2.0 [2]. The main difference between our solution and BERT’s QA solution 1 is BERT use representations of both question and context to vote for the span prediction while we only use that of the context. What we really want to do is to verify our original ideas, to test how well BERT can let the context "aware" the existence of question and to see whether the two embeddings at different granularity can achieve mutual enhancement. This section is scheduled in the following way: in section3.1, we discuss about our original model architecture in general and our effort to implement this architecture. We go through the essential layers and their back-end techniques but leave the BiDAF[8] details for reference reading. Especially in section3.1.4, we shall focus on our original joint probability predictor and how we tackle the answerable problem and the span prediction problem. In section3.2, we shall show several our original loss functions and their considerations. In section3.3, we introduce the baseline we use and in section3.4, we brieﬂy summarize the workload of our project. Please keep in mind that we deﬁne our terminology incrementally, once a symbol has been deﬁned, it will be used throughout the rest of paper. 1https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_ squad.py 2 3.1 Model Architecture As illustrated in Fig. 1, we keep the main framework of BiDAF [8] but replace its GloVe [4] word embedding with BERT contextual word embedding [2] and stack joint probability predictor on top of it. There are 3 hyper-parameters that determine the computational complexity of our system and we formally deﬁne them to be dlstm, dchar_emb and dbert. dlstm is a uniform hidden dimension for all LSTM components [3]. dchar_emb and dbert are the dimension of the character word embedding (output of character level CNN [10]) and the hidden dimension of the BERTBASE[2], respectively. 3.1.1 Embedding Layer Our embedding layer is built based on BERT base uncased tokenizer. (i) Word Level Embedding: More precisely, this is token level embedding. Yet BERT doesn’t split common words thus it is still can be treated as word representation. (ii) Character Level Embedding: BERT split special words such as name entity into pieces started with "##", e.g "Giselle" to [’gi’, ’##selle’]. However, each piece is still a character string from which a dchar_emb dimensional embedding can be generated with character CNN [10]. Note that "##" in word pieces is ignored. 3.1.2 Joint Contextual Representation Layer In this layer, we denote the lengths of question and context as Lq and Lc respectively, the character level representations of question and context are Rcq ∈RLq×dchar_emb and Rcc ∈RLc×dchar_emb respectively. We apply the "Attention Flow Layer" of BiDAF[8] to obtain TChar ∈R8dchar_emb×Lc as context representation conditioned on question at character level. We simply keep the context part from the ﬁnal encoder layer of BERTBASE to obtain

Appx. A) Ground Truth: El Temür (should be "No Answer") Prediction: El Temür 8 6 Conclusion In this paper, we propose our BERT-BiDAF hybrid architecture, thoroughly explain the intuition, and show the theoretical derivation of our joint probability predictor and its loss functions. Furthermore, we demonstrate that our ideas can obtain decent performance on a difﬁcult and highly reputable problem (SQuAD 2.0) and verify our arguments by ablation experiments. We learn much about model design and how to conduct a effective learning. We identify that the overﬁtting and high variance problems are the two main difﬁculties which prohibit us form further progress. Our next step is to dig deeply into the difﬁculties we are facing and may take question representation into consideration. Acknowledgments Thanks to all the CS224 teaching staff for guidance and Microsoft Azure for their great generosity. References [1] Yiming Cui et al. “Attention-over-attention neural networks for reading comprehension”. In: arXiv preprint arXiv:1607.04423 (2016). [2] Jacob Devlin et al. “Bert: Pre-training of deep bidirectional transformers for language under- standing”. In: arXiv preprint arXiv:1810.04805 (2018). [3] Sepp Hochreiter and Jürgen Schmidhuber. “LSTM can solve hard long time lag problems”. In: Advances in neural information processing systems. 1997, pp. 473–479. [4] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. “GloVe: Global Vectors for Word Representation”. In: Empirical Methods in Natural Language Processing (EMNLP). 2014, pp. 1532–1543. URL: http://www.aclweb.org/anthology/D14-1162. [5] Matthew E Peters et al. “Deep contextualized word representations”. In: arXiv preprint arXiv:1802.05365 (2018). [6] Pranav Rajpurkar, Robin Jia, and Percy Liang. “Know What You Don’t Know: Unanswerable Questions for SQuAD”. In: arXiv preprint arXiv:1806.03822 (2018). [7] Pranav Rajpurkar et al. “Squad: 100,000+ questions for machine comprehension of text”. In: arXiv preprint arXiv:1606.05250 (2016). [8] Minjoon Seo et al. “Bidirectional attention ﬂow for machine comprehension”. In: arXiv preprint arXiv:1611.01603 (2016). [9] Adams Wei Yu et al. “Qanet: Combining local convolution with global self-attention for reading comprehension”. In: arXiv preprint arXiv:1804.09541 (2018). [10] Xiang Zhang, Junbo Zhao, and Yann LeCun. “Character-level convolutional networks for text classiﬁcation”. In: (2015), pp. 649–657. A Appendix: Full Paragraphs in Analysis 1. Harvard University: Other: Civil rights leader W. E. B. Du Bois; philosopher Henry David Thoreau; authors Ralph Waldo Emerson and William S. Burroughs; educators Werner Baer, Harlan Hanson; poets Wallace Stevens, T. S. Eliot and E. E. Cummings; conductor Leonard Bernstein; cellist Yo Yo Ma; pianist and composer Charlie Albright; composer John Alden Carpenter; comedian, television show host and writer Conan O’Brien; actors Tatyana Ali, Nestor Carbonell, Matt Damon, Fred Gwynne, Hill Harper, Rashida Jones, Tommy Lee Jones, Ashley Judd, Jack Lemmon, Natalie Portman, Mira Sorvino, Elisabeth Shue, and Scottie Thompson; ﬁlm directors Darren Aronofsky, Terrence Malick, Mira Nair, and Whit Stillman; architect Philip Johnson; musicians Rivers Cuomo, Tom Morello, and Gram Parsons; musician, producer and composer Ryan Leslie; serial killer Ted Kaczynski; programmer and activist Richard Stallman; NFL quarterback Ryan Fitzpatrick; NFL center Matt Birk; NBA player Jeremy Lin; US Ski Team skier Ryan Max Riley; physician Sachin H. Jain; physicist J. Robert Oppenheimer; computer pioneer and inventor An Wang; Tibetologist George de Roerich; and Marshall


Query 5: What is Unsupervised Data Augmentation for Consistency Training?

Answer: 

Related Work Existing works in consistency training does make use of data augmentation [32, 51]; however, they only apply weak augmentation methods such as random translations and cropping. In parallel to our work, ICT [60] and MixMatch [3] also show improvements for semi-supervised learning. These methods employ mixup [70] on top of simple augmentations such as ﬂipping and cropping; instead, UDA emphasizes on the use of state-of-the-art data augmentations, leading to signiﬁcantly better results on CIFAR-10 and SVHN. In addition, UDA is also applicable to language domain and can also scale well to more challenging vision datasets, such as ImageNet. Other works in the consistency training family mostly differ in how the noise is deﬁned: Pseudo- ensemble [2] directly applies Gaussian noise and Dropout noise; VAT [41, 40] deﬁnes the noise by approximating the direction of change in the input space that the model is most sensitive to; Cross-view training [7] masks out part of the input data. Apart from enforcing consistency on the input examples and the hidden representations, another line of research enforces consistency on the model parameter space. Works in this category include Mean Teacher [58], fast-Stochastic Weight Averaging [1] and Smooth Neighbors on Teacher Graphs [35]. For a complete version of related work, please refer to Appendix D. 6 Conclusion In this paper, we show that data augmentation and semi-supervised learning are well connected: better data augmentation can lead to signiﬁcantly better semi-supervised learning. Our method, UDA, employs state-of-the-art data augmentation found in supervised learning to generate diverse and realistic noise and enforces the model to be consistent with respect to these noise. For text, UDA combines well with representation learning, e.g., BERT. For vision, UDA outperforms prior works by a clear margin and nearly matches the performance of the fully supervised models trained on the full labeled sets which are one order of magnitude larger. We hope that UDA will encourage future research to transfer advanced supervised augmentation to semi-supervised setting for different tasks. 9 Acknowledgements We want to thank Hieu Pham, Adams Wei Yu, Zhilin Yang and Ekin Dogus Cubuk for their tireless help to the authors on different stages of this project and thank Colin Raffel for pointing out the connections between our work and previous works. We also would like to thank Olga Wichrowska, Barret Zoph, Jiateng Xie, Guokun Lai, Yulun Du, Chen Dan, David Berthelot, Avital Oliver, Trieu Trinh, Ran Zhao, Ola Spyra, Brandon Yang, Daiyi Peng, Andrew Dai, Samy Bengio, Jeff Dean and the Google Brain team for insightful discussions and support to the work. Lastly, we thank anonymous reviewers for their valueable feedbacks. Broader Impact This work show that it is possible to achieve great performance with limited labeled data. Hence groups/institutes with limited budgets for annotating data may beneﬁt from this research. To the best of our knowledge, nobody will be put at disadvantage from this research. Our method does not leverage biases in the data. Our tasks include standard benchmarks such as IMDb, CIFAR-10, SVHN and ImageNet. References [1] Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson.

question of how the form or “quality” of the noising operation q can inﬂuence the performance of this consistency training framework. Speciﬁcally, to enforce consistency, prior methods generally employ simple noise injection methods such as adding Gaussian noise, simple input augmentations to noise unlabeled examples. In contrast, we hypothesize that stronger data augmentations in supervised learning can also lead to superior performance when used to noise unlabeled examples in the semi-supervised consistency training framework, since it has been shown that more advanced data augmentations that are more diverse and natural can lead to signiﬁcant performance gain in the supervised setting. Following this idea, we propose to use a rich set of state-of-the-art data augmentations veriﬁed in various supervised settings to inject noise and optimize the same consistency training objective on unlabeled examples. When jointly trained with labeled examples, we utilize a weighting factor λ to balance the supervised cross entropy and the unsupervised consistency training loss, which is illustrated in Figure 1. Formally, the full objective can be written as follows: min θ J (θ) = Ex1∼pL(x) [−log pθ(f ∗(x1) | x1)]+λEx2∼pU(x)Eˆx∼q(ˆx|x2)  CE  p˜θ(y | x2)∥pθ(y | ˆx)  (1) where CE denotes cross entropy, q(ˆx | x) is a data augmentation transformation and ˜θ is a ﬁxed copy of the current parameters θ indicating that the gradient is not propagated through ˜θ, as suggested by VAT [41]. We set λ to 1 for most of our experiments. In practice, in each iteration, we compute the supervised loss on a mini-batch of labeled examples and compute the consistency loss on a mini-batch of unlabeled data. The two losses are then summed for the ﬁnal loss. We use a larger batch size for the consistency loss. In the vision domain, simple augmentations including cropping and ﬂipping are applied to labeled examples. To minimize the discrepancy between supervised training and prediction on unlabeled examples, we apply the same simple augmentations to unlabeled examples for computing p˜θ(y | x). Discussion. Before detailing the augmentation operations used in this work, we ﬁrst provide some intuitions on how more advanced data augmentations can provide extra advantages over simple ones used in earlier works from three aspects: • Valid noise: Advanced data augmentation methods that achieve great performance in supervised learning usually generate realistic augmented examples that share the same ground-truth labels with the original example. Thus, it is safe to encourage the consistency between predictions on the original unlabeled example and the augmented unlabeled examples. • Diverse noise: Advanced data augmentation can generate a diverse set of examples since it can make large modiﬁcations to the input example without changing its label, while simple Gaussian noise only make local changes. Encouraging consistency on a diverse set of augmented examples can signiﬁcantly improve the sample efﬁciency. 3 • Targeted inductive biases: Different tasks require different inductive biases. Data augmentation operations that work well in supervised training essentially provides the missing inductive biases. 2.3 Augmentation Strategies for Different Tasks We now detail the augmentation methods, tailored for different tasks, that we use in this work. RandAugment for Image

methods with high quality data augmentation methods in order to improve consistency training. To emphasize the 1Code is available at https://github.com/google-research/uda. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1904.12848v6 [cs.LG] 5 Nov 2020 use of better data augmentation in consistency training, we name our method Unsupervised Data Augmentation or UDA. We evaluate UDA on a wide variety of language and vision tasks. On six text classiﬁcation tasks, our method achieves signiﬁcant improvements over state-of-the-art models. Notably, on IMDb, UDA with 20 labeled examples outperforms the state-of-the-art model trained on 1250x more labeled data. On standard semi-supervised learning benchmarks CIFAR-10 and SVHN, UDA outperforms all existing semi-supervised learning methods by signiﬁcant margins and achieves an error rate of 5.43 and 2.72 with 250 labeled examples respectively. Finally, we also ﬁnd UDA to be beneﬁcial when there is a large amount of supervised data. For instance, on ImageNet, UDA leads to improvements of top-1 accuracy from 58.84 to 68.78 with 10% of the labeled set and from 78.43 to 79.05 when we use the full labeled set and an external dataset with 1.3M unlabeled examples. Our key contributions and ﬁndings can be summarized as follows: • First, we show that state-of-the-art data augmentations found in supervised learning can also serve as a superior source of noise under the consistency enforcing semi-supervised framework. See results in Table 1 and Table 2. • Second, we show that UDA can match and even outperform purely supervised learning that uses orders of magnitude more labeled data. See results in Table 4 and Figure 4. State-of-the-art results for both vision and language tasks are reported in Table 3 and 4. The effectiveness of UDA across different training data sizes are highlighted in Figure 4 and 7. • Third, we show that UDA combines well with transfer learning, e.g., when ﬁne-tuning from BERT (see Table 4), and is effective at high-data regime, e.g. on ImageNet (see Table 5). • Lastly, we also provide a theoretical analysis of how UDA improves the classiﬁcation performance and the corresponding role of the state-of-the-art augmentation in Section 3. 2 Unsupervised Data Augmentation (UDA) In this section, we ﬁrst formulate our task and then present the key method and insights behind UDA. Throughout this paper, we focus on classiﬁcation problems and will use x to denote the input and y∗to denote its ground-truth prediction target. We are interested in learning a model pθ(y | x) to predict y∗based on the input x, where θ denotes the model parameters. Finally, we will use pL(x) and pU(x) to denote the distributions of labeled and unlabeled examples respectively and use f ∗to denote the perfect classiﬁer that we hope to learn. 2.1 Background: Supervised Data Augmentation Data augmentation aims at creating novel and realistic-looking training data by applying a trans- formation to an example, without changing its label. Formally, let q(ˆx | x) be the augmentation transformation from which one can draw augmented examples ˆx based on an original example x. For an augmentation transformation to be valid, it is required that any example

