{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Interactive Notebook Demo, Jupyter notebook for querying the RAG system\n",
    "\n",
    "# Cell 1: Imports and Setup\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Cell 2: Load Model and Index\n",
    "print(\"Loading model and index...\")\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Model loaded: {model.get_sentence_embedding_dimension()}D embeddings\")\n",
    "print(f\"✓ Index loaded: {index.ntotal} vectors\")\n",
    "print(f\"✓ Chunks loaded: {len(chunks)} chunks\")\n",
    "\n",
    "# Cell 3: Define Search Function\n",
    "def search_papers(query, k=3):\n",
    "    \"\"\"\n",
    "    Search for relevant paper chunks\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of results with metadata\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Collect results\n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        if idx < len(chunks):\n",
    "            chunk = chunks[idx]\n",
    "            results.append({\n",
    "                'chunk_id': chunk['chunk_id'],\n",
    "                'source': chunk['source_id'],\n",
    "                'text': chunk['text'],\n",
    "                'distance': float(distance),\n",
    "                'similarity': 1 / (1 + distance)  # Convert distance to similarity\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Cell 4: Helper Function for Display\n",
    "def display_results(query, results):\n",
    "    \"\"\"\n",
    "    Display search results in a formatted way\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{'─'*80}\")\n",
    "        print(f\"RESULT #{i}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        print(f\"Source Paper: {result['source']}\")\n",
    "        print(f\"Chunk ID: {result['chunk_id']}\")\n",
    "        print(f\"Similarity Score: {result['similarity']:.4f}\")\n",
    "        print(f\"\\nContent:\")\n",
    "        print(result['text'])\n",
    "        print()\n",
    "\n",
    "# Cell 5: Example Queries\n",
    "# Try different queries to test the system\n",
    "\n",
    "# Query 1: Transformers\n",
    "query1 = \"What are transformer models and how do they work?\"\n",
    "results1 = search_papers(query1, k=3)\n",
    "display_results(query1, results1)\n",
    "\n",
    "# Cell 6: Query 2 - Attention Mechanism\n",
    "query2 = \"Explain the attention mechanism in neural networks\"\n",
    "results2 = search_papers(query2, k=3)\n",
    "display_results(query2, results2)\n",
    "\n",
    "# Cell 7: Query 3 - Training Methods\n",
    "query3 = \"How to train large language models efficiently?\"\n",
    "results3 = search_papers(query3, k=3)\n",
    "display_results(query3, results3)\n",
    "\n",
    "# Cell 8: Interactive Search Widget\n",
    "from ipywidgets import interact, Text, IntSlider\n",
    "\n",
    "@interact(\n",
    "    query=Text(value=\"\", description=\"Query:\", placeholder=\"Enter your question...\"),\n",
    "    k=IntSlider(value=3, min=1, max=10, description=\"Results:\")\n",
    ")\n",
    "def interactive_search(query, k):\n",
    "    if query:\n",
    "        results = search_papers(query, k)\n",
    "        display_results(query, results)\n",
    "    else:\n",
    "        print(\"Enter a query to search the arXiv papers!\")\n",
    "\n",
    "# Cell 9: Statistics and Analysis\n",
    "def analyze_index():\n",
    "    \"\"\"\n",
    "    Show statistics about the indexed papers\n",
    "    \"\"\"\n",
    "    # Count papers\n",
    "    unique_sources = set(chunk['source_id'] for chunk in chunks)\n",
    "    \n",
    "    # Token statistics\n",
    "    token_counts = [chunk['token_count'] for chunk in chunks]\n",
    "    \n",
    "    print(\"Index Statistics:\")\n",
    "    print(f\"  Total papers: {len(unique_sources)}\")\n",
    "    print(f\"  Total chunks: {len(chunks)}\")\n",
    "    print(f\"  Avg chunks per paper: {len(chunks) / len(unique_sources):.1f}\")\n",
    "    print(f\"\\nChunk Size Statistics:\")\n",
    "    print(f\"  Mean tokens: {np.mean(token_counts):.1f}\")\n",
    "    print(f\"  Median tokens: {np.median(token_counts):.1f}\")\n",
    "    print(f\"  Min tokens: {np.min(token_counts)}\")\n",
    "    print(f\"  Max tokens: {np.max(token_counts)}\")\n",
    "    \n",
    "    # Show sample papers\n",
    "    print(f\"\\nSample Papers:\")\n",
    "    for i, source in enumerate(list(unique_sources)[:5], 1):\n",
    "        print(f\"  {i}. {source}\")\n",
    "\n",
    "analyze_index()\n",
    "\n",
    "# Cell 10: Advanced: Retrieve full paper context\n",
    "def get_paper_context(chunk_id, context_chunks=2):\n",
    "    \"\"\"\n",
    "    Get surrounding chunks from the same paper for more context\n",
    "    \"\"\"\n",
    "    # Find the target chunk\n",
    "    target_chunk = next((c for c in chunks if c['chunk_id'] == chunk_id), None)\n",
    "    \n",
    "    if not target_chunk:\n",
    "        return None\n",
    "    \n",
    "    # Find all chunks from same paper\n",
    "    paper_chunks = [c for c in chunks if c['source_id'] == target_chunk['source_id']]\n",
    "    paper_chunks.sort(key=lambda x: x['chunk_index'])\n",
    "    \n",
    "    # Find target index\n",
    "    target_idx = next(i for i, c in enumerate(paper_chunks) if c['chunk_id'] == chunk_id)\n",
    "    \n",
    "    # Get surrounding chunks\n",
    "    start_idx = max(0, target_idx - context_chunks)\n",
    "    end_idx = min(len(paper_chunks), target_idx + context_chunks + 1)\n",
    "    \n",
    "    return paper_chunks[start_idx:end_idx]\n",
    "\n",
    "# Example: Get context for first result\n",
    "if results1:\n",
    "    print(\"Getting extended context for first result...\")\n",
    "    context = get_paper_context(results1[0]['chunk_id'], context_chunks=2)\n",
    "    \n",
    "    if context:\n",
    "        print(f\"\\nExtended context ({len(context)} chunks):\")\n",
    "        for chunk in context:\n",
    "            marker = \">>> \" if chunk['chunk_id'] == results1[0]['chunk_id'] else \"    \"\n",
    "            print(f\"{marker}Chunk {chunk['chunk_index']}: {chunk['text'][:100]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
