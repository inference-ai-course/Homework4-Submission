# Week 4: RAG Pipeline for arXiv Papers

A complete Retrieval-Augmented Generation (RAG) system for searching scientific papers using semantic similarity.

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ arxiv_scraper.py           # Scrape paper metadata from arXiv
â”œâ”€â”€ arxiv_pdf_downloader.py    # Download PDFs from arXiv
â”œâ”€â”€ pdf_processor.py           # Extract and chunk text from PDFs
â”œâ”€â”€ embedding_indexer.py       # Generate embeddings and build FAISS index
â”œâ”€â”€ main.py                    # FastAPI service for search
â”œâ”€â”€ rag_demo.ipynb            # Jupyter notebook demo
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Data Collection

```bash
# Step 1: Scrape paper metadata from arXiv cs.CL
python arxiv_scraper.py
# Output: arxiv_clean.json (metadata for ~200 papers)

# Step 2: Download PDFs (downloads 50 papers by default)
python arxiv_pdf_downloader.py
# Output: arxiv_pdfs/ directory with PDF files
```

### 3. Build RAG Pipeline

```bash
# Step 3: Extract text and create chunks
python pdf_processor.py
# Output: processed_chunks.json (chunks with metadata)

# Step 4: Generate embeddings and build FAISS index
python embedding_indexer.py
# Output: faiss_index.bin, index_metadata.pkl
```

### 4. Run the API Service

```bash
# Start FastAPI server
python main.py
```

The API will be available at:
- **Base URL**: http://localhost:8000
- **Interactive Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

### 5. Test with Jupyter Notebook

```bash
jupyter notebook rag_demo.ipynb
```

## ğŸ“¡ API Endpoints

### Search Endpoint
```bash
# Search for relevant passages
curl "http://localhost:8000/search?q=attention%20mechanisms&k=3"
```

**Response:**
```json
{
  "query": "attention mechanisms",
  "num_results": 3,
  "results": [
    {
      "rank": 1,
      "distance": 0.3245,
      "chunk_id": "2401.12345_chunk_10",
      "paper_id": "2401.12345",
      "text": "Attention mechanisms allow models to focus on...",
      "token_count": 487
    }
  ]
}
```

### Statistics Endpoint
```bash
curl "http://localhost:8000/stats"
```

### Paper Chunks Endpoint
```bash
# Get all chunks for a specific paper
curl "http://localhost:8000/paper/2401.12345"
```

## ğŸ”§ Configuration

### Chunking Parameters

Edit `pdf_processor.py`:
```python
processor = PDFProcessor(
    max_tokens=512,  # Maximum tokens per chunk
    overlap=50       # Overlapping tokens between chunks
)
```

### Embedding Model

Edit `embedding_indexer.py`:
```python
indexer = EmbeddingIndexer(
    model_name='all-MiniLM-L6-v2'  # Fast, 384-dim embeddings
    # Alternative: 'all-mpnet-base-v2'  # Better quality, 768-dim
)
```

### Search Parameters

```python
# In notebook or API
results = indexer.search(query, k=3)  # Top-3 results
```

## ğŸ“Š Example Queries

The system works well with queries like:

1. **Specific Topics**: "What are attention mechanisms in transformers?"
2. **Methods**: "How does fine-tuning work for language models?"
3. **Concepts**: "Explain zero-shot learning"
4. **Applications**: "What are challenges in machine translation?"
5. **Comparisons**: "Difference between BERT and GPT"

## ğŸ“ Output Files

### processed_chunks.json
```json
[
  {
    "paper_id": "2401.12345",
    "chunk_id": "2401.12345_chunk_0",
    "chunk_index": 0,
    "text": "Full chunk text here...",
    "token_count": 487
  }
]
```

### retrieval_report.json
Generated by the notebook with:
- Query metadata
- Top-k results for each query
- Distance scores
- Retrieved passages

## ğŸ¯ Performance Tips

1. **Chunk Size**: 256-512 tokens works best for most queries
2. **Overlap**: 50-100 tokens helps with context preservation
3. **Index Type**: `IndexFlatL2` is accurate; use `IndexIVFFlat` for speed with large datasets
4. **Batch Processing**: Process embeddings in batches of 32-64 for efficiency

## ğŸ§ª Testing the Pipeline

```python
# In Python or notebook
from embedding_indexer import EmbeddingIndexer

# Load index
indexer = EmbeddingIndexer()
indexer.load_index('faiss_index.bin', 'index_metadata.pkl')

# Search
results = indexer.search("transformer attention", k=3)

# Display
for r in results:
    print(f"Paper: {r['paper_id']}")
    print(f"Distance: {r['distance']:.4f}")
    print(f"Text: {r['text'][:200]}...\n")
```

## ğŸ“ Deliverables Checklist

- [x] **Code**: All Python files for RAG pipeline
- [x] **Data**: FAISS index and processed chunks
- [x] **Report**: Retrieval report with 5+ example queries
- [x] **API**: FastAPI service with /search endpoint
- [x] **Demo**: Jupyter notebook demonstration

## ğŸ”„ Pipeline Flow

```
1. arxiv_scraper.py
   â†“ (arxiv_clean.json)
2. arxiv_pdf_downloader.py
   â†“ (arxiv_pdfs/*.pdf)
3. pdf_processor.py
   â†“ (processed_chunks.json)
4. embedding_indexer.py
   â†“ (faiss_index.bin, index_metadata.pkl)
5. main.py (FastAPI) OR rag_demo.ipynb
   â†“
   Search Results!
```

## ğŸ› Troubleshooting

### PDFs not downloading
- Check internet connection
- Verify paper IDs in arxiv_clean.json
- arXiv may rate limit; add delays between requests

### FAISS index errors
- Ensure embeddings are float32: `embeddings.astype('float32')`
- Check dimension matches: embedding model output = FAISS index dimension

### Out of memory
- Reduce batch_size in embedding generation
- Process papers in smaller batches
- Use a smaller embedding model

### API not starting
- Check if port 8000 is available
- Verify index files exist: `faiss_index.bin`, `index_metadata.pkl`
- Run embedding_indexer.py first

## ğŸ“š Resources

- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [Sentence Transformers](https://www.sbert.net/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [arXiv API Guide](https://info.arxiv.org/help/api/index.html)

## ğŸ“ Next Steps (Week 5)

- Add BM25 sparse retrieval for hybrid search
- Implement re-ranking with cross-encoders
- Add document metadata filtering
- Build conversation history tracking
- Deploy to cloud service

## ğŸ“„ License

MIT License - Feel free to use for educational purposes